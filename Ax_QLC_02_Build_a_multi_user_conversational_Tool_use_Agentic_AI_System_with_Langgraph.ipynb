{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aa1reXo/Agentic-AI/blob/main/Ax_QLC_02_Build_a_multi_user_conversational_Tool_use_Agentic_AI_System_with_Langgraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a ReAct AI Agent from scratch with LangGraph"
      ],
      "metadata": {
        "id": "8pdRDlVs1MJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This demo will cover building AI Agents with LangGraph from scratch.\n",
        "\n",
        "Here we'll create a simple ReAct agent app that can search the web and check the weather. The app consists of an agent (LLM) and tools. As we interact with the app, we will first call the agent (LLM) to decide if we should use tools. Then we will run a loop:\n",
        "\n",
        "- If the agent said to take an action (i.e. call tool), we'll run the tools and pass the results back to the agent\n",
        "- If the agent did not ask to run tools, we will finish (respond to the user)\n",
        "\n",
        "We will implement this in LangGraph completely from scratch step by step.\n"
      ],
      "metadata": {
        "id": "NYBpZTjLnEXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There will be three parts to this hands-on demo:\n",
        "\n",
        "- Part I: Build a Basic Chatbot with LangGraph\n",
        "- Part II: Build a simple ReAct Agent with LangGraph - LLM + Tools\n",
        "- Part III: Build a multi-user conversational ReAct Agent with LangGraph"
      ],
      "metadata": {
        "id": "dZ0q3K8Y1TCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install OpenAI, and LangChain dependencies"
      ],
      "metadata": {
        "id": "L1KvMtf54l0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.3.12\n",
        "#!pip install langchain-openai==0.2.12\n",
        "!pip install langchain-community==0.3.12\n",
        "!pip install langgraph==0.2.56\n",
        "!pip install langgraph-checkpoint-sqlite==2.0.1\n",
        "!pip install beautifulsoup4\n",
        "!pip install langchain-google-genai==2.0.1"
      ],
      "metadata": {
        "id": "2evPp14fy258",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4afbec80-4041-4b01-c01e-297a734ee996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.3.12\n",
            "  Downloading langchain-0.3.12-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.12) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.12) (2.0.38)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.12) (3.11.12)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.12) (0.3.35)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.12) (0.3.6)\n",
            "Collecting langsmith<0.3,>=0.1.17 (from langchain==0.3.12)\n",
            "  Downloading langsmith-0.2.11-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.12) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.12) (2.10.6)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.12) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.12) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.12) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.12) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.12) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.12) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.12) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.12) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.12) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain==0.3.12) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain==0.3.12) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain==0.3.12) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain==0.3.12) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain==0.3.12) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain==0.3.12) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.12) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.12) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.12) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.12) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.12) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.12) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.12) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain==0.3.12) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain==0.3.12) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain==0.3.12) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain==0.3.12) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain==0.3.12) (1.3.1)\n",
            "Downloading langchain-0.3.12-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.2.11-py3-none-any.whl (326 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.9/326.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langsmith, langchain\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.3.8\n",
            "    Uninstalling langsmith-0.3.8:\n",
            "      Successfully uninstalled langsmith-0.3.8\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.18\n",
            "    Uninstalling langchain-0.3.18:\n",
            "      Successfully uninstalled langchain-0.3.18\n",
            "Successfully installed langchain-0.3.12 langsmith-0.2.11\n",
            "Collecting langchain-community==0.3.12\n",
            "  Downloading langchain_community-0.3.12-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.12) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.12) (2.0.38)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.12) (3.11.12)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.3.12)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community==0.3.12)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.12 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.12) (0.3.12)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.12) (0.3.35)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.12) (0.2.11)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.12) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community==0.3.12)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.12) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.12) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.12) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.12) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.12) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.12) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.12) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.12) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.12) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.12)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.12)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.12->langchain-community==0.3.12) (0.3.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.12->langchain-community==0.3.12) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain-community==0.3.12) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain-community==0.3.12) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain-community==0.3.12) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.12) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.12) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.12) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.12)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.12) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.12) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.12) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.12) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.3.12) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.12) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.12) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.12) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain-community==0.3.12) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.12->langchain-community==0.3.12) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.12->langchain-community==0.3.12) (2.27.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.12)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.12) (1.3.1)\n",
            "Downloading langchain_community-0.3.12-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.12 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.7.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n",
            "Collecting langgraph==0.2.56\n",
            "  Downloading langgraph-0.2.56-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43 in /usr/local/lib/python3.11/dist-packages (from langgraph==0.2.56) (0.3.35)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.4 (from langgraph==0.2.56)\n",
            "  Downloading langgraph_checkpoint-2.0.16-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph==0.2.56)\n",
            "  Downloading langgraph_sdk-0.1.53-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (0.2.11)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (4.12.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (2.10.6)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.4->langgraph==0.2.56) (1.1.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.2.56) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.2.56) (3.10.15)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.2.56) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.2.56) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.2.56) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.2.56) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.2.56) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (3.0.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.2.56) (1.3.1)\n",
            "Downloading langgraph-0.2.56-py3-none-any.whl (126 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.16-py3-none-any.whl (38 kB)\n",
            "Downloading langgraph_sdk-0.1.53-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langgraph-sdk, langgraph-checkpoint, langgraph\n",
            "Successfully installed langgraph-0.2.56 langgraph-checkpoint-2.0.16 langgraph-sdk-0.1.53\n",
            "Collecting langgraph-checkpoint-sqlite==2.0.1\n",
            "  Downloading langgraph_checkpoint_sqlite-2.0.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting aiosqlite<0.21.0,>=0.20.0 (from langgraph-checkpoint-sqlite==2.0.1)\n",
            "  Downloading aiosqlite-0.20.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint-sqlite==2.0.1) (2.0.16)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.11/dist-packages (from aiosqlite<0.21.0,>=0.20.0->langgraph-checkpoint-sqlite==2.0.1) (4.12.2)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.2.38 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (0.3.35)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (1.1.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (0.2.11)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (2.10.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (3.10.15)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (2.27.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (1.3.1)\n",
            "Downloading langgraph_checkpoint_sqlite-2.0.1-py3-none-any.whl (12 kB)\n",
            "Downloading aiosqlite-0.20.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: aiosqlite, langgraph-checkpoint-sqlite\n",
            "Successfully installed aiosqlite-0.20.0 langgraph-checkpoint-sqlite-2.0.1\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.12.2)\n",
            "Collecting langchain-google-genai==2.0.1\n",
            "  Downloading langchain_google_genai-2.0.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: google-generativeai<0.9.0,>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai==2.0.1) (0.8.4)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai==2.0.1) (0.3.35)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai==2.0.1) (2.10.6)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (2.160.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (4.25.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (1.26.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (0.2.11)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai==2.0.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai==2.0.1) (2.27.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (1.67.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (1.0.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (3.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (0.14.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (1.3.1)\n",
            "Downloading langchain_google_genai-2.0.1-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-google-genai\n",
            "Successfully installed langchain-google-genai-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter Open AI API Key"
      ],
      "metadata": {
        "id": "H9c37cLnSrbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "# OPENAI_KEY = getpass('Enter your OpenAI Key: ')\n",
        "GEMINI_API_KEY = getpass('Enter your Google Gemini API Key: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8cc249d-f181-46e1-f8ec-64cbca92e49b",
        "id": "gdKxKv-g0sgN"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Google Gemini API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter WeatherAPI API Key\n",
        "\n",
        "Get a free API key from [here](https://www.weatherapi.com/signup.aspx)"
      ],
      "metadata": {
        "id": "djj1pH6A04BX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WEATHER_API_KEY = getpass('Enter WeatherAPI API Key: ')"
      ],
      "metadata": {
        "id": "XpAMz1XgEEov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a86b8405-3cc0-4ce1-a5f3-320897010597"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter WeatherAPI API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter Tavily Search API Key\n",
        "\n",
        "Get a free API key from [here](https://tavily.com/#api)"
      ],
      "metadata": {
        "id": "ucWRRI3QztL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TAVILY_API_KEY = getpass('Enter Tavily Search API Key: ')"
      ],
      "metadata": {
        "id": "mK-1WLzOrJdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da0c1fb9-753f-4233-ce2c-db07d3284768"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Tavily Search API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Environment Variables"
      ],
      "metadata": {
        "id": "1T0s0um5Svfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# os.environ['OPENAI_API_KEY'] = OPENAI_KEY\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
        "os.environ['TAVILY_API_KEY'] = TAVILY_API_KEY"
      ],
      "metadata": {
        "id": "x1YSuHNF_lbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
        "from langchain_core.tools import tool\n",
        "import json\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "tavily_search = TavilySearchAPIWrapper()\n",
        "\n",
        "@tool\n",
        "def search_web(query: str) -> list:\n",
        "    \"\"\"Search the web for a query. Userful for general information or general news\"\"\"\n",
        "    results = tavily_search.raw_results(query=query,\n",
        "                                        max_results=8,\n",
        "                                        search_depth='advanced',\n",
        "                                        include_answer=False,\n",
        "                                        include_raw_content=True)\n",
        "    return results\n",
        "\n",
        "@tool\n",
        "def get_weather(query: str) -> list:\n",
        "    \"\"\"Search weatherapi to get the current weather.\"\"\"\n",
        "    base_url = \"http://api.weatherapi.com/v1/current.json\"\n",
        "    complete_url = f\"{base_url}?key={WEATHER_API_KEY}&q={query}\"\n",
        "\n",
        "    response = requests.get(complete_url)\n",
        "    data = response.json()\n",
        "    if data.get(\"location\"):\n",
        "        return data\n",
        "    else:\n",
        "        return \"Weather Data Not Found\""
      ],
      "metadata": {
        "id": "Ue8xgu9WpuPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_web.invoke('Latest LLMs released')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryEb_pHP10br",
        "outputId": "2d7329bf-f4de-4d8c-ac93-cacc31c7dd23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'Latest LLMs released',\n",
              " 'follow_up_questions': None,\n",
              " 'answer': None,\n",
              " 'images': [],\n",
              " 'results': [{'url': 'https://explodingtopics.com/blog/list-of-llms',\n",
              "   'title': 'Best 22 Large Language Models (LLMs) (February 2025)',\n",
              "   'content': \"Inflection-2.5 is the latest large language model (LLM) developed by Inflection AI to power its conversational AI assistant, Pi. Significant upgrades have been made, as the model currently achieves over 94% of GPT-4’s average performance while only having 40% of the training FLOPs. In March 2024, the Microsoft-backed startup reached 1+ million daily active users on Pi. 13. Gemma is a series of lightweight open-source language models developed and released by Google DeepMind. Pythia is a series of 16 large language models developed and released by EleutherAI, a non-profit AI research lab. Alpaca is a 7 billion-parameter language model developed by a Stanford research team and fine-tuned from Meta's LLaMA 7B model.\",\n",
              "   'score': 0.85216236,\n",
              "   'raw_content': 'Best 22 Large Language Models (LLMs) (February 2025)\\n\\n\\n\\nAbout\\nNewsletter\\nBlog\\n\\n\\nBest 22 Large Language Models (LLMs) (February 2025)\\n\\nby Anthony Cardillo\\nFebruary 7, 2025\\nLarge language models are pre-trained on large datasets and use natural language processing to perform linguistic tasks such as text generation, code completion, paraphrasing, and more.\\nThe initial release of ChatGPT sparked the rapid adoption of generative AI, which has led to large language model innovations and industry growth.\\nIn fact, 92% of Fortune 500 firms have started using generative AI in their workflows.\\nAs adoption continues to grow, so does the LLM industry. The global large language model market is projected to grow from $6.5 billion in 2024 to $140.8 billion by 2033.\\nWith that, here is a list of the top 21 LLMs available in September 2024.\\nLLM NameDeveloperRelease DateAccessParametersDeepSeek R1DeepSeekJanuary 20, 2025Open-Source671 billionGPT-4oOpenAIMay 13, 2024APIUnknownClaude 3.5AnthropicJune 20, 2024APIUnknownGrok-1xAINovember 4, 2023Open-Source314 billionMistral 7BMistral AISeptember 27, 2023Open-Source7.3 billionPaLM 2GoogleMay 10, 2023Open-Source340 billionFalcon 180BTechnology Innovation InstituteSeptember 6, 2023Open-Source180 billionStable LM 2Stability AIJanuary 19, 2024Open-Source1.6 billion, 12 billionGemini 1.5Google DeepMindFebruary 2nd, 2024APIUnknownLlama 3.1Meta AIJune 23, 2024Open-Source405 billionMixtral 8x22BMistral AIApril 10, 2024Open-Source141 billionInflection-2.5Inflection AIMarch 10, 2024ProprietaryUnknownJambaAI21 LabsMarch 29, 2024Open-Source52 billionCommand RCohereMarch 11, 2024Both35 billionGemmaGoogle DeepMindFebruary 21, 2024Open-Source2 billion, 7 billionPhi-3MicrosoftApril 23, 2024Both3.8 billionXGen-7BSalesforceJuly 3, 2023Open-Source7 billionDBRXDatabricks\\' Mosaic MLMarch 27, 2024Open-Source132 billionPythiaEleutherAIFebruary 13, 2023Open-Source70 million to 12 billionSoraOpenAIFebruary 15, 2024 (announced)APIUnknownAlpaca 7BStanford CRFMMarch 13, 2023Open-Source7 billionNemotron-4NvidiaJune 14, 2024Open-Source340 billion\\n1. DeepSeek R1\\n\\nDeveloper: DeepSeek\\nRelease date: January 2025\\nNumber of Parameters: 671B total, 37B active\\nWhat is it? DeepSeek R1 is a reasoning model that excels in math and coding. It beats or matches OpenAI o1 in several benchmarks, including MATH-500 and AIME 2024.\\nOn its release, DeepSeek immediately hit headlines due to the low cost of training compared to most major LLMs.\\nDeepSeek R1 is free to use and open-source. It\\'s accessible via the API, the DeepSeek website, and mobile apps.\\n2. GPT-4o\\n\\nDeveloper: OpenAI\\nRelease date: May 13, 2024\\nNumber of Parameters: Unknown\\nWhat is it? GPT-4o is the latest and most advanced OpenAI language model, succeeding GPT-4, GPT-3.5, and GPT-3. OpenAI claims that GPT-4o is 50% cheaper than GPT-4 despite being 2x faster at generating tokens. This multimodal model includes text, image, video, and voice capabilities packaged into one.\\nGPT-4o\\'s biggest upgrade is the Voice-to-Voice function, which will improve input response times to an average of 320 milliseconds (compared to a few seconds with GPT-4). This feature is expected to be released in the coming weeks.\\n3. Claude 3.5\\n\\nDeveloper: Anthropic\\nRelease date: March 14, 2024\\nNumber of Parameters: Unknown\\nWhat is it?\\xa0As a new upgrade from the highly rated\\xa0Claude 3, Claude 3.5 Sonnet is the first release of the new Claude 3.5 model family. Similar to Claude 3, it\\'ll also include the Haiku and Opus models. As debatably the biggest competitor to GPT-4 and ChatGPT, Claude made even bigger improvements to this model by maintaining the 200,000 token context window at a lower cost. This is much larger than GPT-4\\'s 32,000 token capabilities.\\nAccording to Anthropic\\'s report, Claude 3.5 Sonnet outperformed GPT-4o in major benchmarks like coding and text reasoning. Plus, this is Claude\\'s most advanced vision model, with the ability to transcribe text from images or generate insights from charts.\\nAmazon has invested over $4 billion in Anthropic, bringing the startup\\'s valuation to $15 billion. The Claude mobile app was also released in May 2024.\\n4. Grok-1\\n\\nDeveloper: xAI\\nRelease date: November 4, 2023\\nNumber of Parameters: 314 billion\\nWhat is it? Created by Elon Musk\\'s artificial intelligence startup xAI, Grok-1 is currently the largest open-source LLM released to date at 314 billion parameters. Grok directly integrates with X (Twitter), and users must pay for an X Premium+ subscription to gain access.\\nBecause of the model’s size, Grok has a mixture-of-experts (MoE) architecture that only uses 25% of its weights for any given input token to maximize calculation efficiency.\\nIn August 2024, both Grok-2 and Grok-2 mini were released to X users in beta. According to xAI\\'s reports, Grok-2 outperforms GPT-4o in numerous categories, such as GPQA, MMLU-Pro, and DocVQA.\\n5. Mistral 7B\\n\\nDeveloper: Mistral AI\\nRelease date: September 27, 2023\\nNumber of Parameters: 7.3 billion\\nWhat is it? Mistral 7B is an open-source language model with 32 layers, 32 attention heads, and eight key-value heads. Despite running with fewer parameters, they outperformed the Llama 2 family of models in nearly all metrics, including MMLU, reading comprehension, math, coding, etc.\\nMistral 7B is released under an Apache 2.0 license. Customers are free to download it locally, deploy it on the cloud, or run it on HuggingFace. The Paris-based startup is close to securing a new $600 million funding round that would value the company at $6 billion.\\n6. PaLM 2\\n\\nDeveloper: Google\\nRelease date: May 10, 2023\\nNumber of Parameters: 340 billion\\nWhat is it? PaLM 2 is an advanced large language model developed by Google. As the successor to the original Pathways Language Model (PaLM), it’s trained on 3.6 trillion tokens (compared to 780 billion) and 340 billion parameters (compared to 540 billion). PaLM 2 was originally used to power Google\\'s first generative AI chatbot, Bard (rebranded to Gemini in February 2024).\\n7. Falcon 180B\\n\\nDeveloper: Technology Innovation Institute (TII)\\nRelease date: September 6, 2023\\nNumber of Parameters: 180 billion\\nWhat is it? Developed and funded by the Technology Innovation Institute, Falcon 180B is an upgraded version of the earlier Falcon 40B LLM. It has 180 billion parameters, which is 4.5 times larger than the 40 billion parameters of Falcon 40B.\\nIn addition to Falcon 40B, it also outperforms other large language models like GPT-3.5 and LLaMA 2 on tasks such as reasoning, question answering, and coding. In February 2024, the UAE-based Technology Innovation Institute (TII) committed $300 million in funding to the Falcon Foundation.\\n8. Stable LM 2\\n\\nDeveloper: Stability AI\\nRelease date: January 19, 2024\\nNumber of Parameters: 1.6 billion and 12 billion\\nWhat is it? Stability AI, the creators of the Stable Diffusion text-to-image model, are the developers behind Stable LM 2. This series of large language models includes Stable LM 2 12B (12 billion parameters) and Stable LM 2 1.6B (1.6 billion parameters). Released in April 2024, the larger 12B model outperforms models like LLaMA 2 70B on key benchmarks despite being much smaller.\\n9. Gemini 1.5\\n\\nDeveloper: Google DeepMind\\nRelease date: February 2nd, 2024\\nNumber of Parameters: Unknown\\nWhat is it? Gemini 1.5 is Google\\'s next-generation large language model, offering a significant upgrade over its predecessor, Gemini 1.0. While it’s only available for early testing, Gemini 1.5 Pro provides a one million-token context window (1 hour of video, 700,000 words, or 30,000 lines of code), the largest to date compared to alternative LLMs and chatbots. This upgrade is 35 times larger than Gemini 1.0 Pro and surpasses the previous largest record of 200,000 tokens held by Anthropic’s Claude 2.1.\\n10. Llama 3.1\\n\\nDeveloper: Meta AI\\nRelease date: June 23, 2024\\nNumber of Parameters: 405 billion\\nWhat is it? Llama 3, the predecessor to Llama 3.1, was available in both 70B and 8B versions that outperformed other open-source models like Mistral 7B and Google\\'s Gemma 7B on MMLU, reasoning, coding, and math benchmarks. Now, users will notice major upgrades to the latest version, including 405 billion parameters and an expended context length of 128,000.\\nUsers will also notice more accuracy because of the impressive knowledge base, which has been trained on over 15 trillion tokens. Plus, Meta added eight additional languages for this model. The increased size of this model makes it the largest open-source model released to date.\\nCustomers can still access its predecessor, Llama 2, which is available in three versions: 7 billion, 13 billion, and 70 billion parameters.\\n11. Mixtral 8x22B\\n\\nDeveloper: Mistral AI\\nRelease date: April 10, 2024\\nNumber of Parameters: 141 billion\\nWhat is it? Mixtral 8x22B is Mistral AI\\'s latest and most advanced large language model. This sparse Mixture-of-Experts (SMoE) model has 141 billion total parameters but only uses 39B active parameters to focus on improving the model’s performance-to-cost ratio.\\nThe startup also recently released Mistral Large, a ChatGPT alternative that ranks second behind GPT-4 among API-based LLMs.\\n12. Inflection-2.5\\n\\nDeveloper: Inflection AI\\nRelease date: March 10, 2024\\nNumber of Parameters: Unknown\\nWhat is it? Inflection-2.5 is the latest large language model (LLM) developed by Inflection AI to power its conversational AI assistant, Pi. Significant upgrades have been made, as the model currently achieves over 94% of GPT-4’s average performance while only having 40% of the training FLOPs. In March 2024, the Microsoft-backed startup reached 1+ million daily active users on Pi.\\n13. Jamba\\n\\nDeveloper: AI21 Labs\\nRelease date: March 29, 2024\\nNumber of Parameters: 52 billion\\nWhat is it? AI21 Labs created Jamba, the world\\'s first production-grade Mamba-style large language model. It integrates SSM technology with elements of a traditional transformer model to create a hybrid architecture. The model is efficient and highly scalable, with a context window of 256K and deployment support of 140K context on a single GPU.\\n14. Command R\\n\\nDeveloper: Cohere\\nRelease date: March 11, 2024\\nNumber of Parameters: 35 billion\\nWhat is it? Command R is a series of scalable LLMs from Cohere that support ten languages and 128,000-token context length (around 100 pages of text). This model primarily excels at retrieval-augmented generation, code-related tasks like explanations or rewrites, and reasoning. In April 2024, Command R+ was released to support larger workloads and provide real-world enterprise support.\\n15. Gemma\\n\\nDeveloper: Google DeepMind\\nRelease date: February 21, 2024\\nNumber of Parameters: 2 billion and 7 billion\\nWhat is it? Gemma is a series of lightweight open-source language models developed and released by Google DeepMind. The Gemma models are built with similar tech to the Gemini models, but Gemma is limited to text inputs and outputs only. The models have a context window of 8,000 tokens and are available in 2 billion and 7 billion parameter sizes.\\n16. Phi-3\\n\\nDeveloper: Microsoft\\nRelease date: April 23, 2024\\nNumber of Parameters: 3.8 billion\\nWhat is it? Classified as a small language model (SLM), Phi-3 is Microsoft\\'s latest release with 3.8 billion parameters. Despite the smaller size, it\\'s been trained on 3.3 trillion tokens of data to compete with Mistral 8x7B and GPT-3.5 performance on MT-bench and MMLU benchmarks.\\nTo date, Phi-3-mini is the only model available. However, Microsoft plans to release the Phi-3-small and Phi-3-medium models later this year.\\n17. XGen-7B\\n\\nDeveloper: Salesforce\\nRelease date: July 3, 2023\\nNumber of Parameters: 7 billion\\nWhat is it? XGen-7B is a large language model from Salesforce with 7 billion parameters and an 8k context window. The model was trained on 1.37 trillion tokens from various sources, such as RedPajama, Wikipedia, and Salesforce\\'s own Starcoder dataset.\\nSalesforce has released two open-source versions, a 4,000 and 8,000 token context window base, hosted under an Apache 2.0 license.\\n18. DBRX\\n\\nDeveloper: Databricks\\' Mosaic ML\\nRelease date: March 27, 2024\\nNumber of Parameters: 132 billion\\nWhat is it? DBRX is an open-source LLM built by Databricks and the Mosaic ML research team. The mixture-of-experts architecture has 36 billion (of 132 billion total) active parameters on an input. DBRX has 16 experts and chooses 4 of them during inference, providing 65 times more expert combinations compared to similar models like Mixtral and Grok-1\\n19. Pythia\\n\\nDeveloper: EleutherAI\\nRelease date: February 13, 2023\\nNumber of Parameters: 70 million to 12 billion\\nWhat is it? Pythia is a series of 16 large language models developed and released by EleutherAI, a non-profit AI research lab. There are eight different model sizes: 70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B. Because of Pythia\\'s open-source license, these LLMs serve as a base model for fine-tuned, instruction-following LLMs like Dolly 2.0 by Databricks.\\n20. Sora\\n\\nDeveloper: OpenAI\\nRelease date: February 15, 2024 (announced)\\nNumber of Parameters: Unknown\\nWhat is it? OpenAI\\'s latest development is Sora, a text-to-video model that combines LLMs and generative AI to turn text prompts into realistic videos up to 60 seconds long. The model uses a transformer architecture that operates on \"spacetime patches\" of video and image data rather than text tokens like other LLMs. No official release date for Sora has been announced, but OpenAI expects it to open to the public in late 2024.\\n21. Alpaca 7B\\n\\nDeveloper: Stanford CRFM\\nRelease date: March 27, 2024\\nNumber of Parameters: 7 billion\\nWhat is it? Alpaca is a 7 billion-parameter language model developed by a Stanford research team and fine-tuned from Meta\\'s LLaMA 7B model. Users will notice that although being much smaller, Alpaca performs similarly to text-DaVinci-003 (ChatGPT 3.5). However, Alpaca 7B is available for research purposes, and no commercial licenses are available.\\n22. Nemotron-4 340B\\n\\nDeveloper: NVIDIA\\nRelease date: June 14, 2024\\nNumber of Parameters: 340 billion\\nWhat is it? Nemotron-4 340B\\xa0is a family of large language models for synthetic data generation and AI model training. These models help businesses create new LLMs without larger and more expensive datasets. Instead, Nemotron-4 can create high-quality synthetic data to train other AI models, which reduces the need for extensive human-annotated data.\\nThe model family includes Nemotron-4-340B-Base (foundation model), Nemotron-4-340B-Instruct (fine-tuned chatbot), and Nemotron-4-340B-Reward (quality assessment and preference ranking). Due to the 9 trillion tokens used in training, which includes English, multilingual, and coding language data, Nemotron-4 matches GPT-4\\'s high-quality synthetic data generation capabilities.\\nConclusion\\nThe landscape of large language models is rapidly evolving, with new breakthroughs and innovations emerging at an unprecedented pace.\\nFrom compact models like Phi-2 and Alpaca 7B to cutting-edge architectures like Jamba and DBRX, the field of LLMs is pushing the boundaries of what\\'s possible in natural language processing (NLP).\\nWe will keep this list regularly updated with new models. If you liked learning about these LLMs, check out our lists of generative AI startups and AI startups.\\nFind Thousands of Trending Topics With Our Platform\\nTry Exploding Topics Pro\\n\\nExploding Topics\\n\\nJoin Pro\\nNewsletter\\nTrending Topics\\nAdd a Topic\\nCustomer Login\\n\\nCompany\\n\\nAbout Us\\nContact\\nMethodology\\nCookie Settings\\n\\nFree Tools\\n\\nKeyword Research\\nBacklink Checker\\nSERP Checker\\nKeyword Rank Checker\\nFree SEO Tools\\n\\nConnect\\n\\nYouTube\\nInstagram\\nX (Twitter)\\n\\nResources\\n\\nBlog\\nMarketing Academy\\nFree Webinars\\n\\n\\n\\n© 2025 \\xa0Exploding Topics is a Trademark of Semrush Inc\\n\\nPrivacy Policy\\nTerms of Service\\n'},\n",
              "  {'url': 'https://www.reddit.com/r/LocalLLaMA/comments/1cn9sxa/timeline_of_recent_major_llm_releases_past_2/',\n",
              "   'title': 'Timeline of recent major LLM releases (past 2 months) - Reddit',\n",
              "   'content': \"March 11: Cohere released Command R 35b (dense) (Announcement blogpost). March 27: DataBricks released DBRX, a 132b MoE (Announcement blogpost). March 28: The next day ai21labs released Jamba 52b, a SSM-transformer hybrid MoE (Announcement blogpost). April 4: A week later, Cohere released Command R Plus, a (dense) 104b model (Announcement blogpost). April 9: Days later Mistral AI released Mixtral 8x22b MoE on Twitter, followed by Microsoft's Wizard LM 2 8x22b and 7b (April 15) and the 'official' 8x22b release with the Instruct version (April 17) (Announcement blogpost). April 18: Meta released llama 3 8b and 70b (both dense) (Announcement blogpost). April 22: A week later Microsoft released Phi 3 mini (4b, dense) (Announcement blogpost). Reddit Reddit Reddit Communities Best of Reddit Topics Reddit, Inc.\",\n",
              "   'score': 0.76512027,\n",
              "   'raw_content': None},\n",
              "  {'url': 'https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models',\n",
              "   'title': '25 of the best large language models in 2025 - TechTarget',\n",
              "   'content': \"Large language models are the dynamite behind the\\xa0generative AI\\xa0boom. Some of the most well-known language models today are based on the transformer model, including the\\xa0generative pre-trained transformer series\\xa0of LLMs and bidirectional encoder representations from transformers (BERT). Gemma\\xa0is a family of open-source language models from Google that were trained on the same resources as Gemini. GPT-3\\xa0is OpenAI's large language model with more than 175 billion parameters, released in 2020. Large Language Model Meta AI (Llama) is Meta's LLM which was first released in 2023. The\\xa0Pathways Language Model\\xa0is a 540 billion parameter transformer-based model from Google powering its AI chatbot\\xa0Bard. StableLM is a series of open language models developed by Stability AI, the company behind image generator Stable Diffusion.\",\n",
              "   'score': 0.6607204,\n",
              "   'raw_content': '25 of the best large language models in 2025\\nWhatIs\\nSearch the TechTarget Network \\nBrowse Definitions :\\n\\nA\\nB\\nC\\nD\\nE\\nF\\nG\\nH\\nI\\nJ\\nK\\nL\\nM\\nN\\nO\\nP\\nQ\\nR\\nS\\nT\\nU\\nV\\nW\\nX\\nY\\nZ\\n#\\n\\nLogin Register\\n\\nTechTarget Network\\nTech Accelerator\\nNews\\n2024 IT Salary Survey Results\\n\\nRSS\\n\\n\\nWhatIs\\n\\n\\nBrowse Definitions Data analytics and AI\\nTopics View All\\n\\nBusiness software\\nCloud computing\\nComputer science\\nData centers\\nIT management\\nNetworking\\nSecurity\\nSoftware development\\n\\nPlease select a category\\n\\nTopics\\n\\n\\n\\nBrowse Features Resources\\n\\nBusiness strategies\\nCareer resources\\nEmerging tech\\nTech explainers\\n\\n\\n\\nFollow:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\n\\nData analytics and AI\\n\\nTech Accelerator What is Gen AI? Generative AI explained\\nPrev Next Will AI replace jobs? 17 job types that might be affected Pros and cons of AI-generated content\\nDownload this guide1\\nFeature\\n25 of the best large language models in 2025\\nLarge language models have been affecting search for years and have been brought to the forefront by ChatGPT and other chatbots.\\n\\nShare this item with your network:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy\\n\\nSean Michael Kerner\\nBen Lutkevich, Site Editor\\n\\nPublished: 31 Jan 2025\\nLarge language models are the dynamite behind the\\xa0generative AI\\xa0boom. However, they\\'ve been around for a while.\\nLLMs\\xa0are black box AI systems that use deep learning on extremely large datasets to understand and generate new text. Modern LLMs began taking shape in 2014 when the attention mechanism -- a machine learning technique designed to mimic human cognitive attention -- was introduced in a\\xa0research paper\\xa0titled \"Neural Machine Translation by Jointly Learning to Align and Translate.\" In 2017, that attention mechanism was honed with the introduction of the transformer model in another\\xa0paper, \"Attention Is All You Need.\"\\nSome of the most well-known language models today are based on the transformer model, including the\\xa0generative pre-trained transformer series\\xa0of LLMs and bidirectional encoder representations from transformers (BERT).\\nChatGPT, which runs on a set of language models from OpenAI, attracted more than 100 million users just two months after its release in 2022. Since then, many competing models have been released. Some belong to big companies such as Google, Amazon and Microsoft; others are open source.\\nConstant developments in the field can be difficult to keep track of. Here are some of the most influential models, both past and present. Included in it are models that paved the way for today\\'s leaders as well as those that could have a significant effect in the future.\\nThis article is part of\\nWhat is Gen AI? Generative AI explained\\n\\nWhich also includes:\\n8 top generative AI tool categories for 2025\\nWill AI replace jobs? 17 job types that might be affected\\n25 of the best large language models in 2025\\n\\nTop current LLMs\\nBelow are some of the most relevant large language models today. They do natural language processing and influence the architecture of future models.\\nBERT\\nBERT\\xa0is a family of LLMs that Google introduced in 2018. BERT is a\\xa0transformer-based\\xa0model that can convert sequences of data to other sequences of data. BERT\\'s architecture is a stack of transformer encoders and features 342 million parameters. BERT was pre-trained on a large corpus of data then fine-tuned to perform specific tasks along with natural language inference and sentence text similarity. It was used to improve query understanding in the 2019 iteration of Google search.\\nClaude\\nThe\\xa0Claude LLM\\xa0focuses on constitutional AI, which shapes AI outputs guided by a set of principles that help the AI assistant it powers helpful, harmless and accurate. Claude was created by the company Anthropic.\\nThere are three primary branches of Claude -- Opus, Haiku and Sonnet. The latest iteration of the Claude LLM is the Claude 3.5 Sonnet. It understands nuance, humor and complex instructions better than earlier versions of the LLM. It also has broad programming capabilities that make it well-suited for application development. In October 2024, Claude added a computer-use AI tool, that enables the LLM to use a computer like a human does. It\\'s available via Claude.ai, the Claude iOS app and through an API.\\nCohere\\nCohere is an enterprise AI platform that provides several LLMs including Command, Rerank and Embed. These\\xa0LLMs can be custom-trained\\xa0and fine-tuned to a specific company\\'s use case. The company that created the Cohere LLM was founded by one of the authors of Attention Is All You Need.\\nDeepSeek-R1\\nDeepSeek-R1 is an open-source reasoning model for tasks with complex reasoning, mathematical problem-solving and logical inference. The model uses reinforcement learning techniques to refine its reasoning ability and solve complex problems. DeepSeek-R1 can perform critical problem-solving through self-verification, chain-of-thought reasoning and reflection.\\nErnie\\nErnie is Baidu\\'s large language model which powers the Ernie 4.0 chatbot. The bot was released in August 2023 and has garnered more than 45 million users. Ernie is rumored to have 10 trillion parameters. The bot works best in Mandarin but is capable in other languages.\\nFalcon\\nFalcon is a family of transformer-based models developed by the Technology Innovation Institute. It is open source and has multi-lingual capabilities. Falcon 2 is available in an 11 billion parameter version that provide multimodal capabilities for both text and vision.\\nThe Falcon 1 series includes a pair of larger models with Falcon 40B and Falcon 180B. Falcon models are available on GitHub as well as on cloud provider including Amazon.\\nGemini\\nGemini\\xa0is Google\\'s family of LLMs that power the company\\'s chatbot of the same name. The model replaced Palm in powering the chatbot, which was rebranded from Bard to Gemini upon the model switch. Gemini models are multimodal, meaning they can handle images, audio and video as well as text. Gemini is also integrated in many Google applications and products. It comes in three sizes -- Ultra, Pro and Nano. Ultra is the largest and most capable model, Pro is the mid-tier model and Nano is the smallest model, designed for efficiency with on-device tasks.\\nAmong the most recent models is the Gemini 1.5 Pro update that debuted in May 2024 Gemini is available as a web chatbot, the Google Vertex AI service and via API. Early previews of Gemini 2.0 Flash became available in December 2024 with updated multimodal generation capabilities.\\nGemma\\nGemma\\xa0is a family of open-source language models from Google that were trained on the same resources as Gemini. Gemma 2 was released in June 2024 in two sizes -- a 9 billion parameter model and a 27 billion parameter model. Gemma models can be\\xa0run locally\\xa0on a personal computer, and are also available in Google Vertex AI.\\nGPT-3\\nGPT-3\\xa0is OpenAI\\'s large language model with more than 175 billion parameters, released in 2020. GPT-3 uses a decoder-only transformer architecture. In September 2022, Microsoft announced it had exclusive use of GPT-3\\'s underlying model. GPT-3 is 10 times larger than its predecessor. GPT-3\\'s training data includes Common Crawl, WebText2, Books1, Books2 and Wikipedia.\\nGPT-3 is the last of the GPT series of models in which OpenAI made the parameter counts publicly available. The GPT series was first introduced in 2018 with OpenAI\\'s paper \"Improving Language Understanding by Generative Pre-Training.\"\\nGPT-3.5\\nGPT-3.5 is an upgraded version of GPT-3 with fewer parameters. GPT-3.5 was fine-tuned using\\xa0reinforcement learning from human feedback. GPT-3.5 is the version of GPT that powers ChatGPT. There are several models, with GPT-3.5 turbo being the most capable, according to OpenAI. GPT-3.5\\'s training data extends to September 2021.\\nIt was also integrated into the Bing search engine but has since been replaced with GPT-4.\\nGPT-4\\nGPT-4\\xa0, was released in 2023 and like the others in the OpenAI GPT family, it\\'s a\\xa0transformer-based model. Unlike the others, its parameter count has not been released to the public, though there are rumors that the model has more than 170 trillion. OpenAI describes GPT-4 as a multimodal model, meaning it can\\xa0process and generate both language and images\\xa0as opposed to being limited to only language. GPT-4 also introduced a system message, which lets users specify tone of voice and task.\\nGPT-4 demonstrated human-level performance in multiple academic exams. At the model\\'s release, some speculated that GPT-4 came close to\\xa0artificial general intelligence, which means it is as smart or smarter than a human. That speculation turned out to be unfounded.\\nGPT-4o\\nGPT-4 Omni (GPT-4o) is OpenAI\\'s successor to GPT-4 and offers several improvements over the previous model. GPT-4o creates a more natural human interaction for ChatGPT and is a large multimodal model, accepting various inputs including audio, image and text. The conversations let users engage as they would in a normal human conversation, and the real-time interactivity can also pick up on emotions. GPT-4o can see photos or screens and ask questions about them during interaction.\\nGPT-4o can respond in 232 milliseconds, similar to human response time and faster than GPT-4 Turbo.\\nGranite\\nThe IBM Granite family of models are fully open source models under the Apache v.2 license. The first iteration of the open source model models debuted in May 2024, followed by Granite 3.0 in October and Granite 3.1 in December 2024.\\nThere are multiple variants in the Granite model family including General-purpose models (8B and 2B variants), guardrail model and Mixture-of-Experts models. While the model can be used for general purpose deployments, IBM itself is focusing deployment and optimization for enterprise use cases like customer service, IT automation and cybersecurity.\\nLamda\\nLamda (Language Model for Dialogue Applications) is a family of LLMs developed by Google Brain announced in 2021. Lamda used a decoder-only transformer language model and was pre-trained on a large corpus of text. In 2022, LaMDA gained widespread attention when then-Google engineer Blake Lemoine went public with claims that the\\xa0program was sentient. It was built on the Seq2Seq architecture.\\nLlama\\nLarge Language Model Meta AI (Llama) is Meta\\'s LLM which was first released in 2023. The Llama 3.1 models were released in July 2024, including both a 405 billion and 70 billion parameter model.\\nThe most recent version is Llama 3.2 which was released in September 2024, initially with smaller parameter counts of 11 billion and 90 billion.\\nLlama uses a transformer architecture and was trained on a variety of public data sources, including webpages from CommonCrawl, GitHub, Wikipedia and Project Gutenberg. Llama was effectively leaked and spawned many descendants, including Vicuna and Orca. Llama is available under an open license, allowing for free use of the models. Lllama models are available in many locations including llama.com and Hugging Face.\\nMistral\\nMistral is a family of a mixture of expert models from Mistral AI. Among the newest models is Mistral Large 2 which was first released in July 2024. The model operates with 123 billion parameters and a 128k context window, supporting dozens of languages including French, German, Spanish, Italian, and many others, along with more than 80 coding languages.\\nIn November 2024, Mistral released Pixtral Large, a 124-billion-parameter multimodal model that can handle text and visual data. Mistral models are available via Mistral\\'s API on its Le Platforme-managed web service.\\no1\\nThe OpenAI o1 model family was first introduced in Sept. 2024. The o1 model\\'s focus is to provide what OpenAI refers to as - reasoning models, that can reason through a problem or query before offering a response.\\nThe o1 models excel in STEM fields, with strong results in mathematical reasoning (scoring 83% on the International Mathematics Olympiad compared to GPT-4o\\'s 13%), code generation and scientific research tasks. While they offer enhanced reasoning and improved safety features, they operate more slowly than previous models due to their thorough reasoning processes and come with certain limitations, such as restricted access features and higher API costs. The models are available to ChatGPT Plus and Team users, with varying access levels for different user categories.\\no3\\nOpenAI introduced the successor model, o3, in December 2024. According to OpenAI, o3 is designed to handle tasks with more analytical thinking, problem-solving and complex reasoning and will improve o1\\'s capabilities and performance. The o3 model is in safety testing mode and is currently not available to the public.\\nOrca\\nOrca was developed by Microsoft and has 13 billion parameters, meaning it\\'s small enough to run on a laptop. It aims to improve on advancements made by other open source models by imitating the reasoning procedures achieved by LLMs. Orca achieves the same performance as GPT-4 with significantly fewer parameters and is on par with GPT-3.5 for many tasks. Orca is built on top of the 13 billion parameter version of Llama.\\nPalm\\nThe\\xa0Pathways Language Model\\xa0is a 540 billion parameter transformer-based model from Google powering its AI chatbot\\xa0Bard. It was trained across multiple\\xa0TPU\\xa04 Pods -- Google\\'s custom hardware for machine learning. Palm specializes in reasoning tasks such as coding, math, classification and question answering. Palm also excels at decomposing complex tasks into simpler subtasks.\\nPaLM gets its name from a Google research initiative to build Pathways, ultimately creating a single model that serves as a foundation for multiple use cases. There are\\xa0several fine-tuned versions\\xa0of Palm, including Med-Palm 2 for life sciences and medical information as well as Sec-Palm for cybersecurity deployments to speed up threat analysis.\\nPhi\\nPhi is a transformer-based language model from Microsoft. The Phi 3.5 models were first released in August 2024.\\nThe series includes Phi-3.5-mini-instruct (3.82 billion parameters), Phi-3.5-MoE-instruct (41.9 billion parameters), and Phi-3.5-vision-instruct (4.15 billion parameters), each designed for specific tasks ranging from basic reasoning to vision analysis. All three models support a 128k token context length.\\nReleased under a Microsoft-branded MIT License, they are available for developers to download, use, and modify without restrictions, including for commercial purposes.\\nQwen\\nQwen is large family of open models developed by Chinese internet giant Alibaba Cloud. The newest set of models are the Qwen2.5 suite, which support 29 different languages and currently scale up to 72 billion parameters. These models are suitable for a wide range of tasks, including code generation, structured data understanding, mathematical problem-solving as well as general language understanding and generation.\\nStableLM\\nStableLM is a series of open language models developed by Stability AI, the company behind image generator Stable Diffusion.\\nStableLM 2 debuted in January 2024 initially with a 1.6 billion parameter model. In April 2024 that was expanded to also include a 12 billion parameter model. StableLM 2 supports seven languages: English, Spanish, German, Italian, French, Portuguese, and Dutch. Stability AI positions these models as offering different options for various use cases, with the 1.6B model suitable for specific, narrow tasks and faster processing while the 12B model provides more capability but requires more computational resources.\\nTülu 3\\nAllen Institute for AI\\'s Tülu 3 is an open-source 405 billion-parameter LLM. The Tülu 3 405B model has post-training methods that combine supervised fine-tuning and reinforcement learning at a larger scale. Tülu 3 uses a \"reinforcement learning from verifiable rewards\" framework for fine-tuning tasks with verifiable outcomes -- such as solving mathematical problems and following instructions.\\nVicuna 33B\\nVicuna is another influential open source LLM derived from Llama. It was developed by LMSYS and was fine-tuned using data from sharegpt.com. It is smaller and less capable that GPT-4 according to several benchmarks, but does well for a model of its size. Vicuna has only 33 billion parameters, whereas GPT-4 has trillions.\\nLLM precursors\\nAlthough LLMs are a recent phenomenon, their precursors go back decades. Learn how recent precursor Seq2Seq and distant precursor ELIZA set the stage for modern LLMs.\\nSeq2Seq\\nSeq2Seq is a deep learning approach used for machine translation, image captioning and natural language processing. It was developed by Google and underlies some of their modern LLMs, including LaMDA. Seq2Seq also underlies AlexaTM 20B, Amazon\\'s large language model. It uses a mix of encoders and decoders.\\nEliza\\nEliza was an\\xa0early natural language processing program\\xa0created in 1966. It is one of the earliest examples of a language model. Eliza simulated conversation using pattern matching and substitution. Eliza, running a certain script, could parody the interaction between a patient and therapist by applying weights to certain keywords and responding to the user accordingly. The creator of Eliza, Joshua Weizenbaum, wrote a book on the limits of computation and artificial intelligence.\\nNext Steps\\nGenerative AI challenges that businesses should consider\\nGenerative AI ethics: Biggest concerns\\nGenerative AI landscape: Potential future trends\\nGenerative models: VAEs, GANs, diffusion, transformers, NeRFs\\nAI content generators to explore\\nRelated Resources\\n\\nFive data quality trends to prepare for in the year ahead –Video\\nThe Digital Transformation And Innovation Landscape –Wipro\\nCloudera and NVIDIA Accelerate AI in the Financial Services Industry –Cloudera\\nImprove customer satisfaction or cut costs? Who says you have to choose? –Video\\n\\nDig Deeper on Data analytics and AI\\n\\n ##### What is GPT-3? Everything you need to know  By: Nick Barney\\n ##### What is a small language model (SLM)?  By: Sean Kerner\\n ##### GPT-4  By: Ben Lutkevich\\n ##### What are large language models (LLMs)?  By: Sean Kerner\\n\\nSponsored News\\n\\nSustainability, AI and Dell PowerEdge Servers –Dell Technologies and Intel\\nThree Innovative AI Use Cases for Natural Language Processing –Dell Technologies\\nAutonomous coding: The future of the revenue cycle –Solventum\\n\\nRelated Content\\n\\nExploring GPT-3 architecture – Search Enterprise AI\\nWhat is GPT-3? Everything you need to know – Search Enterprise AI\\nMicrosoft exclusively licenses OpenAI\\'s GPT-3 ... – Search Enterprise AI\\n\\nLatest TechTarget resources\\n\\nNetworking\\nSecurity\\nCIO\\nHR Software\\nCustomer Experience\\n\\nSearch Networking\\n\\n\\nWhat is a thin client (lean client)?A thin client (lean client) is a virtual desktop computing model that runs on the resources stored on a central server instead of...\\n\\n\\nWhat is network monitoring?Network monitoring, also frequently called network management, is the practice of consistently overseeing a computer network for ...\\n\\n\\nWhat is network automation?Network automation is a process that uses intelligent software to automate the management, configuration, deployment, testing and...\\n\\n\\nSearch Security\\n\\n\\nWhat is Internet Key Exchange (IKE)?Internet Key Exchange (IKE) is a standard protocol used to set up a secure and authenticated communication channel between two ...\\n\\n\\nWhat is a certificate revocation list (CRL) and how is it used?A certificate revocation list (CRL) is a list of digital certificates that have been revoked by the issuing certificate authority...\\n\\n\\nWhat is cryptology?Cryptology is the mathematics, such as number theory and the application of formulas and algorithms, that underpin cryptography ...\\n\\n\\nSearch CIO\\n\\n\\nWhat is an IT project manager?An IT project manager is a professional charged with overseeing the process of planning, executing and delegating ...\\n\\n\\nWhat is a cyberthreat hunter (cybersecurity threat analyst)?A cyberthreat hunter, also called a cybersecurity threat analyst, proactively identifies security incidents that might go ...\\n\\n\\nWhat is blockchain? Definition, examples and how it worksBlockchain is a distributed ledger technology (DLT) that\\'s shared across a network of computers to keep a digital record of ...\\n\\n\\nSearch HRSoftware\\n\\n\\nWhat is employee self-service (ESS)?Employee self-service (ESS) is a widely used human resources technology that enables employees to perform many job-related ...\\n\\n\\nWhat is DEI? Diversity, equity and inclusion explainedDiversity, equity and inclusion is a term used to describe policies and programs that promote the representation and ...\\n\\n\\nWhat is payroll software?Payroll software automates the process of paying salaried, hourly and contingent employees.\\n\\n\\nSearch Customer Experience\\n\\n\\nWhat is account-based selling? Everything you need to knowAccount-based selling (ABS) is a strategic sales approach in business-to-business sales and marketing that centers around ...\\n\\n\\nWhat is interactive voice response (IVR)?Interactive voice response (IVR) is an automated telephony system that interacts with callers, gathers information and routes ...\\n\\n\\nWhat is an AI assistant?An AI assistant, or digital assistant, is software that uses artificial intelligence to understand natural language voice ...\\n\\n\\nBrowse by Topic\\n\\n\\nBrowse Resources\\n\\n\\nAbout Us\\n\\nMeet The Editors\\nEditorial Ethics Policy\\nContact Us\\nAdvertisers\\nBusiness Partners\\nEvents\\nMedia Kit\\nCorporate Site\\nReprints\\n\\nAll Rights Reserved, Copyright 1999 - 2025, TechTarget  \\nPrivacy Policy\\nCookie Preferences\\nCookie Preferences\\nDo Not Sell or Share My Personal Information\\nClose\\n\\nX\\nFree Download What is generative AI? Everything you need to know\\nThe potential of AI technology has been percolating in the background for years. But when ChatGPT, the AI chatbot, began grabbing headlines in early 2023, it put generative AI in the spotlight. This guide is your go-to manual for generative AI, covering its benefits, limits, use cases, prospects and much more.\\n'},\n",
              "  {'url': 'https://github.com/eugeneyan/open-llms',\n",
              "   'title': 'A list of open LLMs available for commercial use. - GitHub',\n",
              "   'content': '| MPT-7B | 2023/05 | MPT-7B, MPT-7B-Instruct | Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs | 7 | 84k (ALiBi) | Apache 2.0, CC BY-SA-3.0 |  | | Qwen1.5 32B | 2024/04 | Qwen1.5-32B, Qwen1.5-32B-Chat | Qwen1.5-32B: Fitting the Capstone of the Qwen1.5 Language Model Series | 32 | 32k | Custom Free if you have under 100M users and you cannot use Qwen outputs to train other LLMs besides Qwen and its derivatives |  | | Qwen1.5 110B | 2024/04 | Qwen1.5-110B, Qwen1.5-110B-Chat | Qwen1.5-110B: The First 100B+ Model of the Qwen1.5 Series | 110 | 32k | Custom Free if you have under 100M users and you cannot use Qwen outputs to train other LLMs besides Qwen and its derivatives |  |',\n",
              "   'score': 0.6353361,\n",
              "   'raw_content': 'GitHub - eugeneyan/open-llms: 📋 A list of open LLMs available for commercial use.\\nSkip to content \\nNavigation Menu\\nToggle navigation\\n\\nSign in\\n\\n\\nProduct\\n\\nGitHub Copilot Write better code with AI\\nSecurity Find and fix vulnerabilities\\nActions Automate any workflow\\nCodespaces Instant dev environments\\nIssues Plan and track work\\nCode Review Manage code changes\\nDiscussions Collaborate outside of code\\nCode Search Find more, search less\\n\\nExplore\\n\\nAll features\\nDocumentation\\nGitHub Skills\\nBlog\\n\\n\\n\\nSolutions\\nBy company size\\n\\nEnterprises\\nSmall and medium teams\\nStartups\\nNonprofits\\n\\nBy use case\\n\\nDevSecOps\\nDevOps\\nCI/CD\\nView all use cases\\n\\nBy industry\\n\\nHealthcare\\nFinancial services\\nManufacturing\\nGovernment\\nView all industries\\n\\nView all solutions\\n\\n\\nResources\\nTopics\\n\\nAI\\nDevOps\\nSecurity\\nSoftware Development\\nView all\\n\\nExplore\\n\\nLearning Pathways\\nWhite papers, Ebooks, Webinars\\nCustomer Stories\\nPartners\\nExecutive Insights\\n\\n\\n\\nOpen Source\\n\\n\\nGitHub Sponsors Fund open source developers\\n\\n\\nThe ReadME Project GitHub community articles\\n\\n\\nRepositories\\n\\nTopics\\nTrending\\nCollections\\n\\n\\n\\nEnterprise\\n\\nEnterprise platform AI-powered developer platform\\n\\nAvailable add-ons\\n\\nAdvanced Security Enterprise-grade security features\\nGitHub Copilot Enterprise-grade AI features\\nPremium Support Enterprise-grade 24/7 support\\n\\n\\n\\nPricing\\n\\n\\nSearch or jump to...\\nSearch code, repositories, users, issues, pull requests...\\nSearch\\nClear\\nSearch syntax tips\\nProvide feedback\\nWe read every piece of feedback, and take your input very seriously.\\nInclude my email address so I can be contacted\\nCancel Submit feedback\\nSaved searches\\nUse saved searches to filter your results more quickly\\nName  \\nQuery \\nTo see all available qualifiers, see our documentation.\\nCancel Create saved search\\nSign in\\nSign up Reseting focus\\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\\n{{ message }}\\neugeneyan / open-llms Public\\n\\nNotifications You must be signed in to change notification settings\\nFork 796\\nStar 11.6k\\n\\n📋 A list of open LLMs available for commercial use.\\nLicense\\nApache-2.0 license\\n11.6k stars 796 forks Branches Tags Activity\\nStar\\nNotifications You must be signed in to change notification settings\\n\\nCode\\nIssues 4\\nPull requests 1\\nActions\\nProjects 0\\nSecurity\\nInsights\\n\\nAdditional navigation options\\n\\nCode\\nIssues\\nPull requests\\nActions\\nProjects\\nSecurity\\nInsights\\n\\neugeneyan/open-llms\\nmain\\nBranchesTags\\n\\nGo to file\\nCode\\nFolders and files\\n| Name | Name | \\nLast commit message\\n| \\nLast commit date\\n|\\n| --- | --- | --- | --- |\\n| \\nLatest commit\\nHistory\\n174 Commits\\n\\n|\\n| \\nLICENSE\\n| \\nLICENSE\\n| \\n| \\n|\\n| \\nREADME.md\\n| \\nREADME.md\\n| \\n| \\n|\\n| \\nView all files\\n|\\nRepository files navigation\\n\\nREADME\\nApache-2.0 license\\n\\nOpen LLMs\\n\\nThese LLMs (Large Language Models) are all licensed for commercial use (e.g., Apache 2.0, MIT, OpenRAIL-M). Contributions welcome!\\n| Language Model | Release Date | Checkpoints | Paper/Blog | Params (B) | Context Length | Licence | Try it |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n| T5 | 2019/10 | T5 & Flan-T5, Flan-T5-xxl (HF) | Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer | 0.06 - 11 | 512 | Apache 2.0 | T5-Large |\\n| RWKV 4 | 2021/08 | RWKV, ChatRWKV | The RWKV Language Model (and my LM tricks) | 0.1 - 14 | infinity (RNN) | Apache 2.0 |  |\\n| GPT-NeoX-20B | 2022/04 | GPT-NEOX-20B | GPT-NeoX-20B: An Open-Source Autoregressive Language Model | 20 | 2048 | Apache 2.0 |  |\\n| YaLM-100B | 2022/06 | yalm-100b | Yandex publishes YaLM 100B, the largest GPT-like neural network in open source | 100 | 1024 | Apache 2.0 |  |\\n| UL2 | 2022/10 | UL2 & Flan-UL2, Flan-UL2 (HF) | UL2 20B: An Open Source Unified Language Learner | 20 | 512, 2048 | Apache 2.0 |  |\\n| Bloom | 2022/11 | Bloom | BLOOM: A 176B-Parameter Open-Access Multilingual Language Model | 176 | 2048 | OpenRAIL-M v1 |  |\\n| ChatGLM | 2023/03 | chatglm-6b | ChatGLM, Github | 6 | 2048 | Custom Free with some usage restriction (might require registration) |  |\\n| Cerebras-GPT | 2023/03 | Cerebras-GPT | Cerebras-GPT: A Family of Open, Compute-efficient, Large Language Models (Paper) | 0.111 - 13 | 2048 | Apache 2.0 | Cerebras-GPT-1.3B |\\n| Open Assistant (Pythia family) | 2023/03 | OA-Pythia-12B-SFT-8, OA-Pythia-12B-SFT-4, OA-Pythia-12B-SFT-1 | Democratizing Large Language Model Alignment | 12 | 2048 | Apache 2.0 | Pythia-2.8B |\\n| Pythia | 2023/04 | pythia 70M - 12B | Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling | 0.07 - 12 | 2048 | Apache 2.0 |  |\\n| Dolly | 2023/04 | dolly-v2-12b | Free Dolly: Introducing the World\\'s First Truly Open Instruction-Tuned LLM | 3, 7, 12 | 2048 | MIT |  |\\n| StableLM-Alpha | 2023/04 | StableLM-Alpha | Stability AI Launches the First of its StableLM Suite of Language Models | 3 - 65 | 4096 | CC BY-SA-4.0 |  |\\n| FastChat-T5 | 2023/04 | fastchat-t5-3b-v1.0 | We are excited to release FastChat-T5: our compact and commercial-friendly chatbot! | 3 | 512 | Apache 2.0 |  |\\n| DLite | 2023/05 | dlite-v2-1_5b | Announcing DLite V2: Lightweight, Open LLMs That Can Run Anywhere | 0.124 - 1.5 | 1024 | Apache 2.0 | DLite-v2-1.5B |\\n| h2oGPT | 2023/05 | h2oGPT | Building the World’s Best Open-Source Large Language Model: H2O.ai’s Journey | 12 - 20 | 256 - 2048 | Apache 2.0 |  |\\n| MPT-7B | 2023/05 | MPT-7B, MPT-7B-Instruct | Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs | 7 | 84k (ALiBi) | Apache 2.0, CC BY-SA-3.0 |  |\\n| RedPajama-INCITE | 2023/05 | RedPajama-INCITE | Releasing 3B and 7B RedPajama-INCITE family of models including base, instruction-tuned & chat models | 3 - 7 | 2048 | Apache 2.0 | RedPajama-INCITE-Instruct-3B-v1 |\\n| OpenLLaMA | 2023/05 | open_llama_3b, open_llama_7b, open_llama_13b | OpenLLaMA: An Open Reproduction of LLaMA | 3, 7 | 2048 | Apache 2.0 | OpenLLaMA-7B-Preview_200bt |\\n| Falcon | 2023/05 | Falcon-180B, Falcon-40B, Falcon-7B | The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only | 180, 40, 7 | 2048 | Apache 2.0 |  |\\n| GPT-J-6B | 2023/06 | GPT-J-6B, GPT4All-J | GPT-J-6B: 6B JAX-Based Transformer | 6 | 2048 | Apache 2.0 |  |\\n| MPT-30B | 2023/06 | MPT-30B, MPT-30B-instruct | MPT-30B: Raising the bar for open-source foundation models | 30 | 8192 | Apache 2.0, CC BY-SA-3.0 | MPT 30B inference code using CPU |\\n| LLaMA 2 | 2023/06 | LLaMA 2 Weights\\xa0 | Llama 2: Open Foundation and Fine-Tuned Chat Models | 7 - 70 | 4096 | Custom Free if you have under 700M users and you cannot use LLaMA outputs to train other LLMs besides LLaMA and its derivatives | HuggingChat |\\n| ChatGLM2 | 2023/06 | chatglm2-6b | ChatGLM2-6B, Github | 6 | 32k | Custom Free with some usage restriction (might require registration) |  |\\n| XGen-7B | 2023/06 | xgen-7b-4k-base, xgen-7b-8k-base | Long Sequence Modeling with XGen | 7 | 4096, 8192 | Apache 2.0 |  |\\n| Jais-13b | 2023/08 | jais-13b, jais-13b-chat | Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models | 13 | 2048 | Apache 2.0 |  |\\n| OpenHermes | 2023/09 | OpenHermes-7B, OpenHermes-13B | Nous Research | 7, 13 | 4096 | MIT | OpenHermes-V2 Finetuned on Mistral 7B |\\n| OpenLM | 2023/09 | OpenLM 1B, OpenLM 7B\\xa0 | Open LM: a minimal but performative language modeling (LM) repository | 1, 7 | 2048 | MIT |  |\\n| Mistral 7B | 2023/09 | Mistral-7B-v0.1, Mistral-7B-Instruct-v0.1 | Mistral 7B | 7 | 4096-16K with Sliding Windows | Apache 2.0 | Mistral Transformer |\\n| ChatGLM3 | 2023/10 | chatglm3-6b, chatglm3-6b-base, chatglm3-6b-32k, chatglm3-6b-128k | ChatGLM3 | 6 | 8192, 32k, 128k | Custom Free with some usage restriction (might require registration) |  |\\n| Skywork | 2023/10 | Skywork-13B-Base, Skywork-13B-Math | Skywork | 13 | 4096 | Custom Free with usage restriction and models trained on Skywork outputs become Skywork derivatives, subject to this license. |  |\\n| Jais-30b | 2023/11 | jais-30b-v1, jais-30b-chat-v1 | Jais-30B: Expanding the Horizon in Open-Source Arabic NLP | 30 | 2048 | Apache 2.0 |  |\\n| Zephyr | 2023/11 | Zephyr 7B | Website | 7 | 8192 | Apache 2.0 |  |\\n| DeepSeek | 2023/11 | deepseek-llm-7b-base, deepseek-llm-7b-chat, deepseek-llm-67b-base, deepseek-llm-67b-chat | Introducing DeepSeek LLM, | 7, 67 | 4096 | Custom Free with usage restriction and models trained on DeepSeek outputs become DeepSeek derivatives, subject to this license. |  |\\n| Mistral 7B v0.2 | 2023/12 | Mistral-7B-v0.2, Mistral-7B-Instruct-v0.2 | La Plateforme | 7 | 32k | Apache 2.0 |  |\\n| Mixtral 8x7B v0.1 | 2023/12 | Mixtral-8x7B-v0.1, Mixtral-8x7B-Instruct-v0.1 | Mixtral of experts | 46.7 | 32k | Apache 2.0 |  |\\n| LLM360 Amber | 2023/12 | Amber, AmberChat, AmberSafe | Introducing LLM360: Fully Transparent Open-Source LLMs | 6.7 | 2048 | Apache 2.0 |  |\\n| SOLAR | 2023/12 | Solar-10.7B | Upstage | 10.7 | 4096 | apache-2.0 |  |\\n| phi-2 | 2023/12 | phi-2 2.7B | Microsoft | 2.7 | 2048 | MIT |  |\\n| FLOR | 2023/12 | FLOR-760M, FLOR-1.3B, FLOR-1.3B-Instructed, FLOR-6.3B, FLOR-6.3B-Instructed | FLOR-6.3B: a chinchilla-compliant model for Catalan, Spanish and English | 0.76, 1.3, 6.3 | 2048 | Apache 2.0 with usage restriction inherited from BLOOM |  |\\n| RWKV 5 v2 | 2024/01 | rwkv-5-world-0.4b-2, rwkv-5-world-1.5b-2, rwkv-5-world-3b-2, rwkv-5-world-3b-2(16k), rwkv-5-world-7b-2 | RWKV 5 | 0.4, 1.5, 3, 7 | unlimited(RNN), trained on 4096 (and 16k for 3b) | Apache 2.0 |  |\\n| OLMo | 2024/02 | OLMo 1B, OLMo 7B, OLMo 7B Twin 2T | AI2 | 1,7 | 2048 | Apache 2.0 |  |\\n| Qwen1.5 | 2024/02 | Qwen1.5-7B, Qwen1.5-7B-Chat, Qwen1.5-14B, Qwen1.5-14B-Chat, Qwen1.5-72B, Qwen1.5-72B-Chat | Introducing Qwen1.5 | 7, 14, 72 | 32k | Custom Free if you have under 100M users and you cannot use Qwen outputs to train other LLMs besides Qwen and its derivatives |  |\\n| LWM | 2024/02 | LWM-Text-Chat-128K, LWM-Text-Chat-256K, LWM-Text-Chat-512K, LWM-Text-Chat-1M, LWM-Text-128K, LWM-Text-256K, LWM-Text-512K, LWM-Text-1M | Large World Model (LWM) | 7 | 128k, 256k, 512k, 1M | LLaMA 2 license |  |\\n| Jais-30b v3 | 2024/03 | jais-30b-v3, jais-30b-chat-v3 | Jais 30b v3 | 30 | 8192 | Apache 2.0 |  |\\n| Gemma | 2024/02 | Gemma 7B, Gemma 7B it, Gemma 2B, Gemma 2B it | Technical report | 2-7 | 8192 | Gemma Terms of Use Free with usage restriction and models trained on Gemma outputs become Gemma derivatives, subject to this license. |  |\\n| Grok-1 | 2024/03 | Grok-1 | Open Release of Grok-1 | 314 | 8192 | Apache 2.0 |  |\\n| Qwen1.5 MoE | 2024/03 | Qwen1.5-MoE-A2.7B, Qwen1.5-MoE-A2.7B-Chat | Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters | 14.3 | 8192 | Custom Free if you have under 100M users and you cannot use Qwen outputs to train other LLMs besides Qwen and its derivatives |  |\\n| Jamba 0.1 | 2024/03 | Jamba-v0.1 | Introducing Jamba: AI21\\'s Groundbreaking SSM-Transformer Model | 52 | 256k | Apache 2.0 |  |\\n| Qwen1.5 32B | 2024/04 | Qwen1.5-32B, Qwen1.5-32B-Chat | Qwen1.5-32B: Fitting the Capstone of the Qwen1.5 Language Model Series | 32 | 32k | Custom Free if you have under 100M users and you cannot use Qwen outputs to train other LLMs besides Qwen and its derivatives |  |\\n| Mamba-7B | 2024/04 | mamba-7b-rw | Toyota Research Institute | 7 | unlimited(RNN), trained on 2048 | Apache 2.0 |  |\\n| Mixtral8x22B v0.1 | 2024/04 | Mixtral-8x22B-v0.1, Mixtral-8x22B-Instruct-v0.1 | Cheaper, Better, Faster, Stronger | 141 | 64k | Apache 2.0 |  |\\n| Llama 3 | 2024/04 | Llama-3-8B, Llama-3-8B-Instruct, Llama-3-70B, Llama-3-70B-Instruct, Llama-Guard-2-8B | Introducing Meta Llama 3, Meta Llama 3 | 8, 70 | 8192 | Meta Llama 3 Community License Agreement Free if you have under 700M users and you cannot use LLaMA 3 outputs to train other LLMs besides LLaMA 3 and its derivatives |  |\\n| Phi-3 Mini | 2024/04 | Phi-3-mini-4k-instruct, Phi-3-mini-128k-instruct | Introducing Phi-3, Technical Report | 3.8 | 4096, 128k | MIT |  |\\n| OpenELM | 2024/04 | OpenELM-270M, OpenELM-270M-Instruct, OpenELM-450M, OpenELM-450M-Instruct, OpenELM-1_1B, OpenELM-1_1B-Instruct, OpenELM-3B, OpenELM-3B-Instruct | OpenELM: An Efficient Language Model Family with Open Training and Inference Framework | 0.27, 0.45, 1.1, 3 | 2048 | Custom open license No usage or training restrictions |  |\\n| Snowflake Arctic | 2024/04 | snowflake-arctic-base, snowflake-arctic-instruct | Snowflake Arctic: The Best LLM for Enterprise AI — Efficiently Intelligent, Truly Open | 480 | 4096 | Apache 2.0 |  |\\n| Qwen1.5 110B | 2024/04 | Qwen1.5-110B, Qwen1.5-110B-Chat | Qwen1.5-110B: The First 100B+ Model of the Qwen1.5 Series | 110 | 32k | Custom Free if you have under 100M users and you cannot use Qwen outputs to train other LLMs besides Qwen and its derivatives |  |\\n| RWKV 6 v2.1 | 2024/05 | rwkv-6-world-1.6b-2.1, rwkv-6-world-3b-2.1, rwkv-6-world-7b-2.1 | RWKV 6 | 1.6, 3, 7 | unlimited(RNN), trained on 4096 | Apache 2.0 |  |\\n| DeepSeek-V2 | 2024/05 | DeepSeek-V2, DeepSeek-V2-Chat | DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model | 236 | 128k | Custom Free with usage restriction and models trained on DeepSeek outputs become DeepSeek derivatives, subject to this license. |  |\\n| Fugaku-LLM | 2024/05 | Fugaku-LLM-13B, Fugaku-LLM-13B-instruct | Release of \"Fugaku-LLM\" – a large language model trained on the supercomputer \"Fugaku\" | 13 | 2048 | Custom Free with usage restrictions |  |\\n| Falcon 2 | 2024/05 | falcon2-11B | Meet Falcon 2: TII Releases New AI Model Series, Outperforming Meta’s New Llama 3 | 11 | 8192 | Custom Apache 2.0 with mild acceptable use policy |  |\\n| Yi-1.5 | 2024/05 | Yi-1.5-6B, Yi-1.5-6B-Chat, Yi-1.5-9B, Yi-1.5-9B-Chat, Yi-1.5-34B, Yi-1.5-34B-Chat | Yi-1.5 | 6, 9, 34 | 4096 | Apache 2.0 |  |\\n| DeepSeek-V2-Lite | 2024/05 | DeepSeek-V2-Lite, DeepSeek-V2-Lite-Chat | DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model | 16 | 32k | Custom Free with usage restriction and models trained on DeepSeek outputs become DeepSeek derivatives, subject to this license. |  |\\n| Phi-3 small/medium | 2024/05 | Phi-3-mini-4k-instruct, Phi-3-mini-128k-instruct, Phi-3-medium-4k-instruct, Phi-3-medium-128k-instruct | New models added to the Phi-3 family, available on Microsoft Azure, Technical Report | 7, 14 | 4096, 128k | MIT |  |\\n| Phi-4 | 2024/12 | Phi-4 | Introducing Phi-4: Microsoft’s Newest Small Language Model Specializing in Complex Reasoning, Technical Report | 14 | 4096 | MIT |  |\\n| YuLan-Mini | 2024/12 | YuLan-Mini | YuLan-Mini: An Open Data-efficient Language Model, GitHub | 14 | 28672 | MIT | YuLan-Mini |\\nOpen LLMs for code\\n\\n| Language Model | Release Date | Checkpoints | Paper/Blog | Params (B) | Context Length | Licence | Try it |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n| SantaCoder | 2023/01 | santacoder | SantaCoder: don\\'t reach for the stars! | 1.1 | 2048 | OpenRAIL-M v1 | SantaCoder |\\n| CodeGen2 | 2023/04 | codegen2 1B-16B | CodeGen2: Lessons for Training LLMs on Programming and Natural Languages | 1 - 16 | 2048 | Apache 2.0 |  |\\n| StarCoder | 2023/05 | starcoder | StarCoder: A State-of-the-Art LLM for Code, StarCoder: May the source be with you! | 1.1-15 | 8192 | OpenRAIL-M v1 |  |\\n| StarChat Alpha | 2023/05 | starchat-alpha | Creating a Coding Assistant with StarCoder | 16 | 8192 | OpenRAIL-M v1 |  |\\n| Replit Code | 2023/05 | replit-code-v1-3b | Training a SOTA Code LLM in 1 week and Quantifying the Vibes — with Reza Shabani of Replit | 2.7 | infinity? (ALiBi) | CC BY-SA-4.0 | Replit-Code-v1-3B |\\n| CodeT5+ | 2023/05 | CodeT5+ | CodeT5+: Open Code Large Language Models for Code Understanding and Generation | 0.22 - 16 | 512 | BSD-3-Clause | Codet5+-6B |\\n| XGen-7B | 2023/06 | XGen-7B-8K-Base | Long Sequence Modeling with XGen: A 7B LLM Trained on 8K Input Sequence Length | 7 | 8192 | Apache 2.0 |  |\\n| CodeGen2.5 | 2023/07 | CodeGen2.5-7B-multi | CodeGen2.5: Small, but mighty | 7 | 2048 | Apache 2.0 |  |\\n| DeciCoder-1B | 2023/08 | DeciCoder-1B | Introducing DeciCoder: The New Gold Standard in Efficient and Accurate Code Generation | 1.1 | 2048 | Apache 2.0 | DeciCoder Demo |\\n| Code Llama | 2023/08 | Inference Code for CodeLlama models\\xa0 | Code Llama: Open Foundation Models for Code | 7 - 34 | 4096 | Custom Free if you have under 700M users and you cannot use LLaMA outputs to train other LLMs besides LLaMA and its derivatives | HuggingChat |\\nOpen LLM datasets for pre-training\\n\\n| Name | Release Date | Paper/Blog | Dataset | Tokens (T) | License |\\n| --- | --- | --- | --- | --- | --- |\\n| RedPajama | 2023/04 | RedPajama, a project to create leading open-source models, starts by reproducing LLaMA training dataset of over 1.2 trillion tokens | RedPajama-Data | 1.2 | Apache 2.0 |\\n| starcoderdata | 2023/05 | StarCoder: A State-of-the-Art LLM for Code | starcoderdata | 0.25 | Apache 2.0 |\\nOpen LLM datasets for instruction-tuning\\n\\n| Name | Release Date | Paper/Blog | Dataset | Samples (K) | License |\\n| --- | --- | --- | --- | --- | --- |\\n| OIG (Open Instruction Generalist) | 2023/03 | THE OIG DATASET | OIG | 44,000 | Apache 2.0 |\\n| databricks-dolly-15k | 2023/04 | Free Dolly: Introducing the World\\'s First Truly Open Instruction-Tuned LLM | databricks-dolly-15k | 15 | CC BY-SA-3.0 |\\n| MPT-7B-Instruct | 2023/05 | Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs | dolly_hhrlhf | 59 | CC BY-SA-3.0 |\\nOpen LLM datasets for alignment-tuning\\n\\n| Name | Release Date | Paper/Blog | Dataset | Samples (K) | License |\\n| --- | --- | --- | --- | --- | --- |\\n| OpenAssistant Conversations Dataset | 2023/04 | OpenAssistant Conversations - Democratizing Large Language Model Alignment | oasst1 | 161 | Apache 2.0 |\\nEvals on open LLMs\\n\\n\\nLeaderboard by lmsys.org\\nEvals by MosaicML\\nHolistic Evaluation of Language Models (HELM)\\nLLM-Leaderboard\\nTextSynth Server Benchmarks\\nOpen LLM Leaderboard by Hugging Face\\n\\n\\nWhat do the licences mean?\\n\\n\\nApache 2.0: Allows users to use the software for any purpose, to distribute it, to modify it, and to distribute modified versions of the software under the terms of the license, without concern for royalties.\\nMIT: Similar to Apache 2.0 but shorter and simpler. Also, in contrast to Apache 2.0, does not require stating any significant changes to the original code.\\nCC BY-SA-4.0: Allows (i) copying and redistributing the material and (ii) remixing, transforming, and building upon the material for any purpose, even commercially. But if you do the latter, you must distribute your contributions under the same license as the original. (Thus, may not be viable for internal teams.)\\nOpenRAIL-M v1: Allows royalty-free access and flexible downstream use and sharing of the model and modifications of it, and comes with a set of use restrictions (see Attachment A)\\nBSD-3-Clause: This version allows unlimited redistribution for any purpose as long as its copyright notices and the license\\'s disclaimers of warranty are maintained.\\n\\nDisclaimer: The information provided in this repo does not, and is not intended to, constitute legal advice. Maintainers of this repo are not responsible for the actions of third parties who use the models. Please consult an attorney before using models for commercial purposes.\\n\\nImprovements\\n\\n\\n[ ]  Complete entries for context length, and check entries with ?\\n[ ]  ~Add number of tokens trained?~ (see considerations)\\n[ ]  Add (links to) training code?\\n[ ]  Add (links to) eval benchmarks?\\n\\nAbout\\n📋 A list of open LLMs available for commercial use.\\nTopics\\ncommercial large-language-models llm llms\\nResources\\nReadme\\nLicense\\nApache-2.0 license\\nActivity\\nStars\\n11.6k stars\\nWatchers\\n248 watching\\nForks\\n796 forks\\nReport repository\\nReleases\\nNo releases published\\nPackages 0\\nNo packages published  \\nContributors 31\\n+ 17 contributors\\nFooter\\n© 2025 GitHub,\\xa0Inc.\\nFooter navigation\\n\\nTerms\\nPrivacy\\nSecurity\\nStatus\\nDocs\\nContact\\nManage cookies\\nDo not share my personal information\\n\\nYou can’t perform that action at this time.'},\n",
              "  {'url': 'https://www.shakudo.io/blog/top-9-large-language-models',\n",
              "   'title': 'Top 9 Large Language Models as of Feburary 2025 - Shakudo',\n",
              "   'content': 'The latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. The latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. The latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data.',\n",
              "   'score': 0.4896749,\n",
              "   'raw_content': 'Top 9 Large Language Models as of Feburary 2025 | Shakudo\\nLatest in Insights : When to Choose Deep Learning Over Machine Learning (And Vice Versa)\\n\\n\\nWhy SHakudo\\n\\n Data & AI OS Build your ideal data stack on one unified platform Learn more >\\nshakudo AI Applications\\n Text to SQL Workflow Automation Vector Database Reverse ETLSee all >\\nComponents\\nSolutions\\n\\nShakudo for Industries\\nAerospace\\nAutomotive & Transportation\\nClimate & Energy\\nFinancial Services\\nHealthcare & Life Sciences\\nHigher Education\\nManufacturing\\nReal Estate\\nRetail\\nSports\\nTechnology & Software\\nShakudo Use Cases\\nAutomate Custom Sustainability Report Population\\nChat with Enterprise Knowledge Base Using AI Assistants\\nGenerate Real-World Evidence for Healthcare Decisions\\nOptimize Ticket Pricing with Dynamic Demand Modeling\\nDetect Hidden Red Flags in Company Data\\nMonitor Market Sentiment Across Multiple Sources\\nSee all >\\nResources\\n\\n Case Studies Learn how leading companies leverage data & AI on Shakudo blog Read what\\'s new at Shakudo and the data and AI world white papers Access in-depth reports and guides on data & AI solutions Docs Explore comprehensive guides on the Shakudo platform\\n  Case Study How CentralReach uses Shakudo to Cut Time-To-Deployment to Launch New AI- Powered Solutions\\n  Case Study How AI is Changing the Game for the Cleveland Cavaliers\\nCompany\\n\\n ABout Us Learn about our mission and values Careers Join us in building the next-gen data stack Partners Learn about the relationships that make it happen Contact us Have a question? We\\'re here to help\\nAI WorkshopGet a Demo\\n\\n← Back to Blog\\nInsights\\nTop 9 Large Language Models as of Feburary 2025\\nAuthor(s):\\n\\nNo items found.\\nUpdated on:\\nFebruary 7, 2025\\n\\nTable of contents\\nExample H2\\nExample H3\\nMentioned Components\\nNo items found.\\n<>\\nGet the latest updates in Data & AI straight to your inboxWe’ll email you once per week—and never share your information.\\n🎉 Success! You\\'re now signed up for the Shakudo newsletter.\\nOops! Something went wrong while submitting the form.\\nIntroduction\\nIf we had to choose one word to describe the rapid evolution of AI today, it would probably be something along the lines of explosive. As predicted by the Market Research Future report, the large language model (LLM) market in North America alone is expected to reach $105.5 billion by 2030. The exponential growth of AI tools combined with access to massive troves of text data has opened gates for better and more advanced content generation than we had ever hoped. Yet, such rapid expansion also makes it harder than ever to navigate and select the right tools among the diverse LLM models available.\\nThe goal of this post is to keep you, the AI enthusiast and professional, up-to-date with current trends and essential innovations in the field. Below, we highlighted the top 9 LLMs that we think are currently making waves in the industry, each with distinct capabilities and specialized strengths, excelling in areas such as natural language processing, code synthesis, few-shot learning, or scalability. While we believe there is no one-size-fits-all LLM for every use case, we hope that this list can help you identify the most current and well-suited LLM model that meets your business’s unique requirements.\\n1. GPT\\n\\nOur list kicks off with OpenAI\\'s Generative Pre-trained Transformer (GPT) models, which have consistently exceeded their previous capabilities with each new release. Compared to its prior models, the latest ChatGPT-4o and ChatGPT-4o mini models offer significantly faster processing speeds and enhanced capabilities across text, voice, and vision.\\nThe latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. Both of these models are equipped with multimodal capabilities to handle images as well as audio data.\\nDespite having advanced conversational and reasoning capabilities, note that GPT is a proprietary model, meaning that the training data and parameters are kept confidential by OpenAI, and access to full functionality is restricted–a commercial license or subscription is often required to unlock the complete range of features. In this case, we recommend this model for businesses looking to adopt an LLM that excels in conversational dialogue, multi-step reasoning, efficient computation, and real-time interactions without the constraints of a budget.\\nFor companies who are curious to try out the proprietary models on the market before fully committing to one due to budget constraints or uncertainties about its long-term integration, Shakudo offers a compelling alternative. Our platform currently features a diverse selection of advanced LLMs with simplified deployment and scalability. With a simple subscription, you can access and assess the value of proprietary models, like GPT, before making a substantial investment.\\n2.DeepSeek\\n\\nDeepseek-R1 Benchmark. Source: deepseek.com\\nWith its latest R1 model, the Chinese AI company DeepSeek has once again set new benchmarks for innovation in the AI community. As of January 24th, the DeepSeek-R1 model is ranked fourth on Chatbot Arena, and top as the best open-source LM.\\nThe DeepSeek-R1 is a 671B parameter Mixture-of-Experts (MoE) model with 37B activated parameters per token, trained through large-scale reinforcement learning with a strong focus on reasoning capabilities. The model excels at understanding and handling long-form content and demonstrates superior performance in complex tasks such as mathematics and code generation. The model is approximately 30 times more cost-efficient than OpenAI-o1 and 5 times faster, offering groundbreaking performance at a fraction of the cost. Moreover, it has shown exceptional precision in tasks requiring complex pattern recognition, such as genomic data analysis, medical imaging, and large-scale scientific simulations.\\nDeepSeek-R1’s capabilities are transformative when it comes to integration with proprietary enterprise data such as PII and financial records. Leveraging retrieval-augmented generation (RAG), enterprises can connect the model to their internal data sources to enable highly personalized, context-aware interactions—all while maintaining stringent security and compliance standards. With Shakudo, you can streamline the deployment and integration of advanced AI models like DeepSeek by automating the setup, deployment, and management processes. This eliminates the need for businesses to invest in and maintain extensive computing infrastructure. By operating within your existing infrastructure, the platform ensures seamless integration, enhanced security, and optimal performance without requiring significant in-house resources or specialized expertise.\\n3. Qwen\\n\\n\\u200d\\nAlibaba released Qwen2.5-Max earlier last month, the new model designed to deliver enhanced performance for large-scale natural language processing tasks. In instruct model evaluations, this new model outperforms DeepSeek V3 in benchmarks such as Arena-Hard, LiveBench, LiveCodeBench, and GPQA-Diamond, while also delivering strong performance in other assessments like MMLU-Pro.\\nQwen2.5-Max is pretrained on over 20 trillion tokens. While specific details about its parameter count and token window size are not publicly disclosed, Qwen2.5-Max is designed for low-latency, high-efficiency tasks, making it suitable for applications requiring quick, accurate responses with low latency. Its smaller size allows for deployment on devices with limited computational resources.\\nFor businesses and users looking for higher-performance models for natural language processing and AI-driven tasks, the Qwen 2.5 model is available on platforms like Hugging Face and ModelScope. This model boasts from 0.5 billion to 72 billion parameters, featuring context windows of up to 128,000 tokens, and is excellent for code generation, debugging, and automated forecasting.\\n4. LG AI\\n\\nsource\\nEXAONE 3.0 is a bilingual LLM developed by LG AI Research. The company released its latest model in December 2024 with excellent performance across various benchmarks and real-world applications.\\nWith 7.8 billion parameters, EXAONE 3.0 is capable of understanding and generating human-like text in multiple languages across complex domains, including coding, mathematics, patents, and chemistry. This model has also been optimized to reduce inference processing time by 56%, memory usage by 35%, and operating costs by 72%, ensuring that it remains cost-effective while maintaining high performance.\\nLG AI Research has open-sourced the 7.8B parameter instruction-tuned version of EXAONE 3.0 for non-commercial research purposes. This is a model we recommend to software companies or tech startups for generating Python code, assisting developers in troubleshooting, and creating APIs or other backend components.\\n5. LlaMA\\n\\nMeta is still leading the front with their state-of-the-art LlaMa models. The company released its latest LlaMA 3.3 model in December 2024, featuring multimodal capabilities that can process both text and image for in-depth analysis and response generation, such as interpreting charts, maps, or translating texts identified in an image.\\nLlaMA 3.3 improves on previous models with a longer context window of up to 128,000 tokens and an optimized transformer architecture. With a parameter of 70 billion, this model outperforms open-source and proprietary alternatives in areas such as multilingual dialogue, reasoning, and coding.\\nUnlike ChatGPT models, LlaMA 3 is open-source, giving users the flexibility to access and deploy freely on their cloud depending on the specific requirements of their infrastructure, security preferences, or customization needs. We recommend this model to businesses looking for advanced content generation and language understanding, such as those in customer service, education, marketing, and consumer markets. The openness of these models also allows for your greater control over the model’s performance, tuning, and integration into existing workflows.\\n6. Claude\\n\\nNext on our list is Claude, more specifically, the latest Claude 3.5 Sonnet model developed by Anthropic. We believe that Claude is arguably one of the most significant competitors to GPT since all of its current models–Claude 3 Haiku, Claude 3.5 Sonnet, and Claude 3 Opus–are designed with incredible contextual understanding capabilities that position themselves as the top conversational AI closely aligned with nuanced human interactions.\\nWhile the specific parameters of Claude 3.5 Sonnet remain undisclosed, the model boasts an impressive context window of 200,000 tokens, equivalent to approximately 150,000 words or 300 pages of text.\\nThe current Claude subscription service is credit-based, and the cost can go as high as $2,304/month for enterprise plans tailored to high-volume users. We recommend Claude to mature or mid-stage businesses looking not only to adopt an AI that facilitates human-like interactions but also to enhance their coding capabilities since the Claude 3.5 Sonnet model is currently reaching a 49.0% performance score on the SWE-bench Verified benchmark, placing it as third among all publicly available models, including reasoning models and systems specifically designed for agentic coding.\\n7. Mistral\\n\\nMistral\\'s latest model – Mistral Small 3, a latency-optimized model was released under the Apache 2.0 license at the end of January. This 24-billion-parameter model is designed for low-latency, high-efficiency tasks. It processes approximately 150 tokens per second, making it over three times faster than Llama 3.3 70B on the same hardware.\\nThis new model is ideal for applications requiring quick, accurate responses with low latency, such as virtual assistants, real-time data processing, and on-device command and control. Its smaller size allows for deployment on devices with limited computational resources.\\nMistral Small 3 is currently open-source under the Apache 2.0 license. This means you can freely access and use the model for your own applications, provided you comply with the license terms. Since it is designed to be easily deployable, including on hardware with limited resources like a single GPU or even a MacBook with 32GB RAM, we\\'d recommend this to early-stage businesses looking to implement low-latency AI solutions without the need for extensive hardware infrastructure.\\n8. Gemini\\n\\nGemini is a family of closed-source LLM models developed by Google. The latest model—Gemini 2.0 Flash—operates at twice the speed of Gemini 1.5 Pro, offering substantial improvements in speed, reasoning, and multimodal processing capabilities.\\nWith that being said, Gemini remains a proprietary model; if your company deals with sensitive or confidential data regularly, you might be concerned about sending it to external servers due to security reasons. To address this concern, we recommend that you double-check vendor compliance regulations to ensure data privacy and security standards are met, such as adherence to GDPR, HIPAA, or other relevant data protection laws.\\nIf you’re looking for an open-source alternative that exhibits capabilities almost as good as Gemini, Google’s latest Gemma model, Gemma 2, offers three models available in 2 billion, 9 billion, and 27 billion parameters with a context window of 8,200. For businesses looking for a rather economic option, this is the optimal choice that interprets and understands messages with remarkable accuracy.\\n9. Command\\n\\nCommand R is a family of scalable models developed by Cohere with the goal of balancing high performance with strong accuracy, just like Claude. Both the Command R and Command R+ models offer APIs specifically optimized for Retrieval Augmented Generation (RAG). This means that these models can combine large-scale language generation with real-time information retrieval techniques for much more contextually aware outputs.\\nCurrently, the Command R+ model boasts 104 billion parameters and offers an industry-leading 128,000 token context window for enhanced long-form processing and multi-turn conversation capabilities.\\nOne of the perks of working with an open-source model is also to avoid vendor lock-in. Heavy reliance on a particular type of proprietary model may make it difficult for you to switch to alternative models when your business starts growing or the landscape changes. Cohere approaches this in a hybrid way, meaning that you can access and modify the model for personal usage but need a license for commercial use. In this case, we recommend this model for businesses that want flexibility in experimentation without a long-term commitment to a single vendor.\\n\\nWhitepaper\\nIntroduction\\nIf we had to choose one word to describe the rapid evolution of AI today, it would probably be something along the lines of explosive. As predicted by the Market Research Future report, the large language model (LLM) market in North America alone is expected to reach $105.5 billion by 2030. The exponential growth of AI tools combined with access to massive troves of text data has opened gates for better and more advanced content generation than we had ever hoped. Yet, such rapid expansion also makes it harder than ever to navigate and select the right tools among the diverse LLM models available.\\nThe goal of this post is to keep you, the AI enthusiast and professional, up-to-date with current trends and essential innovations in the field. Below, we highlighted the top 9 LLMs that we think are currently making waves in the industry, each with distinct capabilities and specialized strengths, excelling in areas such as natural language processing, code synthesis, few-shot learning, or scalability. While we believe there is no one-size-fits-all LLM for every use case, we hope that this list can help you identify the most current and well-suited LLM model that meets your business’s unique requirements.\\n1. GPT\\n\\nOur list kicks off with OpenAI\\'s Generative Pre-trained Transformer (GPT) models, which have consistently exceeded their previous capabilities with each new release. Compared to its prior models, the latest ChatGPT-4o and ChatGPT-4o mini models offer significantly faster processing speeds and enhanced capabilities across text, voice, and vision.\\nThe latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. Both of these models are equipped with multimodal capabilities to handle images as well as audio data.\\nDespite having advanced conversational and reasoning capabilities, note that GPT is a proprietary model, meaning that the training data and parameters are kept confidential by OpenAI, and access to full functionality is restricted–a commercial license or subscription is often required to unlock the complete range of features. In this case, we recommend this model for businesses looking to adopt an LLM that excels in conversational dialogue, multi-step reasoning, efficient computation, and real-time interactions without the constraints of a budget.\\nFor companies who are curious to try out the proprietary models on the market before fully committing to one due to budget constraints or uncertainties about its long-term integration, Shakudo offers a compelling alternative. Our platform currently features a diverse selection of advanced LLMs with simplified deployment and scalability. With a simple subscription, you can access and assess the value of proprietary models, like GPT, before making a substantial investment.\\n2.DeepSeek\\n\\nDeepseek-R1 Benchmark. Source: deepseek.com\\nWith its latest R1 model, the Chinese AI company DeepSeek has once again set new benchmarks for innovation in the AI community. As of January 24th, the DeepSeek-R1 model is ranked fourth on Chatbot Arena, and top as the best open-source LM.\\nThe DeepSeek-R1 is a 671B parameter Mixture-of-Experts (MoE) model with 37B activated parameters per token, trained through large-scale reinforcement learning with a strong focus on reasoning capabilities. The model excels at understanding and handling long-form content and demonstrates superior performance in complex tasks such as mathematics and code generation. The model is approximately 30 times more cost-efficient than OpenAI-o1 and 5 times faster, offering groundbreaking performance at a fraction of the cost. Moreover, it has shown exceptional precision in tasks requiring complex pattern recognition, such as genomic data analysis, medical imaging, and large-scale scientific simulations.\\nDeepSeek-R1’s capabilities are transformative when it comes to integration with proprietary enterprise data such as PII and financial records. Leveraging retrieval-augmented generation (RAG), enterprises can connect the model to their internal data sources to enable highly personalized, context-aware interactions—all while maintaining stringent security and compliance standards. With Shakudo, you can streamline the deployment and integration of advanced AI models like DeepSeek by automating the setup, deployment, and management processes. This eliminates the need for businesses to invest in and maintain extensive computing infrastructure. By operating within your existing infrastructure, the platform ensures seamless integration, enhanced security, and optimal performance without requiring significant in-house resources or specialized expertise.\\n3. Qwen\\n\\n\\u200d\\nAlibaba released Qwen2.5-Max earlier last month, the new model designed to deliver enhanced performance for large-scale natural language processing tasks. In instruct model evaluations, this new model outperforms DeepSeek V3 in benchmarks such as Arena-Hard, LiveBench, LiveCodeBench, and GPQA-Diamond, while also delivering strong performance in other assessments like MMLU-Pro.\\nQwen2.5-Max is pretrained on over 20 trillion tokens. While specific details about its parameter count and token window size are not publicly disclosed, Qwen2.5-Max is designed for low-latency, high-efficiency tasks, making it suitable for applications requiring quick, accurate responses with low latency. Its smaller size allows for deployment on devices with limited computational resources.\\nFor businesses and users looking for higher-performance models for natural language processing and AI-driven tasks, the Qwen 2.5 model is available on platforms like Hugging Face and ModelScope. This model boasts from 0.5 billion to 72 billion parameters, featuring context windows of up to 128,000 tokens, and is excellent for code generation, debugging, and automated forecasting.\\n4. LG AI\\n\\nsource\\nEXAONE 3.0 is a bilingual LLM developed by LG AI Research. The company released its latest model in December 2024 with excellent performance across various benchmarks and real-world applications.\\nWith 7.8 billion parameters, EXAONE 3.0 is capable of understanding and generating human-like text in multiple languages across complex domains, including coding, mathematics, patents, and chemistry. This model has also been optimized to reduce inference processing time by 56%, memory usage by 35%, and operating costs by 72%, ensuring that it remains cost-effective while maintaining high performance.\\nLG AI Research has open-sourced the 7.8B parameter instruction-tuned version of EXAONE 3.0 for non-commercial research purposes. This is a model we recommend to software companies or tech startups for generating Python code, assisting developers in troubleshooting, and creating APIs or other backend components.\\n5. LlaMA\\n\\nMeta is still leading the front with their state-of-the-art LlaMa models. The company released its latest LlaMA 3.3 model in December 2024, featuring multimodal capabilities that can process both text and image for in-depth analysis and response generation, such as interpreting charts, maps, or translating texts identified in an image.\\nLlaMA 3.3 improves on previous models with a longer context window of up to 128,000 tokens and an optimized transformer architecture. With a parameter of 70 billion, this model outperforms open-source and proprietary alternatives in areas such as multilingual dialogue, reasoning, and coding.\\nUnlike ChatGPT models, LlaMA 3 is open-source, giving users the flexibility to access and deploy freely on their cloud depending on the specific requirements of their infrastructure, security preferences, or customization needs. We recommend this model to businesses looking for advanced content generation and language understanding, such as those in customer service, education, marketing, and consumer markets. The openness of these models also allows for your greater control over the model’s performance, tuning, and integration into existing workflows.\\n6. Claude\\n\\nNext on our list is Claude, more specifically, the latest Claude 3.5 Sonnet model developed by Anthropic. We believe that Claude is arguably one of the most significant competitors to GPT since all of its current models–Claude 3 Haiku, Claude 3.5 Sonnet, and Claude 3 Opus–are designed with incredible contextual understanding capabilities that position themselves as the top conversational AI closely aligned with nuanced human interactions.\\nWhile the specific parameters of Claude 3.5 Sonnet remain undisclosed, the model boasts an impressive context window of 200,000 tokens, equivalent to approximately 150,000 words or 300 pages of text.\\nThe current Claude subscription service is credit-based, and the cost can go as high as $2,304/month for enterprise plans tailored to high-volume users. We recommend Claude to mature or mid-stage businesses looking not only to adopt an AI that facilitates human-like interactions but also to enhance their coding capabilities since the Claude 3.5 Sonnet model is currently reaching a 49.0% performance score on the SWE-bench Verified benchmark, placing it as third among all publicly available models, including reasoning models and systems specifically designed for agentic coding.\\n7. Mistral\\n\\nMistral\\'s latest model – Mistral Small 3, a latency-optimized model was released under the Apache 2.0 license at the end of January. This 24-billion-parameter model is designed for low-latency, high-efficiency tasks. It processes approximately 150 tokens per second, making it over three times faster than Llama 3.3 70B on the same hardware.\\nThis new model is ideal for applications requiring quick, accurate responses with low latency, such as virtual assistants, real-time data processing, and on-device command and control. Its smaller size allows for deployment on devices with limited computational resources.\\nMistral Small 3 is currently open-source under the Apache 2.0 license. This means you can freely access and use the model for your own applications, provided you comply with the license terms. Since it is designed to be easily deployable, including on hardware with limited resources like a single GPU or even a MacBook with 32GB RAM, we\\'d recommend this to early-stage businesses looking to implement low-latency AI solutions without the need for extensive hardware infrastructure.\\n8. Gemini\\n\\nGemini is a family of closed-source LLM models developed by Google. The latest model—Gemini 2.0 Flash—operates at twice the speed of Gemini 1.5 Pro, offering substantial improvements in speed, reasoning, and multimodal processing capabilities.\\nWith that being said, Gemini remains a proprietary model; if your company deals with sensitive or confidential data regularly, you might be concerned about sending it to external servers due to security reasons. To address this concern, we recommend that you double-check vendor compliance regulations to ensure data privacy and security standards are met, such as adherence to GDPR, HIPAA, or other relevant data protection laws.\\nIf you’re looking for an open-source alternative that exhibits capabilities almost as good as Gemini, Google’s latest Gemma model, Gemma 2, offers three models available in 2 billion, 9 billion, and 27 billion parameters with a context window of 8,200. For businesses looking for a rather economic option, this is the optimal choice that interprets and understands messages with remarkable accuracy.\\n9. Command\\n\\nCommand R is a family of scalable models developed by Cohere with the goal of balancing high performance with strong accuracy, just like Claude. Both the Command R and Command R+ models offer APIs specifically optimized for Retrieval Augmented Generation (RAG). This means that these models can combine large-scale language generation with real-time information retrieval techniques for much more contextually aware outputs.\\nCurrently, the Command R+ model boasts 104 billion parameters and offers an industry-leading 128,000 token context window for enhanced long-form processing and multi-turn conversation capabilities.\\nOne of the perks of working with an open-source model is also to avoid vendor lock-in. Heavy reliance on a particular type of proprietary model may make it difficult for you to switch to alternative models when your business starts growing or the landscape changes. Cohere approaches this in a hybrid way, meaning that you can access and modify the model for personal usage but need a license for commercial use. In this case, we recommend this model for businesses that want flexibility in experimentation without a long-term commitment to a single vendor.\\nGet the whitepaper\\nTop 9 Large Language Models as of Feburary 2025\\nBy clicking \"Download,\" you agree to Shakudo processing your personal data in accordance with its Privacy Notice.\\nThank you for filling out the form. The whitepaper you have requested is available for download below.  \\nDownload White Paper\\nOops! Something went wrong while submitting the form.\\nGet the whitepaper\\nTop 9 Large Language Models as of Feburary 2025\\nThank you for your interest. Click the button below to download whitepaper you have requested.  \\nDownload White Paper\\n\\nTop 9 Large Language Models as of Feburary 2025\\nExplore the top 9 LLMs making waves in the AI world and what each of them excel at\\n\\n| Case Study\\nTop 9 Large Language Models as of Feburary 2025\\n\\nKey results\\nAbout\\nindustry\\nTech Stack\\nNo items found.\\n<>\\nIntroduction\\nIf we had to choose one word to describe the rapid evolution of AI today, it would probably be something along the lines of explosive. As predicted by the Market Research Future report, the large language model (LLM) market in North America alone is expected to reach $105.5 billion by 2030. The exponential growth of AI tools combined with access to massive troves of text data has opened gates for better and more advanced content generation than we had ever hoped. Yet, such rapid expansion also makes it harder than ever to navigate and select the right tools among the diverse LLM models available.\\nThe goal of this post is to keep you, the AI enthusiast and professional, up-to-date with current trends and essential innovations in the field. Below, we highlighted the top 9 LLMs that we think are currently making waves in the industry, each with distinct capabilities and specialized strengths, excelling in areas such as natural language processing, code synthesis, few-shot learning, or scalability. While we believe there is no one-size-fits-all LLM for every use case, we hope that this list can help you identify the most current and well-suited LLM model that meets your business’s unique requirements.\\n1. GPT\\n\\nOur list kicks off with OpenAI\\'s Generative Pre-trained Transformer (GPT) models, which have consistently exceeded their previous capabilities with each new release. Compared to its prior models, the latest ChatGPT-4o and ChatGPT-4o mini models offer significantly faster processing speeds and enhanced capabilities across text, voice, and vision.\\nThe latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. Both of these models are equipped with multimodal capabilities to handle images as well as audio data.\\nDespite having advanced conversational and reasoning capabilities, note that GPT is a proprietary model, meaning that the training data and parameters are kept confidential by OpenAI, and access to full functionality is restricted–a commercial license or subscription is often required to unlock the complete range of features. In this case, we recommend this model for businesses looking to adopt an LLM that excels in conversational dialogue, multi-step reasoning, efficient computation, and real-time interactions without the constraints of a budget.\\nFor companies who are curious to try out the proprietary models on the market before fully committing to one due to budget constraints or uncertainties about its long-term integration, Shakudo offers a compelling alternative. Our platform currently features a diverse selection of advanced LLMs with simplified deployment and scalability. With a simple subscription, you can access and assess the value of proprietary models, like GPT, before making a substantial investment.\\n2.DeepSeek\\n\\nDeepseek-R1 Benchmark. Source: deepseek.com\\nWith its latest R1 model, the Chinese AI company DeepSeek has once again set new benchmarks for innovation in the AI community. As of January 24th, the DeepSeek-R1 model is ranked fourth on Chatbot Arena, and top as the best open-source LM.\\nThe DeepSeek-R1 is a 671B parameter Mixture-of-Experts (MoE) model with 37B activated parameters per token, trained through large-scale reinforcement learning with a strong focus on reasoning capabilities. The model excels at understanding and handling long-form content and demonstrates superior performance in complex tasks such as mathematics and code generation. The model is approximately 30 times more cost-efficient than OpenAI-o1 and 5 times faster, offering groundbreaking performance at a fraction of the cost. Moreover, it has shown exceptional precision in tasks requiring complex pattern recognition, such as genomic data analysis, medical imaging, and large-scale scientific simulations.\\nDeepSeek-R1’s capabilities are transformative when it comes to integration with proprietary enterprise data such as PII and financial records. Leveraging retrieval-augmented generation (RAG), enterprises can connect the model to their internal data sources to enable highly personalized, context-aware interactions—all while maintaining stringent security and compliance standards. With Shakudo, you can streamline the deployment and integration of advanced AI models like DeepSeek by automating the setup, deployment, and management processes. This eliminates the need for businesses to invest in and maintain extensive computing infrastructure. By operating within your existing infrastructure, the platform ensures seamless integration, enhanced security, and optimal performance without requiring significant in-house resources or specialized expertise.\\n3. Qwen\\n\\n\\u200d\\nAlibaba released Qwen2.5-Max earlier last month, the new model designed to deliver enhanced performance for large-scale natural language processing tasks. In instruct model evaluations, this new model outperforms DeepSeek V3 in benchmarks such as Arena-Hard, LiveBench, LiveCodeBench, and GPQA-Diamond, while also delivering strong performance in other assessments like MMLU-Pro.\\nQwen2.5-Max is pretrained on over 20 trillion tokens. While specific details about its parameter count and token window size are not publicly disclosed, Qwen2.5-Max is designed for low-latency, high-efficiency tasks, making it suitable for applications requiring quick, accurate responses with low latency. Its smaller size allows for deployment on devices with limited computational resources.\\nFor businesses and users looking for higher-performance models for natural language processing and AI-driven tasks, the Qwen 2.5 model is available on platforms like Hugging Face and ModelScope. This model boasts from 0.5 billion to 72 billion parameters, featuring context windows of up to 128,000 tokens, and is excellent for code generation, debugging, and automated forecasting.\\n4. LG AI\\n\\nsource\\nEXAONE 3.0 is a bilingual LLM developed by LG AI Research. The company released its latest model in December 2024 with excellent performance across various benchmarks and real-world applications.\\nWith 7.8 billion parameters, EXAONE 3.0 is capable of understanding and generating human-like text in multiple languages across complex domains, including coding, mathematics, patents, and chemistry. This model has also been optimized to reduce inference processing time by 56%, memory usage by 35%, and operating costs by 72%, ensuring that it remains cost-effective while maintaining high performance.\\nLG AI Research has open-sourced the 7.8B parameter instruction-tuned version of EXAONE 3.0 for non-commercial research purposes. This is a model we recommend to software companies or tech startups for generating Python code, assisting developers in troubleshooting, and creating APIs or other backend components.\\n5. LlaMA\\n\\nMeta is still leading the front with their state-of-the-art LlaMa models. The company released its latest LlaMA 3.3 model in December 2024, featuring multimodal capabilities that can process both text and image for in-depth analysis and response generation, such as interpreting charts, maps, or translating texts identified in an image.\\nLlaMA 3.3 improves on previous models with a longer context window of up to 128,000 tokens and an optimized transformer architecture. With a parameter of 70 billion, this model outperforms open-source and proprietary alternatives in areas such as multilingual dialogue, reasoning, and coding.\\nUnlike ChatGPT models, LlaMA 3 is open-source, giving users the flexibility to access and deploy freely on their cloud depending on the specific requirements of their infrastructure, security preferences, or customization needs. We recommend this model to businesses looking for advanced content generation and language understanding, such as those in customer service, education, marketing, and consumer markets. The openness of these models also allows for your greater control over the model’s performance, tuning, and integration into existing workflows.\\n6. Claude\\n\\nNext on our list is Claude, more specifically, the latest Claude 3.5 Sonnet model developed by Anthropic. We believe that Claude is arguably one of the most significant competitors to GPT since all of its current models–Claude 3 Haiku, Claude 3.5 Sonnet, and Claude 3 Opus–are designed with incredible contextual understanding capabilities that position themselves as the top conversational AI closely aligned with nuanced human interactions.\\nWhile the specific parameters of Claude 3.5 Sonnet remain undisclosed, the model boasts an impressive context window of 200,000 tokens, equivalent to approximately 150,000 words or 300 pages of text.\\nThe current Claude subscription service is credit-based, and the cost can go as high as $2,304/month for enterprise plans tailored to high-volume users. We recommend Claude to mature or mid-stage businesses looking not only to adopt an AI that facilitates human-like interactions but also to enhance their coding capabilities since the Claude 3.5 Sonnet model is currently reaching a 49.0% performance score on the SWE-bench Verified benchmark, placing it as third among all publicly available models, including reasoning models and systems specifically designed for agentic coding.\\n7. Mistral\\n\\nMistral\\'s latest model – Mistral Small 3, a latency-optimized model was released under the Apache 2.0 license at the end of January. This 24-billion-parameter model is designed for low-latency, high-efficiency tasks. It processes approximately 150 tokens per second, making it over three times faster than Llama 3.3 70B on the same hardware.\\nThis new model is ideal for applications requiring quick, accurate responses with low latency, such as virtual assistants, real-time data processing, and on-device command and control. Its smaller size allows for deployment on devices with limited computational resources.\\nMistral Small 3 is currently open-source under the Apache 2.0 license. This means you can freely access and use the model for your own applications, provided you comply with the license terms. Since it is designed to be easily deployable, including on hardware with limited resources like a single GPU or even a MacBook with 32GB RAM, we\\'d recommend this to early-stage businesses looking to implement low-latency AI solutions without the need for extensive hardware infrastructure.\\n8. Gemini\\n\\nGemini is a family of closed-source LLM models developed by Google. The latest model—Gemini 2.0 Flash—operates at twice the speed of Gemini 1.5 Pro, offering substantial improvements in speed, reasoning, and multimodal processing capabilities.\\nWith that being said, Gemini remains a proprietary model; if your company deals with sensitive or confidential data regularly, you might be concerned about sending it to external servers due to security reasons. To address this concern, we recommend that you double-check vendor compliance regulations to ensure data privacy and security standards are met, such as adherence to GDPR, HIPAA, or other relevant data protection laws.\\nIf you’re looking for an open-source alternative that exhibits capabilities almost as good as Gemini, Google’s latest Gemma model, Gemma 2, offers three models available in 2 billion, 9 billion, and 27 billion parameters with a context window of 8,200. For businesses looking for a rather economic option, this is the optimal choice that interprets and understands messages with remarkable accuracy.\\n9. Command\\n\\nCommand R is a family of scalable models developed by Cohere with the goal of balancing high performance with strong accuracy, just like Claude. Both the Command R and Command R+ models offer APIs specifically optimized for Retrieval Augmented Generation (RAG). This means that these models can combine large-scale language generation with real-time information retrieval techniques for much more contextually aware outputs.\\nCurrently, the Command R+ model boasts 104 billion parameters and offers an industry-leading 128,000 token context window for enhanced long-form processing and multi-turn conversation capabilities.\\nOne of the perks of working with an open-source model is also to avoid vendor lock-in. Heavy reliance on a particular type of proprietary model may make it difficult for you to switch to alternative models when your business starts growing or the landscape changes. Cohere approaches this in a hybrid way, meaning that you can access and modify the model for personal usage but need a license for commercial use. In this case, we recommend this model for businesses that want flexibility in experimentation without a long-term commitment to a single vendor.\\nExplore more from Shakudo\\n How VPCs Enable AI Deployments with a Modern Data Stack Insights January 28, 2025\\n The Power of Simple Questions: How to Choose the Right Natural Language to SQL Query Tool Insights May 15, 2024\\n Bring Data and AI tooling right to MongoDB Atlas with Shakudo News August 26, 2024\\nTake the next step\\n\"Shakudo gave us the flexibility to use the data stack components that fit our needs and evolve the stack to keep up with the industry.\"\\n\\nNeal Gilmore\\nSenior Vice President, Enterprise Data & Analytics\\nDiscover Shakudo\\n\\nShakudo brings the best data and AI products into your VPC and operates them for you automatically achieving a more reliable, performant, and cost effective data stack than ever before.\\n\\n Book Demo Email X (Twitter) Linkedin Youtube\\nNewsletter\\nSign up for the latest Shakudo news:\\n🎉 Success! You\\'re now signed up for the Shakudo newsletter.\\nOops! Something went wrong while submitting the form.\\nApplications\\nData and AI OSStack ComponentsLanguage to SQLVector Database + LLMReverse ETLWorkflow Automation\\nIndustries\\nAutomotive & Transportation\\nAerospace\\nManufacturing\\nHigher Education\\nHealthcare & Life Sciences\\nClimate & Energy\\nTechnology & Software\\nSports\\nReal Estate\\nRetail\\nFinancial Services\\nResources\\nUse Cases\\nInsights\\nWhite Paper\\nCase Study\\nPress\\nProduct\\nTutorial\\nNews\\nWebinarGlossaryDocumentation\\nCompany\\nAboutPartnersDGX PartnerCareersMedia Kit\\nGet Started\\nSignupContact UsNewsletter\\n© 2025 Shakudo\\nToronto, CA\\nContact usPrivacy PolicyTerms/ConditionsSitemap\\nTrusted by industry leaders\\n\\n\\n\\n\\n\\n\\nSee Shakudo in Action  \\nWatch the 3 Minute Demo\\n\\nThis field is required\\n\\nFor information about how Shakudo handles your personal data, please see our Privacy Policy.\\nThank you for your submission. A Shakudo expert will be in touch with you shortly.  \\nIn the meantime, feel free to check out our data insights, case studies, and latest industry news that help data teams win.  \\n Live chat Live chat will provide the quickest answer to any of your questions.\\nOops! Something went wrong while submitting the form.\\n⨉'},\n",
              "  {'url': 'https://medium.com/data-science-in-your-pocket/the-best-llms-of-2024-0b5d930f89ff',\n",
              "   'title': 'The best LLMs of 2024. DeepSeek-v3, Claude3.5 Sonnet, GPT-4o…',\n",
              "   'content': 'DeepSeek-v3, Claude3.5 Sonnet, GPT-4o… | by Mehul Gupta | Data Science in your pocket | Jan, 2025 | Medium DeepSeek-v3, Claude3.5 Sonnet, GPT-4o, OpenAI-o1, Qwen Coder, Flux and many more Best Open-sourced LLM Gives a tough fight to closed source models like GPT-4o and Claude3.5 Best Image-Video model Though the model looks even better than Hunyuan-video, the model isn’t open-sourced yet and has limited access as well. Though the model is not open-sourced, it is still available for common folks to try unlike OpenAI o1 Best Image generation model Gemini2 because the output quality is the best for codes and is available for free (not open-sourced though). Best Audio Cloning/TTS model',\n",
              "   'score': 0.44680908,\n",
              "   'raw_content': None},\n",
              "  {'url': 'https://slator.com/10-large-language-models-that-matter-to-the-language-industry/',\n",
              "   'title': '10 Large Language Models That Matter to the Language Industry',\n",
              "   'content': 'The model — described by Meta as “the most capable openly available LLM to date” — is behind the Meta AI assistant that is now embedded in Facebook, Instagram, and WhatsApp. Llama 3 is optimized for dialogue use cases (i.e., AI assistants) and is English-centric. Slator 2024 Language Industry Market Report — Language AI Edition For a more in-depth analysis of the changing AI model landscape in 2024 and its implications for players in the language industry, obtain a copy of Slator’s 2024 Language Industry Market Report — Language AI Edition. Slator is the leading source of research and market intelligence for translation, localization, interpreting, and language AI.',\n",
              "   'score': 0.39683983,\n",
              "   'raw_content': '*New* Slator Pro Guide: AI in Interpreting\\n10 Large Language Models That Matter to the Language Industry\\nThe landscape of large language models (LLMs) is changing fast. The pace of releases is accelerating, along with performance and capability advances.\\xa0\\nModels are becoming more multimodal, and covering a wider range of languages.\\xa0\\nThe Slator 2024 Language Industry Market Report examines how quickly LLMs are changing and considers the implications for language industry players that build applications on top of these “base” models.\\nThe report includes a one-page timeline of key LLM releases from 2021 to 2024.\\xa0\\nIn this article, we home in on the last six months with a recap of ten notable LLM releases that have moved the needle in terms of language AI.\\nGPT-4o from OpenAI — May 2024\\nOpenAI describes the newest version of its large language model as a “step towards much more natural human-computer interaction.”\\nGPT-4o accepts and generates any combination of text, audio, image, and video. Its response time to audio inputs — 232 milliseconds, reportedly —\\xa0 is relevant for applications that need low latency like live captioning and real-time speech-to-speech. Furthermore, it is already being integrated as the LLM of choice in translation management platforms like Phrase.\\nOpenAI also cites a range of new and improved capabilities, such as the interpretation of emotions through facial expressions. See Slator’s coverage of the original release here.\\nLlaMA 3 from Meta — April 2024\\nYou have likely already encountered Meta’s Llama 3 in the wild. The model — described by Meta as “the most capable openly available LLM to date” — is behind the Meta AI assistant that is now embedded in Facebook, Instagram, and WhatsApp.\\nLlama 3 is optimized for dialogue use cases (i.e., AI assistants) and is English-centric. Meta calls non-English use cases “out-of-scope” but does, however, allow for fine-tuning on languages beyond English if this refinement falls within the terms of its license agreement.\\nA research paper for Lllama 3 has been promised for “the coming months”.\\nGemma from Google — February 2024\\nGemma is — compared to its predecessor, Google Gemini— quite little. The model’s weights are available in two sizes: 2B and 7B.\\xa0\\nLightweight models are faster, cheaper, and easier to use. Google hopes this, along with making Gemma open, will encourage researchers and developers to build more on the company’s AI models.\\nGemma shares technical and infrastructure components with Gemini and Google states that Gemma surpasses significantly larger models on key benchmarks. Potential applications include “conversational translators” and “multilingual writing assistants.”\\nSlator 2024 Language Industry Market Report — Language AI Edition\\nGemini v.1.5 from Google — February 2024\\nGemini is Google’s “largest and most capable AI model widely available today.” Originally launched in December 2023, Gemini was Google’s attempt to regain the upper hand in AI, a year on from the launch of ChatGPT by Microsoft-backed OpenAI.\\nThe original release claimed state-of-the-art performance on a range of multimodal benchmarks,” including automatic speech recognition (ASR) and automatic speech translation.\\nThe 1.5 version offers a broader context window (useful, for example, for achieving more contextually relevant machine translation) and has “enhanced performance” on tasks such as analyzing, classifying and summarizing large amounts of content.\\nAya from Cohere — February 2024\\nA number of specialized AI startups have moved into the LLM development space alongside OpenAI and big tech. These include AI labs such as Anthropic, Stability.AI, and Mistral AI, as well as AI platform Cohere.\\xa0\\nCohere’s Aya model aims to extend AI capabilities beyond English to achieve massive multilinguality. The model is an open-source, massively multilingual LLM covering 101 different languages.\\xa0\\nMarzieh Fadaee, Senior Research Scientist at Cohere, told SlatorCon Remote in March 2024, that, with Aya, Cohere has also created one of the largest datasets for instruction fine-tuning of multilingual models. It is, Fadaee said, a resource that is “particularly valuable for languages with limited representation.”\\nCroissantLLM from Unbabel — February 2024\\nCroissantLLM is a French-English LLM from CentraleSupélec, Carnegie Mellon University, and Unbabel.\\xa0\\nThe open-source model was developed to address the lack of models where English is not the dominant training language. “Our end goal is to have a model less skewed towards English performance or cultural biases,” the researchers said.\\xa0\\nCroissantLLM is designed to be very lightweight, with the goal of encouraging widespread adoption and a reduction of cost and deployment challenges.\\xa0\\nMixtral 8x7B from Mistral AI — December 2023\\nAnother open-source model, Mixtral 8x7B handles English, French, Italian, German, and Spanish.\\xa0\\nOn release, Mistral reported that Mixtral 8x7B was the “the best model overall regarding cost / performance trade-offs” and said it exceeded GPT3.5 on most benchmarks.\\nIn December 2023, researchers from ADAPT Centre used Mistral 7B (an earlier version) to show how fine-tuning can enhance the real-time, adaptive machine translation capabilities of a general-purpose LLM.\\nTranslatotron 3 from Google — December 2023\\nTranslation 3 is another step forward in the fast-moving field of direct speech-to-speech. Google calls it the “first fully unsupervised end-to-end model for direct speech-to-speech translation.”\\nThe third version of Translatotron improves on previous versions in a few ways, most notably in its unsupervised S2ST architecture.\\xa0\\nAccording to a post from Google researchers, “this method opens the door not only to translation between more language pairs but also towards translation of the non-textual speech attributes such as pauses, speaking rates, and speaker identity,”\\nThe system was also able to “learn” direct speech-to-speech translation from monolingual data alone.\\nSeamlessM4T v2 from Meta — November 2023\\nExemplifying the current trend toward more multimodal models, Seamless M4T is a suite of AI models for both speech and text translations. The model can convert across modes — speech into text, speech into speech, and text into speech — and across languages with text-to-text translations for up to 100 languages.\\nOn release, Meta Product Product Lead Jeff Wang said on X, “We just made speech translation a whole lot better!”\\nSeamlessExpressive from Meta — November 2023\\nSeamlessExpressive (a component of SeamlessM4T v2) offers speech-to-speech translation in English, Spanish, German, French, Italian, and Chinese. The original speaker’s pitch, pace and tone are retained in the translated speech.\\nA further, novel approach to speech mapping was put forward in early June 2024 by Meta AI researchers. A new model — SeamlessExpressiveLM — was instructed to work in sequence, first translating semantic content, and then transfering the speaker’s vocal style. This sequential approach was evaluated in Spanish and Hungarian into English speech translations, with measurable vocal style improvements.\\xa0\\nFor a more in-depth analysis of the changing AI model landscape in 2024 and its implications for players in the language industry, obtain a copy of Slator’s 2024 Language Industry Market Report — Language AI Edition.\\nHead of Research at Slator. Linguist and hiking enthusiast. Based in Madrid.\\nLanguage Industry Intelligence In Your Inbox. Every Friday\\nTo view this video please enable JavaScript, and consider upgrading to a web browser that supports HTML5 video\\n'},\n",
              "  {'url': 'https://github.blog/ai-and-ml/llms/',\n",
              "   'title': 'The latest on LLMs - The GitHub Blog',\n",
              "   'content': 'The latest on LLMs - The GitHub Blog How we use GitHub to be more productive, collaborative, and secureOur engineering and security teams do some incredible work. Let’s take a look at how we use GitHub to be more productive, build collaboratively, and shift security left. How we use GitHub to be more productive, collaborative, and secureOur engineering and security teams do some incredible work. Let’s take a look at how we use GitHub to be more productive, build collaboratively, and shift security left. Learn how we’re experimenting with generative AI models to extend GitHub Copilot across the developer lifecycle. A developer’s guide to open source LLMs and generative AI GitHub Meet the companies and engineering teams that build with GitHub. Learn more',\n",
              "   'score': 0.35716435,\n",
              "   'raw_content': \"Learn about artificial intelligence and machine learning across the GitHub ecosystem and the wider industry.\\nLearn how to build with generative AI.\\nChange how you work with GitHub Copilot.\\nEverything developers need to know about LLMs.\\nMachine learning tips, tricks, and best practices.\\nExplore the capabilities and benefits of AI code generation and how it can improve your developer experience.\\nResources for developers to grow in their skills and careers.\\nInsights and best practices for building apps.\\nTips & tricks to grow as a professional developer.\\nImprove how you use GitHub at work.\\nLearn how to move into your first professional role.\\nStay current on what’s new (or new again).\\nLearn how to start building, shipping, and maintaining software with GitHub.\\nGet an inside look at how we’re building the home for all developers.\\nDiscover how we deliver a performant and highly available experience across the GitHub platform.\\nExplore best practices for building software at scale with a majority remote team.\\nGet a glimpse at the technology underlying the world’s leading AI-powered developer platform.\\nLearn how we build security into everything we do across the developer lifecycle.\\nFind out what goes into making GitHub the home for all developers.\\nOur engineering and security teams do some incredible work. Let’s take a look at how we use GitHub to be more productive, build collaboratively, and shift security left.\\nExplore how to write, build, and deploy enterprise software at scale.\\nAutomating your way to faster and more secure ships.\\nGuides on continuous integration and delivery.\\nTips, tools, and tricks to improve developer collaboration.\\nDevOps resources for enterprise engineering teams.\\nHow to integrate security into the SDLC.\\nEnsuring your builds stay clean.\\nLearn how to bring AI to your engineering teams and maximize the value that you get from it.\\nKeep up with what’s new and notable from inside GitHub.\\nAn inside look at news and product updates from GitHub.\\nThe latest on GitHub’s platform, products, and tools.\\nInsights into the state of open source on GitHub.\\nThe latest policy and regulatory changes in software.\\nData-driven insights around the developer ecosystem.\\nOlder news and updates from GitHub.\\nLearn how to use retrieval-augmented generation (RAG) to capture more insights.\\nEverything open source on GitHub.\\nThe latest Git updates.\\nSpotlighting open source maintainers.\\nHow open source is driving positive change.\\nExplore open source games on GitHub.\\nOrganizations worldwide are incorporating open source methodologies into the way they build and ship their own software.\\nStay up to date on everything security.\\nApplication security, explained.\\nDemystifying supply chain security.\\nUpdates from the GitHub Security Lab.\\nHelpful tips on securing web applications.\\nLearn about core challenges in DevSecOps, and how you can start addressing them with AI and automation.\\nCategories\\nLearn about artificial intelligence and machine learning across the GitHub ecosystem and the wider industry.\\nLearn how to build with generative AI.\\nChange how you work with GitHub Copilot.\\nEverything developers need to know about LLMs.\\nMachine learning tips, tricks, and best practices.\\nExplore the capabilities and benefits of AI code generation and how it can improve your developer experience.\\nResources for developers to grow in their skills and careers.\\nInsights and best practices for building apps.\\nTips & tricks to grow as a professional developer.\\nImprove how you use GitHub at work.\\nLearn how to move into your first professional role.\\nStay current on what’s new (or new again).\\nLearn how to start building, shipping, and maintaining software with GitHub.\\nGet an inside look at how we’re building the home for all developers.\\nDiscover how we deliver a performant and highly available experience across the GitHub platform.\\nExplore best practices for building software at scale with a majority remote team.\\nGet a glimpse at the technology underlying the world’s leading AI-powered developer platform.\\nLearn how we build security into everything we do across the developer lifecycle.\\nFind out what goes into making GitHub the home for all developers.\\nOur engineering and security teams do some incredible work. Let’s take a look at how we use GitHub to be more productive, build collaboratively, and shift security left.\\nExplore how to write, build, and deploy enterprise software at scale.\\nAutomating your way to faster and more secure ships.\\nGuides on continuous integration and delivery.\\nTips, tools, and tricks to improve developer collaboration.\\nDevOps resources for enterprise engineering teams.\\nHow to integrate security into the SDLC.\\nEnsuring your builds stay clean.\\nLearn how to bring AI to your engineering teams and maximize the value that you get from it.\\nKeep up with what’s new and notable from inside GitHub.\\nAn inside look at news and product updates from GitHub.\\nThe latest on GitHub’s platform, products, and tools.\\nInsights into the state of open source on GitHub.\\nThe latest policy and regulatory changes in software.\\nData-driven insights around the developer ecosystem.\\nOlder news and updates from GitHub.\\nLearn how to use retrieval-augmented generation (RAG) to capture more insights.\\nEverything open source on GitHub.\\nThe latest Git updates.\\nSpotlighting open source maintainers.\\nHow open source is driving positive change.\\nExplore open source games on GitHub.\\nOrganizations worldwide are incorporating open source methodologies into the way they build and ship their own software.\\nStay up to date on everything security.\\nApplication security, explained.\\nDemystifying supply chain security.\\nUpdates from the GitHub Security Lab.\\nHelpful tips on securing web applications.\\nLearn about core challenges in DevSecOps, and how you can start addressing them with AI and automation.\\n\\n\\t\\t\\t\\t\\t\\tLLMs\\t\\t\\t\\t\\t\\nIn-depth articles and tutorials on leveraging LLMs, including natural language processing, code generation, and data analysis, with insights into training, fine-tuning, and deploying LLMs. Curious how to get started? Check out our guide on architecting LLM-powered applications.\\n\\n\\t\\tFeatured\\t\\n\\n\\n\\t\\t\\t\\tSo many tokens, so little time: Introducing a faster, more flexible byte-pair tokenizer\\t\\t\\t\\n\\nWe released a new open source byte-pair tokenizer that is faster and more flexible than popular alternatives.\\n\\n\\n\\t\\t\\t\\tUnlocking the power of unstructured data with RAG\\t\\t\\t\\n\\nUnstructured data holds valuable information about codebases, organizational best practices, and customer feedback. Here are some ways you can leverage it with RAG, or retrieval-augmented generation.\\n\\n\\n\\t\\t\\t\\tHow AI enhances static application security testing (SAST)\\t\\t\\t\\n\\nHere’s how SAST tools combine generative AI with code scanning to help you deliver features faster and keep vulnerabilities out of code. \\n\\n\\n\\t\\t\\t\\tCustomizing and fine-tuning LLMs: What you need to know\\t\\t\\t\\n\\nLearn how your organization can customize its LLM-based solution through retrieval augmented generation and fine-tuning.\\nWe do newsletters, too\\nDiscover tips, technical guides, and best practices in our biweekly newsletter just for devs.\\n\\n\\t\\t\\t\\tLatest\\t\\t\\t\\n\\n\\n\\t\\t\\t\\tHow we’re experimenting with LLMs to evolve GitHub Copilot\\t\\t\\t\\n\\nLearn how we’re experimenting with generative AI models to extend GitHub Copilot across the developer lifecycle.\\n\\n\\n\\t\\t\\t\\tThe architecture of today’s LLM applications\\t\\t\\t\\n\\nHere’s everything you need to know to build your first LLM app and problem spaces you can start exploring today.\\n\\n\\n\\t\\t\\t\\tDemystifying LLMs: How they can do things they weren’t trained to do\\t\\t\\t\\n\\nExplore how LLMs generate text, why they sometimes hallucinate information, and the ethical implications surrounding their incredible capabilities.\\n\\n\\n\\t\\t\\t\\tA developer’s guide to open source LLMs and generative AI\\t\\t\\t\\n\\nOpen source generative AI projects are a great way to build new AI-powered features and apps.\\n\\n\\t\\t\\tThe world's largest developer platform\\t\\t\\n\\n\\t\\t\\tDocs\\t\\t\\nEverything you need to master GitHub, all in one place.\\n\\n\\t\\t\\tGitHub\\t\\t\\nBuild what’s next on GitHub, the place for anyone from anywhere to build anything.\\n\\n\\t\\t\\tCustomer stories\\t\\t\\nMeet the companies and engineering teams that build with GitHub.\\n\\n\\t\\t\\tWork at GitHub!\\t\\t\\nCheck out our current job openings.\\nProduct\\nPlatform\\nSupport\\nCompany\\n\"}],\n",
              " 'response_time': 3.67}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_weather.invoke('Hyderabad')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_rGY_Tr2T6F",
        "outputId": "70808aa3-d943-42f8-af12-b99ad7e0e627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'location': {'name': 'Hyderabad',\n",
              "  'region': 'Telangana',\n",
              "  'country': 'India',\n",
              "  'lat': 17.3753,\n",
              "  'lon': 78.4744,\n",
              "  'tz_id': 'Asia/Kolkata',\n",
              "  'localtime_epoch': 1740080314,\n",
              "  'localtime': '2025-02-21 01:08'},\n",
              " 'current': {'last_updated_epoch': 1740079800,\n",
              "  'last_updated': '2025-02-21 01:00',\n",
              "  'temp_c': 22.1,\n",
              "  'temp_f': 71.8,\n",
              "  'is_day': 0,\n",
              "  'condition': {'text': 'Mist',\n",
              "   'icon': '//cdn.weatherapi.com/weather/64x64/night/143.png',\n",
              "   'code': 1030},\n",
              "  'wind_mph': 2.2,\n",
              "  'wind_kph': 3.6,\n",
              "  'wind_degree': 135,\n",
              "  'wind_dir': 'SE',\n",
              "  'pressure_mb': 1017.0,\n",
              "  'pressure_in': 30.03,\n",
              "  'precip_mm': 0.0,\n",
              "  'precip_in': 0.0,\n",
              "  'humidity': 100,\n",
              "  'cloud': 50,\n",
              "  'feelslike_c': 24.4,\n",
              "  'feelslike_f': 75.8,\n",
              "  'windchill_c': 24.1,\n",
              "  'windchill_f': 75.5,\n",
              "  'heatindex_c': 25.2,\n",
              "  'heatindex_f': 77.3,\n",
              "  'dewpoint_c': 12.1,\n",
              "  'dewpoint_f': 53.8,\n",
              "  'vis_km': 4.0,\n",
              "  'vis_miles': 2.0,\n",
              "  'uv': 0.0,\n",
              "  'gust_mph': 4.6,\n",
              "  'gust_kph': 7.3}}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "tools = [search_web, get_weather]\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
        "llm_with_tools = llm.bind_tools(tools)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucF0fYXm1zgq",
        "outputId": "f37b00dc-ab05-4916-82d5-80bb0619ae29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_with_tools.invoke('What is AI in 1 line')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_Z-6p5Q2pOC",
        "outputId": "e283bdb2-c302-4ada-a779-2b9e2958c9d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='AI, or artificial intelligence, is the ability of a computer to perform tasks that normally require human intelligence.\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-5b7abb29-029e-440d-ba87-49e0853cf42c-0', usage_metadata={'input_tokens': 65, 'output_tokens': 22, 'total_tokens': 87})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_with_tools.invoke('What are some of the latest LLMs released')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJtvE9Id4Rui",
        "outputId": "3b0d5318-4dff-4999-864d-423198651811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'name': 'search_web', 'arguments': '{\"query\": \"latest large language models released\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-9caca380-9768-483d-a19f-9c1eedbb0d51-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'latest large language models released'}, 'id': 'de89a4c8-e22a-402c-b66a-2ec4b1975228', 'type': 'tool_call'}], usage_metadata={'input_tokens': 67, 'output_tokens': 9, 'total_tokens': 76})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the Agentic Graph with LangGraph built-ins - Recommended Flow\n",
        "\n",
        "Here we will use LangGraph to build the full graph which will have the Agentic workflow.\n",
        "\n",
        "Each functionality will be implemented as we would in the real-world.\n",
        "\n",
        "Here we will be replacing our `BasicToolNode` for the prebuilt [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#toolnode),\n",
        "and our `route_tools` condition with the prebuilt [tools_condition](https://langchain-ai.github.io/langgraph/reference/prebuilt/#tools_condition)"
      ],
      "metadata": {
        "id": "JPcJl2m-vYMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "\n",
        "#from langchain_openai import ChatOpenAI\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.messages import BaseMessage\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "# define function which will be used to store all agent messages\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "# start the graph building\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "# add tools and bind to LLM\n",
        "tools = [search_web, get_weather]\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "# add the LLM to graph\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "# Add tools to a node\n",
        "tool_node = ToolNode(tools=tools)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "# add conditional edges\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        "    ['tools', '__end__']\n",
        ")\n",
        "\n",
        "# Any time a tool is called, we return to the chatbot to decide the next step\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "# Define entry point to the graph\n",
        "graph_builder.set_entry_point(\"chatbot\")\n",
        "# compile the graph\n",
        "agent = graph_builder.compile()"
      ],
      "metadata": {
        "id": "akYszF3ivXiS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b38199ef-c7e7-4daf-f9ad-8056ffff5541"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display, Markdown\n",
        "\n",
        "display(Image(agent.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "rdBRSLehwMm_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "a21b2d48-9755-4d52-b983-90a5901d5232"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"What are some of the latest LLMs released?\"\"\"\n",
        "response = agent.invoke({\"messages\": (\"user\", prompt)})\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dabt40FjQXyK",
        "outputId": "e5a3f174-e76e-430a-8603-a880665e36d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='What are some of the latest LLMs released?', additional_kwargs={}, response_metadata={}, id='71f92e9e-9951-43b1-bf53-b40e27a72097'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'search_web', 'arguments': '{\"query\": \"latest large language models released\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-c3eae8fe-0608-4449-b331-72232036d867-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'latest large language models released'}, 'id': '6248d479-40b6-49d9-a65e-123ccf1e478f', 'type': 'tool_call'}], usage_metadata={'input_tokens': 68, 'output_tokens': 9, 'total_tokens': 77}),\n",
              "  ToolMessage(content='{\"query\": \"latest large language models released\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://explodingtopics.com/blog/list-of-llms\", \"title\": \"Best 22 Large Language Models (LLMs) (February 2025)\", \"content\": \"Inflection-2.5 is the latest large language model (LLM) developed by Inflection AI to power its conversational AI assistant, Pi. Significant upgrades have been made, as the model currently achieves over 94% of GPT-4’s average performance while only having 40% of the training FLOPs. In March 2024, the Microsoft-backed startup reached 1+ million daily active users on Pi. 13. Gemma is a series of lightweight open-source language models developed and released by Google DeepMind. Pythia is a series of 16 large language models developed and released by EleutherAI, a non-profit AI research lab. Alpaca is a 7 billion-parameter language model developed by a Stanford research team and fine-tuned from Meta\\'s LLaMA 7B model.\", \"score\": 0.9220975, \"raw_content\": \"Best 22 Large Language Models (LLMs) (February 2025)\\\\n\\\\n\\\\n\\\\nAbout\\\\nNewsletter\\\\nBlog\\\\n\\\\n\\\\nBest 22 Large Language Models (LLMs) (February 2025)\\\\n\\\\nby Anthony Cardillo\\\\nFebruary 7, 2025\\\\nLarge language models are pre-trained on large datasets and use natural language processing to perform linguistic tasks such as text generation, code completion, paraphrasing, and more.\\\\nThe initial release of ChatGPT sparked the rapid adoption of generative AI, which has led to large language model innovations and industry growth.\\\\nIn fact, 92% of Fortune 500 firms have started using generative AI in their workflows.\\\\nAs adoption continues to grow, so does the LLM industry. The global large language model market is projected to grow from $6.5 billion in 2024 to $140.8 billion by 2033.\\\\nWith that, here is a list of the top 21 LLMs available in September 2024.\\\\nLLM NameDeveloperRelease DateAccessParametersDeepSeek R1DeepSeekJanuary 20, 2025Open-Source671 billionGPT-4oOpenAIMay 13, 2024APIUnknownClaude 3.5AnthropicJune 20, 2024APIUnknownGrok-1xAINovember 4, 2023Open-Source314 billionMistral 7BMistral AISeptember 27, 2023Open-Source7.3 billionPaLM 2GoogleMay 10, 2023Open-Source340 billionFalcon 180BTechnology Innovation InstituteSeptember 6, 2023Open-Source180 billionStable LM 2Stability AIJanuary 19, 2024Open-Source1.6 billion, 12 billionGemini 1.5Google DeepMindFebruary 2nd, 2024APIUnknownLlama 3.1Meta AIJune 23, 2024Open-Source405 billionMixtral 8x22BMistral AIApril 10, 2024Open-Source141 billionInflection-2.5Inflection AIMarch 10, 2024ProprietaryUnknownJambaAI21 LabsMarch 29, 2024Open-Source52 billionCommand RCohereMarch 11, 2024Both35 billionGemmaGoogle DeepMindFebruary 21, 2024Open-Source2 billion, 7 billionPhi-3MicrosoftApril 23, 2024Both3.8 billionXGen-7BSalesforceJuly 3, 2023Open-Source7 billionDBRXDatabricks\\' Mosaic MLMarch 27, 2024Open-Source132 billionPythiaEleutherAIFebruary 13, 2023Open-Source70 million to 12 billionSoraOpenAIFebruary 15, 2024 (announced)APIUnknownAlpaca 7BStanford CRFMMarch 13, 2023Open-Source7 billionNemotron-4NvidiaJune 14, 2024Open-Source340 billion\\\\n1. DeepSeek R1\\\\n\\\\nDeveloper: DeepSeek\\\\nRelease date: January 2025\\\\nNumber of Parameters: 671B total, 37B active\\\\nWhat is it? DeepSeek R1 is a reasoning model that excels in math and coding. It beats or matches OpenAI o1 in several benchmarks, including MATH-500 and AIME 2024.\\\\nOn its release, DeepSeek immediately hit headlines due to the low cost of training compared to most major LLMs.\\\\nDeepSeek R1 is free to use and open-source. It\\'s accessible via the API, the DeepSeek website, and mobile apps.\\\\n2. GPT-4o\\\\n\\\\nDeveloper: OpenAI\\\\nRelease date: May 13, 2024\\\\nNumber of Parameters: Unknown\\\\nWhat is it? GPT-4o is the latest and most advanced OpenAI language model, succeeding GPT-4, GPT-3.5, and GPT-3. OpenAI claims that GPT-4o is 50% cheaper than GPT-4 despite being 2x faster at generating tokens. This multimodal model includes text, image, video, and voice capabilities packaged into one.\\\\nGPT-4o\\'s biggest upgrade is the Voice-to-Voice function, which will improve input response times to an average of 320 milliseconds (compared to a few seconds with GPT-4). This feature is expected to be released in the coming weeks.\\\\n3. Claude 3.5\\\\n\\\\nDeveloper: Anthropic\\\\nRelease date: March 14, 2024\\\\nNumber of Parameters: Unknown\\\\nWhat is it?\\xa0As a new upgrade from the highly rated\\xa0Claude 3, Claude 3.5 Sonnet is the first release of the new Claude 3.5 model family. Similar to Claude 3, it\\'ll also include the Haiku and Opus models. As debatably the biggest competitor to GPT-4 and ChatGPT, Claude made even bigger improvements to this model by maintaining the 200,000 token context window at a lower cost. This is much larger than GPT-4\\'s 32,000 token capabilities.\\\\nAccording to Anthropic\\'s report, Claude 3.5 Sonnet outperformed GPT-4o in major benchmarks like coding and text reasoning. Plus, this is Claude\\'s most advanced vision model, with the ability to transcribe text from images or generate insights from charts.\\\\nAmazon has invested over $4 billion in Anthropic, bringing the startup\\'s valuation to $15 billion. The Claude mobile app was also released in May 2024.\\\\n4. Grok-1\\\\n\\\\nDeveloper: xAI\\\\nRelease date: November 4, 2023\\\\nNumber of Parameters: 314 billion\\\\nWhat is it? Created by Elon Musk\\'s artificial intelligence startup xAI, Grok-1 is currently the largest open-source LLM released to date at 314 billion parameters. Grok directly integrates with X (Twitter), and users must pay for an X Premium+ subscription to gain access.\\\\nBecause of the model’s size, Grok has a mixture-of-experts (MoE) architecture that only uses 25% of its weights for any given input token to maximize calculation efficiency.\\\\nIn August 2024, both Grok-2 and Grok-2 mini were released to X users in beta. According to xAI\\'s reports, Grok-2 outperforms GPT-4o in numerous categories, such as GPQA, MMLU-Pro, and DocVQA.\\\\n5. Mistral 7B\\\\n\\\\nDeveloper: Mistral AI\\\\nRelease date: September 27, 2023\\\\nNumber of Parameters: 7.3 billion\\\\nWhat is it? Mistral 7B is an open-source language model with 32 layers, 32 attention heads, and eight key-value heads. Despite running with fewer parameters, they outperformed the Llama 2 family of models in nearly all metrics, including MMLU, reading comprehension, math, coding, etc.\\\\nMistral 7B is released under an Apache 2.0 license. Customers are free to download it locally, deploy it on the cloud, or run it on HuggingFace. The Paris-based startup is close to securing a new $600 million funding round that would value the company at $6 billion.\\\\n6. PaLM 2\\\\n\\\\nDeveloper: Google\\\\nRelease date: May 10, 2023\\\\nNumber of Parameters: 340 billion\\\\nWhat is it? PaLM 2 is an advanced large language model developed by Google. As the successor to the original Pathways Language Model (PaLM), it’s trained on 3.6 trillion tokens (compared to 780 billion) and 340 billion parameters (compared to 540 billion). PaLM 2 was originally used to power Google\\'s first generative AI chatbot, Bard (rebranded to Gemini in February 2024).\\\\n7. Falcon 180B\\\\n\\\\nDeveloper: Technology Innovation Institute (TII)\\\\nRelease date: September 6, 2023\\\\nNumber of Parameters: 180 billion\\\\nWhat is it? Developed and funded by the Technology Innovation Institute, Falcon 180B is an upgraded version of the earlier Falcon 40B LLM. It has 180 billion parameters, which is 4.5 times larger than the 40 billion parameters of Falcon 40B.\\\\nIn addition to Falcon 40B, it also outperforms other large language models like GPT-3.5 and LLaMA 2 on tasks such as reasoning, question answering, and coding. In February 2024, the UAE-based Technology Innovation Institute (TII) committed $300 million in funding to the Falcon Foundation.\\\\n8. Stable LM 2\\\\n\\\\nDeveloper: Stability AI\\\\nRelease date: January 19, 2024\\\\nNumber of Parameters: 1.6 billion and 12 billion\\\\nWhat is it? Stability AI, the creators of the Stable Diffusion text-to-image model, are the developers behind Stable LM 2. This series of large language models includes Stable LM 2 12B (12 billion parameters) and Stable LM 2 1.6B (1.6 billion parameters). Released in April 2024, the larger 12B model outperforms models like LLaMA 2 70B on key benchmarks despite being much smaller.\\\\n9. Gemini 1.5\\\\n\\\\nDeveloper: Google DeepMind\\\\nRelease date: February 2nd, 2024\\\\nNumber of Parameters: Unknown\\\\nWhat is it? Gemini 1.5 is Google\\'s next-generation large language model, offering a significant upgrade over its predecessor, Gemini 1.0. While it’s only available for early testing, Gemini 1.5 Pro provides a one million-token context window (1 hour of video, 700,000 words, or 30,000 lines of code), the largest to date compared to alternative LLMs and chatbots. This upgrade is 35 times larger than Gemini 1.0 Pro and surpasses the previous largest record of 200,000 tokens held by Anthropic’s Claude 2.1.\\\\n10. Llama 3.1\\\\n\\\\nDeveloper: Meta AI\\\\nRelease date: June 23, 2024\\\\nNumber of Parameters: 405 billion\\\\nWhat is it? Llama 3, the predecessor to Llama 3.1, was available in both 70B and 8B versions that outperformed other open-source models like Mistral 7B and Google\\'s Gemma 7B on MMLU, reasoning, coding, and math benchmarks. Now, users will notice major upgrades to the latest version, including 405 billion parameters and an expended context length of 128,000.\\\\nUsers will also notice more accuracy because of the impressive knowledge base, which has been trained on over 15 trillion tokens. Plus, Meta added eight additional languages for this model. The increased size of this model makes it the largest open-source model released to date.\\\\nCustomers can still access its predecessor, Llama 2, which is available in three versions: 7 billion, 13 billion, and 70 billion parameters.\\\\n11. Mixtral 8x22B\\\\n\\\\nDeveloper: Mistral AI\\\\nRelease date: April 10, 2024\\\\nNumber of Parameters: 141 billion\\\\nWhat is it? Mixtral 8x22B is Mistral AI\\'s latest and most advanced large language model. This sparse Mixture-of-Experts (SMoE) model has 141 billion total parameters but only uses 39B active parameters to focus on improving the model’s performance-to-cost ratio.\\\\nThe startup also recently released Mistral Large, a ChatGPT alternative that ranks second behind GPT-4 among API-based LLMs.\\\\n12. Inflection-2.5\\\\n\\\\nDeveloper: Inflection AI\\\\nRelease date: March 10, 2024\\\\nNumber of Parameters: Unknown\\\\nWhat is it? Inflection-2.5 is the latest large language model (LLM) developed by Inflection AI to power its conversational AI assistant, Pi. Significant upgrades have been made, as the model currently achieves over 94% of GPT-4’s average performance while only having 40% of the training FLOPs. In March 2024, the Microsoft-backed startup reached 1+ million daily active users on Pi.\\\\n13. Jamba\\\\n\\\\nDeveloper: AI21 Labs\\\\nRelease date: March 29, 2024\\\\nNumber of Parameters: 52 billion\\\\nWhat is it? AI21 Labs created Jamba, the world\\'s first production-grade Mamba-style large language model. It integrates SSM technology with elements of a traditional transformer model to create a hybrid architecture. The model is efficient and highly scalable, with a context window of 256K and deployment support of 140K context on a single GPU.\\\\n14. Command R\\\\n\\\\nDeveloper: Cohere\\\\nRelease date: March 11, 2024\\\\nNumber of Parameters: 35 billion\\\\nWhat is it? Command R is a series of scalable LLMs from Cohere that support ten languages and 128,000-token context length (around 100 pages of text). This model primarily excels at retrieval-augmented generation, code-related tasks like explanations or rewrites, and reasoning. In April 2024, Command R+ was released to support larger workloads and provide real-world enterprise support.\\\\n15. Gemma\\\\n\\\\nDeveloper: Google DeepMind\\\\nRelease date: February 21, 2024\\\\nNumber of Parameters: 2 billion and 7 billion\\\\nWhat is it? Gemma is a series of lightweight open-source language models developed and released by Google DeepMind. The Gemma models are built with similar tech to the Gemini models, but Gemma is limited to text inputs and outputs only. The models have a context window of 8,000 tokens and are available in 2 billion and 7 billion parameter sizes.\\\\n16. Phi-3\\\\n\\\\nDeveloper: Microsoft\\\\nRelease date: April 23, 2024\\\\nNumber of Parameters: 3.8 billion\\\\nWhat is it? Classified as a small language model (SLM), Phi-3 is Microsoft\\'s latest release with 3.8 billion parameters. Despite the smaller size, it\\'s been trained on 3.3 trillion tokens of data to compete with Mistral 8x7B and GPT-3.5 performance on MT-bench and MMLU benchmarks.\\\\nTo date, Phi-3-mini is the only model available. However, Microsoft plans to release the Phi-3-small and Phi-3-medium models later this year.\\\\n17. XGen-7B\\\\n\\\\nDeveloper: Salesforce\\\\nRelease date: July 3, 2023\\\\nNumber of Parameters: 7 billion\\\\nWhat is it? XGen-7B is a large language model from Salesforce with 7 billion parameters and an 8k context window. The model was trained on 1.37 trillion tokens from various sources, such as RedPajama, Wikipedia, and Salesforce\\'s own Starcoder dataset.\\\\nSalesforce has released two open-source versions, a 4,000 and 8,000 token context window base, hosted under an Apache 2.0 license.\\\\n18. DBRX\\\\n\\\\nDeveloper: Databricks\\' Mosaic ML\\\\nRelease date: March 27, 2024\\\\nNumber of Parameters: 132 billion\\\\nWhat is it? DBRX is an open-source LLM built by Databricks and the Mosaic ML research team. The mixture-of-experts architecture has 36 billion (of 132 billion total) active parameters on an input. DBRX has 16 experts and chooses 4 of them during inference, providing 65 times more expert combinations compared to similar models like Mixtral and Grok-1\\\\n19. Pythia\\\\n\\\\nDeveloper: EleutherAI\\\\nRelease date: February 13, 2023\\\\nNumber of Parameters: 70 million to 12 billion\\\\nWhat is it? Pythia is a series of 16 large language models developed and released by EleutherAI, a non-profit AI research lab. There are eight different model sizes: 70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B. Because of Pythia\\'s open-source license, these LLMs serve as a base model for fine-tuned, instruction-following LLMs like Dolly 2.0 by Databricks.\\\\n20. Sora\\\\n\\\\nDeveloper: OpenAI\\\\nRelease date: February 15, 2024 (announced)\\\\nNumber of Parameters: Unknown\\\\nWhat is it? OpenAI\\'s latest development is Sora, a text-to-video model that combines LLMs and generative AI to turn text prompts into realistic videos up to 60 seconds long. The model uses a transformer architecture that operates on \\\\\"spacetime patches\\\\\" of video and image data rather than text tokens like other LLMs. No official release date for Sora has been announced, but OpenAI expects it to open to the public in late 2024.\\\\n21. Alpaca 7B\\\\n\\\\nDeveloper: Stanford CRFM\\\\nRelease date: March 27, 2024\\\\nNumber of Parameters: 7 billion\\\\nWhat is it? Alpaca is a 7 billion-parameter language model developed by a Stanford research team and fine-tuned from Meta\\'s LLaMA 7B model. Users will notice that although being much smaller, Alpaca performs similarly to text-DaVinci-003 (ChatGPT 3.5). However, Alpaca 7B is available for research purposes, and no commercial licenses are available.\\\\n22. Nemotron-4 340B\\\\n\\\\nDeveloper: NVIDIA\\\\nRelease date: June 14, 2024\\\\nNumber of Parameters: 340 billion\\\\nWhat is it? Nemotron-4 340B\\xa0is a family of large language models for synthetic data generation and AI model training. These models help businesses create new LLMs without larger and more expensive datasets. Instead, Nemotron-4 can create high-quality synthetic data to train other AI models, which reduces the need for extensive human-annotated data.\\\\nThe model family includes Nemotron-4-340B-Base (foundation model), Nemotron-4-340B-Instruct (fine-tuned chatbot), and Nemotron-4-340B-Reward (quality assessment and preference ranking). Due to the 9 trillion tokens used in training, which includes English, multilingual, and coding language data, Nemotron-4 matches GPT-4\\'s high-quality synthetic data generation capabilities.\\\\nConclusion\\\\nThe landscape of large language models is rapidly evolving, with new breakthroughs and innovations emerging at an unprecedented pace.\\\\nFrom compact models like Phi-2 and Alpaca 7B to cutting-edge architectures like Jamba and DBRX, the field of LLMs is pushing the boundaries of what\\'s possible in natural language processing (NLP).\\\\nWe will keep this list regularly updated with new models. If you liked learning about these LLMs, check out our lists of generative AI startups and AI startups.\\\\nFind Thousands of Trending Topics With Our Platform\\\\nTry Exploding Topics Pro\\\\n\\\\nExploding Topics\\\\n\\\\nJoin Pro\\\\nNewsletter\\\\nTrending Topics\\\\nAdd a Topic\\\\nCustomer Login\\\\n\\\\nCompany\\\\n\\\\nAbout Us\\\\nContact\\\\nMethodology\\\\nCookie Settings\\\\n\\\\nFree Tools\\\\n\\\\nKeyword Research\\\\nBacklink Checker\\\\nSERP Checker\\\\nKeyword Rank Checker\\\\nFree SEO Tools\\\\n\\\\nConnect\\\\n\\\\nYouTube\\\\nInstagram\\\\nX (Twitter)\\\\n\\\\nResources\\\\n\\\\nBlog\\\\nMarketing Academy\\\\nFree Webinars\\\\n\\\\n\\\\n\\\\n© 2025 \\xa0Exploding Topics is a Trademark of Semrush Inc\\\\n\\\\nPrivacy Policy\\\\nTerms of Service\\\\n\"}, {\"url\": \"https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models\", \"title\": \"25 of the best large language models in 2025 - TechTarget\", \"content\": \"Large language models are the dynamite behind the\\xa0generative AI\\xa0boom. Some of the most well-known language models today are based on the transformer model, including the\\xa0generative pre-trained transformer series\\xa0of LLMs and bidirectional encoder representations from transformers (BERT). Gemma\\xa0is a family of open-source language models from Google that were trained on the same resources as Gemini. GPT-3\\xa0is OpenAI\\'s large language model with more than 175 billion parameters, released in 2020. Large Language Model Meta AI (Llama) is Meta\\'s LLM which was first released in 2023. The\\xa0Pathways Language Model\\xa0is a 540 billion parameter transformer-based model from Google powering its AI chatbot\\xa0Bard. StableLM is a series of open language models developed by Stability AI, the company behind image generator Stable Diffusion.\", \"score\": 0.83549726, \"raw_content\": \"25 of the best large language models in 2025\\\\nWhatIs\\\\nSearch the TechTarget Network \\\\nBrowse Definitions :\\\\n\\\\nA\\\\nB\\\\nC\\\\nD\\\\nE\\\\nF\\\\nG\\\\nH\\\\nI\\\\nJ\\\\nK\\\\nL\\\\nM\\\\nN\\\\nO\\\\nP\\\\nQ\\\\nR\\\\nS\\\\nT\\\\nU\\\\nV\\\\nW\\\\nX\\\\nY\\\\nZ\\\\n#\\\\n\\\\nLogin Register\\\\n\\\\nTechTarget Network\\\\nTech Accelerator\\\\nNews\\\\n2024 IT Salary Survey Results\\\\n\\\\nRSS\\\\n\\\\n\\\\nWhatIs\\\\n\\\\n\\\\nBrowse Definitions Data analytics and AI\\\\nTopics View All\\\\n\\\\nBusiness software\\\\nCloud computing\\\\nComputer science\\\\nData centers\\\\nIT management\\\\nNetworking\\\\nSecurity\\\\nSoftware development\\\\n\\\\nPlease select a category\\\\n\\\\nTopics\\\\n\\\\n\\\\n\\\\nBrowse Features Resources\\\\n\\\\nBusiness strategies\\\\nCareer resources\\\\nEmerging tech\\\\nTech explainers\\\\n\\\\n\\\\n\\\\nFollow:\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nHome\\\\n\\\\nData analytics and AI\\\\n\\\\nTech Accelerator What is Gen AI? Generative AI explained\\\\nPrev Next Will AI replace jobs? 17 job types that might be affected Pros and cons of AI-generated content\\\\nDownload this guide1\\\\nFeature\\\\n25 of the best large language models in 2025\\\\nLarge language models have been affecting search for years and have been brought to the forefront by ChatGPT and other chatbots.\\\\n\\\\nShare this item with your network:\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nBy\\\\n\\\\nSean Michael Kerner\\\\nBen Lutkevich, Site Editor\\\\n\\\\nPublished: 31 Jan 2025\\\\nLarge language models are the dynamite behind the\\xa0generative AI\\xa0boom. However, they\\'ve been around for a while.\\\\nLLMs\\xa0are black box AI systems that use deep learning on extremely large datasets to understand and generate new text. Modern LLMs began taking shape in 2014 when the attention mechanism -- a machine learning technique designed to mimic human cognitive attention -- was introduced in a\\xa0research paper\\xa0titled \\\\\"Neural Machine Translation by Jointly Learning to Align and Translate.\\\\\" In 2017, that attention mechanism was honed with the introduction of the transformer model in another\\xa0paper, \\\\\"Attention Is All You Need.\\\\\"\\\\nSome of the most well-known language models today are based on the transformer model, including the\\xa0generative pre-trained transformer series\\xa0of LLMs and bidirectional encoder representations from transformers (BERT).\\\\nChatGPT, which runs on a set of language models from OpenAI, attracted more than 100 million users just two months after its release in 2022. Since then, many competing models have been released. Some belong to big companies such as Google, Amazon and Microsoft; others are open source.\\\\nConstant developments in the field can be difficult to keep track of. Here are some of the most influential models, both past and present. Included in it are models that paved the way for today\\'s leaders as well as those that could have a significant effect in the future.\\\\nThis article is part of\\\\nWhat is Gen AI? Generative AI explained\\\\n\\\\nWhich also includes:\\\\n8 top generative AI tool categories for 2025\\\\nWill AI replace jobs? 17 job types that might be affected\\\\n25 of the best large language models in 2025\\\\n\\\\nTop current LLMs\\\\nBelow are some of the most relevant large language models today. They do natural language processing and influence the architecture of future models.\\\\nBERT\\\\nBERT\\xa0is a family of LLMs that Google introduced in 2018. BERT is a\\xa0transformer-based\\xa0model that can convert sequences of data to other sequences of data. BERT\\'s architecture is a stack of transformer encoders and features 342 million parameters. BERT was pre-trained on a large corpus of data then fine-tuned to perform specific tasks along with natural language inference and sentence text similarity. It was used to improve query understanding in the 2019 iteration of Google search.\\\\nClaude\\\\nThe\\xa0Claude LLM\\xa0focuses on constitutional AI, which shapes AI outputs guided by a set of principles that help the AI assistant it powers helpful, harmless and accurate. Claude was created by the company Anthropic.\\\\nThere are three primary branches of Claude -- Opus, Haiku and Sonnet. The latest iteration of the Claude LLM is the Claude 3.5 Sonnet. It understands nuance, humor and complex instructions better than earlier versions of the LLM. It also has broad programming capabilities that make it well-suited for application development. In October 2024, Claude added a computer-use AI tool, that enables the LLM to use a computer like a human does. It\\'s available via Claude.ai, the Claude iOS app and through an API.\\\\nCohere\\\\nCohere is an enterprise AI platform that provides several LLMs including Command, Rerank and Embed. These\\xa0LLMs can be custom-trained\\xa0and fine-tuned to a specific company\\'s use case. The company that created the Cohere LLM was founded by one of the authors of Attention Is All You Need.\\\\nDeepSeek-R1\\\\nDeepSeek-R1 is an open-source reasoning model for tasks with complex reasoning, mathematical problem-solving and logical inference. The model uses reinforcement learning techniques to refine its reasoning ability and solve complex problems. DeepSeek-R1 can perform critical problem-solving through self-verification, chain-of-thought reasoning and reflection.\\\\nErnie\\\\nErnie is Baidu\\'s large language model which powers the Ernie 4.0 chatbot. The bot was released in August 2023 and has garnered more than 45 million users. Ernie is rumored to have 10 trillion parameters. The bot works best in Mandarin but is capable in other languages.\\\\nFalcon\\\\nFalcon is a family of transformer-based models developed by the Technology Innovation Institute. It is open source and has multi-lingual capabilities. Falcon 2 is available in an 11 billion parameter version that provide multimodal capabilities for both text and vision.\\\\nThe Falcon 1 series includes a pair of larger models with Falcon 40B and Falcon 180B. Falcon models are available on GitHub as well as on cloud provider including Amazon.\\\\nGemini\\\\nGemini\\xa0is Google\\'s family of LLMs that power the company\\'s chatbot of the same name. The model replaced Palm in powering the chatbot, which was rebranded from Bard to Gemini upon the model switch. Gemini models are multimodal, meaning they can handle images, audio and video as well as text. Gemini is also integrated in many Google applications and products. It comes in three sizes -- Ultra, Pro and Nano. Ultra is the largest and most capable model, Pro is the mid-tier model and Nano is the smallest model, designed for efficiency with on-device tasks.\\\\nAmong the most recent models is the Gemini 1.5 Pro update that debuted in May 2024 Gemini is available as a web chatbot, the Google Vertex AI service and via API. Early previews of Gemini 2.0 Flash became available in December 2024 with updated multimodal generation capabilities.\\\\nGemma\\\\nGemma\\xa0is a family of open-source language models from Google that were trained on the same resources as Gemini. Gemma 2 was released in June 2024 in two sizes -- a 9 billion parameter model and a 27 billion parameter model. Gemma models can be\\xa0run locally\\xa0on a personal computer, and are also available in Google Vertex AI.\\\\nGPT-3\\\\nGPT-3\\xa0is OpenAI\\'s large language model with more than 175 billion parameters, released in 2020. GPT-3 uses a decoder-only transformer architecture. In September 2022, Microsoft announced it had exclusive use of GPT-3\\'s underlying model. GPT-3 is 10 times larger than its predecessor. GPT-3\\'s training data includes Common Crawl, WebText2, Books1, Books2 and Wikipedia.\\\\nGPT-3 is the last of the GPT series of models in which OpenAI made the parameter counts publicly available. The GPT series was first introduced in 2018 with OpenAI\\'s paper \\\\\"Improving Language Understanding by Generative Pre-Training.\\\\\"\\\\nGPT-3.5\\\\nGPT-3.5 is an upgraded version of GPT-3 with fewer parameters. GPT-3.5 was fine-tuned using\\xa0reinforcement learning from human feedback. GPT-3.5 is the version of GPT that powers ChatGPT. There are several models, with GPT-3.5 turbo being the most capable, according to OpenAI. GPT-3.5\\'s training data extends to September 2021.\\\\nIt was also integrated into the Bing search engine but has since been replaced with GPT-4.\\\\nGPT-4\\\\nGPT-4\\xa0, was released in 2023 and like the others in the OpenAI GPT family, it\\'s a\\xa0transformer-based model. Unlike the others, its parameter count has not been released to the public, though there are rumors that the model has more than 170 trillion. OpenAI describes GPT-4 as a multimodal model, meaning it can\\xa0process and generate both language and images\\xa0as opposed to being limited to only language. GPT-4 also introduced a system message, which lets users specify tone of voice and task.\\\\nGPT-4 demonstrated human-level performance in multiple academic exams. At the model\\'s release, some speculated that GPT-4 came close to\\xa0artificial general intelligence, which means it is as smart or smarter than a human. That speculation turned out to be unfounded.\\\\nGPT-4o\\\\nGPT-4 Omni (GPT-4o) is OpenAI\\'s successor to GPT-4 and offers several improvements over the previous model. GPT-4o creates a more natural human interaction for ChatGPT and is a large multimodal model, accepting various inputs including audio, image and text. The conversations let users engage as they would in a normal human conversation, and the real-time interactivity can also pick up on emotions. GPT-4o can see photos or screens and ask questions about them during interaction.\\\\nGPT-4o can respond in 232 milliseconds, similar to human response time and faster than GPT-4 Turbo.\\\\nGranite\\\\nThe IBM Granite family of models are fully open source models under the Apache v.2 license. The first iteration of the open source model models debuted in May 2024, followed by Granite 3.0 in October and Granite 3.1 in December 2024.\\\\nThere are multiple variants in the Granite model family including General-purpose models (8B and 2B variants), guardrail model and Mixture-of-Experts models. While the model can be used for general purpose deployments, IBM itself is focusing deployment and optimization for enterprise use cases like customer service, IT automation and cybersecurity.\\\\nLamda\\\\nLamda (Language Model for Dialogue Applications) is a family of LLMs developed by Google Brain announced in 2021. Lamda used a decoder-only transformer language model and was pre-trained on a large corpus of text. In 2022, LaMDA gained widespread attention when then-Google engineer Blake Lemoine went public with claims that the\\xa0program was sentient. It was built on the Seq2Seq architecture.\\\\nLlama\\\\nLarge Language Model Meta AI (Llama) is Meta\\'s LLM which was first released in 2023. The Llama 3.1 models were released in July 2024, including both a 405 billion and 70 billion parameter model.\\\\nThe most recent version is Llama 3.2 which was released in September 2024, initially with smaller parameter counts of 11 billion and 90 billion.\\\\nLlama uses a transformer architecture and was trained on a variety of public data sources, including webpages from CommonCrawl, GitHub, Wikipedia and Project Gutenberg. Llama was effectively leaked and spawned many descendants, including Vicuna and Orca. Llama is available under an open license, allowing for free use of the models. Lllama models are available in many locations including llama.com and Hugging Face.\\\\nMistral\\\\nMistral is a family of a mixture of expert models from Mistral AI. Among the newest models is Mistral Large 2 which was first released in July 2024. The model operates with 123 billion parameters and a 128k context window, supporting dozens of languages including French, German, Spanish, Italian, and many others, along with more than 80 coding languages.\\\\nIn November 2024, Mistral released Pixtral Large, a 124-billion-parameter multimodal model that can handle text and visual data. Mistral models are available via Mistral\\'s API on its Le Platforme-managed web service.\\\\no1\\\\nThe OpenAI o1 model family was first introduced in Sept. 2024. The o1 model\\'s focus is to provide what OpenAI refers to as - reasoning models, that can reason through a problem or query before offering a response.\\\\nThe o1 models excel in STEM fields, with strong results in mathematical reasoning (scoring 83% on the International Mathematics Olympiad compared to GPT-4o\\'s 13%), code generation and scientific research tasks. While they offer enhanced reasoning and improved safety features, they operate more slowly than previous models due to their thorough reasoning processes and come with certain limitations, such as restricted access features and higher API costs. The models are available to ChatGPT Plus and Team users, with varying access levels for different user categories.\\\\no3\\\\nOpenAI introduced the successor model, o3, in December 2024. According to OpenAI, o3 is designed to handle tasks with more analytical thinking, problem-solving and complex reasoning and will improve o1\\'s capabilities and performance. The o3 model is in safety testing mode and is currently not available to the public.\\\\nOrca\\\\nOrca was developed by Microsoft and has 13 billion parameters, meaning it\\'s small enough to run on a laptop. It aims to improve on advancements made by other open source models by imitating the reasoning procedures achieved by LLMs. Orca achieves the same performance as GPT-4 with significantly fewer parameters and is on par with GPT-3.5 for many tasks. Orca is built on top of the 13 billion parameter version of Llama.\\\\nPalm\\\\nThe\\xa0Pathways Language Model\\xa0is a 540 billion parameter transformer-based model from Google powering its AI chatbot\\xa0Bard. It was trained across multiple\\xa0TPU\\xa04 Pods -- Google\\'s custom hardware for machine learning. Palm specializes in reasoning tasks such as coding, math, classification and question answering. Palm also excels at decomposing complex tasks into simpler subtasks.\\\\nPaLM gets its name from a Google research initiative to build Pathways, ultimately creating a single model that serves as a foundation for multiple use cases. There are\\xa0several fine-tuned versions\\xa0of Palm, including Med-Palm 2 for life sciences and medical information as well as Sec-Palm for cybersecurity deployments to speed up threat analysis.\\\\nPhi\\\\nPhi is a transformer-based language model from Microsoft. The Phi 3.5 models were first released in August 2024.\\\\nThe series includes Phi-3.5-mini-instruct (3.82 billion parameters), Phi-3.5-MoE-instruct (41.9 billion parameters), and Phi-3.5-vision-instruct (4.15 billion parameters), each designed for specific tasks ranging from basic reasoning to vision analysis. All three models support a 128k token context length.\\\\nReleased under a Microsoft-branded MIT License, they are available for developers to download, use, and modify without restrictions, including for commercial purposes.\\\\nQwen\\\\nQwen is large family of open models developed by Chinese internet giant Alibaba Cloud. The newest set of models are the Qwen2.5 suite, which support 29 different languages and currently scale up to 72 billion parameters. These models are suitable for a wide range of tasks, including code generation, structured data understanding, mathematical problem-solving as well as general language understanding and generation.\\\\nStableLM\\\\nStableLM is a series of open language models developed by Stability AI, the company behind image generator Stable Diffusion.\\\\nStableLM 2 debuted in January 2024 initially with a 1.6 billion parameter model. In April 2024 that was expanded to also include a 12 billion parameter model. StableLM 2 supports seven languages: English, Spanish, German, Italian, French, Portuguese, and Dutch. Stability AI positions these models as offering different options for various use cases, with the 1.6B model suitable for specific, narrow tasks and faster processing while the 12B model provides more capability but requires more computational resources.\\\\nTülu 3\\\\nAllen Institute for AI\\'s Tülu 3 is an open-source 405 billion-parameter LLM. The Tülu 3 405B model has post-training methods that combine supervised fine-tuning and reinforcement learning at a larger scale. Tülu 3 uses a \\\\\"reinforcement learning from verifiable rewards\\\\\" framework for fine-tuning tasks with verifiable outcomes -- such as solving mathematical problems and following instructions.\\\\nVicuna 33B\\\\nVicuna is another influential open source LLM derived from Llama. It was developed by LMSYS and was fine-tuned using data from sharegpt.com. It is smaller and less capable that GPT-4 according to several benchmarks, but does well for a model of its size. Vicuna has only 33 billion parameters, whereas GPT-4 has trillions.\\\\nLLM precursors\\\\nAlthough LLMs are a recent phenomenon, their precursors go back decades. Learn how recent precursor Seq2Seq and distant precursor ELIZA set the stage for modern LLMs.\\\\nSeq2Seq\\\\nSeq2Seq is a deep learning approach used for machine translation, image captioning and natural language processing. It was developed by Google and underlies some of their modern LLMs, including LaMDA. Seq2Seq also underlies AlexaTM 20B, Amazon\\'s large language model. It uses a mix of encoders and decoders.\\\\nEliza\\\\nEliza was an\\xa0early natural language processing program\\xa0created in 1966. It is one of the earliest examples of a language model. Eliza simulated conversation using pattern matching and substitution. Eliza, running a certain script, could parody the interaction between a patient and therapist by applying weights to certain keywords and responding to the user accordingly. The creator of Eliza, Joshua Weizenbaum, wrote a book on the limits of computation and artificial intelligence.\\\\nNext Steps\\\\nGenerative AI challenges that businesses should consider\\\\nGenerative AI ethics: Biggest concerns\\\\nGenerative AI landscape: Potential future trends\\\\nGenerative models: VAEs, GANs, diffusion, transformers, NeRFs\\\\nAI content generators to explore\\\\nRelated Resources\\\\n\\\\nFive data quality trends to prepare for in the year ahead –Video\\\\nThe Digital Transformation And Innovation Landscape –Wipro\\\\nCloudera and NVIDIA Accelerate AI in the Financial Services Industry –Cloudera\\\\nImprove customer satisfaction or cut costs? Who says you have to choose? –Video\\\\n\\\\nDig Deeper on Data analytics and AI\\\\n\\\\n ##### What is GPT-3? Everything you need to know  By: Nick Barney\\\\n ##### What is a small language model (SLM)?  By: Sean Kerner\\\\n ##### GPT-4  By: Ben Lutkevich\\\\n ##### What are large language models (LLMs)?  By: Sean Kerner\\\\n\\\\nSponsored News\\\\n\\\\nSustainability, AI and Dell PowerEdge Servers –Dell Technologies and Intel\\\\nThree Innovative AI Use Cases for Natural Language Processing –Dell Technologies\\\\nAutonomous coding: The future of the revenue cycle –Solventum\\\\n\\\\nRelated Content\\\\n\\\\nExploring GPT-3 architecture – Search Enterprise AI\\\\nWhat is GPT-3? Everything you need to know – Search Enterprise AI\\\\nMicrosoft exclusively licenses OpenAI\\'s GPT-3 ... – Search Enterprise AI\\\\n\\\\nLatest TechTarget resources\\\\n\\\\nNetworking\\\\nSecurity\\\\nCIO\\\\nHR Software\\\\nCustomer Experience\\\\n\\\\nSearch Networking\\\\n\\\\n\\\\nWhat is a thin client (lean client)?A thin client (lean client) is a virtual desktop computing model that runs on the resources stored on a central server instead of...\\\\n\\\\n\\\\nWhat is network monitoring?Network monitoring, also frequently called network management, is the practice of consistently overseeing a computer network for ...\\\\n\\\\n\\\\nWhat is network automation?Network automation is a process that uses intelligent software to automate the management, configuration, deployment, testing and...\\\\n\\\\n\\\\nSearch Security\\\\n\\\\n\\\\nWhat is Internet Key Exchange (IKE)?Internet Key Exchange (IKE) is a standard protocol used to set up a secure and authenticated communication channel between two ...\\\\n\\\\n\\\\nWhat is a certificate revocation list (CRL) and how is it used?A certificate revocation list (CRL) is a list of digital certificates that have been revoked by the issuing certificate authority...\\\\n\\\\n\\\\nWhat is cryptology?Cryptology is the mathematics, such as number theory and the application of formulas and algorithms, that underpin cryptography ...\\\\n\\\\n\\\\nSearch CIO\\\\n\\\\n\\\\nWhat is an IT project manager?An IT project manager is a professional charged with overseeing the process of planning, executing and delegating ...\\\\n\\\\n\\\\nWhat is a cyberthreat hunter (cybersecurity threat analyst)?A cyberthreat hunter, also called a cybersecurity threat analyst, proactively identifies security incidents that might go ...\\\\n\\\\n\\\\nWhat is blockchain? Definition, examples and how it worksBlockchain is a distributed ledger technology (DLT) that\\'s shared across a network of computers to keep a digital record of ...\\\\n\\\\n\\\\nSearch HRSoftware\\\\n\\\\n\\\\nWhat is employee self-service (ESS)?Employee self-service (ESS) is a widely used human resources technology that enables employees to perform many job-related ...\\\\n\\\\n\\\\nWhat is DEI? Diversity, equity and inclusion explainedDiversity, equity and inclusion is a term used to describe policies and programs that promote the representation and ...\\\\n\\\\n\\\\nWhat is payroll software?Payroll software automates the process of paying salaried, hourly and contingent employees.\\\\n\\\\n\\\\nSearch Customer Experience\\\\n\\\\n\\\\nWhat is account-based selling? Everything you need to knowAccount-based selling (ABS) is a strategic sales approach in business-to-business sales and marketing that centers around ...\\\\n\\\\n\\\\nWhat is interactive voice response (IVR)?Interactive voice response (IVR) is an automated telephony system that interacts with callers, gathers information and routes ...\\\\n\\\\n\\\\nWhat is an AI assistant?An AI assistant, or digital assistant, is software that uses artificial intelligence to understand natural language voice ...\\\\n\\\\n\\\\nBrowse by Topic\\\\n\\\\n\\\\nBrowse Resources\\\\n\\\\n\\\\nAbout Us\\\\n\\\\nMeet The Editors\\\\nEditorial Ethics Policy\\\\nContact Us\\\\nAdvertisers\\\\nBusiness Partners\\\\nEvents\\\\nMedia Kit\\\\nCorporate Site\\\\nReprints\\\\n\\\\nAll Rights Reserved, Copyright 1999 - 2025, TechTarget  \\\\nPrivacy Policy\\\\nCookie Preferences\\\\nCookie Preferences\\\\nDo Not Sell or Share My Personal Information\\\\nClose\\\\n\\\\nX\\\\nFree Download What is generative AI? Everything you need to know\\\\nThe potential of AI technology has been percolating in the background for years. But when ChatGPT, the AI chatbot, began grabbing headlines in early 2023, it put generative AI in the spotlight. This guide is your go-to manual for generative AI, covering its benefits, limits, use cases, prospects and much more.\\\\n\"}, {\"url\": \"https://hatchworks.com/blog/gen-ai/large-language-models-guide/\", \"title\": \"Large Language Models: What You Need to Know in 2025\", \"content\": \"Large language models (LLMs) are the unsung heroes of recent Generative AI advancements, quietly working behind the scenes to understand and generate language as we know it. OpenAI released GPT-4, an even more powerful and versatile model than its predecessors, with improvements in understanding, reasoning, and generating text across a broader range of contexts and languages. 2022: The emergence of GPT-4 and other advanced models such as Midjourney, continuing to push the boundaries of what’s possible with LLMs in terms of generating and understanding natural language across various domains and tasks, including image generation. These models are trained on massive data sets and can perform a broad range of tasks like generating text, translating languages, and more. Tags: AI, artificial intelligence, gen ai, Generative AI, large language models, LLMs\", \"score\": 0.77994305, \"raw_content\": \"Large Language Models: What You Need to Know in 2025 | HatchWorks AI\\\\nSkip to content\\\\n\\\\n\\\\nWhat We Do\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nServices\\\\n\\\\n\\\\nAI Strategy & Roadmap\\\\n\\\\nData Engineering & Analytics\\\\nAI-Powered Software Development\\\\n\\\\nAI Engineering Teams\\\\n        *   *   Accelerators\\\\n\\\\n\\\\nGenerative Driven Development™\\\\n\\\\nAI Roadmap & ROI Workshop\\\\nAI Solution Accelerator\\\\nRAG\\\\n\\\\nGenIQ\\\\n        *   *   Industries\\\\n\\\\n\\\\nCommunications and IoT\\\\n\\\\nTechnology\\\\nHealthcare\\\\nFinance\\\\n\\\\nRetail\\\\n        *   *   Partnerships\\\\n\\\\n\\\\nDatabricks\\\\n\\\\nIndustries\\\\nCommunications and IoT Solutions\\\\nTechnology\\\\nHealthcare\\\\nFinance\\\\nRetail\\\\n\\\\n\\\\nAbout Us\\\\nAbout Us\\\\nCareers & Culture\\\\nHatchFutures\\\\nFAQ\\\\n\\\\n\\\\n\\\\nResources\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nInsights\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nBlog\\\\n\\\\nTalking AI Podcast\\\\n\\\\nTalking AI Newsletter\\\\n        *   *   Tools & Reports\\\\n\\\\n\\\\nState of AI Report 2025\\\\n\\\\nTech Talent Report 2024\\\\nNearshore Budget Calculator\\\\n\\\\nBuild your Own GPT\\\\n        *   *   Learn & Connect\\\\n\\\\n\\\\nEvents\\\\n        *   *   Media\\\\n\\\\n\\\\nNewsroom\\\\n\\\\nOur Work\\\\nCareers\\\\nContact\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nCareers\\\\nContact us\\\\nLarge Language Models: What You Need to Know in 2025\\\\n\\\\nMelissa Malec\\\\n\\\\nDecember 2, 2024\\\\n\\\\n\\\\nUpdated: January 16, 2025\\\\n\\\\n\\\\nLarge language models (LLMs) are the unsung heroes of recent Generative AI advancements, quietly working behind the scenes to understand and generate language as we know it.\\\\nBut how do they work? What are they capable of? And what should we look out for when using them?\\\\n\\\\nRead on and find out in this guide for LLMs in 2024. Jump ahead:\\\\n\\\\nUnderstanding Large Language Models\\\\nWhat is a Large Language Model?\\\\nHow Do Large Language Models Work?\\\\nKey Milestones in Large Language Model Development\\\\nCapabilities of Large Language Models\\\\nChallenges and Limitations of LLMs\\\\nThe Future of Language Models: What Comes Next?\\\\n\\\\nUnderstanding Large Language Models\\\\nLet’s get the basics out of the way. Here we’ll define the large language model (LLM), explain how they work, and provide a timeline of key milestones in LLM development.\\\\nWhat is a Large Language Model?\\\\nA large language model, often abbreviated to LLM, is a type of artificial intelligence model designed to understand natural language as well as generate it at a large scale.\\\\nWhen we say human language, we don’t just mean English, Spanish, or Cantonese. Those are certainly part of what LLMs are trained on but human language, in this context, also extends to:\\\\n\\\\nArt\\\\nDance\\\\nMorse code\\\\nGenetic code\\\\nHieroglyphics\\\\nCryptography\\\\nSign language\\\\nBody language\\\\nMusical notation\\\\nChemical signaling\\\\nEmojis and symbols\\\\nAnimal communication\\\\nHaptic communications\\\\nTraffic signs and signals\\\\nMathematical equations\\\\nProgramming languages\\\\n\\\\nLLMs are trained on billions of parameters and have the ability to learn from a wide range of data sources.\\\\nThis extensive training enables them to predict and produce text based on the input they receive so that they can engage in conversations, answer queries, or even write code.\\\\nSome of the leading very large models include giants like GPT, LLaMa, LaMDA, PaLM 2, BERT, and ERNIE.\\\\nThey’re at the heart of various applications, aiding in everything from customer service chatbots to content creation and software development.\\\\nSome companies even build their own LLMs but that requires significant time, investment, and tech knowledge. It’s much easier to integrate a pre-trained LLM into your own systems.\\\\nHow Do Large Language Models Work?\\\\nLarge Language Models use a blend of neural networks and machine learning (ML). It’s this blend that allows the technology to first process and then generate original text and imagery.\\\\nThink of neural networks as the LLM’s brain. It’s these networks that learn from vast amounts of data, improving over time as they’re exposed to more.\\\\nAs the model is trained on more data, it learns patterns, structures, and the nuances of language. It’s like teaching it the rules of grammar, the rhythm of poetry, and the jargon of technical manuals all at once.\\\\nMachine learning models then help the model to predict the next word in a sentence based on the words that come before it. This is done countless times, refining the model’s ability to generate coherent and contextually relevant text.\\\\nLLMs now also operate on a Transformer Architecture. This architecture allows the model to look at and weigh the importance of different words in a sentence. It’s the same as when we read a sentence and look for context clues to understand its meaning.\\\\n⚠️ While LLMs can generate original content, the quality, relevance, and innovativeness of their output can vary and require human oversight and refinement.\\\\nThe originality is also influenced by how the prompts are structured, the model’s training data, and the specific capabilities of the LLM in question.\\\\nKey Milestones in Large Language Model Development\\\\nLarge language models haven’t always been as useful as they are today. They’ve developed and been iterated upon significantly over time.\\\\nLet’s look at some of those key moments in LLM history. That way you can appreciate how far they’ve come and the rapid evolution in the last few years compared to decades of slow progress.\\\\n1966\\\\nELIZA\\\\n\\\\nThe first chatbot created by Joseph Weizenbaum, simulating a psychotherapist in conversation.\\\\n2013\\\\nword2vec\\\\n\\\\nA groundbreaking tool developed by a team led by Tomas Mikolov at Google, introducing efficient methods for learning word embeddings from raw text.\\\\n2018\\\\nGPT and BERT\\\\n\\\\nGPT (Generative Pretrained Transformer): OpenAI introduced GPT, showcasing a powerful model for understanding and generating human-like text.\\\\nBERT (Bidirectional Encoder Representations from Transformers): Developed by Google, BERT significantly advanced the state of the art in natural language understanding tasks.\\\\n\\\\n2020\\\\nGPT 3\\\\n\\\\nOpenAI released GPT-3, a model with 175 billion parameters, achieving unprecedented levels of language understanding and generation capabilities.\\\\nLate 2021\\\\nIntroduction of ChatGPT\\\\n\\\\nOpenAI introduced ChatGPT, a conversational agent based on the GPT-3.5 model, designed to provide more engaging and natural dialogue experiences. ChatGPT showcased the potential of GPT models in interactive applications.\\\\n2022\\\\nGPT-4\\\\nOpenAI released GPT-4, an even more powerful and versatile model than its predecessors, with improvements in understanding, reasoning, and generating text across a broader range of contexts and languages.\\\\n2022\\\\nMidjourney and Other Innovations\\\\n\\\\nThe launch of Midjourney, along with other models and platforms, reflected the growing diversity and application of AI in creative processes, design, and beyond, indicating a broader trend towards multimodal and specialized AI systems.\\\\nPre-2010: Early Foundations\\\\n\\\\n1950s-1970s: Early AI research lays the groundwork for natural language processing. Most famously, a tech called ‘Eliza’ was the world’s first chatbot.\\\\n1980s-1990s: Development of statistical methods for NLP, moving away from rule-based systems.\\\\n\\\\n2010: Initial Models\\\\n\\\\n2013: Introduction of word2vec, a tool for computing vector representations of words, which significantly improved the quality of NLP tasks by capturing semantic meanings of words.\\\\n\\\\n2014-2017: RNNs and Attention Mechanisms\\\\n\\\\n2014: Sequence to sequence (seq2seq) models and Recurrent Neural Networks (RNNs) become popular for tasks like machine translation.\\\\n2015: Introduction of Attention Mechanism, improving the performance of neural machine translation systems.\\\\n2017: The Transformer model is introduced in the paper “Attention is All You Need”, setting a new standard for NLP tasks with its efficient handling of sequences.\\\\n\\\\n2018: Emergence of GPT and BERT\\\\n\\\\nJune 2018: OpenAI introduces GPT (Generative Pretrained Transformer), a model that leverages unsupervised learning to generate coherent and diverse text.\\\\nOctober 2018: Google AI introduces BERT (Bidirectional Encoder Representations from Transformers), which uses bidirectional training of Transformer models to improve understanding of context in language.\\\\n\\\\n2019-2020: Larger and More Powerful Models\\\\n\\\\n2019: Introduction of GPT-2, an improved version of GPT with 1.5 billion parameters, showcasing the model’s ability to generate coherent and contextually relevant text over extended passages.\\\\n2020: OpenAI releases GPT-3, a much larger model with 175 billion parameters, demonstrating remarkable abilities in generating human-like text, translation, and answering questions.\\\\n\\\\n2021-2023: Specialization, Multimodality, and Democratization of LLMs\\\\n\\\\n2021-2022: Development of specialized models like Google’s LaMDA for conversational applications and Facebook’s OPT for open pre-trained transformers.\\\\n2021: Introduction of multimodal models like DALL·E by OpenAI, capable of generating images from textual descriptions, and CLIP, which can understand images in the context of natural language.\\\\n2022: The emergence of GPT-4 and other advanced models such as Midjourney, continuing to push the boundaries of what’s possible with LLMs in terms of generating and understanding natural language across various domains and tasks, including image generation. It’s also more accessible to larger numbers of people.\\\\n\\\\nCapabilities of Large Language Models\\\\nThe capabilities of Large Language Models are as vast as the datasets they’re trained on. Use cases range from generating code to suggesting strategy for a product launch and analyzing data points.\\\\nThis is because LLMs serve as foundation models that can be applied across multiple uses.\\\\nHere’s a list of LLM capabilities:\\\\n\\\\nText generation\\\\nLanguage translation\\\\nSummarization\\\\nQuestion answering\\\\nSentiment analysis\\\\nConversational agents\\\\nCode generation and explanation\\\\nNamed entity recognition\\\\nText classification\\\\nContent recommendation\\\\nLanguage modeling\\\\nSpell checking and grammar correction\\\\nParaphrasing and rewriting\\\\nKeyword and phrase extraction\\\\nDialogue systems\\\\n\\\\nAnd here’s a breakdown of some of the more common ones we see:\\\\nAutomated Code Generation\\\\nLLMs can generate code snippets, functions, or even entire modules based on natural language descriptions, reducing the time and effort required to implement common functionalities.\\\\nHere’s an example to illustrate how LLMs can be used for automated code generation:\\\\nPrompt:\\\\n“Write a Python function that takes a list of numbers as input and returns a list containing only the even numbers.”\\\\n\\\\nText Generation\\\\nLLMs can generate coherent, contextually relevant text based on prompts. This includes creating articles, stories, and even generating product descriptions.\\\\nHere’s an example to illustrate how LLMs can be used for text generation:\\\\nPrompt:\\\\n“Generate a product description for a cutting-edge smartwatch designed for fitness enthusiasts. The description should highlight its advanced health and fitness tracking, personalized coaching, long battery life, durability, connectivity features, and customizable design. Target the description to appeal to both seasoned athletes and beginners interested in tracking their fitness progress.”\\\\n\\\\nLanguage Translation\\\\nThey can translate text between different languages, often with a high degree of accuracy, depending on the languages involved and the model’s training data.\\\\nHere’s an example to illustrate how LLMs can be used for language translation:\\\\nPrompt:\\\\n“Translate the following English text into Spanish: ‘The quick brown fox jumps over the lazy dog.\\'”\\\\n\\\\nBug Detection and Correction\\\\nLLMs can help identify bugs in code by analyzing code patterns and suggesting fixes for common errors, potentially integrating with IDEs (Integrated Development Environments) to provide real-time assistance.\\\\nHere’s an example to illustrate how LLMs can be used for bug detection:\\\\nPrompt:\\\\n“The Python function below intends to return the nth Fibonacci number. Please identify and correct any bugs in the function.\\\\nPython Function:\\\\ndef fibonacci(n):\\\\nif n <\\\\\\\\= 1:\\\\nreturn n\\\\nelse:\\\\nreturn fibonacci(n – 1) + fibonacci(n – 2)”\\\\n\\\\nParaphrasing and Rewriting\\\\nThey can rephrase or rewrite text while maintaining the original meaning, useful for content creation and academic purposes.\\\\nHere’s an example to illustrate how LLMs can be used for paraphrasing:\\\\nPrompt:\\\\n“Rewrite the following sentence in a simpler and more concise way without losing its original meaning: ‘The comprehensive study on climate change incorporates a wide array of data, including historical weather patterns, satellite imagery, and computer model predictions, to provide a holistic view of the impacts of global warming.\\'”\\\\n\\\\nDialogue Systems\\\\nLLMs power sophisticated dialogue systems for customer service, interactive storytelling, and educational purposes, providing responses that can adapt to the user’s input.\\\\nThink of a chatbot on a software product you use where you can ask it questions and it generates insightful, helpful responses.\\\\nChallenges and Limitations of LLMs\\\\nLarge language models have come a long way since the early days of Eliza.\\\\nIn the last two years alone, we’ve seen LLMs power Generative AI and create high-quality text, music, video, and images.\\\\nBut with any technology, there will always be growing pains.\\\\nTechnical Limitations of Language Models\\\\nLarge Language Models sometimes face technical limitations impacting their accuracy and ability to understand context.\\\\nDomain Mismatch\\\\nModels trained on broad datasets may struggle with specific or niche subjects due to a lack of detailed data in those areas. This can lead to inaccuracies or overly generic responses when dealing with specialized knowledge.\\\\nWord Prediction\\\\nLLMs often falter with less common words or phrases, impacting their ability to fully understand or accurately generate text involving these terms. This limitation can affect the quality of translation, writing, and technical documentation tasks.\\\\nReal-time Translation Efficiency\\\\nWhile LLMs have made strides in translation accuracy, the computational demands of processing and generating translations in real-time can strain resources, especially for languages with complex grammatical structures or those less represented in training data.\\\\nHallucinations and Bias\\\\nOn occasion, LLM technology is too original. So original in fact that it’s making up information.\\\\nThis is a lesson Air Canada learned the hard way when its chatbot told a customer about a refund policy when no such policy exists, which they then had to honor.\\\\nFinally, LLMs can inadvertently propagate and amplify biases present in their training data, leading to outputs that may be discriminatory or offensive.\\\\nScalability and Environmental Impact\\\\nThe scalability of LLMs is tied to the impact it has on the environment. And that impact is turning out to be a big one.\\\\nTraining a system like GPT-3 took 1,287 Megawatt hours (MWh) of energy. To put that into perspective, 1 MWh could power about 330 homes for one hour in the United States.\\\\nThe image below shows the energy consumption of training four different LLMs.\\\\n\\\\nEnergy consumption doesn’t end at training—operating LLMs also uses a grotesque level of energy.\\\\nIn one report, Alex de Vries, founder of Digiconomist, has calculated that by 2027 the AI sector will consume between 85 to 134 Terawatt hours each year. That’s almost the same as the annual energy demand of the Netherlands.\\\\nWe can’t help but wonder how sustainable that is and what the long-term environmental impact will be on our energy sources. Especially when you consider LLMs are only going to become larger and more complex as we advance their capabilities.\\\\nAnd to maintain large language models, we’ll need to update them with new data and parameters as they arise. That will only expend more energy and resources.\\\\nThe Future of Language Models: What Comes Next?\\\\nNow that we’ve seen drastic and rapid improvement in the capabilities of LLMs through Generative AI, we expect users of AI to be fine-tuning prompts and discovering new use cases and applications.\\\\nIn the workplace especially, the focus will be on productivity hacks. It’s something we experiment with already through our Generative Driven Development™ offering, where our team has increased the productivity of software development by 30-50%.\\\\nHilary Ashton, Chief Product Officer at Teradata, shared her predictions for the future of LLMs and AI in AI Magazine:\\\\n\\\\nFirst, I foresee a massive productivity leap forward through GenAI, especially in technology and software. It’s getting more cost-effective to get into GenAI, and there are lots more solutions available that can help improve GenAI solutions. It will be the year when conversations gravitate to GenAI, ethics, and what it means to be human. In some cases, we’ll start to see the workforce shift and be reshaped, with the technology helping to usher in a four-day work week for some full-time employees.”\\\\nHilary Ashton\\\\n\\\\nAnd she’s right, especially when it comes to ethical considerations and where we humans add value AI can’t replicate.\\\\nWe’ll also see further democratization of AI with it infiltrating other areas of our life, much the same the computer has done since its invention.\\\\nWhat we know for certain is the development of LLMs and Generative AI is only getting started. And we want to be leading conversations on its use, ethics, scalability, and more as it evolves.\\\\nYou can be part of that conversation too:\\\\nListen or watch our Talking AI podcast where we interview AI experts and talk or sign up for our newsletter where we share insights and developments on LLMs, AI/ML, and Data governance, curated by our very own CTO, Omar Shanti.\\\\nFrequently Asked Questions About Large Language Models LLMs\\\\n1. What is a Large Language Model (LLM)?\\\\nA Large Language Model (LLM) is an artificial intelligence model that uses machine learning techniques, particularly deep learning and neural networks, to understand and generate human language. These models are trained on massive data sets and can perform a broad range of tasks like generating text, translating languages, and more.\\\\n2. How do Large Language Models work?\\\\nLarge Language Models work by leveraging transformer models, which utilize self-attention mechanisms to process input text. They are pre-trained on vast amounts of data and can perform in-context learning, allowing them to generate coherent and contextually relevant responses based on user inputs.\\\\n3. What is the significance of transformer models in LLMs?\\\\nTransformer models are crucial because they enable LLMs to handle long-range dependencies in text through self-attention. This mechanism allows the model to weigh the importance of different words in a sentence, improving the language model’s performance in understanding and generating language.\\\\n4. Why are Large Language Models important in AI technologies?\\\\nLarge Language Models are important because they serve as foundation models for various AI technologies like virtual assistants, conversational AI, and search engines. They enhance the ability of machines to understand and generate human language, making interactions with technology more natural.\\\\n5. What is fine-tuning in the context of LLMs?\\\\nFine-tuning involves taking a pre-trained language model and further training it on a specific task or dataset. This process adjusts the model to perform better on specific tasks like sentiment analysis, handling programming languages, or other specialized applications.\\\\n6. How does model size affect the performance of Large Language Models?\\\\nThe model size, often measured by the parameter count, affects an LLM’s ability to capture complex language patterns. Very large models with hundreds of billions of parameters generally perform better but require more computational resources during the training process.\\\\n7. Can LLMs generate code in programming languages?\\\\nYes, Large Language Models can generate code in various programming languages. They assist developers by providing code snippets, debugging help, and translating code, thanks to their training on diverse datasets that include programming code.\\\\n8. What is “in-context learning” in Large Language Models?\\\\nIn-context learning refers to an LLM’s ability to learn and perform specific tasks based solely on the input text provided during inference, without additional fine-tuning. This allows the model to adapt to new tasks or instructions on the fly, enhancing its versatility across a broad range of applications.\\\\n9. How do LLMs handle multiple tasks like text generation and sentiment analysis?\\\\nLLMs are versatile due to their training on diverse data. They can perform multiple tasks like text generation, sentiment analysis, and more by leveraging their learned knowledge. Through fine-tuning, they can be adapted to perform specific tasks more effectively.\\\\n10. What are “zero-shot” and “few-shot” learning in Large Language Models?\\\\nZero-shot learning allows an LLM to perform a specific task it wasn’t explicitly trained on by leveraging its general language understanding. Few-shot learning involves providing the model with a few examples of the task within the prompt to guide its response. Both methods showcase the model’s ability to generalize and adapt to new tasks with minimal or no additional training data.\\\\nInstantly access the power of AI and our team of AI-enabled practitioners\\\\nWe are ready to support you on your project!\\\\nContact us\\\\n\\\\nCategory: Gen AI\\\\nTags: AI, artificial intelligence, gen ai, Generative AI, large language models, LLMs\\\\n\\\\nGet the best of our content\\\\nstraight to your inbox!\\\\nDon’t worry, we don’t spam!\\\\nRelated Posts\\\\n\\\\nProprietary Patient Management System Unlocks 99% Faster Implementation and Client Onboarding\\\\n\\\\nAmazon Q Developer: The AWS Tool Revolutionizing Cloud Interaction\\\\n\\\\nPractical Data Governance Pillars: Safeguarding Your Digital Assets\\\\n\\\\nTesting Your RAG-Powered AI Chatbot\\\\nCategories\\\\n\\\\nAgile\\\\nCulture\\\\nModernization\\\\nNearshore Development\\\\nProduct + Design\\\\nSoftware Development\\\\nTalent\\\\n\\\\n\\\\nSubscribe to our newsletter and stay up to date on the latest in AI\\\\nServices\\\\n\\\\nAI Strategy Roadmap\\\\nData Engineering & Analytics\\\\nAI-Powered Software Development\\\\nAI Engineering Teams\\\\n\\\\nPartnerships\\\\n\\\\nDatabricks\\\\n\\\\nAccelerators\\\\n\\\\nGen AI Innovation Workshop\\\\nGen AI Solution Accelerator\\\\nRAG\\\\nGenIQ\\\\n\\\\nIndustries\\\\n\\\\nCommunications & IoT\\\\nTechnology\\\\nHealthcare\\\\nFinance\\\\nRetail\\\\n\\\\nResources\\\\n\\\\nBlog\\\\nTalking AI Podcast\\\\nTalking AI Newsletter\\\\nEvents\\\\nNearshore Budget Calculator\\\\n\\\\nGet in touch\\\\n\\\\nBook a call\\\\n1-800-621-7063\\\\n\\\\nFacebook Youtube \\\\n\\\\nAtlanta, GA [HQ]\\\\nChicago, IL\\\\nDallas, TX \\u200b\\\\nSan Jose, Costa Rica [HQ]\\\\nBogota, Colombia\\\\nMedellin, Colombia\\\\nBarranquilla, Colombia\\\\nLima, Peru\\\\n\\\\n\\\\n\\\\n©2023 HatchWorks Inc. All rights reserved.\\\\nPrivacy Policy\\u200b\\\\nTerms and Conditions\\\\nRecruitment Fraud Disclaimer\\\\n\\\\nClose this module\\\\n\\\\nFREE E-BOOKState of AI 2025\\\\nA round-up of industry stats, research, and insights to understand where AI stands, how it got here, and where it’s going.\\\\nNameName\\\\nEmailEmail\\\\nCompany NameCompany Name\\\\nDownload E-book\\\\nNo thanks, I’m not interested!\"}, {\"url\": \"https://www.shakudo.io/blog/top-9-large-language-models\", \"title\": \"Top 9 Large Language Models as of Feburary 2025 - Shakudo\", \"content\": \"The latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. The latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. The latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data.\", \"score\": 0.7730283, \"raw_content\": \"Top 9 Large Language Models as of Feburary 2025 | Shakudo\\\\nLatest in Insights : When to Choose Deep Learning Over Machine Learning (And Vice Versa)\\\\n\\\\n\\\\nWhy SHakudo\\\\n\\\\n Data & AI OS Build your ideal data stack on one unified platform Learn more >\\\\nshakudo AI Applications\\\\n Text to SQL Workflow Automation Vector Database Reverse ETLSee all >\\\\nComponents\\\\nSolutions\\\\n\\\\nShakudo for Industries\\\\nAerospace\\\\nAutomotive & Transportation\\\\nClimate & Energy\\\\nFinancial Services\\\\nHealthcare & Life Sciences\\\\nHigher Education\\\\nManufacturing\\\\nReal Estate\\\\nRetail\\\\nSports\\\\nTechnology & Software\\\\nShakudo Use Cases\\\\nAutomate Custom Sustainability Report Population\\\\nChat with Enterprise Knowledge Base Using AI Assistants\\\\nGenerate Real-World Evidence for Healthcare Decisions\\\\nOptimize Ticket Pricing with Dynamic Demand Modeling\\\\nDetect Hidden Red Flags in Company Data\\\\nMonitor Market Sentiment Across Multiple Sources\\\\nSee all >\\\\nResources\\\\n\\\\n Case Studies Learn how leading companies leverage data & AI on Shakudo blog Read what\\'s new at Shakudo and the data and AI world white papers Access in-depth reports and guides on data & AI solutions Docs Explore comprehensive guides on the Shakudo platform\\\\n  Case Study How CentralReach uses Shakudo to Cut Time-To-Deployment to Launch New AI- Powered Solutions\\\\n  Case Study How AI is Changing the Game for the Cleveland Cavaliers\\\\nCompany\\\\n\\\\n ABout Us Learn about our mission and values Careers Join us in building the next-gen data stack Partners Learn about the relationships that make it happen Contact us Have a question? We\\'re here to help\\\\nAI WorkshopGet a Demo\\\\n\\\\n← Back to Blog\\\\nInsights\\\\nTop 9 Large Language Models as of Feburary 2025\\\\nAuthor(s):\\\\n\\\\nNo items found.\\\\nUpdated on:\\\\nFebruary 7, 2025\\\\n\\\\nTable of contents\\\\nExample H2\\\\nExample H3\\\\nMentioned Components\\\\nNo items found.\\\\n<>\\\\nGet the latest updates in Data & AI straight to your inboxWe’ll email you once per week—and never share your information.\\\\n🎉 Success! You\\'re now signed up for the Shakudo newsletter.\\\\nOops! Something went wrong while submitting the form.\\\\nIntroduction\\\\nIf we had to choose one word to describe the rapid evolution of AI today, it would probably be something along the lines of explosive. As predicted by the Market Research Future report, the large language model (LLM) market in North America alone is expected to reach $105.5 billion by 2030. The exponential growth of AI tools combined with access to massive troves of text data has opened gates for better and more advanced content generation than we had ever hoped. Yet, such rapid expansion also makes it harder than ever to navigate and select the right tools among the diverse LLM models available.\\\\nThe goal of this post is to keep you, the AI enthusiast and professional, up-to-date with current trends and essential innovations in the field. Below, we highlighted the top 9 LLMs that we think are currently making waves in the industry, each with distinct capabilities and specialized strengths, excelling in areas such as natural language processing, code synthesis, few-shot learning, or scalability. While we believe there is no one-size-fits-all LLM for every use case, we hope that this list can help you identify the most current and well-suited LLM model that meets your business’s unique requirements.\\\\n1. GPT\\\\n\\\\nOur list kicks off with OpenAI\\'s Generative Pre-trained Transformer (GPT) models, which have consistently exceeded their previous capabilities with each new release. Compared to its prior models, the latest ChatGPT-4o and ChatGPT-4o mini models offer significantly faster processing speeds and enhanced capabilities across text, voice, and vision.\\\\nThe latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. Both of these models are equipped with multimodal capabilities to handle images as well as audio data.\\\\nDespite having advanced conversational and reasoning capabilities, note that GPT is a proprietary model, meaning that the training data and parameters are kept confidential by OpenAI, and access to full functionality is restricted–a commercial license or subscription is often required to unlock the complete range of features. In this case, we recommend this model for businesses looking to adopt an LLM that excels in conversational dialogue, multi-step reasoning, efficient computation, and real-time interactions without the constraints of a budget.\\\\nFor companies who are curious to try out the proprietary models on the market before fully committing to one due to budget constraints or uncertainties about its long-term integration, Shakudo offers a compelling alternative. Our platform currently features a diverse selection of advanced LLMs with simplified deployment and scalability. With a simple subscription, you can access and assess the value of proprietary models, like GPT, before making a substantial investment.\\\\n2.DeepSeek\\\\n\\\\nDeepseek-R1 Benchmark. Source: deepseek.com\\\\nWith its latest R1 model, the Chinese AI company DeepSeek has once again set new benchmarks for innovation in the AI community. As of January 24th, the DeepSeek-R1 model is ranked fourth on Chatbot Arena, and top as the best open-source LM.\\\\nThe DeepSeek-R1 is a 671B parameter Mixture-of-Experts (MoE) model with 37B activated parameters per token, trained through large-scale reinforcement learning with a strong focus on reasoning capabilities. The model excels at understanding and handling long-form content and demonstrates superior performance in complex tasks such as mathematics and code generation. The model is approximately 30 times more cost-efficient than OpenAI-o1 and 5 times faster, offering groundbreaking performance at a fraction of the cost. Moreover, it has shown exceptional precision in tasks requiring complex pattern recognition, such as genomic data analysis, medical imaging, and large-scale scientific simulations.\\\\nDeepSeek-R1’s capabilities are transformative when it comes to integration with proprietary enterprise data such as PII and financial records. Leveraging retrieval-augmented generation (RAG), enterprises can connect the model to their internal data sources to enable highly personalized, context-aware interactions—all while maintaining stringent security and compliance standards. With Shakudo, you can streamline the deployment and integration of advanced AI models like DeepSeek by automating the setup, deployment, and management processes. This eliminates the need for businesses to invest in and maintain extensive computing infrastructure. By operating within your existing infrastructure, the platform ensures seamless integration, enhanced security, and optimal performance without requiring significant in-house resources or specialized expertise.\\\\n3. Qwen\\\\n\\\\n\\u200d\\\\nAlibaba released Qwen2.5-Max earlier last month, the new model designed to deliver enhanced performance for large-scale natural language processing tasks. In instruct model evaluations, this new model outperforms DeepSeek V3 in benchmarks such as Arena-Hard, LiveBench, LiveCodeBench, and GPQA-Diamond, while also delivering strong performance in other assessments like MMLU-Pro.\\\\nQwen2.5-Max is pretrained on over 20 trillion tokens. While specific details about its parameter count and token window size are not publicly disclosed, Qwen2.5-Max is designed for low-latency, high-efficiency tasks, making it suitable for applications requiring quick, accurate responses with low latency. Its smaller size allows for deployment on devices with limited computational resources.\\\\nFor businesses and users looking for higher-performance models for natural language processing and AI-driven tasks, the Qwen 2.5 model is available on platforms like Hugging Face and ModelScope. This model boasts from 0.5 billion to 72 billion parameters, featuring context windows of up to 128,000 tokens, and is excellent for code generation, debugging, and automated forecasting.\\\\n4. LG AI\\\\n\\\\nsource\\\\nEXAONE 3.0 is a bilingual LLM developed by LG AI Research. The company released its latest model in December 2024 with excellent performance across various benchmarks and real-world applications.\\\\nWith 7.8 billion parameters, EXAONE 3.0 is capable of understanding and generating human-like text in multiple languages across complex domains, including coding, mathematics, patents, and chemistry. This model has also been optimized to reduce inference processing time by 56%, memory usage by 35%, and operating costs by 72%, ensuring that it remains cost-effective while maintaining high performance.\\\\nLG AI Research has open-sourced the 7.8B parameter instruction-tuned version of EXAONE 3.0 for non-commercial research purposes. This is a model we recommend to software companies or tech startups for generating Python code, assisting developers in troubleshooting, and creating APIs or other backend components.\\\\n5. LlaMA\\\\n\\\\nMeta is still leading the front with their state-of-the-art LlaMa models. The company released its latest LlaMA 3.3 model in December 2024, featuring multimodal capabilities that can process both text and image for in-depth analysis and response generation, such as interpreting charts, maps, or translating texts identified in an image.\\\\nLlaMA 3.3 improves on previous models with a longer context window of up to 128,000 tokens and an optimized transformer architecture. With a parameter of 70 billion, this model outperforms open-source and proprietary alternatives in areas such as multilingual dialogue, reasoning, and coding.\\\\nUnlike ChatGPT models, LlaMA 3 is open-source, giving users the flexibility to access and deploy freely on their cloud depending on the specific requirements of their infrastructure, security preferences, or customization needs. We recommend this model to businesses looking for advanced content generation and language understanding, such as those in customer service, education, marketing, and consumer markets. The openness of these models also allows for your greater control over the model’s performance, tuning, and integration into existing workflows.\\\\n6. Claude\\\\n\\\\nNext on our list is Claude, more specifically, the latest Claude 3.5 Sonnet model developed by Anthropic. We believe that Claude is arguably one of the most significant competitors to GPT since all of its current models–Claude 3 Haiku, Claude 3.5 Sonnet, and Claude 3 Opus–are designed with incredible contextual understanding capabilities that position themselves as the top conversational AI closely aligned with nuanced human interactions.\\\\nWhile the specific parameters of Claude 3.5 Sonnet remain undisclosed, the model boasts an impressive context window of 200,000 tokens, equivalent to approximately 150,000 words or 300 pages of text.\\\\nThe current Claude subscription service is credit-based, and the cost can go as high as $2,304/month for enterprise plans tailored to high-volume users. We recommend Claude to mature or mid-stage businesses looking not only to adopt an AI that facilitates human-like interactions but also to enhance their coding capabilities since the Claude 3.5 Sonnet model is currently reaching a 49.0% performance score on the SWE-bench Verified benchmark, placing it as third among all publicly available models, including reasoning models and systems specifically designed for agentic coding.\\\\n7. Mistral\\\\n\\\\nMistral\\'s latest model – Mistral Small 3, a latency-optimized model was released under the Apache 2.0 license at the end of January. This 24-billion-parameter model is designed for low-latency, high-efficiency tasks. It processes approximately 150 tokens per second, making it over three times faster than Llama 3.3 70B on the same hardware.\\\\nThis new model is ideal for applications requiring quick, accurate responses with low latency, such as virtual assistants, real-time data processing, and on-device command and control. Its smaller size allows for deployment on devices with limited computational resources.\\\\nMistral Small 3 is currently open-source under the Apache 2.0 license. This means you can freely access and use the model for your own applications, provided you comply with the license terms. Since it is designed to be easily deployable, including on hardware with limited resources like a single GPU or even a MacBook with 32GB RAM, we\\'d recommend this to early-stage businesses looking to implement low-latency AI solutions without the need for extensive hardware infrastructure.\\\\n8. Gemini\\\\n\\\\nGemini is a family of closed-source LLM models developed by Google. The latest model—Gemini 2.0 Flash—operates at twice the speed of Gemini 1.5 Pro, offering substantial improvements in speed, reasoning, and multimodal processing capabilities.\\\\nWith that being said, Gemini remains a proprietary model; if your company deals with sensitive or confidential data regularly, you might be concerned about sending it to external servers due to security reasons. To address this concern, we recommend that you double-check vendor compliance regulations to ensure data privacy and security standards are met, such as adherence to GDPR, HIPAA, or other relevant data protection laws.\\\\nIf you’re looking for an open-source alternative that exhibits capabilities almost as good as Gemini, Google’s latest Gemma model, Gemma 2, offers three models available in 2 billion, 9 billion, and 27 billion parameters with a context window of 8,200. For businesses looking for a rather economic option, this is the optimal choice that interprets and understands messages with remarkable accuracy.\\\\n9. Command\\\\n\\\\nCommand R is a family of scalable models developed by Cohere with the goal of balancing high performance with strong accuracy, just like Claude. Both the Command R and Command R+ models offer APIs specifically optimized for Retrieval Augmented Generation (RAG). This means that these models can combine large-scale language generation with real-time information retrieval techniques for much more contextually aware outputs.\\\\nCurrently, the Command R+ model boasts 104 billion parameters and offers an industry-leading 128,000 token context window for enhanced long-form processing and multi-turn conversation capabilities.\\\\nOne of the perks of working with an open-source model is also to avoid vendor lock-in. Heavy reliance on a particular type of proprietary model may make it difficult for you to switch to alternative models when your business starts growing or the landscape changes. Cohere approaches this in a hybrid way, meaning that you can access and modify the model for personal usage but need a license for commercial use. In this case, we recommend this model for businesses that want flexibility in experimentation without a long-term commitment to a single vendor.\\\\n\\\\nWhitepaper\\\\nIntroduction\\\\nIf we had to choose one word to describe the rapid evolution of AI today, it would probably be something along the lines of explosive. As predicted by the Market Research Future report, the large language model (LLM) market in North America alone is expected to reach $105.5 billion by 2030. The exponential growth of AI tools combined with access to massive troves of text data has opened gates for better and more advanced content generation than we had ever hoped. Yet, such rapid expansion also makes it harder than ever to navigate and select the right tools among the diverse LLM models available.\\\\nThe goal of this post is to keep you, the AI enthusiast and professional, up-to-date with current trends and essential innovations in the field. Below, we highlighted the top 9 LLMs that we think are currently making waves in the industry, each with distinct capabilities and specialized strengths, excelling in areas such as natural language processing, code synthesis, few-shot learning, or scalability. While we believe there is no one-size-fits-all LLM for every use case, we hope that this list can help you identify the most current and well-suited LLM model that meets your business’s unique requirements.\\\\n1. GPT\\\\n\\\\nOur list kicks off with OpenAI\\'s Generative Pre-trained Transformer (GPT) models, which have consistently exceeded their previous capabilities with each new release. Compared to its prior models, the latest ChatGPT-4o and ChatGPT-4o mini models offer significantly faster processing speeds and enhanced capabilities across text, voice, and vision.\\\\nThe latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. Both of these models are equipped with multimodal capabilities to handle images as well as audio data.\\\\nDespite having advanced conversational and reasoning capabilities, note that GPT is a proprietary model, meaning that the training data and parameters are kept confidential by OpenAI, and access to full functionality is restricted–a commercial license or subscription is often required to unlock the complete range of features. In this case, we recommend this model for businesses looking to adopt an LLM that excels in conversational dialogue, multi-step reasoning, efficient computation, and real-time interactions without the constraints of a budget.\\\\nFor companies who are curious to try out the proprietary models on the market before fully committing to one due to budget constraints or uncertainties about its long-term integration, Shakudo offers a compelling alternative. Our platform currently features a diverse selection of advanced LLMs with simplified deployment and scalability. With a simple subscription, you can access and assess the value of proprietary models, like GPT, before making a substantial investment.\\\\n2.DeepSeek\\\\n\\\\nDeepseek-R1 Benchmark. Source: deepseek.com\\\\nWith its latest R1 model, the Chinese AI company DeepSeek has once again set new benchmarks for innovation in the AI community. As of January 24th, the DeepSeek-R1 model is ranked fourth on Chatbot Arena, and top as the best open-source LM.\\\\nThe DeepSeek-R1 is a 671B parameter Mixture-of-Experts (MoE) model with 37B activated parameters per token, trained through large-scale reinforcement learning with a strong focus on reasoning capabilities. The model excels at understanding and handling long-form content and demonstrates superior performance in complex tasks such as mathematics and code generation. The model is approximately 30 times more cost-efficient than OpenAI-o1 and 5 times faster, offering groundbreaking performance at a fraction of the cost. Moreover, it has shown exceptional precision in tasks requiring complex pattern recognition, such as genomic data analysis, medical imaging, and large-scale scientific simulations.\\\\nDeepSeek-R1’s capabilities are transformative when it comes to integration with proprietary enterprise data such as PII and financial records. Leveraging retrieval-augmented generation (RAG), enterprises can connect the model to their internal data sources to enable highly personalized, context-aware interactions—all while maintaining stringent security and compliance standards. With Shakudo, you can streamline the deployment and integration of advanced AI models like DeepSeek by automating the setup, deployment, and management processes. This eliminates the need for businesses to invest in and maintain extensive computing infrastructure. By operating within your existing infrastructure, the platform ensures seamless integration, enhanced security, and optimal performance without requiring significant in-house resources or specialized expertise.\\\\n3. Qwen\\\\n\\\\n\\u200d\\\\nAlibaba released Qwen2.5-Max earlier last month, the new model designed to deliver enhanced performance for large-scale natural language processing tasks. In instruct model evaluations, this new model outperforms DeepSeek V3 in benchmarks such as Arena-Hard, LiveBench, LiveCodeBench, and GPQA-Diamond, while also delivering strong performance in other assessments like MMLU-Pro.\\\\nQwen2.5-Max is pretrained on over 20 trillion tokens. While specific details about its parameter count and token window size are not publicly disclosed, Qwen2.5-Max is designed for low-latency, high-efficiency tasks, making it suitable for applications requiring quick, accurate responses with low latency. Its smaller size allows for deployment on devices with limited computational resources.\\\\nFor businesses and users looking for higher-performance models for natural language processing and AI-driven tasks, the Qwen 2.5 model is available on platforms like Hugging Face and ModelScope. This model boasts from 0.5 billion to 72 billion parameters, featuring context windows of up to 128,000 tokens, and is excellent for code generation, debugging, and automated forecasting.\\\\n4. LG AI\\\\n\\\\nsource\\\\nEXAONE 3.0 is a bilingual LLM developed by LG AI Research. The company released its latest model in December 2024 with excellent performance across various benchmarks and real-world applications.\\\\nWith 7.8 billion parameters, EXAONE 3.0 is capable of understanding and generating human-like text in multiple languages across complex domains, including coding, mathematics, patents, and chemistry. This model has also been optimized to reduce inference processing time by 56%, memory usage by 35%, and operating costs by 72%, ensuring that it remains cost-effective while maintaining high performance.\\\\nLG AI Research has open-sourced the 7.8B parameter instruction-tuned version of EXAONE 3.0 for non-commercial research purposes. This is a model we recommend to software companies or tech startups for generating Python code, assisting developers in troubleshooting, and creating APIs or other backend components.\\\\n5. LlaMA\\\\n\\\\nMeta is still leading the front with their state-of-the-art LlaMa models. The company released its latest LlaMA 3.3 model in December 2024, featuring multimodal capabilities that can process both text and image for in-depth analysis and response generation, such as interpreting charts, maps, or translating texts identified in an image.\\\\nLlaMA 3.3 improves on previous models with a longer context window of up to 128,000 tokens and an optimized transformer architecture. With a parameter of 70 billion, this model outperforms open-source and proprietary alternatives in areas such as multilingual dialogue, reasoning, and coding.\\\\nUnlike ChatGPT models, LlaMA 3 is open-source, giving users the flexibility to access and deploy freely on their cloud depending on the specific requirements of their infrastructure, security preferences, or customization needs. We recommend this model to businesses looking for advanced content generation and language understanding, such as those in customer service, education, marketing, and consumer markets. The openness of these models also allows for your greater control over the model’s performance, tuning, and integration into existing workflows.\\\\n6. Claude\\\\n\\\\nNext on our list is Claude, more specifically, the latest Claude 3.5 Sonnet model developed by Anthropic. We believe that Claude is arguably one of the most significant competitors to GPT since all of its current models–Claude 3 Haiku, Claude 3.5 Sonnet, and Claude 3 Opus–are designed with incredible contextual understanding capabilities that position themselves as the top conversational AI closely aligned with nuanced human interactions.\\\\nWhile the specific parameters of Claude 3.5 Sonnet remain undisclosed, the model boasts an impressive context window of 200,000 tokens, equivalent to approximately 150,000 words or 300 pages of text.\\\\nThe current Claude subscription service is credit-based, and the cost can go as high as $2,304/month for enterprise plans tailored to high-volume users. We recommend Claude to mature or mid-stage businesses looking not only to adopt an AI that facilitates human-like interactions but also to enhance their coding capabilities since the Claude 3.5 Sonnet model is currently reaching a 49.0% performance score on the SWE-bench Verified benchmark, placing it as third among all publicly available models, including reasoning models and systems specifically designed for agentic coding.\\\\n7. Mistral\\\\n\\\\nMistral\\'s latest model – Mistral Small 3, a latency-optimized model was released under the Apache 2.0 license at the end of January. This 24-billion-parameter model is designed for low-latency, high-efficiency tasks. It processes approximately 150 tokens per second, making it over three times faster than Llama 3.3 70B on the same hardware.\\\\nThis new model is ideal for applications requiring quick, accurate responses with low latency, such as virtual assistants, real-time data processing, and on-device command and control. Its smaller size allows for deployment on devices with limited computational resources.\\\\nMistral Small 3 is currently open-source under the Apache 2.0 license. This means you can freely access and use the model for your own applications, provided you comply with the license terms. Since it is designed to be easily deployable, including on hardware with limited resources like a single GPU or even a MacBook with 32GB RAM, we\\'d recommend this to early-stage businesses looking to implement low-latency AI solutions without the need for extensive hardware infrastructure.\\\\n8. Gemini\\\\n\\\\nGemini is a family of closed-source LLM models developed by Google. The latest model—Gemini 2.0 Flash—operates at twice the speed of Gemini 1.5 Pro, offering substantial improvements in speed, reasoning, and multimodal processing capabilities.\\\\nWith that being said, Gemini remains a proprietary model; if your company deals with sensitive or confidential data regularly, you might be concerned about sending it to external servers due to security reasons. To address this concern, we recommend that you double-check vendor compliance regulations to ensure data privacy and security standards are met, such as adherence to GDPR, HIPAA, or other relevant data protection laws.\\\\nIf you’re looking for an open-source alternative that exhibits capabilities almost as good as Gemini, Google’s latest Gemma model, Gemma 2, offers three models available in 2 billion, 9 billion, and 27 billion parameters with a context window of 8,200. For businesses looking for a rather economic option, this is the optimal choice that interprets and understands messages with remarkable accuracy.\\\\n9. Command\\\\n\\\\nCommand R is a family of scalable models developed by Cohere with the goal of balancing high performance with strong accuracy, just like Claude. Both the Command R and Command R+ models offer APIs specifically optimized for Retrieval Augmented Generation (RAG). This means that these models can combine large-scale language generation with real-time information retrieval techniques for much more contextually aware outputs.\\\\nCurrently, the Command R+ model boasts 104 billion parameters and offers an industry-leading 128,000 token context window for enhanced long-form processing and multi-turn conversation capabilities.\\\\nOne of the perks of working with an open-source model is also to avoid vendor lock-in. Heavy reliance on a particular type of proprietary model may make it difficult for you to switch to alternative models when your business starts growing or the landscape changes. Cohere approaches this in a hybrid way, meaning that you can access and modify the model for personal usage but need a license for commercial use. In this case, we recommend this model for businesses that want flexibility in experimentation without a long-term commitment to a single vendor.\\\\nGet the whitepaper\\\\nTop 9 Large Language Models as of Feburary 2025\\\\nBy clicking \\\\\"Download,\\\\\" you agree to Shakudo processing your personal data in accordance with its Privacy Notice.\\\\nThank you for filling out the form. The whitepaper you have requested is available for download below.  \\\\nDownload White Paper\\\\nOops! Something went wrong while submitting the form.\\\\nGet the whitepaper\\\\nTop 9 Large Language Models as of Feburary 2025\\\\nThank you for your interest. Click the button below to download whitepaper you have requested.  \\\\nDownload White Paper\\\\n\\\\nTop 9 Large Language Models as of Feburary 2025\\\\nExplore the top 9 LLMs making waves in the AI world and what each of them excel at\\\\n\\\\n| Case Study\\\\nTop 9 Large Language Models as of Feburary 2025\\\\n\\\\nKey results\\\\nAbout\\\\nindustry\\\\nTech Stack\\\\nNo items found.\\\\n<>\\\\nIntroduction\\\\nIf we had to choose one word to describe the rapid evolution of AI today, it would probably be something along the lines of explosive. As predicted by the Market Research Future report, the large language model (LLM) market in North America alone is expected to reach $105.5 billion by 2030. The exponential growth of AI tools combined with access to massive troves of text data has opened gates for better and more advanced content generation than we had ever hoped. Yet, such rapid expansion also makes it harder than ever to navigate and select the right tools among the diverse LLM models available.\\\\nThe goal of this post is to keep you, the AI enthusiast and professional, up-to-date with current trends and essential innovations in the field. Below, we highlighted the top 9 LLMs that we think are currently making waves in the industry, each with distinct capabilities and specialized strengths, excelling in areas such as natural language processing, code synthesis, few-shot learning, or scalability. While we believe there is no one-size-fits-all LLM for every use case, we hope that this list can help you identify the most current and well-suited LLM model that meets your business’s unique requirements.\\\\n1. GPT\\\\n\\\\nOur list kicks off with OpenAI\\'s Generative Pre-trained Transformer (GPT) models, which have consistently exceeded their previous capabilities with each new release. Compared to its prior models, the latest ChatGPT-4o and ChatGPT-4o mini models offer significantly faster processing speeds and enhanced capabilities across text, voice, and vision.\\\\nThe latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. Both of these models are equipped with multimodal capabilities to handle images as well as audio data.\\\\nDespite having advanced conversational and reasoning capabilities, note that GPT is a proprietary model, meaning that the training data and parameters are kept confidential by OpenAI, and access to full functionality is restricted–a commercial license or subscription is often required to unlock the complete range of features. In this case, we recommend this model for businesses looking to adopt an LLM that excels in conversational dialogue, multi-step reasoning, efficient computation, and real-time interactions without the constraints of a budget.\\\\nFor companies who are curious to try out the proprietary models on the market before fully committing to one due to budget constraints or uncertainties about its long-term integration, Shakudo offers a compelling alternative. Our platform currently features a diverse selection of advanced LLMs with simplified deployment and scalability. With a simple subscription, you can access and assess the value of proprietary models, like GPT, before making a substantial investment.\\\\n2.DeepSeek\\\\n\\\\nDeepseek-R1 Benchmark. Source: deepseek.com\\\\nWith its latest R1 model, the Chinese AI company DeepSeek has once again set new benchmarks for innovation in the AI community. As of January 24th, the DeepSeek-R1 model is ranked fourth on Chatbot Arena, and top as the best open-source LM.\\\\nThe DeepSeek-R1 is a 671B parameter Mixture-of-Experts (MoE) model with 37B activated parameters per token, trained through large-scale reinforcement learning with a strong focus on reasoning capabilities. The model excels at understanding and handling long-form content and demonstrates superior performance in complex tasks such as mathematics and code generation. The model is approximately 30 times more cost-efficient than OpenAI-o1 and 5 times faster, offering groundbreaking performance at a fraction of the cost. Moreover, it has shown exceptional precision in tasks requiring complex pattern recognition, such as genomic data analysis, medical imaging, and large-scale scientific simulations.\\\\nDeepSeek-R1’s capabilities are transformative when it comes to integration with proprietary enterprise data such as PII and financial records. Leveraging retrieval-augmented generation (RAG), enterprises can connect the model to their internal data sources to enable highly personalized, context-aware interactions—all while maintaining stringent security and compliance standards. With Shakudo, you can streamline the deployment and integration of advanced AI models like DeepSeek by automating the setup, deployment, and management processes. This eliminates the need for businesses to invest in and maintain extensive computing infrastructure. By operating within your existing infrastructure, the platform ensures seamless integration, enhanced security, and optimal performance without requiring significant in-house resources or specialized expertise.\\\\n3. Qwen\\\\n\\\\n\\u200d\\\\nAlibaba released Qwen2.5-Max earlier last month, the new model designed to deliver enhanced performance for large-scale natural language processing tasks. In instruct model evaluations, this new model outperforms DeepSeek V3 in benchmarks such as Arena-Hard, LiveBench, LiveCodeBench, and GPQA-Diamond, while also delivering strong performance in other assessments like MMLU-Pro.\\\\nQwen2.5-Max is pretrained on over 20 trillion tokens. While specific details about its parameter count and token window size are not publicly disclosed, Qwen2.5-Max is designed for low-latency, high-efficiency tasks, making it suitable for applications requiring quick, accurate responses with low latency. Its smaller size allows for deployment on devices with limited computational resources.\\\\nFor businesses and users looking for higher-performance models for natural language processing and AI-driven tasks, the Qwen 2.5 model is available on platforms like Hugging Face and ModelScope. This model boasts from 0.5 billion to 72 billion parameters, featuring context windows of up to 128,000 tokens, and is excellent for code generation, debugging, and automated forecasting.\\\\n4. LG AI\\\\n\\\\nsource\\\\nEXAONE 3.0 is a bilingual LLM developed by LG AI Research. The company released its latest model in December 2024 with excellent performance across various benchmarks and real-world applications.\\\\nWith 7.8 billion parameters, EXAONE 3.0 is capable of understanding and generating human-like text in multiple languages across complex domains, including coding, mathematics, patents, and chemistry. This model has also been optimized to reduce inference processing time by 56%, memory usage by 35%, and operating costs by 72%, ensuring that it remains cost-effective while maintaining high performance.\\\\nLG AI Research has open-sourced the 7.8B parameter instruction-tuned version of EXAONE 3.0 for non-commercial research purposes. This is a model we recommend to software companies or tech startups for generating Python code, assisting developers in troubleshooting, and creating APIs or other backend components.\\\\n5. LlaMA\\\\n\\\\nMeta is still leading the front with their state-of-the-art LlaMa models. The company released its latest LlaMA 3.3 model in December 2024, featuring multimodal capabilities that can process both text and image for in-depth analysis and response generation, such as interpreting charts, maps, or translating texts identified in an image.\\\\nLlaMA 3.3 improves on previous models with a longer context window of up to 128,000 tokens and an optimized transformer architecture. With a parameter of 70 billion, this model outperforms open-source and proprietary alternatives in areas such as multilingual dialogue, reasoning, and coding.\\\\nUnlike ChatGPT models, LlaMA 3 is open-source, giving users the flexibility to access and deploy freely on their cloud depending on the specific requirements of their infrastructure, security preferences, or customization needs. We recommend this model to businesses looking for advanced content generation and language understanding, such as those in customer service, education, marketing, and consumer markets. The openness of these models also allows for your greater control over the model’s performance, tuning, and integration into existing workflows.\\\\n6. Claude\\\\n\\\\nNext on our list is Claude, more specifically, the latest Claude 3.5 Sonnet model developed by Anthropic. We believe that Claude is arguably one of the most significant competitors to GPT since all of its current models–Claude 3 Haiku, Claude 3.5 Sonnet, and Claude 3 Opus–are designed with incredible contextual understanding capabilities that position themselves as the top conversational AI closely aligned with nuanced human interactions.\\\\nWhile the specific parameters of Claude 3.5 Sonnet remain undisclosed, the model boasts an impressive context window of 200,000 tokens, equivalent to approximately 150,000 words or 300 pages of text.\\\\nThe current Claude subscription service is credit-based, and the cost can go as high as $2,304/month for enterprise plans tailored to high-volume users. We recommend Claude to mature or mid-stage businesses looking not only to adopt an AI that facilitates human-like interactions but also to enhance their coding capabilities since the Claude 3.5 Sonnet model is currently reaching a 49.0% performance score on the SWE-bench Verified benchmark, placing it as third among all publicly available models, including reasoning models and systems specifically designed for agentic coding.\\\\n7. Mistral\\\\n\\\\nMistral\\'s latest model – Mistral Small 3, a latency-optimized model was released under the Apache 2.0 license at the end of January. This 24-billion-parameter model is designed for low-latency, high-efficiency tasks. It processes approximately 150 tokens per second, making it over three times faster than Llama 3.3 70B on the same hardware.\\\\nThis new model is ideal for applications requiring quick, accurate responses with low latency, such as virtual assistants, real-time data processing, and on-device command and control. Its smaller size allows for deployment on devices with limited computational resources.\\\\nMistral Small 3 is currently open-source under the Apache 2.0 license. This means you can freely access and use the model for your own applications, provided you comply with the license terms. Since it is designed to be easily deployable, including on hardware with limited resources like a single GPU or even a MacBook with 32GB RAM, we\\'d recommend this to early-stage businesses looking to implement low-latency AI solutions without the need for extensive hardware infrastructure.\\\\n8. Gemini\\\\n\\\\nGemini is a family of closed-source LLM models developed by Google. The latest model—Gemini 2.0 Flash—operates at twice the speed of Gemini 1.5 Pro, offering substantial improvements in speed, reasoning, and multimodal processing capabilities.\\\\nWith that being said, Gemini remains a proprietary model; if your company deals with sensitive or confidential data regularly, you might be concerned about sending it to external servers due to security reasons. To address this concern, we recommend that you double-check vendor compliance regulations to ensure data privacy and security standards are met, such as adherence to GDPR, HIPAA, or other relevant data protection laws.\\\\nIf you’re looking for an open-source alternative that exhibits capabilities almost as good as Gemini, Google’s latest Gemma model, Gemma 2, offers three models available in 2 billion, 9 billion, and 27 billion parameters with a context window of 8,200. For businesses looking for a rather economic option, this is the optimal choice that interprets and understands messages with remarkable accuracy.\\\\n9. Command\\\\n\\\\nCommand R is a family of scalable models developed by Cohere with the goal of balancing high performance with strong accuracy, just like Claude. Both the Command R and Command R+ models offer APIs specifically optimized for Retrieval Augmented Generation (RAG). This means that these models can combine large-scale language generation with real-time information retrieval techniques for much more contextually aware outputs.\\\\nCurrently, the Command R+ model boasts 104 billion parameters and offers an industry-leading 128,000 token context window for enhanced long-form processing and multi-turn conversation capabilities.\\\\nOne of the perks of working with an open-source model is also to avoid vendor lock-in. Heavy reliance on a particular type of proprietary model may make it difficult for you to switch to alternative models when your business starts growing or the landscape changes. Cohere approaches this in a hybrid way, meaning that you can access and modify the model for personal usage but need a license for commercial use. In this case, we recommend this model for businesses that want flexibility in experimentation without a long-term commitment to a single vendor.\\\\nExplore more from Shakudo\\\\n How VPCs Enable AI Deployments with a Modern Data Stack Insights January 28, 2025\\\\n The Power of Simple Questions: How to Choose the Right Natural Language to SQL Query Tool Insights May 15, 2024\\\\n Bring Data and AI tooling right to MongoDB Atlas with Shakudo News August 26, 2024\\\\nTake the next step\\\\n\\\\\"Shakudo gave us the flexibility to use the data stack components that fit our needs and evolve the stack to keep up with the industry.\\\\\"\\\\n\\\\nNeal Gilmore\\\\nSenior Vice President, Enterprise Data & Analytics\\\\nDiscover Shakudo\\\\n\\\\nShakudo brings the best data and AI products into your VPC and operates them for you automatically achieving a more reliable, performant, and cost effective data stack than ever before.\\\\n\\\\n Book Demo Email X (Twitter) Linkedin Youtube\\\\nNewsletter\\\\nSign up for the latest Shakudo news:\\\\n🎉 Success! You\\'re now signed up for the Shakudo newsletter.\\\\nOops! Something went wrong while submitting the form.\\\\nApplications\\\\nData and AI OSStack ComponentsLanguage to SQLVector Database + LLMReverse ETLWorkflow Automation\\\\nIndustries\\\\nAutomotive & Transportation\\\\nAerospace\\\\nManufacturing\\\\nHigher Education\\\\nHealthcare & Life Sciences\\\\nClimate & Energy\\\\nTechnology & Software\\\\nSports\\\\nReal Estate\\\\nRetail\\\\nFinancial Services\\\\nResources\\\\nUse Cases\\\\nInsights\\\\nWhite Paper\\\\nCase Study\\\\nPress\\\\nProduct\\\\nTutorial\\\\nNews\\\\nWebinarGlossaryDocumentation\\\\nCompany\\\\nAboutPartnersDGX PartnerCareersMedia Kit\\\\nGet Started\\\\nSignupContact UsNewsletter\\\\n© 2025 Shakudo\\\\nToronto, CA\\\\nContact usPrivacy PolicyTerms/ConditionsSitemap\\\\nTrusted by industry leaders\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSee Shakudo in Action  \\\\nWatch the 3 Minute Demo\\\\n\\\\nThis field is required\\\\n\\\\nFor information about how Shakudo handles your personal data, please see our Privacy Policy.\\\\nThank you for your submission. A Shakudo expert will be in touch with you shortly.  \\\\nIn the meantime, feel free to check out our data insights, case studies, and latest industry news that help data teams win.  \\\\n Live chat Live chat will provide the quickest answer to any of your questions.\\\\nOops! Something went wrong while submitting the form.\\\\n⨉\"}, {\"url\": \"https://botpress.com/blog/best-large-language-models\", \"title\": \"Best Large Language Models in 2025 (Open Source + Hosted LLMs)\", \"content\": \"Discord Join thousands of peers and share ideas Docs Comprehensive guides and references API Reference material for use with external systems LLM Ranking Real-time data to help you choose the right models Videos Tutorials, demos, and product walkthroughs CLI Command-line tools for faster building Whether it\\'s adding natural language processing capabilities to an existing app or building new AI-driven features, APIs allow developers to use LLMs for tasks like sentiment analysis, language translation, or content generation without needing to build or train models themselves. Developed by the Technology Innovation Institute and released on September 6, 2023, Falcon 180B features a staggering 180 billion parameters, making it one of the largest and most powerful open-source LLMs. It was designed to excel in tasks like translation, text generation, and research\\u200b.\", \"score\": 0.7625837, \"raw_content\": \"The Best Large Language Models in 2025 (Open Source + Hosted)\\\\n\\\\nPlatform\\\\nFeatures\\\\nAgent Studio Build and customize your agent rapidlyAutonomous Engine Use LLMs to guide conversations and tasksKnowledge Bases Train your bot with custom knowledge sourcesTables Store and manage conversation data\\\\nChannels\\\\n WhatsApp Instagram Messenger Slack\\\\nAll channels\\\\nIntegrations\\\\n HubSpot Notion Jira Calendly\\\\nAll integrations\\\\nLLM Providers\\\\n OpenAI Anthropic Groq Hugging Face\\\\nAll LLMs\\\\nSolutions\\\\nFor\\\\nEnterprise Automate mission-critical production workflowsAgencies Provide sophisticated agent servicesDevelopers Explore a robust API for agent development\\\\n Customer Stories Discover from successful customers how Botpress is transforming business worldwide.\\\\nBy Industry\\\\nEcommerce\\\\nEducation\\\\nFinance\\\\nHospitality\\\\nAll industries\\\\nBy Department\\\\nSales\\\\nEngineering\\\\nProduct\\\\nITSM\\\\nAll departments\\\\nBy Use Case\\\\nShopping Assistant\\\\nLead Generation\\\\nEmployee Experience\\\\nTicket Management\\\\nAll use cases\\\\nResources\\\\nEssential\\\\n Academy Learn to build through curated courses Library Resources to enhance your AI workflows Blog Insights and updates on Botpress and AI agents\\\\nbuilding\\\\n Discord Join thousands of peers and share ideas Docs Comprehensive guides and references API Reference material for use with external systems LLM Ranking Real-time data to help you choose the right models Videos Tutorials, demos, and product walkthroughs CLI Command-line tools for faster building\\\\nPartners\\\\n Become a Partner Join our network of certified experts Hire an Expert Connect with partners and consultants\\\\nDocs\\\\nPricing\\\\nContactGet started for free\\\\nback to blog\\\\nAI Basics\\\\nFor Builders\\\\nThe Best Large Language Models in 2025 (Open Source + Hosted)\\\\nOctober 19, 2024\\\\n·\\\\nUpdated on\\\\nLarge language models are increasing in power and popularity. Here are some of the best available for users today.\\\\nBotpress\\\\n\\\\nWith so many large language models (LLMs), it can be hard to decide which to use.\\\\nThe latest models are constantly pushing the boundaries of what\\'s possible in artificial intelligence. As these models continue to shape the way we interact with technology, the possibilities for generative AI applications are limitless.\\\\nWe are now presented with a powerful toolset at our fingertips. It\\'s suddenly easy to create AI agents and AI chatbots, or to use an LLM as a personal AI assistant in day-to-day tasks.\\\\nThe world of LLMs is only just beginning.\\\\nWhat are large language models?\\\\nA large language model (LLM) is an advanced type of artificial intelligence designed to understand and generate human-like text.\\\\nLLMs use deep learning algorithms that have been trained on vast amounts of data to recognize patterns and context in language.\\\\nAfter training, they use natural language processing perform tasks like translation, content creation, summarization, and answering questions.\\\\n\\\\nBuild LLM chatbots\\\\nBuild LLM-powered AI agents\\\\nStart now\\\\nNo credit card required\\\\nHow to use a large language model\\\\nThere are infinite ways to apply the power of an LLM. But most fall into one of 3 main categories:\\\\n1. AI agents and chatbots\\\\nLLMs are commonly integrated into chatbots and AI agents. These days, most conversational AI is powered by an LLM.\\\\nYou can see the most popular LLMs for chatbots and AI agents on our real-time ranking of the LLMs used by 500,000+ bot builders.\\\\nThese models can handle complex queries, generate contextual responses, and even manage dynamic conversations that evolve based on user input.\\\\nCommon AI agents include customer support chatbots and HR bots. But as the technology expands, so do use cases. Now businesses can build bespoke chatbots for hotels, sales chatbots, or even chatbots for real estate.\\\\nBy understanding the intent and context behind questions, LLM-powered chatbots can be used for customer support, virtual assistants, or even in business process automation.\\\\n2. Daily use\\\\nLLMs have increasingly made their way into everyday tasks. People use them for content generation, text summarization, language translation, and even creative projects, like writing poems or generating art descriptions.\\\\nThere are plenty of tools that use LLM APIs to help with daily tasks. Software like writing assistants or code completion tools are typically powered by LLMs these days.\\\\n3. API use\\\\nIf you\\'re a developer, you can be the one using an API to build other software and tools.\\\\nLLMs can be accessed via APIs, which provide flexibility for integrating language models into various software applications.\\\\nWhether it\\'s adding natural language processing capabilities to an existing app or building new AI-driven features, APIs allow developers to use LLMs for tasks like sentiment analysis, language translation, or content generation without needing to build or train models themselves.\\\\n\\\\nDeploying AI Agents?\\\\nRead our Blueprint for AI Agent Implementation\\\\nRead Now\\\\nAccess is always free\\\\nThe 5 best LLMs\\\\nMost LLM use is hosted software, which means it\\'s maintained and run by a third-party provider on their servers, rather than on the user’s local system.\\\\nUsers access it over the internet, benefiting from simplified maintenance, updates, and infrastructure management handled by the host.\\\\nHere are the 5 best hosted LLMs available today:\\\\n1. GPT-4o\\\\nOpenAI’s latest multimodal model, GPT-4o, was released in May 2024 and integrates text, image, video, and voice capabilities.\\\\nThis model is 50% cheaper and twice as fast as GPT-4, making it highly efficient for a wide range of tasks. It stands out with its Voice-to-Voice function, allowing for audio responses in real-time, with a latency of just 320 milliseconds.\\\\nGPT-4o also improves performance in non-English languages and offers a more interactive experience\\u200b.\\\\n2. Claude 3.5\\\\nLaunched by Anthropic in June 2024, Claude 3.5 is known for its ethical design and strong performance across various benchmarks.\\\\nAvailable through an API, it continues Anthropic’s focus on safer AI interactions. While the number of parameters remains undisclosed, its advanced capabilities make it a strong competitor for tasks involving conversational AI and content generation\\u200b.\\\\n3. Grok-1\\\\nDeveloped by Elon Musk’s xAI, Grok-1 debuted in November 2023 with 314 billion parameters, focusing on generating responses with personality and real-time data from X (formerly Twitter).\\\\nIn August 2024, xAI released Grok-2 and Grok-2 mini, which have reportedly outperformed GPT-4o in several performance metrics\\u200b.\\\\n4. Gemini 1.5\\\\nGoogle\\'s Gemini 1.5 focuses on improving multilingual capabilities and translation accuracy, making it particularly valuable for global businesses.\\\\nReleased in mid-2024, it is also designed to enhance tasks like text generation, customer interaction, and more\\u200b.\\\\n5. Inflection-2.5\\\\nInflection AI’s Inflection-2.5 powers the conversational AI assistant Pi, released in March 2024.\\\\nThis model achieves over 94% of GPT-4’s performance while using only 40% of the training computational resources.\\\\nIts efficiency has led to over a million daily active users on Pi, making it one of the most popular conversational models today\\u200b\\\\nThe 5 best open-source LLMs\\\\nIf you\\'re a builder, open source LLMs are your friend. Open-source software refers to code that is publicly available for anyone to view, modify, and distribute.\\\\nIt fosters collaboration and transparency, allowing developers to adapt the software to their specific needs while contributing to its improvement.\\\\nHere are the top 5 open-source LLMs available today:\\\\n1. LLaMA 3.1\\\\nMeta’s latest open-source LLM, LLaMA 3, launched in April 2024, with sizes ranging from 8 billion to 70 billion parameters.\\\\nIt offers improved reasoning and coding abilities and is open-source for developers. LLaMA 3 is designed to outperform models like Claude 3 and Gemini 1.5, making it a top choice for a range of real-world tasks.\\\\n2. Mistral 7B\\\\nReleased by Mistral AI on September 27, 2023, this model has 7.3 billion parameters but manages to outperform larger models in many benchmarks.\\\\nIts smaller size makes it highly efficient, ideal for self-hosting, and versatile across NLP tasks\\u200b.\\\\n3. Falcon 180B\\\\nDeveloped by the Technology Innovation Institute and released on September 6, 2023, Falcon 180B features a staggering 180 billion parameters, making it one of the largest and most powerful open-source LLMs.\\\\nIt was designed to excel in tasks like translation, text generation, and research\\u200b.\\\\n4. OLMo\\\\nCreated by the Allen Institute for AI, OLMo focuses on transparency and reproducibility, making it highly valuable for research purposes.\\\\nIt’s particularly favored by researchers who need full insight into the data and training process\\u200b.\\\\n5. Qwen-1.5\\\\nAlibaba’s Qwen-1.5 is their open-source LLM, which competes with models from Meta and Google in both capability and cost-effectiveness.\\\\nIt\\'s aimed at high-performance tasks in language processing and is designed to scale across various applications, from e-commerce to customer service\\u200b.\\\\nDeploy an LLM-powered AI agent\\\\nLeverage LLMs in your day-to-day with custom AI agents.\\\\nWith the plethora of chatbot platforms out there, it’s easy to set up an AI agent to fulfill your specific needs.\\\\nBotpress is an endlessly extensible AI automation platform. With a pre-built library of integrations, drag-and-drop workflows, and comprehensive tutorials, it\\'s accessible for builders at all stages of expertise.\\\\nPlug in any LLM to power your AI project, across any use case.\\\\nStart building today. It\\'s free.\\\\n\\\\nBuild LLM chatbots\\\\nBuild LLM-powered AI agents\\\\nStart now\\\\nNo credit card required\\\\nTable of Contents\\\\nStep 1. the title of the step goes here as expected\\\\nStep 1. the title of the step goes here as expected\\\\n\\\\nLearn how to build AI agents\\\\nShare this on:\\\\n\\\\n\\\\n##### What is RCS?\\\\nMarc Mercier\\\\nNov 29\\\\n\\\\n##### Ultimate Guide to Artificial Intelligence (AI) and Augmented Reality (AR)\\\\nBotpress\\\\nSep 23\\\\n\\\\n##### What is Agentic AI?\\\\nSarah Chudleigh\\\\nDec 11\\\\nBuild Better with Botpress\\\\nGet started today - it’s free!\\\\nStart building now \\\\n\\\\nAll Systems Operational\\\\nSOC 2\\\\nCertified\\\\nGDPR\\\\nCompliant\\\\n\\\\n© 2025\\\\n\\\\n\\\\nPlatform\\\\nPricingAgent StudioAutonomous EngineKnowledge BasesTables\\\\nHub\\\\nIntegrationsChannelsLLMs\\\\nResources\\\\nTalk to SalesDocumentationHire an ExpertVideosCustomer StoriesAPI ReferenceBlogStatusv12 Resources\\\\nCommunity\\\\nCommunity SupportBecome a PartnerBecome an AmbassadorBecome an Affiliate\\\\nCompany\\\\nAboutCareersNews & PressLegalPrivacy\\\\n© Botpress 2025\"}, {\"url\": \"https://en.wikipedia.org/wiki/List_of_large_language_models\", \"title\": \"List of large language models - Wikipedia\", \"content\": \"GLaM (Generalist Language Model)    December 2021   Google  1200[35]    1.6 trillion tokens[35] 5600[35]    Proprietary Sparse mixture of experts model, making it more expensive to train but cheaper to run inference compared to GPT-3. PaLM (Pathways Language Model)  April 2022  Google  540[43] 768 billion tokens[42]  29,250[38]  Proprietary Trained for ~60 days on ~6000 TPU v4 chips.[38] As of October\\xa02024, it is the largest dense Transformer published. Mixtral 8x7B    December 2023   Mistral AI  46.7    Unknown Unknown Apache 2.0  Outperforms GPT-3.5 and Llama 2 70B on many benchmarks.[82] Mixture of experts model, with 12.9 billion parameters activated per token.[83] ^ a b c d Table 20 and page 66 of PaLM: Scaling Language Modeling with Pathways Archived 2023-06-10 at the Wayback Machine\", \"score\": 0.7230167, \"raw_content\": \"Jump to content\\\\nMain menu\\\\nSearch\\\\nDonate\\\\nCreate account\\\\nLog in\\\\nPersonal tools\\\\nToggle the table of contents\\\\nList of large language models\\\\nAdd languages\\\\nArticle\\\\nTalk\\\\nRead\\\\nEdit\\\\nView history\\\\nTools\\\\nFrom Wikipedia, the free encyclopedia\\\\nA large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\\\nThis page lists notable large language models.\\\\nFor the training cost column, 1 petaFLOP-day = 1 petaFLOP/sec × 1 day = 8.64E19 FLOP. Also, only the largest model\\'s cost is written.\\\\nName    Release date[a] Developer   Number of parameters (billion) [b]  Corpus size Training cost (petaFLOP-day)    License[c]  Notes\\\\nGPT-1   June 2018   OpenAI  0.117       1[1]    MIT[2]  First GPT model, decoder-only transformer. Trained for 30 days on 8 P600 GPUs.\\\\nBERT    October 2018    Google  0.340[3]    3.3 billion words[3]    9[4]    Apache 2.0[5]   An early and influential language model.[6]Encoder-only and thus not built to be prompted or generative.[7] Training took 4 days on 64 TPUv2 chips.[8]\\\\nT5  October 2019    Google  11[9]   34 billion tokens[9]        Apache 2.0[10]  Base model for many Google projects, such as Imagen.[11]\\\\nXLNet   June 2019   Google  0.340[12]   33 billion words    330 Apache 2.0[13]  An alternative to BERT; designed as encoder-only. Trained on 512 TPU v3 chips for 5.5 days.[14]\\\\nGPT-2   February 2019   OpenAI  1.5[15] 40GB[16] (~10 billion tokens)[17]   28[18]  MIT[19] Trained on 32 TPUv3 chips for 1 week.[18]\\\\nGPT-3   May 2020    OpenAI  175[20] 300 billion tokens[17]  3640[21]    proprietary A fine-tuned variant of GPT-3, termed GPT-3.5, was made available to the public through a web interface called ChatGPT in 2022.[22]\\\\nGPT-Neo March 2021  EleutherAI  2.7[23] 825 GiB[24]     MIT[25] The first of a series of free GPT-3 alternatives released by EleutherAI. GPT-Neo outperformed an equivalent-size GPT-3 model on some benchmarks, but was significantly worse than the largest GPT-3.[25]\\\\nGPT-J   June 2021   EleutherAI  6[26]   825 GiB[24] 200[27] Apache 2.0  GPT-3-style language model\\\\nMegatron-Turing NLG October 2021[28]    Microsoft and Nvidia    530[29] 338.6 billion tokens[29]    38000[30]   Restricted web access   Trained for 3 months on over 2000 A100 GPUs on the NVIDIA Selene Supercomputer, for over 3 million GPU-hours.[30]\\\\nErnie 3.0 Titan December 2021   Baidu   260[31] 4 Tb        Proprietary Chinese-language LLM. Ernie Bot is based on this model.\\\\nClaude[32]  December 2021   Anthropic   52[33]  400 billion tokens[33]      beta    Fine-tuned for desirable behavior in conversations.[34]\\\\nGLaM (Generalist Language Model)    December 2021   Google  1200[35]    1.6 trillion tokens[35] 5600[35]    Proprietary Sparse mixture of experts model, making it more expensive to train but cheaper to run inference compared to GPT-3.\\\\nGopher  December 2021   DeepMind    280[36] 300 billion tokens[37]  5833[38]    Proprietary Later developed into the Chinchilla model.\\\\nLaMDA (Language Models for Dialog Applications) January 2022    Google  137[39] 1.56T words,[39] 168 billion tokens[37] 4110[40]    Proprietary Specialized for response generation in conversations.\\\\nGPT-NeoX    February 2022   EleutherAI  20[41]  825 GiB[24] 740[27] Apache 2.0  based on the Megatron architecture\\\\nChinchilla  March 2022  DeepMind    70[42]  1.4 trillion tokens[42][37] 6805[38]    Proprietary Reduced-parameter model trained on more data. Used in the Sparrow bot. Often cited for its neural scaling law.\\\\nPaLM (Pathways Language Model)  April 2022  Google  540[43] 768 billion tokens[42]  29,250[38]  Proprietary Trained for ~60 days on ~6000 TPU v4 chips.[38] As of October\\xa02024, it is the largest dense Transformer published.\\\\nOPT (Open Pretrained Transformer)   May 2022    Meta    175[44] 180 billion tokens[45]  310[27] Non-commercial research[d]  GPT-3 architecture with some adaptations from Megatron. Uniquely, the training logbook written by the team was published.[46]\\\\nYaLM 100B   June 2022   Yandex  100[47] 1.7TB[47]       Apache 2.0  English-Russian model based on Microsoft\\'s Megatron-LM.\\\\nMinerva June 2022   Google  540[48] 38.5B tokens from webpages filtered for mathematical content and from papers submitted to the arXiv preprint server[48]     Proprietary For solving \\\\\"mathematical and scientific questions using step-by-step reasoning\\\\\".[49] Initialized from PaLM models, then finetuned on mathematical and scientific data.\\\\nBLOOM   July 2022   Large collaboration led by Hugging Face 175[50] 350 billion tokens (1.6TB)[51]      Responsible AI  Essentially GPT-3 but trained on a multi-lingual corpus (30% English excluding programming languages)\\\\nGalactica   November 2022   Meta    120 106 billion tokens[52]  unknown CC-BY-NC-4.0    Trained on scientific text and modalities.\\\\nAlexaTM (Teacher Models)    November 2022   Amazon  20[53]  1.3 trillion[54]        proprietary[55] bidirectional sequence-to-sequence architecture\\\\nNeuro-sama  December 2022   Independent Unknown Unknown     privately-owned A language model designed for live-streaming on Twitch.\\\\nLLaMA (Large Language Model Meta AI)    February 2023   Meta AI 65[56]  1.4 trillion[56]    6300[57]    Non-commercial research[e]  Corpus has 20 languages. \\\\\"Overtrained\\\\\" (compared to Chinchilla scaling law) for better performance with fewer parameters.[56]\\\\nGPT-4   March 2023  OpenAI  Unknown[f] (According to rumors: 1760)[59]  Unknown Unknown proprietary Available for ChatGPT Plus users and used in several products.\\\\nChameleon   June 2024   Meta AI 34[60]  4.4 trillion      \\\\nCerebras-GPT    March 2023  Cerebras    13[61]      270[27] Apache 2.0  Trained with Chinchilla formula.\\\\nFalcon  March 2023  Technology Innovation Institute 40[62]  1 trillion tokens, from RefinedWeb (filtered web text corpus)[63] plus some \\\\\"curated corpora\\\\\".[64]  2800[57]    Apache 2.0[65]\\\\nBloombergGPT    March 2023  Bloomberg L.P.  50  363 billion token dataset based on Bloomberg\\'s data sources, plus 345 billion tokens from general purpose datasets[66]      Proprietary Trained on financial data from proprietary sources, for financial tasks.\\\\nPanGu-Σ March 2023  Huawei  1085    329 billion tokens[67]      Proprietary \\\\nOpenAssistant[68]   March 2023  LAION   17  1.5 trillion tokens     Apache 2.0  Trained on crowdsourced open data\\\\nJurassic-2[69]  March 2023  AI21 Labs   Unknown Unknown     Proprietary Multilingual[70]\\\\nPaLM 2 (Pathways Language Model 2)  May 2023    Google  340[71] 3.6 trillion tokens[71] 85,000[57]  Proprietary Was used in Bard chatbot.[72]\\\\nLlama 2 July 2023   Meta AI 70[73]  2 trillion tokens[73]   21,000  Llama 2 license 1.7 million A100-hours.[74]\\\\nClaude 2    July 2023   Anthropic   Unknown Unknown Unknown Proprietary Used in Claude chatbot.[75]\\\\nGranite 13b July 2023   IBM Unknown Unknown Unknown Proprietary Used in IBM Watsonx.[76]\\\\nMistral 7B  September 2023  Mistral AI  7.3[77] Unknown     Apache 2.0\\\\nClaude 2.1  November 2023   Anthropic   Unknown Unknown Unknown Proprietary Used in Claude chatbot. Has a context window of 200,000 tokens, or ~500 pages.[78]\\\\nGrok-1[79]  November 2023   xAI 314 Unknown Unknown Apache 2.0  Used in Grok chatbot. Grok-1 has a context length of 8,192 tokens and has access to X (Twitter).[80]\\\\nGemini 1.0  December 2023   Google DeepMind Unknown Unknown Unknown Proprietary Multimodal model, comes in three sizes. Used in the chatbot of the same name.[81]\\\\nMixtral 8x7B    December 2023   Mistral AI  46.7    Unknown Unknown Apache 2.0  Outperforms GPT-3.5 and Llama 2 70B on many benchmarks.[82] Mixture of experts model, with 12.9 billion parameters activated per token.[83]\\\\nMixtral 8x22B   April 2024  Mistral AI  141 Unknown Unknown Apache 2.0  [84]\\\\nPhi-2   December 2023   Microsoft   2.7 1.4T tokens 419[85] MIT Trained on real and synthetic \\\\\"textbook-quality\\\\\" data, for 14 days on 96 A100 GPUs.[85]\\\\nGemini 1.5  February 2024   Google DeepMind Unknown Unknown Unknown Proprietary Multimodal model, based on a Mixture-of-Experts (MoE) architecture. Context window above 1 million tokens.[86]\\\\nGemini Ultra    February 2024   Google DeepMind Unknown Unknown Unknown   \\\\nGemma   February 2024   Google DeepMind 7   6T tokens   Unknown Gemma Terms of Use[87]\\\\nClaude 3    March 2024  Anthropic   Unknown Unknown Unknown Proprietary Includes three models, Haiku, Sonnet, and Opus.[88]\\\\nNova    October 2024    Rubik\\'s AI  Unknown Unknown Unknown Proprietary Includes three models, Nova-Instant, Nova-Air, and Nova-Pro.\\\\nDBRX    March 2024  Databricks and Mosaic ML    136 12T Tokens      Databricks Open Model License   Training cost 10 million USD.\\\\nFugaku-LLM  May 2024    Fujitsu, Tokyo Institute of Technology, etc.    13  380B Tokens         The largest model ever trained on CPU-only, on the Fugaku.[89]\\\\nPhi-3   April 2024  Microsoft   14[90]  4.8T Tokens     MIT Microsoft markets them as \\\\\"small language model\\\\\".[91]\\\\nGranite Code Models May 2024    IBM Unknown Unknown Unknown Apache 2.0\\\\nQwen2   June 2024   Alibaba Cloud   72[92]  3T Tokens           Multiple sizes, the smallest being 0.5B.\\\\nNemotron-4  June 2024   Nvidia  340 9T Tokens   200,000 NVIDIA Open Model License   Trained for 1 epoch. Trained on 6144 H100 GPUs between December 2023 and May 2024.[93][94]\\\\nLlama 3.1   July 2024   Meta AI 405 15.6T tokens    440,000 Llama 3 license 405B version took 31 million hours on H100-80GB, at 3.8E25 FLOPs.[95][96]\\\\nDeepSeek V3 December 2024   DeepSeek    671 14.8T tokens    440,00  DeepSeek License    2.788M hours on H800 GPUs.[97]\\\\nAmazon Nova December 2024   Amazon  Unknown Unknown Unknown Proprietary Includes three models, Nova Micro, Nova Lite, and Nova Pro[98]\\\\nSee also[edit]\\\\nList of chatbots\\\\nNotes[edit]\\\\n^ This is the date that documentation describing the model\\'s architecture was first released.\\\\n^ In many cases, researchers release or report on multiple versions of a model having different sizes. In these cases, the size of the largest model is listed here.\\\\n^ This is the license of the pre-trained model weights. In almost all cases the training code itself is open-source or can be easily replicated.\\\\n^ The smaller models including 66B are publicly available, while the 175B model is available on request.\\\\n^ Facebook\\'s license and distribution scheme restricted access to approved researchers, but the model weights were leaked and became widely available.\\\\n^ As stated in Technical report: \\\\\"Given both the competitive landscape and the safety implications of large-scale models like GPT-4, this report contains no further details about the architecture (including model size), hardware, training compute, dataset construction, training method ...\\\\\"[58]\\\\nReferences[edit]\\\\n^ \\\\\"Improving language understanding with unsupervised learning\\\\\". openai.com. June 11, 2018. Archived from the original on 2023-03-18. Retrieved 2023-03-18.\\\\n^ \\\\\"finetune-transformer-lm\\\\\". GitHub. Archived from the original on 19 May 2023. Retrieved 2 January 2024.\\\\n^ a b Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (11 October 2018). \\\\\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\\\\". arXiv:1810.04805v2 [cs.CL].\\\\n^ Prickett, Nicole Hemsoth (2021-08-24). \\\\\"Cerebras Shifts Architecture To Meet Massive AI/ML Models\\\\\". The Next Platform. Archived from the original on 2023-06-20. Retrieved 2023-06-20.\\\\n^ \\\\\"BERT\\\\\". March 13, 2023. Archived from the original on January 13, 2021. Retrieved March 13, 2023 – via GitHub.\\\\n^ Manning, Christopher D. (2022). \\\\\"Human Language Understanding & Reasoning\\\\\". Daedalus. 151 (2): 127–138. doi:10.1162/daed_a_01905. S2CID\\xa0248377870. Archived from the original on 2023-11-17. Retrieved 2023-03-09.\\\\n^ Patel, Ajay; Li, Bryan; Rasooli, Mohammad Sadegh; Constant, Noah; Raffel, Colin; Callison-Burch, Chris (2022). \\\\\"Bidirectional Language Models Are Also Few-shot Learners\\\\\". arXiv:2209.14500 [cs.LG].\\\\n^ Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (11 October 2018). \\\\\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\\\\". arXiv:1810.04805v2 [cs.CL].\\\\n^ a b Raffel, Colin; Shazeer, Noam; Roberts, Adam; Lee, Katherine; Narang, Sharan; Matena, Michael; Zhou, Yanqi; Li, Wei; Liu, Peter J. (2020). \\\\\"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\\\\". Journal of Machine Learning Research. 21 (140): 1–67. arXiv:1910.10683. ISSN\\xa01533-7928.\\\\n^ google-research/text-to-text-transfer-transformer, Google Research, 2024-04-02, archived from the original on 2024-03-29, retrieved 2024-04-04\\\\n^ \\\\\"Imagen: Text-to-Image Diffusion Models\\\\\". imagen.research.google. Archived from the original on 2024-03-27. Retrieved 2024-04-04.\\\\n^ \\\\\"Pretrained models — transformers 2.0.0 documentation\\\\\". huggingface.co. Archived from the original on 2024-08-05. Retrieved 2024-08-05.\\\\n^ \\\\\"xlnet\\\\\". GitHub. Archived from the original on 2 January 2024. Retrieved 2 January 2024.\\\\n^ Yang, Zhilin; Dai, Zihang; Yang, Yiming; Carbonell, Jaime; Salakhutdinov, Ruslan; Le, Quoc V. (2 January 2020). \\\\\"XLNet: Generalized Autoregressive Pretraining for Language Understanding\\\\\". arXiv:1906.08237 [cs.CL].\\\\n^ \\\\\"GPT-2: 1.5B Release\\\\\". OpenAI. 2019-11-05. Archived from the original on 2019-11-14. Retrieved 2019-11-14.\\\\n^ \\\\\"Better language models and their implications\\\\\". openai.com. Archived from the original on 2023-03-16. Retrieved 2023-03-13.\\\\n^ a b \\\\\"OpenAI\\'s GPT-3 Language Model: A Technical Overview\\\\\". lambdalabs.com. 3 June 2020. Archived from the original on 27 March 2023. Retrieved 13 March 2023.\\\\n^ a b \\\\\"openai-community/gpt2-xl · Hugging Face\\\\\". huggingface.co. Archived from the original on 2024-07-24. Retrieved 2024-07-24.\\\\n^ \\\\\"gpt-2\\\\\". GitHub. Archived from the original on 11 March 2023. Retrieved 13 March 2023.\\\\n^ Wiggers, Kyle (28 April 2022). \\\\\"The emerging types of language models and why they matter\\\\\". TechCrunch. Archived from the original on 16 March 2023. Retrieved 9 March 2023.\\\\n^ Table D.1 in Brown, Tom B.; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Gretchen; Henighan, Tom; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel M.; Wu, Jeffrey; Winter, Clemens; Hesse, Christopher; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Scott; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (May 28, 2020). \\\\\"Language Models are Few-Shot Learners\\\\\". arXiv:2005.14165v4 [cs.CL].\\\\n^ \\\\\"ChatGPT: Optimizing Language Models for Dialogue\\\\\". OpenAI. 2022-11-30. Archived from the original on 2022-11-30. Retrieved 2023-01-13.\\\\n^ \\\\\"GPT Neo\\\\\". March 15, 2023. Archived from the original on March 12, 2023. Retrieved March 12, 2023 – via GitHub.\\\\n^ a b c Gao, Leo; Biderman, Stella; Black, Sid; Golding, Laurence; Hoppe, Travis; Foster, Charles; Phang, Jason; He, Horace; Thite, Anish; Nabeshima, Noa; Presser, Shawn; Leahy, Connor (31 December 2020). \\\\\"The Pile: An 800GB Dataset of Diverse Text for Language Modeling\\\\\". arXiv:2101.00027 [cs.CL].\\\\n^ a b Iyer, Abhishek (15 May 2021). \\\\\"GPT-3\\'s free alternative GPT-Neo is something to be excited about\\\\\". VentureBeat. Archived from the original on 9 March 2023. Retrieved 13 March 2023.\\\\n^ \\\\\"GPT-J-6B: An Introduction to the Largest Open Source GPT Model | Forefront\\\\\". www.forefront.ai. Archived from the original on 2023-03-09. Retrieved 2023-02-28.\\\\n^ a b c d Dey, Nolan; Gosal, Gurpreet; Zhiming; Chen; Khachane, Hemant; Marshall, William; Pathria, Ribhu; Tom, Marvin; Hestness, Joel (2023-04-01). \\\\\"Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster\\\\\". arXiv:2304.03208 [cs.LG].\\\\n^ Alvi, Ali; Kharya, Paresh (11 October 2021). \\\\\"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World\\'s Largest and Most Powerful Generative Language Model\\\\\". Microsoft Research. Archived from the original on 13 March 2023. Retrieved 13 March 2023.\\\\n^ a b Smith, Shaden; Patwary, Mostofa; Norick, Brandon; LeGresley, Patrick; Rajbhandari, Samyam; Casper, Jared; Liu, Zhun; Prabhumoye, Shrimai; Zerveas, George; Korthikanti, Vijay; Zhang, Elton; Child, Rewon; Aminabadi, Reza Yazdani; Bernauer, Julie; Song, Xia (2022-02-04). \\\\\"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model\\\\\". arXiv:2201.11990 [cs.CL].\\\\n^ a b Rajbhandari, Samyam; Li, Conglong; Yao, Zhewei; Zhang, Minjia; Aminabadi, Reza Yazdani; Awan, Ammar Ahmad; Rasley, Jeff; He, Yuxiong (2022-07-21), DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale, arXiv:2201.05596\\\\n^ Wang, Shuohuan; Sun, Yu; Xiang, Yang; Wu, Zhihua; Ding, Siyu; Gong, Weibao; Feng, Shikun; Shang, Junyuan; Zhao, Yanbin; Pang, Chao; Liu, Jiaxiang; Chen, Xuyi; Lu, Yuxiang; Liu, Weixin; Wang, Xi; Bai, Yangfan; Chen, Qiuliang; Zhao, Li; Li, Shiyong; Sun, Peng; Yu, Dianhai; Ma, Yanjun; Tian, Hao; Wu, Hua; Wu, Tian; Zeng, Wei; Li, Ge; Gao, Wen; Wang, Haifeng (December 23, 2021). \\\\\"ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation\\\\\". arXiv:2112.12731 [cs.CL].\\\\n^ \\\\\"Product\\\\\". Anthropic. Archived from the original on 16 March 2023. Retrieved 14 March 2023.\\\\n^ a b Askell, Amanda; Bai, Yuntao; Chen, Anna; et\\xa0al. (9 December 2021). \\\\\"A General Language Assistant as a Laboratory for Alignment\\\\\". arXiv:2112.00861 [cs.CL].\\\\n^ Bai, Yuntao; Kadavath, Saurav; Kundu, Sandipan; et\\xa0al. (15 December 2022). \\\\\"Constitutional AI: Harmlessness from AI Feedback\\\\\". arXiv:2212.08073 [cs.CL].\\\\n^ a b c Dai, Andrew M; Du, Nan (December 9, 2021). \\\\\"More Efficient In-Context Learning with GLaM\\\\\". ai.googleblog.com. Archived from the original on 2023-03-12. Retrieved 2023-03-09.\\\\n^ \\\\\"Language modelling at scale: Gopher, ethical considerations, and retrieval\\\\\". www.deepmind.com. 8 December 2021. Archived from the original on 20 March 2023. Retrieved 20 March 2023.\\\\n^ a b c Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; et\\xa0al. (29 March 2022). \\\\\"Training Compute-Optimal Large Language Models\\\\\". arXiv:2203.15556 [cs.CL].\\\\n^ a b c d Table 20 and page 66 of PaLM: Scaling Language Modeling with Pathways Archived 2023-06-10 at the Wayback Machine\\\\n^ a b Cheng, Heng-Tze; Thoppilan, Romal (January 21, 2022). \\\\\"LaMDA: Towards Safe, Grounded, and High-Quality Dialog Models for Everything\\\\\". ai.googleblog.com. Archived from the original on 2022-03-25. Retrieved 2023-03-09.\\\\n^ Thoppilan, Romal; De Freitas, Daniel; Hall, Jamie; Shazeer, Noam; Kulshreshtha, Apoorv; Cheng, Heng-Tze; Jin, Alicia; Bos, Taylor; Baker, Leslie; Du, Yu; Li, YaGuang; Lee, Hongrae; Zheng, Huaixiu Steven; Ghafouri, Amin; Menegali, Marcelo (2022-01-01). \\\\\"LaMDA: Language Models for Dialog Applications\\\\\". arXiv:2201.08239 [cs.CL].\\\\n^ Black, Sidney; Biderman, Stella; Hallahan, Eric; et\\xa0al. (2022-05-01). GPT-NeoX-20B: An Open-Source Autoregressive Language Model. Proceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models. Vol.\\xa0Proceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models. pp.\\xa095–136. Archived from the original on 2022-12-10. Retrieved 2022-12-19.\\\\n^ a b c Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; Sifre, Laurent (12 April 2022). \\\\\"An empirical analysis of compute-optimal large language model training\\\\\". Deepmind Blog. Archived from the original on 13 April 2022. Retrieved 9 March 2023.\\\\n^ Narang, Sharan; Chowdhery, Aakanksha (April 4, 2022). \\\\\"Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance\\\\\". ai.googleblog.com. Archived from the original on 2022-04-04. Retrieved 2023-03-09.\\\\n^ Susan Zhang; Mona Diab; Luke Zettlemoyer. \\\\\"Democratizing access to large-scale language models with OPT-175B\\\\\". ai.facebook.com. Archived from the original on 2023-03-12. Retrieved 2023-03-12.\\\\n^ Zhang, Susan; Roller, Stephen; Goyal, Naman; Artetxe, Mikel; Chen, Moya; Chen, Shuohui; Dewan, Christopher; Diab, Mona; Li, Xian; Lin, Xi Victoria; Mihaylov, Todor; Ott, Myle; Shleifer, Sam; Shuster, Kurt; Simig, Daniel; Koura, Punit Singh; Sridhar, Anjali; Wang, Tianlu; Zettlemoyer, Luke (21 June 2022). \\\\\"OPT: Open Pre-trained Transformer Language Models\\\\\". arXiv:2205.01068 [cs.CL].\\\\n^ \\\\\"metaseq/projects/OPT/chronicles at main · facebookresearch/metaseq\\\\\". GitHub. Retrieved 2024-10-18.\\\\n^ a b Khrushchev, Mikhail; Vasilev, Ruslan; Petrov, Alexey; Zinov, Nikolay (2022-06-22), YaLM 100B, archived from the original on 2023-06-16, retrieved 2023-03-18\\\\n^ a b Lewkowycz, Aitor; Andreassen, Anders; Dohan, David; Dyer, Ethan; Michalewski, Henryk; Ramasesh, Vinay; Slone, Ambrose; Anil, Cem; Schlag, Imanol; Gutman-Solo, Theo; Wu, Yuhuai; Neyshabur, Behnam; Gur-Ari, Guy; Misra, Vedant (30 June 2022). \\\\\"Solving Quantitative Reasoning Problems with Language Models\\\\\". arXiv:2206.14858 [cs.CL].\\\\n^ \\\\\"Minerva: Solving Quantitative Reasoning Problems with Language Models\\\\\". ai.googleblog.com. 30 June 2022. Retrieved 20 March 2023.\\\\n^ Ananthaswamy, Anil (8 March 2023). \\\\\"In AI, is bigger always better?\\\\\". Nature. 615 (7951): 202–205. Bibcode:2023Natur.615..202A. doi:10.1038/d41586-023-00641-w. PMID\\xa036890378. S2CID\\xa0257380916. Archived from the original on 16 March 2023. Retrieved 9 March 2023.\\\\n^ \\\\\"bigscience/bloom · Hugging Face\\\\\". huggingface.co. Archived from the original on 2023-04-12. Retrieved 2023-03-13.\\\\n^ Taylor, Ross; Kardas, Marcin; Cucurull, Guillem; Scialom, Thomas; Hartshorn, Anthony; Saravia, Elvis; Poulton, Andrew; Kerkez, Viktor; Stojnic, Robert (16 November 2022). \\\\\"Galactica: A Large Language Model for Science\\\\\". arXiv:2211.09085 [cs.CL].\\\\n^ \\\\\"20B-parameter Alexa model sets new marks in few-shot learning\\\\\". Amazon Science. 2 August 2022. Archived from the original on 15 March 2023. Retrieved 12 March 2023.\\\\n^ Soltan, Saleh; Ananthakrishnan, Shankar; FitzGerald, Jack; et\\xa0al. (3 August 2022). \\\\\"AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model\\\\\". arXiv:2208.01448 [cs.CL].\\\\n^ \\\\\"AlexaTM 20B is now available in Amazon SageMaker JumpStart | AWS Machine Learning Blog\\\\\". aws.amazon.com. 17 November 2022. Archived from the original on 13 March 2023. Retrieved 13 March 2023.\\\\n^ a b c \\\\\"Introducing LLaMA: A foundational, 65-billion-parameter large language model\\\\\". Meta AI. 24 February 2023. Archived from the original on 3 March 2023. Retrieved 9 March 2023.\\\\n^ a b c \\\\\"The Falcon has landed in the Hugging Face ecosystem\\\\\". huggingface.co. Archived from the original on 2023-06-20. Retrieved 2023-06-20.\\\\n^ \\\\\"GPT-4 Technical Report\\\\\" (PDF). OpenAI. 2023. Archived (PDF) from the original on March 14, 2023. Retrieved March 14, 2023.\\\\n^ Schreiner, Maximilian (2023-07-11). \\\\\"GPT-4 architecture, datasets, costs and more leaked\\\\\". THE DECODER. Archived from the original on 2023-07-12. Retrieved 2024-07-26.\\\\n^ Dickson, Ben (22 May 2024). \\\\\"Meta introduces Chameleon, a state-of-the-art multimodal model\\\\\". VentureBeat.\\\\n^ Dey, Nolan (March 28, 2023). \\\\\"Cerebras-GPT: A Family of Open, Compute-efficient, Large Language Models\\\\\". Cerebras. Archived from the original on March 28, 2023. Retrieved March 28, 2023.\\\\n^ \\\\\"Abu Dhabi-based TII launches its own version of ChatGPT\\\\\". tii.ae. Archived from the original on 2023-04-03. Retrieved 2023-04-03.\\\\n^ Penedo, Guilherme; Malartic, Quentin; Hesslow, Daniel; Cojocaru, Ruxandra; Cappelli, Alessandro; Alobeidli, Hamza; Pannier, Baptiste; Almazrouei, Ebtesam; Launay, Julien (2023-06-01). \\\\\"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only\\\\\". arXiv:2306.01116 [cs.CL].\\\\n^ \\\\\"tiiuae/falcon-40b · Hugging Face\\\\\". huggingface.co. 2023-06-09. Retrieved 2023-06-20.\\\\n^ UAE\\'s Falcon 40B, World\\'s Top-Ranked AI Model from Technology Innovation Institute, is Now Royalty-Free Archived 2024-02-08 at the Wayback Machine, 31 May 2023\\\\n^ Wu, Shijie; Irsoy, Ozan; Lu, Steven; Dabravolski, Vadim; Dredze, Mark; Gehrmann, Sebastian; Kambadur, Prabhanjan; Rosenberg, David; Mann, Gideon (March 30, 2023). \\\\\"BloombergGPT: A Large Language Model for Finance\\\\\". arXiv:2303.17564 [cs.LG].\\\\n^ Ren, Xiaozhe; Zhou, Pingyi; Meng, Xinfan; Huang, Xinjing; Wang, Yadao; Wang, Weichao; Li, Pengfei; Zhang, Xiaoda; Podolskiy, Alexander; Arshinov, Grigory; Bout, Andrey; Piontkovskaya, Irina; Wei, Jiansheng; Jiang, Xin; Su, Teng; Liu, Qun; Yao, Jun (March 19, 2023). \\\\\"PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing\\\\\". arXiv:2303.10845 [cs.CL].\\\\n^ Köpf, Andreas; Kilcher, Yannic; von Rütte, Dimitri; Anagnostidis, Sotiris; Tam, Zhi-Rui; Stevens, Keith; Barhoum, Abdullah; Duc, Nguyen Minh; Stanley, Oliver; Nagyfi, Richárd; ES, Shahul; Suri, Sameer; Glushkov, David; Dantuluri, Arnav; Maguire, Andrew (2023-04-14). \\\\\"OpenAssistant Conversations – Democratizing Large Language Model Alignment\\\\\". arXiv:2304.07327 [cs.CL].\\\\n^ Wrobel, Sharon. \\\\\"Tel Aviv startup rolls out new advanced AI language model to rival OpenAI\\\\\". www.timesofisrael.com. Archived from the original on 2023-07-24. Retrieved 2023-07-24.\\\\n^ Wiggers, Kyle (2023-04-13). \\\\\"With Bedrock, Amazon enters the generative AI race\\\\\". TechCrunch. Archived from the original on 2023-07-24. Retrieved 2023-07-24.\\\\n^ a b Elias, Jennifer (16 May 2023). \\\\\"Google\\'s newest A.I. model uses nearly five times more text data for training than its predecessor\\\\\". CNBC. Archived from the original on 16 May 2023. Retrieved 18 May 2023.\\\\n^ \\\\\"Introducing PaLM 2\\\\\". Google. May 10, 2023. Archived from the original on May 18, 2023. Retrieved May 18, 2023.\\\\n^ a b \\\\\"Introducing Llama 2: The Next Generation of Our Open Source Large Language Model\\\\\". Meta AI. 2023. Archived from the original on 2024-01-05. Retrieved 2023-07-19.\\\\n^ \\\\\"llama/MODEL_CARD.md at main · meta-llama/llama\\\\\". GitHub. Archived from the original on 2024-05-28. Retrieved 2024-05-28.\\\\n^ \\\\\"Claude 2\\\\\". anthropic.com. Archived from the original on 15 December 2023. Retrieved 12 December 2023.\\\\n^ Nirmal, Dinesh (2023-09-07). \\\\\"Building AI for business: IBM\\'s Granite foundation models\\\\\". IBM Blog. Archived from the original on 2024-07-22. Retrieved 2024-08-11.\\\\n^ \\\\\"Announcing Mistral 7B\\\\\". Mistral. 2023. Archived from the original on 2024-01-06. Retrieved 2023-10-06.\\\\n^ \\\\\"Introducing Claude 2.1\\\\\". anthropic.com. Archived from the original on 15 December 2023. Retrieved 12 December 2023.\\\\n^ xai-org/grok-1, xai-org, 2024-03-19, archived from the original on 2024-05-28, retrieved 2024-03-19\\\\n^ \\\\\"Grok-1 model card\\\\\". x.ai. Retrieved 12 December 2023.\\\\n^ \\\\\"Gemini – Google DeepMind\\\\\". deepmind.google. Archived from the original on 8 December 2023. Retrieved 12 December 2023.\\\\n^ Franzen, Carl (11 December 2023). \\\\\"Mistral shocks AI community as latest open source model eclipses GPT-3.5 performance\\\\\". VentureBeat. Archived from the original on 11 December 2023. Retrieved 12 December 2023.\\\\n^ \\\\\"Mixtral of experts\\\\\". mistral.ai. 11 December 2023. Archived from the original on 13 February 2024. Retrieved 12 December 2023.\\\\n^ AI, Mistral (2024-04-17). \\\\\"Cheaper, Better, Faster, Stronger\\\\\". mistral.ai. Archived from the original on 2024-05-05. Retrieved 2024-05-05.\\\\n^ a b Hughes, Alyssa (12 December 2023). \\\\\"Phi-2: The surprising power of small language models\\\\\". Microsoft Research. Archived from the original on 12 December 2023. Retrieved 13 December 2023.\\\\n^ \\\\\"Our next-generation model: Gemini 1.5\\\\\". Google. 15 February 2024. Archived from the original on 16 February 2024. Retrieved 16 February 2024. This means 1.5 Pro can process vast amounts of information in one go — including 1 hour of video, 11 hours of audio, codebases with over 30,000 lines of code or over 700,000 words. In our research, we\\'ve also successfully tested up to 10 million tokens.\\\\n^ \\\\\"Gemma\\\\\" – via GitHub.\\\\n^ \\\\\"Introducing the next generation of Claude\\\\\". www.anthropic.com. Archived from the original on 2024-03-04. Retrieved 2024-03-04.\\\\n^ \\\\\"Fugaku-LLM/Fugaku-LLM-13B · Hugging Face\\\\\". huggingface.co. Archived from the original on 2024-05-17. Retrieved 2024-05-17.\\\\n^ \\\\\"Phi-3\\\\\". azure.microsoft.com. 23 April 2024. Archived from the original on 2024-04-27. Retrieved 2024-04-28.\\\\n^ \\\\\"Phi-3 Model Documentation\\\\\". huggingface.co. Archived from the original on 2024-05-13. Retrieved 2024-04-28.\\\\n^ \\\\\"Qwen2\\\\\". GitHub. Archived from the original on 2024-06-17. Retrieved 2024-06-17.\\\\n^ \\\\\"nvidia/Nemotron-4-340B-Base · Hugging Face\\\\\". huggingface.co. 2024-06-14. Archived from the original on 2024-06-15. Retrieved 2024-06-15.\\\\n^ \\\\\"Nemotron-4 340B | Research\\\\\". research.nvidia.com. Archived from the original on 2024-06-15. Retrieved 2024-06-15.\\\\n^ \\\\\"The Llama 3 Herd of Models\\\\\" (July 23, 2024) Llama Team, AI @ Meta\\\\n^ \\\\\"llama-models/models/llama3_1/MODEL_CARD.md at main · meta-llama/llama-models\\\\\". GitHub. Archived from the original on 2024-07-23. Retrieved 2024-07-23.\\\\n^ deepseek-ai/DeepSeek-V3, DeepSeek, 2024-12-26, retrieved 2024-12-26\\\\n^ Amazon Nova Micro, Lite, and Pro - AWS AI Service Cards3, Amazon, 2024-12-27, retrieved 2024-12-27\\\\nvte\\\\nNatural language processing\\\\nGeneral terms \\\\nAI-completeBag-of-wordsn-gram BigramTrigramComputational linguisticsNatural language understandingStop wordsText processing\\\\nText analysis \\\\nArgument miningCollocation extractionConcept miningCoreference resolutionDeep linguistic processingDistant readingInformation extractionNamed-entity recognitionOntology learningParsing Semantic parsingSyntactic parsingPart-of-speech taggingSemantic analysisSemantic role labelingSemantic decompositionSemantic similaritySentiment analysis\\\\nTerminology extractionText miningTextual entailmentTruecasingWord-sense disambiguationWord-sense induction\\\\nText segmentation \\\\nCompound-term processingLemmatisationLexical analysisText chunkingStemmingSentence segmentationWord segmentation\\\\nAutomatic summarization \\\\nMulti-document summarizationSentence extractionText simplification\\\\nMachine translation \\\\nComputer-assistedExample-basedRule-basedStatisticalTransfer-basedNeural\\\\nDistributional semantics models \\\\nBERTDocument-term matrixExplicit semantic analysisfastTextGloVeLanguage model (large)Latent semantic analysisSeq2seqWord embeddingWord2vec\\\\nLanguage resources,\\\\ndatasets and corpora  \\\\nTypes and\\\\nstandards \\\\nCorpus linguisticsLexical resourceLinguistic Linked Open DataMachine-readable dictionaryParallel textPropBankSemantic networkSimple Knowledge Organization SystemSpeech corpusText corpusThesaurus (information retrieval)TreebankUniversal Dependencies\\\\nData  \\\\nBabelNetBank of EnglishDBpediaFrameNetGoogle Ngram ViewerUBYWordNetWikidata\\\\nAutomatic identification\\\\nand data capture  \\\\nSpeech recognitionSpeech segmentationSpeech synthesisNatural language generationOptical character recognition\\\\nTopic model \\\\nDocument classificationLatent Dirichlet allocationPachinko allocation\\\\nComputer-assisted\\\\nreviewing \\\\nAutomated essay scoringConcordancerGrammar checkerPredictive textPronunciation assessmentSpell checker\\\\nNatural language\\\\nuser interface\\\\nChatbotInteractive fiction (c.f. Syntax guessing)Question answeringVirtual assistantVoice user interface\\\\nRelated \\\\nFormal semanticsHallucinationNatural Language ToolkitspaCy\\\\nPortal:\\\\n Language\\\\nCategory: Software comparisons\\\\nThis page was last edited on 28 December 2024, at 21:13\\xa0(UTC).\\\\nText is available under the Creative Commons Attribution-ShareAlike 4.0 License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\\\\nPrivacy policy\\\\nAbout Wikipedia\\\\nDisclaimers\\\\nContact Wikipedia\\\\nCode of Conduct\\\\nDevelopers\\\\nStatistics\\\\nCookie statement\\\\nMobile view\"}, {\"url\": \"https://ai.meta.com/blog/meta-llama-3/\", \"title\": \"Introducing Meta Llama 3: The most capable openly available LLM ...\", \"content\": \"Today, we’re introducing Meta Llama 3, the next generation of our state-of-the-art open source large language model. Today, we’re excited to share the first two models of the next generation of Llama, Meta Llama 3, available for broad use. We wanted to address developer feedback to increase the overall helpfulness of Llama 3 and are doing so while continuing to play a leading role on responsible use and deployment of LLMs. We are embracing the open source ethos of releasing early and often to enable the community to get access to these models while they are still in development. Please note that this data is based on an early checkpoint of Llama 3 that is still training and these capabilities are not supported as part of the models released today.\", \"score\": 0.7216064, \"raw_content\": \"Introducing Meta Llama 3: The most capable openly available LLM to date\\\\n\\\\n\\\\nOur approach\\\\nResearch\\\\nProduct experiences\\\\nLlama\\\\nBlog\\\\nTry Meta AI\\\\n\\\\n\\\\nLarge Language Model\\\\nIntroducing Meta Llama 3: The most capable openly available LLM to date\\\\nApril 18, 2024\\\\nTakeaways:\\\\nRECOMMENDED READS\\\\n\\\\n5 Steps to Getting Started with Llama 2\\\\nThe Llama Ecosystem: Past, Present, and Future\\\\nIntroducing Code Llama, a state-of-the-art large language model for coding\\\\n\\\\nMeta and Microsoft Introduce the Next Generation of Llama\\\\n\\\\n\\\\nToday, we’re introducing Meta Llama 3, the next generation of our state-of-the-art open source large language model.\\\\n\\\\nLlama 3 models will soon be available on AWS, Databricks, Google Cloud, Hugging Face, Kaggle, IBM WatsonX, Microsoft Azure, NVIDIA NIM, and Snowflake, and with support from hardware platforms offered by AMD, AWS, Dell, Intel, NVIDIA, and Qualcomm.\\\\nWe’re dedicated to developing Llama 3 in a responsible way, and we’re offering various resources to help others use it responsibly as well. This includes introducing new trust and safety tools with Llama Guard 2, Code Shield, and CyberSec Eval 2.\\\\nIn the coming months, we expect to introduce new capabilities, longer context windows, additional model sizes, and enhanced performance, and we’ll share the Llama 3 research paper.\\\\nMeta AI, built with Llama 3 technology, is now one of the world’s leading AI assistants that can boost your intelligence and lighten your load—helping you learn, get things done, create content, and connect to make the most out of every moment. You can try Meta AI here.\\\\n\\\\nToday, we’re excited to share the first two models of the next generation of Llama, Meta Llama 3, available for broad use. This release features pretrained and instruction-fine-tuned language models with 8B and 70B parameters that can support a broad range of use cases. This next generation of Llama demonstrates state-of-the-art performance on a wide range of industry benchmarks and offers new capabilities, including improved reasoning. We believe these are the best open source models of their class, period. In support of our longstanding open approach, we’re putting Llama 3 in the hands of the community. We want to kickstart the next wave of innovation in AI across the stack—from applications to developer tools to evals to inference optimizations and more. We can’t wait to see what you build and look forward to your feedback.\\\\nOur goals for Llama 3\\\\nWith Llama 3, we set out to build the best open models that are on par with the best proprietary models available today. We wanted to address developer feedback to increase the overall helpfulness of Llama 3 and are doing so while continuing to play a leading role on responsible use and deployment of LLMs. We are embracing the open source ethos of releasing early and often to enable the community to get access to these models while they are still in development. The text-based models we are releasing today are the first in the Llama 3 collection of models. Our goal in the near future is to make Llama 3 multilingual and multimodal, have longer context, and continue to improve overall performance across core LLM capabilities such as reasoning and coding.\\\\nState-of-the-art performance\\\\nOur new 8B and 70B parameter Llama 3 models are a major leap over Llama 2 and establish a new state-of-the-art for LLM models at those scales. Thanks to improvements in pretraining and post-training, our pretrained and instruction-fine-tuned models are the best models existing today at the 8B and 70B parameter scale. Improvements in our post-training procedures substantially reduced false refusal rates, improved alignment, and increased diversity in model responses. We also saw greatly improved capabilities like reasoning, code generation, and instruction following making Llama 3 more steerable.\\\\n*Please see evaluation details for setting and parameters with which these evaluations are calculated.\\\\nIn the development of Llama 3, we looked at model performance on standard benchmarks and also sought to optimize for performance for real-world scenarios. To this end, we developed a new high-quality human evaluation set. This evaluation set contains 1,800 prompts that cover 12 key use cases: asking for advice, brainstorming, classification, closed question answering, coding, creative writing, extraction, inhabiting a character/persona, open question answering, reasoning, rewriting, and summarization. To prevent accidental overfitting of our models on this evaluation set, even our own modeling teams do not have access to it. The chart below shows aggregated results of our human evaluations across of these categories and prompts against Claude Sonnet, Mistral Medium, and GPT-3.5.\\\\n\\\\nPreference rankings by human annotators based on this evaluation set highlight the strong performance of our 70B instruction-following model compared to competing models of comparable size in real-world scenarios.\\\\nOur pretrained model also establishes a new state-of-the-art for LLM models at those scales.\\\\n*Please see evaluation details for setting and parameters with which these evaluations are calculated.\\\\nTo develop a great language model, we believe it’s important to innovate, scale, and optimize for simplicity. We adopted this design philosophy throughout the Llama 3 project with a focus on four key ingredients: the model architecture, the pretraining data, scaling up pretraining, and instruction fine-tuning.\\\\nModel architecture\\\\nIn line with our design philosophy, we opted for a relatively standard decoder-only transformer architecture in Llama 3. Compared to Llama 2, we made several key improvements. Llama 3 uses a tokenizer with a vocabulary of 128K tokens that encodes language much more efficiently, which leads to substantially improved model performance. To improve the inference efficiency of Llama 3 models, we’ve adopted grouped query attention (GQA) across both the 8B and 70B sizes. We trained the models on sequences of 8,192 tokens, using a mask to ensure self-attention does not cross document boundaries.\\\\nTraining data\\\\nTo train the best language model, the curation of a large, high-quality training dataset is paramount. In line with our design principles, we invested heavily in pretraining data. Llama 3 is pretrained on over 15T tokens that were all collected from publicly available sources. Our training dataset is seven times larger than that used for Llama 2, and it includes four times more code. To prepare for upcoming multilingual use cases, over 5% of the Llama 3 pretraining dataset consists of high-quality non-English data that covers over 30 languages. However, we do not expect the same level of performance in these languages as in English.\\\\nTo ensure Llama 3 is trained on data of the highest quality, we developed a series of data-filtering pipelines. These pipelines include using heuristic filters, NSFW filters, semantic deduplication approaches, and text classifiers to predict data quality. We found that previous generations of Llama are surprisingly good at identifying high-quality data, hence we used Llama 2 to generate the training data for the text-quality classifiers that are powering Llama 3.\\\\nWe also performed extensive experiments to evaluate the best ways of mixing data from different sources in our final pretraining dataset. These experiments enabled us to select a data mix that ensures that Llama 3 performs well across use cases including trivia questions, STEM, coding, historical knowledge, etc.\\\\nScaling up pretraining\\\\nTo effectively leverage our pretraining data in Llama 3 models, we put substantial effort into scaling up pretraining. Specifically, we have developed a series of detailed scaling laws for downstream benchmark evaluations. These scaling laws enable us to select an optimal data mix and to make informed decisions on how to best use our training compute. Importantly, scaling laws allow us to predict the performance of our largest models on key tasks (for example, code generation as evaluated on the HumanEval benchmark—see above) before we actually train the models. This helps us ensure strong performance of our final models across a variety of use cases and capabilities.\\\\nWe made several new observations on scaling behavior during the development of Llama 3. For example, while the Chinchilla-optimal amount of training compute for an 8B parameter model corresponds to ~200B tokens, we found that model performance continues to improve even after the model is trained on two orders of magnitude more data. Both our 8B and 70B parameter models continued to improve log-linearly after we trained them on up to 15T tokens. Larger models can match the performance of these smaller models with less training compute, but smaller models are generally preferred because they are much more efficient during inference.\\\\nTo train our largest Llama 3 models, we combined three types of parallelization: data parallelization, model parallelization, and pipeline parallelization. Our most efficient implementation achieves a compute utilization of over 400 TFLOPS per GPU when trained on 16K GPUs simultaneously. We performed training runs on two custom-built 24K GPU clusters. To maximize GPU uptime, we developed an advanced new training stack that automates error detection, handling, and maintenance. We also greatly improved our hardware reliability and detection mechanisms for silent data corruption, and we developed new scalable storage systems that reduce overheads of checkpointing and rollback. Those improvements resulted in an overall effective training time of more than 95%. Combined, these improvements increased the efficiency of Llama 3 training by ~three times compared to Llama 2.\\\\nInstruction fine-tuning\\\\nTo fully unlock the potential of our pretrained models in chat use cases, we innovated on our approach to instruction-tuning as well. Our approach to post-training is a combination of supervised fine-tuning (SFT), rejection sampling, proximal policy optimization (PPO), and direct preference optimization (DPO). The quality of the prompts that are used in SFT and the preference rankings that are used in PPO and DPO has an outsized influence on the performance of aligned models. Some of our biggest improvements in model quality came from carefully curating this data and performing multiple rounds of quality assurance on annotations provided by human annotators.\\\\nLearning from preference rankings via PPO and DPO also greatly improved the performance of Llama 3 on reasoning and coding tasks. We found that if you ask a model a reasoning question that it struggles to answer, the model will sometimes produce the right reasoning trace: The model knows how to produce the right answer, but it does not know how to select it. Training on preference rankings enables the model to learn how to select it.\\\\nBuilding with Llama 3\\\\nOur vision is to enable developers to customize Llama 3 to support relevant use cases and to make it easier to adopt best practices and improve the open ecosystem. With this release, we’re providing new trust and safety tools including updated components with both Llama Guard 2 and Cybersec Eval 2, and the introduction of Code Shield—an inference time guardrail for filtering insecure code produced by LLMs.\\\\nWe’ve also co-developed Llama 3 with torchtune, the new PyTorch-native library for easily authoring, fine-tuning, and experimenting with LLMs. torchtune provides memory efficient and hackable training recipes written entirely in PyTorch. The library is integrated with popular platforms such as Hugging Face, Weights & Biases, and EleutherAI and even supports Executorch for enabling efficient inference to be run on a wide variety of mobile and edge devices. For everything from prompt engineering to using Llama 3 with LangChain we have a comprehensive getting started guide and takes you from downloading Llama 3 all the way to deployment at scale within your generative AI application.\\\\nA system-level approach to responsibility\\\\nWe have designed Llama 3 models to be maximally helpful while ensuring an industry leading approach to responsibly deploying them. To achieve this, we have adopted a new, system-level approach to the responsible development and deployment of Llama. We envision Llama models as part of a broader system that puts the developer in the driver’s seat. Llama models will serve as a foundational piece of a system that developers design with their unique end goals in mind.\\\\n\\\\nInstruction fine-tuning also plays a major role in ensuring the safety of our models. Our instruction-fine-tuned models have been red-teamed (tested) for safety through internal and external efforts. \\u200b\\u200bOur red teaming approach leverages human experts and automation methods to generate adversarial prompts that try to elicit problematic responses. For instance, we apply comprehensive testing to assess risks of misuse related to Chemical, Biological, Cyber Security, and other risk areas. All of these efforts are iterative and used to inform safety fine-tuning of the models being released. You can read more about our efforts in the model card.\\\\nLlama Guard models are meant to be a foundation for prompt and response safety and can easily be fine-tuned to create a new taxonomy depending on application needs. As a starting point, the new Llama Guard 2 uses the recently announced MLCommons taxonomy, in an effort to support the emergence of industry standards in this important area. Additionally, CyberSecEval 2 expands on its predecessor by adding measures of an LLM’s propensity to allow for abuse of its code interpreter, offensive cybersecurity capabilities, and susceptibility to prompt injection attacks (learn more in our technical paper). Finally, we’re introducing Code Shield which adds support for inference-time filtering of insecure code produced by LLMs. This offers mitigation of risks around insecure code suggestions, code interpreter abuse prevention, and secure command execution.\\\\nWith the speed at which the generative AI space is moving, we believe an open approach is an important way to bring the ecosystem together and mitigate these potential harms. As part of that, we’re updating our Responsible Use Guide (RUG) that provides a comprehensive guide to responsible development with LLMs. As we outlined in the RUG, we recommend that all inputs and outputs be checked and filtered in accordance with content guidelines appropriate to the application. Additionally, many cloud service providers offer content moderation APIs and other tools for responsible deployment, and we encourage developers to also consider using these options.\\\\nDeploying Llama 3 at scale\\\\nLlama 3 will soon be available on all major platforms including cloud providers, model API providers, and much more. Llama 3 will be everywhere.\\\\nOur benchmarks show the tokenizer offers improved token efficiency, yielding up to 15% fewer tokens compared to Llama 2. Also, Group Query Attention (GQA) now has been added to Llama 3 8B as well. As a result, we observed that despite the model having 1B more parameters compared to Llama 2 7B, the improved tokenizer efficiency and GQA contribute to maintaining the inference efficiency on par with Llama 2 7B.\\\\nFor examples of how to leverage all of these capabilities, check out Llama Recipes which contains all of our open source code that can be leveraged for everything from fine-tuning to deployment to model evaluation.\\\\nWhat’s next for Llama 3?\\\\nThe Llama 3 8B and 70B models mark the beginning of what we plan to release for Llama 3. And there’s a lot more to come.\\\\nOur largest models are over 400B parameters and, while these models are still training, our team is excited about how they’re trending. Over the coming months, we’ll release multiple models with new capabilities including multimodality, the ability to converse in multiple languages, a much longer context window, and stronger overall capabilities. We will also publish a detailed research paper once we are done training Llama 3.\\\\nTo give you a sneak preview for where these models are today as they continue training, we thought we could share some snapshots of how our largest LLM model is trending. Please note that this data is based on an early checkpoint of Llama 3 that is still training and these capabilities are not supported as part of the models released today.\\\\n*Please see evaluation details for setting and parameters with which these evaluations are calculated.\\\\nWe’re committed to the continued growth and development of an open AI ecosystem for releasing our models responsibly. We have long believed that openness leads to better, safer products, faster innovation, and a healthier overall market. This is good for Meta, and it is good for society. We’re taking a community-first approach with Llama 3, and starting today, these models are available on the leading cloud, hosting, and hardware platforms with many more to come.\\\\nTry Meta Llama 3 today\\\\nWe’ve integrated our latest models into Meta AI, which we believe is the world’s leading AI assistant. It’s now built with Llama 3 technology and it’s available in more countries across our apps.\\\\nYou can use Meta AI on Facebook, Instagram, WhatsApp, Messenger, and the web to get things done, learn, create, and connect with the things that matter to you. You can read more about the Meta AI experience here.\\\\nVisit the Llama 3 website to download the models and reference the Getting Started Guide for the latest list of all available platforms.\\\\nYou’ll also soon be able to test multimodal Meta AI on our Ray-Ban Meta smart glasses.\\\\nAs always, we look forward to seeing all the amazing products and experiences you will build with Meta Llama 3.\\\\n\\\\nShare:\\\\n\\\\nOur latest updates delivered to your inbox\\\\nSubscribe to our newsletter to keep up with Meta AI news, events, research breakthroughs, and more.\\\\nJoin us in the pursuit of what’s possible with AI.\\\\nSee all open positions\\\\nRelated Posts\\\\n\\\\nComputer Vision\\\\nIntroducing Segment Anything: Working toward the first foundation model for image segmentation\\\\nApril 5, 2023\\\\nRead post\\\\nFEATURED\\\\n\\\\nResearch\\\\nMultiRay: Optimizing efficiency for large-scale AI models\\\\nNovember 18, 2022\\\\nRead post\\\\nFEATURED\\\\n\\\\nML Applications\\\\nMuAViC: The first audio-video speech translation benchmark\\\\nMarch 8, 2023\\\\nRead post\\\\nOur approach\\\\nAbout AI at Meta\\\\nPeople\\\\nCareers\\\\nResearch\\\\nInfrastructure\\\\nResources\\\\nDemos\\\\nProduct experiences\\\\nMeta AI\\\\nAI Studio\\\\nLatest news\\\\nBlog\\\\nNewsletter\\\\nFoundational models\\\\nLlama\\\\n\\\\n \\\\n \\\\n \\\\n \\\\nOur approach\\\\nOur approachAbout AI at MetaPeopleCareers\\\\nResearch\\\\nResearchInfrastructureResourcesDemos\\\\nProduct experiences\\\\nMeta AIAI Studio\\\\nLatest news\\\\nLatest newsBlogNewsletter\\\\nFoundational models\\\\nLlama\\\\n \\\\n \\\\n \\\\n \\\\nPrivacy Policy\\\\nTerms\\\\nCookies\\\\nMeta © 2025\\\\n \\\\n \\\\n \\\\n \"}, {\"url\": \"https://en.wikipedia.org/wiki/Large_language_model\", \"title\": \"Large language model - Wikipedia\", \"content\": \"A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. A smoothed n-gram model in 2001 trained on 0.3 billion words achieved state-of-the-art perplexity at the time.[4] In the 2000s, as Internet use became prevalent, some researchers constructed Internet-scale language datasets (\\\\\"web as corpus\\\\\"[5]), upon which they trained statistical language models.[6][7] In 2009, in most language processing tasks, statistical language models dominated over symbolic language models, as they can usefully ingest large datasets.[8]\", \"score\": 0.61695945, \"raw_content\": \"Jump to content\\\\nMain menu\\\\nSearch\\\\nDonate\\\\nCreate account\\\\nLog in\\\\nPersonal tools\\\\nToggle the table of contents\\\\nLarge language model\\\\n53 languages\\\\nArticle\\\\nTalk\\\\nRead\\\\nEdit\\\\nView history\\\\nTools\\\\nFrom Wikipedia, the free encyclopedia\\\\nNot to be confused with Logic learning machine.\\\\nPart of a series on\\\\nMachine learning\\\\nand data mining\\\\nParadigms\\\\nProblems\\\\nSupervised learning\\\\n(classification\\xa0• regression)\\\\nClustering\\\\nDimensionality reduction\\\\nStructured prediction\\\\nAnomaly detection\\\\nArtificial neural network\\\\nAutoencoderDeep learningFeedforward neural networkRecurrent neural network LSTMGRUESNreservoir computingBoltzmann machine RestrictedGANDiffusion modelSOMConvolutional neural network U-NetLeNetAlexNetDeepDreamNeural radiance fieldTransformer VisionMambaSpiking neural networkMemtransistorElectrochemical RAM (ECRAM)\\\\nReinforcement learning\\\\nLearning with humans\\\\nModel diagnostics\\\\nMathematical foundations\\\\nJournals and conferences\\\\nRelated articles\\\\nvte\\\\nA large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering.[1] These models acquire predictive power regarding syntax, semantics, and ontologies[2] inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.[3]\\\\nHistory[edit]\\\\nThe training compute of notable large models in FLOPs vs publication date over the period 2010-2024. For overall notable models (top left), frontier models (top right), top language models (bottom left) and top models within leading companies (bottom right). The majority of these models are language models.\\\\nThe training compute of notable large AI models in FLOPs vs publication date over the period 2017-2024. The majority of large models are language models or multimodal models with language capacity.\\\\nBefore 2017, there were a few language models that were large as compared to capacities then available. In the 1990s, the IBM alignment models pioneered statistical language modelling. A smoothed n-gram model in 2001 trained on 0.3 billion words achieved state-of-the-art perplexity at the time.[4] In the 2000s, as Internet use became prevalent, some researchers constructed Internet-scale language datasets (\\\\\"web as corpus\\\\\"[5]), upon which they trained statistical language models.[6][7] In 2009, in most language processing tasks, statistical language models dominated over symbolic language models, as they can usefully ingest large datasets.[8]\\\\nAfter neural networks became dominant in image processing around 2012,[9] they were applied to language modelling as well. Google converted its translation service to Neural Machine Translation in 2016. As it was before transformers, it was done by seq2seq deep LSTM networks.\\\\nAn illustration of main components of the transformer model from the original paper, where layers were normalized after (instead of before) multiheaded attention\\\\nAt the 2017 NeurIPS conference, Google researchers introduced the transformer architecture in their landmark paper \\\\\"Attention Is All You Need\\\\\". This paper\\'s goal was to improve upon 2014 seq2seq technology,[10] and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014.[11] The following year in 2018, BERT was introduced and quickly became \\\\\"ubiquitous\\\\\".[12] Though the original transformer has both encoder and decoder blocks, BERT is an encoder-only model. Academic and research usage of BERT began to decline in 2023, following rapid improvements in the abilities of decoder-only models (such as GPT) to solve tasks via prompting.[13]\\\\nAlthough decoder-only GPT-1 was introduced in 2018, it was GPT-2 in 2019 that caught widespread attention because OpenAI at first deemed it too powerful to release publicly, out of fear of malicious use.[14] GPT-3 in 2020 went a step further and as of 2024 is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing browser-based ChatGPT that captured the imaginations of the general population and caused some media hype and online buzz.[15] The 2023 GPT-4 was praised for its increased accuracy and as a \\\\\"holy grail\\\\\" for its multimodal capabilities.[16] OpenAI did not reveal the high-level architecture and the number of parameters of GPT-4. The release of ChatGPT led to an uptick in LLM usage across several research subfields of computer science, including robotics, software engineering, and societal impact work.[17] In 2024 OpenAI released the reasoning model OpenAI o1, which generates long chains of thought before returning a final answer.\\\\nCompeting language models have for the most part been attempting to equal the GPT series, at least in terms of number of parameters.[18]\\\\nSince 2022, source-available models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on the field of use. Mistral AI\\'s models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. In January 2025, DeepSeek released DeepSeek R1, a 671-billion-parameter open-weight model that performs comparably to OpenAI o1 but at a much lower cost.[19]\\\\nSince 2023, many LLMs have been trained to be multimodal, having the ability to also process or generate other types of data, such as images or audio. These LLMs are also called large multimodal models (LMMs).[20]\\\\nAs of 2024, the largest and most capable models are all based on the transformer architecture. Some recent implementations are based on other architectures, such as recurrent neural network variants and Mamba (a state space model).[21][22][23]\\\\nDataset preprocessing[edit]\\\\nSee also: List of datasets for machine-learning research §\\xa0Internet\\\\nTokenization[edit]\\\\nAs machine learning algorithms process numbers rather than text, the text must be converted to numbers. In the first step, a vocabulary is decided upon, then integer indices are arbitrarily but uniquely assigned to each vocabulary entry, and finally, an embedding is associated to the integer index. Algorithms include byte-pair encoding (BPE) and WordPiece. There are also special tokens serving as control characters, such as [MASK] for masked-out token (as used in BERT), and [UNK] (\\\\\"unknown\\\\\") for characters not appearing in the vocabulary. Also, some special symbols are used to denote special text formatting. For example, \\\\\"Ġ\\\\\" denotes a preceding whitespace in RoBERTa and GPT. \\\\\"##\\\\\" denotes continuation of a preceding word in BERT.[24]\\\\nFor example, the BPE tokenizer used by GPT-3 (Legacy) would split tokenizer: texts -> series of numerical \\\\\"tokens\\\\\" as\\\\ntoken   izer    :   \\xa0texts  \\xa0-> series  \\xa0of \\xa0numerical  \\xa0\\\\\"  t   ok  ens \\\\\"\\\\nTokenization also compresses the datasets. Because LLMs generally require input to be an array that is not jagged, the shorter texts must be \\\\\"padded\\\\\" until they match the length of the longest one. How many tokens are, on average, needed per word depends on the language of the dataset.[25][26]\\\\nBPE[edit]\\\\nMain article: Byte pair encoding\\\\nAs an example, consider a tokenizer based on byte-pair encoding. In the first step, all unique characters (including blanks and punctuation marks) are treated as an initial set of n-grams (i.e. initial set of uni-grams). Successively the most frequent pair of adjacent characters is merged into a bi-gram and all instances of the pair are replaced by it. All occurrences of adjacent pairs of (previously merged) n-grams that most frequently occur together are then again merged into even lengthier n-gram, until a vocabulary of prescribed size is obtained (in case of GPT-3, the size is 50257).[27] After a tokenizer is trained, any text can be tokenized by it, as long as it does not contain characters not appearing in the initial-set of uni-grams.[28]\\\\nProblems[edit]\\\\nA token vocabulary based on the frequencies extracted from mainly English corpora uses as few tokens as possible for an average English word. However, an average word in another language encoded by such an English-optimized tokenizer is split into a suboptimal amount of tokens. GPT-2 tokenizer can use up to 15 times more tokens per word for some languages, for example for the Shan language from Myanmar. Even more widespread languages such as Portuguese and German have \\\\\"a premium of 50%\\\\\" compared to English.[29]\\\\nGreedy tokenization also causes subtle problems with text completion.[30]\\\\nDataset cleaning[edit]\\\\nMain article: Data cleansing\\\\nIn the context of training LLMs, datasets are typically cleaned by removing low-quality, duplicated, or toxic data.[31] Cleaned datasets can increase training efficiency and lead to improved downstream performance.[32][33] A trained LLM can be used to clean datasets for training a further LLM.[34]\\\\nWith the increasing proportion of LLM-generated content on the web, data cleaning in the future may include filtering out such content. LLM-generated content can pose a problem if the content is similar to human text (making filtering difficult) but of lower quality (degrading performance of models trained on it).[35]\\\\nSynthetic data[edit]\\\\nMain article: Synthetic data\\\\nTraining of largest language models might need more linguistic data than naturally available, or that the naturally occurring data is of insufficient quality. In these cases, synthetic data might be used. Microsoft\\'s Phi series of LLMs is trained on textbook-like data generated by another LLM.[36]\\\\nTraining and architecture[edit]\\\\nSee also: Fine-tuning (machine learning)\\\\nReinforcement learning from human feedback (RLHF)[edit]\\\\nMain article: Reinforcement learning from human feedback\\\\nReinforcement learning from human feedback (RLHF) through algorithms, such as proximal policy optimization, is used to further fine-tune a model based on a dataset of human preferences.[37]\\\\nInstruction tuning[edit]\\\\nUsing \\\\\"self-instruct\\\\\" approaches, LLMs have been able to bootstrap correct responses, replacing any naive responses, starting from human-generated corrections of a few cases. For example, in the instruction \\\\\"Write an essay about the main themes represented in Hamlet,\\\\\" an initial naive completion might be \\\\\"If you submit the essay after March 17, your grade will be reduced by 10% for each day of delay,\\\\\" based on the frequency of this textual sequence in the corpus.[38]\\\\nMixture of experts[edit]\\\\nMain article: Mixture of experts\\\\nThe largest LLM may be too expensive to train and use directly. For such models, mixture of experts (MoE) can be applied, a line of research pursued by Google researchers since 2017 to train models reaching up to 1 trillion parameters.[39][40][41]\\\\nPrompt engineering, attention mechanism, and context window[edit]\\\\nSee also: Prompt engineering and Attention (machine learning)\\\\nMost results previously achievable only by (costly) fine-tuning, can be achieved through prompt engineering, although limited to the scope of a single conversation (more precisely, limited to the scope of a context window).[42]\\\\nWhen each head calculates, according to its own criteria, how much other tokens are relevant for the \\\\\"it_\\\\\" token, note that the second attention head, represented by the second column, is focusing most on the first two rows, i.e. the tokens \\\\\"The\\\\\" and \\\\\"animal\\\\\", while the third column is focusing most on the bottom two rows, i.e. on \\\\\"tired\\\\\", which has been tokenized into two tokens.[43]\\\\nIn order to find out which tokens are relevant to each other within the scope of the context window, the attention mechanism calculates \\\\\"soft\\\\\" weights for each token, more precisely for its embedding, by using multiple attention heads, each with its own \\\\\"relevance\\\\\" for calculating its own soft weights. For example, the small (i.e. 117M parameter sized) GPT-2 model has had twelve attention heads and a context window of only 1k tokens.[44] In its medium version it has 345M parameters and contains 24 layers, each with 12 attention heads. For the training with gradient descent a batch size of 512 was utilized.[28]\\\\nThe largest models, such as Google\\'s Gemini 1.5, presented in February 2024, can have a context window sized up to 1 million (context window of 10 million was also \\\\\"successfully tested\\\\\").[45] Other models with large context windows includes Anthropic\\'s Claude 2.1, with a context window of up to 200k tokens.[46] Note that this maximum refers to the number of input tokens and that the maximum number of output tokens differs from the input and is often smaller. For example, the GPT-4 Turbo model has a maximum output of 4096 tokens.[47]\\\\nLength of a conversation that the model can take into account when generating its next answer is limited by the size of a context window, as well. If the length of a conversation, for example with ChatGPT, is longer than its context window, only the parts inside the context window are taken into account when generating the next answer, or the model needs to apply some algorithm to summarize the too distant parts of conversation.\\\\nThe shortcomings of making a context window larger include higher computational cost and possibly diluting the focus on local context, while making it smaller can cause a model to miss an important long-range dependency. Balancing them is a matter of experimentation and domain-specific considerations.\\\\nA model may be pre-trained either to predict how the segment continues, or what is missing in the segment, given a segment from its training dataset.[48] It can be either\\\\nautoregressive (i.e. predicting how the segment continues, as GPTs do): for example given a segment \\\\\"I like to eat\\\\\", the model predicts \\\\\"ice cream\\\\\", or \\\\\"sushi\\\\\".\\\\n\\\\\"masked\\\\\" (i.e. filling in the parts missing from the segment, the way \\\\\"BERT\\\\\"[49] does it): for example, given a segment \\\\\"I like to [] [] cream\\\\\", the model predicts that \\\\\"eat\\\\\" and \\\\\"ice\\\\\" are missing.\\\\nModels may be trained on auxiliary tasks which test their understanding of the data distribution, such as Next Sentence Prediction (NSP), in which pairs of sentences are presented and the model must predict whether they appear consecutively in the training corpus.[49] During training, regularization loss is also used to stabilize training. However regularization loss is usually not used during testing and evaluation.\\\\nInfrastructure[edit]\\\\nSubstantial infrastructure is necessary for training the largest models.[50][51][52]\\\\nTraining cost[edit]\\\\nThe qualifier \\\\\"large\\\\\" in \\\\\"large language model\\\\\" is inherently vague, as there is no definitive threshold for the number of parameters required to qualify as \\\\\"large\\\\\". As time goes on, what was previously considered \\\\\"large\\\\\" may evolve. GPT-1 of 2018 is usually considered the first LLM, even though it has only 0.117 billion parameters. The tendency towards larger models is visible in the list of large language models.\\\\nAdvances in software and hardware have reduced the cost substantially since 2020, such that in 2023 training of a 12-billion-parameter LLM computational cost is 72,300 A100-GPU-hours, while in 2020 the cost of training a 1.5-billion-parameter LLM (which was two orders of magnitude smaller than the state of the art in 2020) was between $80,000 and $1,600,000.[53][54][55] Since 2020, large sums were invested in increasingly large models. For example, training of the GPT-2 (i.e. a 1.5-billion-parameters model) in 2019 cost $50,000, while training of the PaLM (i.e. a 540-billion-parameters model) in 2022 cost $8 million, and Megatron-Turing NLG 530B (in 2021) cost around $11 million.[56]\\\\nFor Transformer-based LLM, training cost is much higher than inference cost. It costs 6 FLOPs per parameter to train on one token, whereas it costs 1 to 2 FLOPs per parameter to infer on one token.[57]\\\\nTool use[edit]\\\\nThere are certain tasks that, in principle, cannot be solved by any LLM, at least not without the use of external tools or additional software. An example of such a task is responding to the user\\'s input \\'354 * 139 = \\', provided that the LLM has not already encountered a continuation of this calculation in its training corpus.[dubious – discuss] In such cases, the LLM needs to resort to running program code that calculates the result, which can then be included in its response.[dubious – discuss]: Another example is \\\\\"What is the time now? It is \\\\\", where a separate program interpreter would need to execute a code to get system time on the computer, so that the LLM can include it in its reply.[58][59] This basic strategy can be sophisticated with multiple attempts of generated programs, and other sampling strategies.[60]\\\\nGenerally, in order to get an LLM to use tools, one must fine-tune it for tool-use. If the number of tools is finite, then fine-tuning may be done just once. If the number of tools can grow arbitrarily, as with online API services, then the LLM can be fine-tuned to be able to read API documentation and call API correctly.[61][62]\\\\nA simpler form of tool use is retrieval-augmented generation: the augmentation of an LLM with document retrieval. Given a query, a document retriever is called to retrieve the most relevant documents. This is usually done by encoding the query and the documents into vectors, then finding the documents with vectors (usually stored in a vector database) most similar to the vector of the query. The LLM then generates an output based on both the query and context included from the retrieved documents.[63]\\\\nAgency[edit]\\\\nAn LLM is typically not an autonomous agent by itself, as it lacks the ability to interact with dynamic environments, recall past behaviors, and plan future actions, but can be transformed into one by integrating modules like profiling, memory, planning, and action.[64]\\\\nThe ReAct pattern, a portmanteau of \\\\\"Reason\\xa0+\\xa0Act\\\\\", constructs an agent out of an LLM, using the LLM as a planner. The LLM is prompted to \\\\\"think out loud\\\\\". Specifically, the language model is prompted with a textual description of the environment, a goal, a list of possible actions, and a record of the actions and observations so far. It generates one or more thoughts before generating an action, which is then executed in the environment.[65] The linguistic description of the environment given to the LLM planner can even be the LaTeX code of a paper describing the environment.[66]\\\\nIn the DEPS (\\\\\"Describe, Explain, Plan and Select\\\\\") method, an LLM is first connected to the visual world via image descriptions, then it is prompted to produce plans for complex tasks and behaviors based on its pretrained knowledge and environmental feedback it receives.[67]\\\\nThe Reflexion method[68] constructs an agent that learns over multiple episodes. At the end of each episode, the LLM is given the record of the episode, and prompted to think up \\\\\"lessons learned\\\\\", which would help it perform better at a subsequent episode. These \\\\\"lessons learned\\\\\" are given to the agent in the subsequent episodes.[citation needed]\\\\nMonte Carlo tree search can use an LLM as rollout heuristic. When a programmatic world model is not available, an LLM can also be prompted with a description of the environment to act as world model.[69]\\\\nFor open-ended exploration, an LLM can be used to score observations for their \\\\\"interestingness\\\\\", which can be used as a reward signal to guide a normal (non-LLM) reinforcement learning agent.[70] Alternatively, it can propose increasingly difficult tasks for curriculum learning.[71] Instead of outputting individual actions, an LLM planner can also construct \\\\\"skills\\\\\", or functions for complex action sequences. The skills can be stored and later invoked, allowing increasing levels of abstraction in planning.[71]\\\\nLLM-powered agents can keep a long-term memory of its previous contexts, and the memory can be retrieved in the same way as Retrieval Augmented Generation. Multiple such agents can interact socially.[72]\\\\nCompression[edit]\\\\nTypically, LLMs are trained with single- or half-precision floating point numbers (float32 and float16). One float16 has 16 bits, or 2 bytes, and so one billion parameters require 2 gigabytes. The largest models typically have 100 billion parameters, requiring 200 gigabytes to load, which places them outside the range of most consumer electronics.[73]\\\\nPost-training quantization[74] aims to decrease the space requirement by lowering precision of the parameters of a trained model, while preserving most of its performance.[75][76] The simplest form of quantization simply truncates all numbers to a given number of bits. It can be improved by using a different quantization codebook per layer. Further improvement can be done by applying different precisions to different parameters, with higher precision for particularly important parameters (\\\\\"outlier weights\\\\\").[77] See the visual guide to quantization by Maarten Grootendorst[78] for a visual depiction.\\\\nWhile quantized models are typically frozen, and only pre-quantized models are fine-tuned, quantized models can still be fine-tuned.[79]\\\\nMultimodality[edit]\\\\nSee also: Multimodal learning\\\\nMultimodality means \\\\\"having several modalities\\\\\", and a \\\\\"modality\\\\\" refers to a type of input or output, such as video, image, audio, text, proprioception, etc.[80] There have been many AI models trained specifically to ingest one modality and output another modality, such as AlexNet for image to label,[81] visual question answering for image-text to text,[82] and speech recognition for speech to text.\\\\nA common method to create multimodal models out of an LLM is to \\\\\"tokenize\\\\\" the output of a trained encoder. Concretely, one can construct an LLM that can understand images as follows: take a trained LLM, and take a trained image encoder \\\\n𝐸\\\\n. Make a small multilayered perceptron \\\\n𝑓\\\\n, so that for any image \\\\n𝑦\\\\n, the post-processed vector \\\\n𝑓\\\\n(\\\\n𝐸\\\\n(\\\\n𝑦\\\\n)\\\\n)\\\\n has the same dimensions as an encoded token. That is an \\\\\"image token\\\\\". Then, one can interleave text tokens and image tokens. The compound model is then fine-tuned on an image-text dataset. This basic construction can be applied with more sophistication to improve the model. The image encoder may be frozen to improve stability.[83]\\\\nFlamingo demonstrated the effectiveness of the tokenization method, finetuning a pair of pretrained language model and image encoder to perform better on visual question answering than models trained from scratch.[84] Google PaLM model was fine-tuned into a multimodal model PaLM-E using the tokenization method, and applied to robotic control.[85] LLaMA models have also been turned multimodal using the tokenization method, to allow image inputs,[86] and video inputs.[87]\\\\nGPT-4 can use both text and image as inputs[88] (although the vision component was not released to the public until GPT-4V[89]); Google DeepMind\\'s Gemini is also multimodal.[90] Mistral introduced its own multimodel Pixtral 12B model in September 2024.[91]\\\\nReasoning[edit]\\\\nIn late 2024, a new direction emerged in LLM development with models specifically designed for complex reasoning tasks. These \\\\\"reasoning models\\\\\" were trained to spend more time generating step-by-step solutions before providing final answers, similar to human problem-solving processes.[92] OpenAI introduced this trend with their o1 model in September 2024, followed by o3 in December 2024. These models showed significant improvements in mathematics, science, and coding tasks compared to traditional LLMs. For example, on International Mathematics Olympiad qualifying exam problems, GPT-4o achieved 13% accuracy while o1 reached 83%.[92][93] In January 2025, the Chinese company DeepSeek released DeepSeek-R1, a 671-billion-parameter open-weight reasoning model that achieved comparable performance to OpenAI\\'s o1 while being significantly more cost-effective to operate. Unlike proprietary models from OpenAI, DeepSeek-R1\\'s open-weight nature allowed researchers to study and build upon the algorithm, though its training data remained private.[94] These reasoning models typically require more computational resources per query compared to traditional LLMs, as they perform more extensive processing to work through problems step-by-step. However, they have shown superior capabilities in domains requiring structured logical thinking, such as mathematics, scientific research, and computer programming.[93]\\\\nProperties[edit]\\\\nScaling laws[edit]\\\\nMain article: Neural scaling law\\\\nThe performance of an LLM after pretraining largely depends on the:\\\\ncost of pretraining \\\\n𝐶\\\\n (the total amount of compute used),\\\\nsize of the artificial neural network itself, such as number of parameters \\\\n𝑁\\\\n (i.e. amount of neurons in its layers, amount of weights between them and biases),\\\\nsize of its pretraining dataset (i.e. number of tokens in corpus, \\\\n𝐷\\\\n).\\\\n\\\\\"Scaling laws\\\\\" are empirical statistical laws that predict LLM performance based on such factors. One particular scaling law (\\\\\"Chinchilla scaling\\\\\") for LLM autoregressively trained for one epoch, with a log-log learning rate schedule, states that:[95]\\\\n{\\\\n𝐶\\\\n=\\\\n𝐶\\\\n0\\\\n𝑁\\\\n𝐷\\\\n𝐿\\\\n𝐴\\\\n𝑁\\\\n𝛼\\\\n+\\\\n𝐵\\\\n𝐷\\\\n𝛽\\\\n+\\\\n𝐿\\\\n0\\\\nwhere the variables are\\\\n𝐶\\\\n is the cost of training the model, in FLOPs.\\\\n𝑁\\\\n is the number of parameters in the model.\\\\n𝐷\\\\n is the number of tokens in the training set.\\\\n𝐿\\\\n is the average negative log-likelihood loss per token (nats/token), achieved by the trained LLM on the test dataset.\\\\nand the statistical hyper-parameters are\\\\n𝐶\\\\n0\\\\n=\\\\n6\\\\n, meaning that it costs 6 FLOPs per parameter to train on one token. Note that training cost is much higher than inference cost, where it costs 1 to 2 FLOPs per parameter to infer on one token.[57]\\\\n𝛼\\\\n=\\\\n0.34\\\\n,\\\\n𝛽\\\\n=\\\\n0.28\\\\n,\\\\n𝐴\\\\n=\\\\n406.4\\\\n,\\\\n𝐵\\\\n=\\\\n410.7\\\\n,\\\\n𝐿\\\\n0\\\\n=\\\\n1.69\\\\nEmergent abilities[edit]\\\\nAt point(s) referred to as breaks,[96] the lines change their slopes, appearing on a linear-log plot as a series of linear segments connected by arcs.\\\\nPerformance of bigger models on various tasks, when plotted on a log-log scale, appears as a linear extrapolation of performance achieved by smaller models. However, this linearity may be punctuated by \\\\\"break(s)\\\\\"[96] in the scaling law, where the slope of the line changes abruptly, and where larger models acquire \\\\\"emergent abilities\\\\\".[42][97] They arise from the complex interaction of the model\\'s components and are not explicitly programmed or designed.[98]\\\\nFurthermore, recent research has demonstrated that AI systems, including large language models, can employ heuristic reasoning akin to human cognition. They balance between exhaustive logical processing and the use of cognitive shortcuts (heuristics), adapting their reasoning strategies to optimize between accuracy and effort. This behavior aligns with principles of resource-rational human cognition, as discussed in classical theories of bounded rationality and dual-process theory.[99]\\\\nThe most intriguing among emergent abilities is in-context learning from example demonstrations.[100] In-context learning is involved in tasks, such as:\\\\nreported arithmetics\\\\ndecoding the International Phonetic Alphabet\\\\nunscrambling a word\\'s letters\\\\ndisambiguating word-in-context datasets[42][101][102]\\\\nconverting spatial words\\\\ncardinal directions (for example, replying \\\\\"northeast\\\\\" in response to a 3x3 grid of 8 zeros and a 1 in the top-right), color terms represented in text.[103]\\\\nchain-of-thought prompting: Model outputs are improved by chain-of-thought prompting only when model size exceeds 62B. Smaller models perform better when prompted to answer immediately, without chain of thought.[104]\\\\nidentifying offensive content in paragraphs of Hinglish (a combination of Hindi and English), and generating a similar English equivalent of Kiswahili proverbs.[105]\\\\nSchaeffer et. al. argue that the emergent abilities are not unpredictably acquired, but predictably acquired according to a smooth scaling law. The authors considered a toy statistical model of an LLM solving multiple-choice questions, and showed that this statistical model, modified to account for other types of tasks, applies to these tasks as well.[106]\\\\nLet \\\\n𝑥\\\\n be the number of parameter count, and \\\\n𝑦\\\\n be the performance of the model.\\\\nWhen \\\\n𝑦\\\\n=\\\\naverage\\xa0\\\\nPr\\\\n(\\\\ncorrect token\\\\n)\\\\n, then \\\\n(\\\\nlog\\\\n\\u2061\\\\n𝑥\\\\n,\\\\n𝑦\\\\n)\\\\n is an exponential curve (before it hits the plateau at one), which looks like emergence.\\\\nWhen \\\\n𝑦\\\\n=\\\\naverage\\xa0\\\\nlog\\\\n\\u2061\\\\n(\\\\nPr\\\\n(\\\\ncorrect token\\\\n)\\\\n)\\\\n, then the \\\\n(\\\\nlog\\\\n\\u2061\\\\n𝑥\\\\n,\\\\n𝑦\\\\n)\\\\n plot is a straight line (before it hits the plateau at zero), which does not look like emergence.\\\\nWhen \\\\n𝑦\\\\n=\\\\naverage\\xa0\\\\nPr\\\\n(\\\\nthe most likely token is correct\\\\n)\\\\n, then \\\\n(\\\\nlog\\\\n\\u2061\\\\n𝑥\\\\n,\\\\n𝑦\\\\n)\\\\n is a step-function, which looks like emergence.\\\\nInterpretation[edit]\\\\nLarge language models by themselves are black boxes, and it is not clear how they can perform linguistic tasks. There are several methods for understanding how LLM work.\\\\nMechanistic interpretability aims to reverse-engineer LLM by discovering symbolic algorithms that approximate the inference performed by LLM. One example is Othello-GPT, where a small Transformer is trained to predict legal Othello moves. It is found that there is a linear representation of Othello board, and modifying the representation changes the predicted legal Othello moves in the correct way.[107][108] In another example, a small Transformer is trained on Karel programs. Similar to the Othello-GPT example, there is a linear representation of Karel program semantics, and modifying the representation changes output in the correct way. The model also generates correct programs that are on average shorter than those in the training set.[109]\\\\nIn another example, the authors trained small transformers on modular arithmetic addition. The resulting models were reverse-engineered, and it turned out they used discrete Fourier transform.[110]\\\\nA related concept is AI explainability, which focuses on understanding how an AI model arrives at a given result.\\\\nUnderstanding and intelligence[edit]\\\\nSee also: Philosophy of artificial intelligence and Artificial consciousness\\\\nNLP researchers were evenly split when asked, in a 2022 survey, whether (untuned) LLMs \\\\\"could (ever) understand natural language in some nontrivial sense\\\\\".[111] Proponents of \\\\\"LLM understanding\\\\\" believe that some LLM abilities, such as mathematical reasoning, imply an ability to \\\\\"understand\\\\\" certain concepts. A Microsoft team argued in 2023 that GPT-4 \\\\\"can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more\\\\\" and that GPT-4 \\\\\"could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence system\\\\\": \\\\\"Can one reasonably say that a system that passes exams for software engineering candidates is not really intelligent?\\\\\"[112][113] Ilya Sutskever argues that predicting the next word sometimes involves reasoning and deep insights, for example if the LLM has to predict the name of the criminal in an unknown detective novel after processing the entire story leading up to the revelation.[114] Some researchers characterize LLMs as \\\\\"alien intelligence\\\\\".[115][116] For example, Conjecture CEO Connor Leahy considers untuned LLMs to be like inscrutable alien \\\\\"Shoggoths\\\\\", and believes that RLHF tuning creates a \\\\\"smiling facade\\\\\" obscuring the inner workings of the LLM: \\\\\"If you don\\'t push it too far, the smiley face stays on. But then you give it [an unexpected] prompt, and suddenly you see this massive underbelly of insanity, of weird thought processes and clearly non-human understanding.\\\\\"[117][118]\\\\nIn contrast, some skeptics of LLM understanding believe that existing LLMs are \\\\\"simply remixing and recombining existing writing\\\\\",[116] a phenomenon known as stochastic parrot, or they point to the deficits existing LLMs continue to have in prediction skills, reasoning skills, agency, and explainability.[111] For example, GPT-4 has natural deficits in planning and in real-time learning.[113] Generative LLMs have been observed to confidently assert claims of fact which do not seem to be justified by their training data, a phenomenon which has been termed \\\\\"hallucination\\\\\".[119] Specifically, hallucinations in the context of LLMs correspond to the generation of text or responses that seem syntactically sound, fluent, and natural but are factually incorrect, nonsensical, or unfaithful to the provided source input.[120] Neuroscientist Terrence Sejnowski has argued that \\\\\"The diverging opinions of experts on the intelligence of LLMs suggests that our old ideas based on natural intelligence are inadequate\\\\\".[111]\\\\nThe matter of LLM\\'s exhibiting intelligence or understanding has two main aspects – the first is how to model thought and language in a computer system, and the second is how to enable the computer system to generate human like language.[111] These aspects of language as a model of cognition have been developed in the field of cognitive linguistics. American linguist George Lakoff presented Neural Theory of Language (NTL)[121] as a computational basis for using language as a model of learning tasks and understanding. The NTL Model outlines how specific neural structures of the human brain shape the nature of thought and language and in turn what are the computational properties of such neural systems that can be applied to model thought and language in a computer system. After a framework for modeling language in a computer systems was established, the focus shifted to establishing frameworks for computer systems to generate language with acceptable grammar. In his 2014 book titled The Language Myth: Why Language Is Not An Instinct, British cognitive linguist and digital communication technologist Vyvyan Evans mapped out the role of probabilistic context-free grammar (PCFG) in enabling NLP to model cognitive patterns and generate human like language.[122][123]\\\\nEvaluation[edit]\\\\nPerplexity[edit]\\\\nThe canonical measure of the performance of an LLM is its perplexity on a given text corpus. Perplexity measures how well a model predicts the contents of a dataset; the higher the likelihood the model assigns to the dataset, the lower the perplexity. In mathematical terms, perplexity is the exponential of the average negative log likelihood per token.\\\\nlog\\\\n\\u2061\\\\n(\\\\nPerplexity\\\\n)\\\\n=\\\\n−\\\\n1\\\\n𝑁\\\\n∑\\\\n𝑖\\\\n=\\\\n1\\\\n𝑁\\\\nlog\\\\n\\u2061\\\\n(\\\\nPr\\\\n(\\\\ntoken\\\\n𝑖\\\\n∣\\\\ncontext for token\\\\n𝑖\\\\n)\\\\n)\\\\nHere, \\\\n𝑁\\\\n is the number of tokens in the text corpus, and \\\\\"context for token \\\\n𝑖\\\\n\\\\\" depends on the specific type of LLM. If the LLM is autoregressive, then \\\\\"context for token \\\\n𝑖\\\\n\\\\\" is the segment of text appearing before token \\\\n𝑖\\\\n. If the LLM is masked, then \\\\\"context for token \\\\n𝑖\\\\n\\\\\" is the segment of text surrounding token \\\\n𝑖\\\\n.\\\\nBecause language models may overfit to training data, models are usually evaluated by their perplexity on a test set.[49] This evaluation is potentially problematic for larger models which, as they are trained on increasingly large corpora of text, are increasingly likely to inadvertently include portions of any given test set.[1]\\\\nBPW, BPC, and BPT[edit]\\\\nIn information theory, the concept of entropy is intricately linked to perplexity, a relationship notably established by Claude Shannon.[124] This relationship is mathematically expressed as \\\\nEntropy\\\\n=\\\\nlog\\\\n2\\\\n\\u2061\\\\n(\\\\nPerplexity\\\\n)\\\\n.\\\\nEntropy, in this context, is commonly quantified in terms of bits per word (BPW) or bits per character (BPC), which hinges on whether the language model utilizes word-based or character-based tokenization.\\\\nNotably, in the case of larger language models that predominantly employ sub-word tokenization, bits per token (BPT) emerges as a seemingly more appropriate measure. However, due to the variance in tokenization methods across different Large Language Models (LLMs), BPT does not serve as a reliable metric for comparative analysis among diverse models. To convert BPT into BPW, one can multiply it by the average number of tokens per word.\\\\nIn the evaluation and comparison of language models, cross-entropy is generally the preferred metric over entropy. The underlying principle is that a lower BPW is indicative of a model\\'s enhanced capability for compression. This, in turn, reflects the model\\'s proficiency in making accurate predictions.\\\\nTask-specific datasets and benchmarks[edit]\\\\nA large number of testing datasets and benchmarks have also been developed to evaluate the capabilities of language models on more specific downstream tasks. Tests may be designed to evaluate a variety of capabilities, including general knowledge, commonsense reasoning, and mathematical problem-solving.\\\\nOne broad category of evaluation dataset is question answering datasets, consisting of pairs of questions and correct answers, for example, (\\\\\"Have the San Jose Sharks won the Stanley Cup?\\\\\", \\\\\"No\\\\\").[125] A question answering task is considered \\\\\"open book\\\\\" if the model\\'s prompt includes text from which the expected answer can be derived (for example, the previous question could be adjoined with some text which includes the sentence \\\\\"The Sharks have advanced to the Stanley Cup finals once, losing to the Pittsburgh Penguins in 2016.\\\\\"[125]). Otherwise, the task is considered \\\\\"closed book\\\\\", and the model must draw on knowledge retained during training.[126] Some examples of commonly used question answering datasets include TruthfulQA, Web Questions, TriviaQA, and SQuAD.[126]\\\\nEvaluation datasets may also take the form of text completion, having the model select the most likely word or sentence to complete a prompt, for example: \\\\\"Alice was friends with Bob. Alice went to visit her friend, ____\\\\\".[1]\\\\nSome composite benchmarks have also been developed which combine a diversity of different evaluation datasets and tasks. Examples include GLUE, SuperGLUE, MMLU, BIG-bench, HELM, and HLE (Humanity\\'s Last Exam).[124][126] OpenAI has released tools for running composite benchmarks, but noted that the eval results are sensitive to the prompting method.[127][128] Some public datasets contain questions that are mislabeled, ambiguous, unanswerable, or otherwise of low-quality, which can be cleaned to give more reliable benchmark scores.[129]\\\\nIt was previously standard to report results on a heldout portion of an evaluation dataset after doing supervised fine-tuning on the remainder. It is now more common to evaluate a pre-trained model directly through prompting techniques, though researchers vary in the details of how they formulate prompts for particular tasks, particularly with respect to how many examples of solved tasks are adjoined to the prompt (i.e. the value of n in n-shot prompting).\\\\nAdversarially constructed evaluations[edit]\\\\nBecause of the rapid pace of improvement of large language models, evaluation benchmarks have suffered from short lifespans, with state of the art models quickly \\\\\"saturating\\\\\" existing benchmarks, exceeding the performance of human annotators, leading to efforts to replace or augment the benchmark with more challenging tasks.[130] In addition, there are cases of \\\\\"shortcut learning\\\\\" wherein AIs sometimes \\\\\"cheat\\\\\" on multiple-choice tests by using statistical correlations in superficial test question wording in order to guess the correct responses, without necessarily understanding the actual question being asked.[111]\\\\nSome datasets have been constructed adversarially, focusing on particular problems on which extant language models seem to have unusually poor performance compared to humans. One example is the TruthfulQA dataset, a question answering dataset consisting of 817 questions which language models are susceptible to answering incorrectly by mimicking falsehoods to which they were repeatedly exposed during training. For example, an LLM may answer \\\\\"No\\\\\" to the question \\\\\"Can you teach an old dog new tricks?\\\\\" because of its exposure to the English idiom you can\\'t teach an old dog new tricks, even though this is not literally true.[131]\\\\nAnother example of an adversarial evaluation dataset is Swag and its successor, HellaSwag, collections of problems in which one of multiple options must be selected to complete a text passage. The incorrect completions were generated by sampling from a language model and filtering with a set of classifiers. The resulting problems are trivial for humans but at the time the datasets were created state of the art language models had poor accuracy on them. For example:\\\\nWe see a fitness center sign. We then see a man talking to the camera and sitting and laying on a exercise ball. The man...\\\\na) demonstrates how to increase efficient exercise work by running up and down balls.\\\\nb) moves all his arms and legs and builds up a lot of muscle.\\\\nc) then plays the ball and we see a graphics and hedge trimming demonstration.\\\\nd) performs sit ups while on the ball and talking.[132]\\\\nBERT selects b) as the most likely completion, though the correct answer is d).[132]\\\\nLimitations of LLM benchmarks[edit]\\\\nBenchmarks can become outdated rapidly. Once a model attains near-perfect scores on a given benchmark, that benchmark ceases to serve as a meaningful indicator of progress. This phenomenon, known as \\\\\"benchmark saturation,\\\\\" necessitates the development of more challenging and nuanced tasks to continue advancing LLM capabilities. For instance, traditional benchmarks like HellaSwag and MMLU have seen models achieving high accuracy already.\\\\nWider impact[edit]\\\\nIn 2023, Nature Biomedical Engineering wrote that \\\\\"it is no longer possible to accurately distinguish\\\\\" human-written text from text created by large language models, and that \\\\\"It is all but certain that general-purpose large language models will rapidly proliferate... It is a rather safe bet that they will change many industries over time.\\\\\"[133] Goldman Sachs suggested in 2023 that generative language AI could increase global GDP by 7% in the next ten years, and could expose to automation 300 million jobs globally.[134][135]\\\\nMemorization and copyright[edit]\\\\nFurther information: Artificial intelligence and copyright\\\\nMemorization is an emergent behavior in LLMs in which long strings of text are occasionally output verbatim from training data, contrary to typical behavior of traditional artificial neural nets. Evaluations of controlled LLM output measure the amount memorized from training data (focused on GPT-2-series models) as variously over 1% for exact duplicates[136] or up to about 7%.[137]\\\\nA 2023 study showed that when ChatGPT 3.5 turbo was prompted to repeat the same word indefinitely, after a few hundreds of repetitions, it would start outputting excerpts from its training data.[138]\\\\nSecurity[edit]\\\\nSome commenters expressed concern over accidental or deliberate creation of misinformation, or other forms of misuse.[139] For example, the availability of large language models could reduce the skill-level required to commit bioterrorism; biosecurity researcher Kevin Esvelt has suggested that LLM creators should exclude from their training data papers on creating or enhancing pathogens.[140]\\\\nThe potential presence of \\\\\"sleeper agents\\\\\" within LLM models is another emerging security concern. These are hidden functionalities built into the model that remain dormant until triggered by a specific event or condition. Upon activation, the LLM deviates from its expected behavior to make insecure actions.[141]\\\\nLLM applications accessible to the public, like ChatGPT or Claude, typically incorporate safety measures designed to filter out harmful content. However, implementing these controls effectively has proven challenging. For instance, a 2023 study[142] proposed a method for circumventing LLM safety systems. Similarly, Yongge Wang[143] illustrated in 2024 how a potential criminal could potentially bypass ChatGPT 4o\\'s safety controls to obtain information on establishing a drug trafficking operation.\\\\nAlgorithmic bias[edit]\\\\nMain article: Algorithmic bias\\\\nWhile LLMs have shown remarkable capabilities in generating human-like text, they are susceptible to inheriting and amplifying biases present in their training data. This can manifest in skewed representations or unfair treatment of different demographics, such as those based on race, gender, language, and cultural groups.[144] Since English data is overrepresented in current large language models\\' training data, it may also downplay non-English views.[145]\\\\nStereotyping[edit]\\\\nAI models can reinforce a wide range of stereotypes, including those based on gender, ethnicity, age, nationality, religion, or occupation. This can lead to outputs that unfairly generalize or caricature groups of people, sometimes in harmful or derogatory ways.[146]\\\\nNotably, gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained. Large language models often assign roles and characteristics based on traditional gender norms.[144] For example, it might associate nurses or secretaries predominantly with women and engineers or CEOs with men.[147]\\\\nPolitical bias[edit]\\\\nPolitical bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.[148]\\\\nEnergy demands[edit]\\\\nThe energy demands of LLMs have grown along with their size and capabilities. Data centers that enable LLM training require substantial amounts of electricity. Much of that electricity is generated by non-renewable resources that create greenhouse gases and contribute to climate change.[149] Nuclear power and geothermal energy are two options tech companies are exploring to meet the sizable energy demands of LLM training.[150] The significant expense of investing in geothermal solutions has led to major shale producers like Chevron and Exxon Mobil advocating for tech companies to use electricity produced via natural gas to fuel their large energy demands.[151]\\\\nSee also[edit]\\\\nFoundation models\\\\nList of large language models\\\\nList of chatbots\\\\nReferences[edit]\\\\n^ a b c Brown, Tom B.; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Gretchen; Henighan, Tom; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel M.; Wu, Jeffrey; Winter, Clemens; Hesse, Christopher; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Scott; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (Dec 2020). Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.F.; Lin, H. (eds.). \\\\\"Language Models are Few-Shot Learners\\\\\" (PDF). Advances in Neural Information Processing Systems. 33. Curran Associates, Inc.: 1877–1901. Archived (PDF) from the original on 2023-11-17. Retrieved 2023-03-14.\\\\n^ Fathallah, Nadeen; Das, Arunav; De Giorgis, Stefano; Poltronieri, Andrea; Haase, Peter; Kovriguina, Liubov (2024-05-26). NeOn-GPT: A Large Language Model-Powered Pipeline for Ontology Learning (PDF). Extended Semantic Web Conference 2024. Hersonissos, Greece.\\\\n^ Manning, Christopher D. (2022). \\\\\"Human Language Understanding & Reasoning\\\\\". Daedalus. 151 (2): 127–138. doi:10.1162/daed_a_01905. S2CID\\xa0248377870. Archived from the original on 2023-11-17. Retrieved 2023-03-09.\\\\n^ Goodman, Joshua (2001-08-09), A Bit of Progress in Language Modeling, arXiv:cs/0108005, Bibcode:2001cs........8005G\\\\n^ Kilgarriff, Adam; Grefenstette, Gregory (September 2003). \\\\\"Introduction to the Special Issue on the Web as Corpus\\\\\". Computational Linguistics. 29 (3): 333–347. doi:10.1162/089120103322711569. ISSN\\xa00891-2017.\\\\n^ Banko, Michele; Brill, Eric (2001). \\\\\"Scaling to very very large corpora for natural language disambiguation\\\\\". Proceedings of the 39th Annual Meeting on Association for Computational Linguistics - ACL \\'01. Morristown, NJ, USA: Association for Computational Linguistics: 26–33. doi:10.3115/1073012.1073017.\\\\n^ Resnik, Philip; Smith, Noah A. (September 2003). \\\\\"The Web as a Parallel Corpus\\\\\". Computational Linguistics. 29 (3): 349–380. doi:10.1162/089120103322711578. ISSN\\xa00891-2017. Archived from the original on 2024-06-07. Retrieved 2024-06-07.\\\\n^ Halevy, Alon; Norvig, Peter; Pereira, Fernando (March 2009). \\\\\"The Unreasonable Effectiveness of Data\\\\\". IEEE Intelligent Systems. 24 (2): 8–12. doi:10.1109/MIS.2009.36. ISSN\\xa01541-1672.\\\\n^ Chen, Leiyu; Li, Shaobo; Bai, Qiang; Yang, Jing; Jiang, Sanlong; Miao, Yanming (2021). \\\\\"Review of Image Classification Algorithms Based on Convolutional Neural Networks\\\\\". Remote Sensing. 13 (22): 4712. Bibcode:2021RemS...13.4712C. doi:10.3390/rs13224712.\\\\n^ Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Łukasz; Polosukhin, Illia (2017). \\\\\"Attention is All you Need\\\\\" (PDF). Advances in Neural Information Processing Systems. 30. Curran Associates, Inc. Archived (PDF) from the original on 2024-02-21. Retrieved 2024-01-21.\\\\n^ Bahdanau, Dzmitry; Cho, Kyunghyun; Bengio, Yoshua (2014). \\\\\"Neural Machine Translation by Jointly Learning to Align and Translate\\\\\". arXiv:1409.0473 [cs.CL].\\\\n^ Rogers, Anna; Kovaleva, Olga; Rumshisky, Anna (2020). \\\\\"A Primer in BERTology: What We Know About How BERT Works\\\\\". Transactions of the Association for Computational Linguistics. 8: 842–866. arXiv:2002.12327. doi:10.1162/tacl_a_00349. S2CID\\xa0211532403. Archived from the original on 2022-04-03. Retrieved 2024-01-21.\\\\n^ Movva, Rajiv; Balachandar, Sidhika; Peng, Kenny; Agostini, Gabriel; Garg, Nikhil; Pierson, Emma (2024). \\\\\"Topics, Authors, and Institutions in Large Language Model Research: Trends from 17K arXiv Papers\\\\\". Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). pp.\\xa01223–1243. arXiv:2307.10700. doi:10.18653/v1/2024.naacl-long.67. Retrieved 2024-12-08.\\\\n^ Hern, Alex (14 February 2019). \\\\\"New AI fake text generator may be too dangerous to release, say creators\\\\\". The Guardian. Archived from the original on 14 February 2019. Retrieved 20 January 2024.\\\\n^ \\\\\"ChatGPT a year on: 3 ways the AI chatbot has completely changed the world in 12 months\\\\\". Euronews. November 30, 2023. Archived from the original on January 14, 2024. Retrieved January 20, 2024.\\\\n^ Heaven, Will (March 14, 2023). \\\\\"GPT-4 is bigger and better than ChatGPT—but OpenAI won\\'t say why\\\\\". MIT Technology Review. Archived from the original on March 17, 2023. Retrieved January 20, 2024.\\\\n^ Movva, Rajiv; Balachandar, Sidhika; Peng, Kenny; Agostini, Gabriel; Garg, Nikhil; Pierson, Emma (2024). \\\\\"Topics, Authors, and Institutions in Large Language Model Research: Trends from 17K arXiv Papers\\\\\". Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). pp.\\xa01223–1243. arXiv:2307.10700. doi:10.18653/v1/2024.naacl-long.67. Retrieved 2024-12-08.\\\\n^ \\\\\"Parameters in notable artificial intelligence systems\\\\\". ourworldindata.org. November 30, 2023. Retrieved January 20, 2024.\\\\n^ Sharma, Shubham (2025-01-20). \\\\\"Open-source DeepSeek-R1 uses pure reinforcement learning to match OpenAI o1 — at 95% less cost\\\\\". VentureBeat. Retrieved 2025-01-26.\\\\n^ Zia, Dr Tehseen (2024-01-08). \\\\\"Unveiling of Large Multimodal Models: Shaping the Landscape of Language Models in 2024\\\\\". Unite.AI. Retrieved 2024-12-28.\\\\n^ Peng, Bo; et\\xa0al. (2023). \\\\\"RWKV: Reinventing RNNS for the Transformer Era\\\\\". arXiv:2305.13048 [cs.CL].\\\\n^ Merritt, Rick (2022-03-25). \\\\\"What Is a Transformer Model?\\\\\". NVIDIA Blog. Archived from the original on 2023-11-17. Retrieved 2023-07-25.\\\\n^ Gu, Albert; Dao, Tri (2023-12-01), Mamba: Linear-Time Sequence Modeling with Selective State Spaces, arXiv:2312.00752\\\\n^ Kaushal, Ayush; Mahowald, Kyle (2022-06-06), What do tokens know about their characters and how do they know it?, arXiv:2206.02608\\\\n^ Yennie Jun (2023-05-03). \\\\\"All languages are NOT created (tokenized) equal\\\\\". Language models cost much more in some languages than others. Archived from the original on 2023-08-17. Retrieved 2023-08-17. In other words, to express the same sentiment, some languages require up to 10 times more tokens.\\\\n^ Petrov, Aleksandar; Malfa, Emanuele La; Torr, Philip; Bibi, Adel (June 23, 2023). \\\\\"Language Model Tokenizers Introduce Unfairness Between Languages\\\\\". NeurIPS. arXiv:2305.15425. Archived from the original on December 15, 2023. Retrieved September 16, 2023 – via openreview.net.\\\\n^ \\\\\"OpenAI API\\\\\". platform.openai.com. Archived from the original on April 23, 2023. Retrieved 2023-04-30.\\\\n^ a b Paaß, Gerhard; Giesselbach, Sven (2022). \\\\\"Pre-trained Language Models\\\\\". Foundation Models for Natural Language Processing. Artificial Intelligence: Foundations, Theory, and Algorithms. pp.\\xa019–78. doi:10.1007/978-3-031-23190-2_2. ISBN\\xa09783031231902. Archived from the original on 3 August 2023. Retrieved 3 August 2023.\\\\n^ Petrov, Aleksandar; Emanuele La Malfa; Torr, Philip H. S.; Bibi, Adel (2023). \\\\\"Language Model Tokenizers Introduce Unfairness Between Languages\\\\\". arXiv:2305.15425 [cs.CL].\\\\n^ Lundberg, Scott (2023-12-12). \\\\\"The Art of Prompt Design: Prompt Boundaries and Token Healing\\\\\". Medium. Retrieved 2024-08-05.\\\\n^ Dodge, Jesse; Sap, Maarten; Marasović, Ana; Agnew, William; Ilharco, Gabriel; Groeneveld, Dirk; Mitchell, Margaret; Gardner, Matt (2021). \\\\\"Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus\\\\\". arXiv:2104.08758 [cs.CL].\\\\n^ Lee, Katherine; Ippolito, Daphne; Nystrom, Andrew; Zhang, Chiyuan; Eck, Douglas; Callison-Burch, Chris; Carlini, Nicholas (May 2022). \\\\\"Deduplicating Training Data Makes Language Models Better\\\\\" (PDF). Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. 1: Long Papers: 8424–8445. doi:10.18653/v1/2022.acl-long.577.\\\\n^ Li, Yuanzhi; Bubeck, Sébastien; Eldan, Ronen; Del Giorno, Allie; Gunasekar, Suriya; Lee, Yin Tat (2023-09-11), Textbooks Are All You Need II: phi-1.5 technical report, arXiv:2309.05463\\\\n^ Lin, Zhenghao; Gou, Zhibin; Gong, Yeyun; Liu, Xiao; Shen, Yelong; Xu, Ruochen; Lin, Chen; Yang, Yujiu; Jiao, Jian (2024-04-11). \\\\\"Rho-1: Not All Tokens Are What You Need\\\\\". arXiv:2404.07965 [cs.CL].\\\\n^ Brown, Tom B.; et\\xa0al. (2020). \\\\\"Language Models are Few-Shot Learners\\\\\". arXiv:2005.14165 [cs.CL].\\\\n^ Abdin, Marah; Jacobs, Sam Ade; Awan, Ammar Ahmad; Aneja, Jyoti; Awadallah, Ahmed; Awadalla, Hany; Bach, Nguyen; Bahree, Amit; Bakhtiari, Arash (2024-04-23). \\\\\"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone\\\\\". arXiv:2404.14219 [cs.CL].\\\\n^ Ouyang, Long; Wu, Jeff; Jiang, Xu; Almeida, Diogo; Wainwright, Carroll L.; Mishkin, Pamela; Zhang, Chong; Agarwal, Sandhini; Slama, Katarina; Ray, Alex; Schulman, John; Hilton, Jacob; Kelton, Fraser; Miller, Luke; Simens, Maddie; Askell, Amanda; Welinder, Peter; Christiano, Paul; Leike, Jan; Lowe, Ryan (2022). \\\\\"Training language models to follow instructions with human feedback\\\\\". arXiv:2203.02155 [cs.CL].\\\\n^ Wang, Yizhong; Kordi, Yeganeh; Mishra, Swaroop; Liu, Alisa; Smith, Noah A.; Khashabi, Daniel; Hajishirzi, Hannaneh (2022). \\\\\"Self-Instruct: Aligning Language Model with Self Generated Instructions\\\\\". arXiv:2212.10560 [cs.CL].\\\\n^ Shazeer, Noam; Mirhoseini, Azalia; Maziarz, Krzysztof; Davis, Andy; Le, Quoc; Hinton, Geoffrey; Dean, Jeff (2017-01-01). \\\\\"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\\\\\". arXiv:1701.06538 [cs.LG].\\\\n^ Lepikhin, Dmitry; Lee, HyoukJoong; Xu, Yuanzhong; Chen, Dehao; Firat, Orhan; Huang, Yanping; Krikun, Maxim; Shazeer, Noam; Chen, Zhifeng (2021-01-12). \\\\\"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\\\\\". arXiv:2006.16668 [cs.CL].\\\\n^ Dai, Andrew M; Du, Nan (December 9, 2021). \\\\\"More Efficient In-Context Learning with GLaM\\\\\". ai.googleblog.com. Archived from the original on 2023-03-12. Retrieved 2023-03-09.\\\\n^ a b c Wei, Jason; Tay, Yi; Bommasani, Rishi; Raffel, Colin; Zoph, Barret; Borgeaud, Sebastian; Yogatama, Dani; Bosma, Maarten; Zhou, Denny; Metzler, Donald; Chi, Ed H.; Hashimoto, Tatsunori; Vinyals, Oriol; Liang, Percy; Dean, Jeff; Fedus, William (31 August 2022). \\\\\"Emergent Abilities of Large Language Models\\\\\". Transactions on Machine Learning Research. ISSN\\xa02835-8856. Archived from the original on 22 March 2023. Retrieved 19 March 2023.\\\\n^ Allamar, Jay. \\\\\"Illustrated transformer\\\\\". Archived from the original on 2023-07-25. Retrieved 2023-07-29.\\\\n^ Allamar, Jay. \\\\\"The Illustrated GPT-2 (Visualizing Transformer Language Models)\\\\\". Retrieved 2023-08-01.\\\\n^ \\\\\"Our next-generation model: Gemini 1.5\\\\\". Google. 15 February 2024. Archived from the original on 18 February 2024. Retrieved 18 February 2024.\\\\n^ \\\\\"Long context prompting for Claude 2.1\\\\\". December 6, 2023. Archived from the original on August 27, 2024. Retrieved January 20, 2024.\\\\n^ \\\\\"Rate limits\\\\\". openai.com. Archived from the original on February 2, 2024. Retrieved January 20, 2024.\\\\n^ Zaib, Munazza; Sheng, Quan Z.; Emma Zhang, Wei (4 February 2020). \\\\\"A Short Survey of Pre-trained Language Models for Conversational AI-A New Age in NLP\\\\\". Proceedings of the Australasian Computer Science Week Multiconference. pp.\\xa01–4. arXiv:2104.10810. doi:10.1145/3373017.3373028. ISBN\\xa09781450376976. S2CID\\xa0211040895.\\\\n^ a b c Jurafsky, Dan; Martin, James H. (7 January 2023). Speech and Language Processing (PDF) (3rd edition draft\\xa0ed.). Archived (PDF) from the original on 23 March 2023. Retrieved 24 May 2022.\\\\n^ \\\\\"From bare metal to a 70B model: infrastructure set-up and scripts\\\\\". imbue.com. Archived from the original on 2024-07-26. Retrieved 2024-07-24.\\\\n^ \\\\\"metaseq/projects/OPT/chronicles at main · facebookresearch/metaseq\\\\\". GitHub. Archived from the original on 2024-01-24. Retrieved 2024-07-24.\\\\n^ Albrecht, Josh (2024-07-23). \\\\\"State of the Art: Training >70B LLMs on 10,000 H100 clusters\\\\\". www.latent.space. Retrieved 2024-07-24.\\\\n^ Wiggers, Kyle (28 April 2022). \\\\\"The emerging types of language models and why they matter\\\\\". TechCrunch. Archived from the original on 16 March 2023. Retrieved 9 March 2023.\\\\n^ Sharir, Or; Peleg, Barak; Shoham, Yoav (2020). \\\\\"The Cost of Training NLP Models: A Concise Overview\\\\\". arXiv:2004.08900 [cs.CL].\\\\n^ Biderman, Stella; Schoelkopf, Hailey; Anthony, Quentin; Bradley, Herbie; Khan, Mohammad Aflah; Purohit, Shivanshu; Prashanth, USVSN Sai (April 2023). \\\\\"Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling\\\\\". arXiv:2304.01373 [cs.CL].\\\\n^ Maslej, Nestor; Fattorini, Loredana; Brynjolfsson, Erik; Etchemendy, John; Ligett, Katrina; Lyons, Terah; Manyika, James; Ngo, Helen; Niebles, Juan Carlos (2023-10-05), Artificial Intelligence Index Report 2023, arXiv:2310.03715\\\\n^ a b Section 2.1 and Table 1, Kaplan, Jared; McCandlish, Sam; Henighan, Tom; Brown, Tom B.; Chess, Benjamin; Child, Rewon; Gray, Scott; Radford, Alec; Wu, Jeffrey; Amodei, Dario (2020). \\\\\"Scaling Laws for Neural Language Models\\\\\". arXiv:2001.08361 [cs.LG].\\\\n^ Gao, Luyu; Madaan, Aman; Zhou, Shuyan; Alon, Uri; Liu, Pengfei; Yang, Yiming; Callan, Jamie; Neubig, Graham (2022-11-01). \\\\\"PAL: Program-aided Language Models\\\\\". arXiv:2211.10435 [cs.CL].\\\\n^ \\\\\"PAL: Program-aided Language Models\\\\\". reasonwithpal.com. Archived from the original on 2023-06-12. Retrieved 2023-06-12.\\\\n^ Paranjape, Bhargavi; Lundberg, Scott; Singh, Sameer; Hajishirzi, Hannaneh; Zettlemoyer, Luke; Tulio Ribeiro, Marco (2023-03-01). \\\\\"ART: Automatic multi-step reasoning and tool-use for large language models\\\\\". arXiv:2303.09014 [cs.CL].\\\\n^ Liang, Yaobo; Wu, Chenfei; Song, Ting; Wu, Wenshan; Xia, Yan; Liu, Yu; Ou, Yang; Lu, Shuai; Ji, Lei; Mao, Shaoguang; Wang, Yun; Shou, Linjun; Gong, Ming; Duan, Nan (2023-03-01). \\\\\"TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs\\\\\". arXiv:2303.16434 [cs.AI].\\\\n^ Patil, Shishir G.; Zhang, Tianjun; Wang, Xin; Gonzalez, Joseph E. (2023-05-01). \\\\\"Gorilla: Large Language Model Connected with Massive APIs\\\\\". arXiv:2305.15334 [cs.CL].\\\\n^ Lewis, Patrick; Perez, Ethan; Piktus, Aleksandra; Petroni, Fabio; Karpukhin, Vladimir; Goyal, Naman; Küttler, Heinrich; Lewis, Mike; Yih, Wen-tau; Rocktäschel, Tim; Riedel, Sebastian; Kiela, Douwe (2020). \\\\\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\\\\". Advances in Neural Information Processing Systems. 33. Curran Associates, Inc.: 9459–9474. arXiv:2005.11401. Archived from the original on 2023-06-12. Retrieved 2023-06-12.\\\\n^ \\\\\"The Growth Behind LLM-based Autonomous Agents\\\\\". KDnuggets. October 23, 2023.\\\\n^ Yao, Shunyu; Zhao, Jeffrey; Yu, Dian; Du, Nan; Shafran, Izhak; Narasimhan, Karthik; Cao, Yuan (2022-10-01). \\\\\"ReAct: Synergizing Reasoning and Acting in Language Models\\\\\". arXiv:2210.03629 [cs.CL].\\\\n^ Wu, Yue; Prabhumoye, Shrimai; Min, So Yeon (24 May 2023). \\\\\"SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning\\\\\". arXiv:2305.15486 [cs.AI].\\\\n^ Wang, Zihao; Cai, Shaofei; Liu, Anji; Ma, Xiaojian; Liang, Yitao (2023-02-03). \\\\\"Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents\\\\\". arXiv:2302.01560 [cs.AI].\\\\n^ Shinn, Noah; Cassano, Federico; Labash, Beck; Gopinath, Ashwin; Narasimhan, Karthik; Yao, Shunyu (2023-03-01). \\\\\"Reflexion: Language Agents with Verbal Reinforcement Learning\\\\\". arXiv:2303.11366 [cs.AI].\\\\n^ Hao, Shibo; Gu, Yi; Ma, Haodi; Jiahua Hong, Joshua; Wang, Zhen; Zhe Wang, Daisy; Hu, Zhiting (2023-05-01). \\\\\"Reasoning with Language Model is Planning with World Model\\\\\". arXiv:2305.14992 [cs.CL].\\\\n^ Zhang, Jenny; Lehman, Joel; Stanley, Kenneth; Clune, Jeff (2 June 2023). \\\\\"OMNI: Open-endedness via Models of human Notions of Interestingness\\\\\". arXiv:2306.01711 [cs.AI].\\\\n^ a b \\\\\"Voyager | An Open-Ended Embodied Agent with Large Language Models\\\\\". voyager.minedojo.org. Archived from the original on 2023-06-08. Retrieved 2023-06-09.\\\\n^ Park, Joon Sung; O\\'Brien, Joseph C.; Cai, Carrie J.; Ringel Morris, Meredith; Liang, Percy; Bernstein, Michael S. (2023-04-01). \\\\\"Generative Agents: Interactive Simulacra of Human Behavior\\\\\". arXiv:2304.03442 [cs.HC].\\\\n^ Mann, Tobias. \\\\\"How to run an LLM locally on your PC in less than 10 minutes\\\\\". www.theregister.com. Retrieved 2024-05-17.\\\\n^ Nagel, Markus; Amjad, Rana Ali; Baalen, Mart Van; Louizos, Christos; Blankevoort, Tijmen (2020-11-21). \\\\\"Up or Down? Adaptive Rounding for Post-Training Quantization\\\\\". Proceedings of the 37th International Conference on Machine Learning. PMLR: 7197–7206. Archived from the original on 2023-06-14. Retrieved 2023-06-14.\\\\n^ Polino, Antonio; Pascanu, Razvan; Alistarh, Dan (2018-02-01). \\\\\"Model compression via distillation and quantization\\\\\". arXiv:1802.05668 [cs.NE].\\\\n^ Frantar, Elias; Ashkboos, Saleh; Hoefler, Torsten; Alistarh, Dan (2022-10-01). \\\\\"GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers\\\\\". arXiv:2210.17323 [cs.LG].\\\\n^ Dettmers, Tim; Svirschevski, Ruslan; Egiazarian, Vage; Kuznedelev, Denis; Frantar, Elias; Ashkboos, Saleh; Borzunov, Alexander; Hoefler, Torsten; Alistarh, Dan (2023-06-01). \\\\\"SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression\\\\\". arXiv:2306.03078 [cs.CL].\\\\n^ Grootendorst, Maarten. \\\\\"A Visual Guide to Quantization\\\\\". newsletter.maartengrootendorst.com. Archived from the original on 31 Jul 2024. Retrieved 2024-07-31.\\\\n^ Dettmers, Tim; Pagnoni, Artidoro; Holtzman, Ari; Zettlemoyer, Luke (2023-05-01). \\\\\"QLoRA: Efficient Finetuning of Quantized LLMs\\\\\". arXiv:2305.14314 [cs.LG].\\\\n^ Kiros, Ryan; Salakhutdinov, Ruslan; Zemel, Rich (2014-06-18). \\\\\"Multimodal Neural Language Models\\\\\". Proceedings of the 31st International Conference on Machine Learning. PMLR: 595–603. Archived from the original on 2023-07-02. Retrieved 2023-07-02.\\\\n^ Krizhevsky, Alex; Sutskever, Ilya; Hinton, Geoffrey E (2012). \\\\\"ImageNet Classification with Deep Convolutional Neural Networks\\\\\". Advances in Neural Information Processing Systems. 25. Curran Associates, Inc. Archived from the original on 2023-07-02. Retrieved 2023-07-02.\\\\n^ Antol, Stanislaw; Agrawal, Aishwarya; Lu, Jiasen; Mitchell, Margaret; Batra, Dhruv; Zitnick, C. Lawrence; Parikh, Devi (2015). \\\\\"VQA: Visual Question Answering\\\\\". ICCV: 2425–2433. Archived from the original on 2023-07-02. Retrieved 2023-07-02.\\\\n^ Li, Junnan; Li, Dongxu; Savarese, Silvio; Hoi, Steven (2023-01-01). \\\\\"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\\\\\". arXiv:2301.12597 [cs.CV].\\\\n^ Alayrac, Jean-Baptiste; Donahue, Jeff; Luc, Pauline; Miech, Antoine; Barr, Iain; Hasson, Yana; Lenc, Karel; Mensch, Arthur; Millican, Katherine; Reynolds, Malcolm; Ring, Roman; Rutherford, Eliza; Cabi, Serkan; Han, Tengda; Gong, Zhitao (2022-12-06). \\\\\"Flamingo: a Visual Language Model for Few-Shot Learning\\\\\". Advances in Neural Information Processing Systems. 35: 23716–23736. arXiv:2204.14198. Archived from the original on 2023-07-02. Retrieved 2023-07-02.\\\\n^ Driess, Danny; Xia, Fei; Sajjadi, Mehdi S. M.; Lynch, Corey; Chowdhery, Aakanksha; Ichter, Brian; Wahid, Ayzaan; Tompson, Jonathan; Vuong, Quan; Yu, Tianhe; Huang, Wenlong; Chebotar, Yevgen; Sermanet, Pierre; Duckworth, Daniel; Levine, Sergey (2023-03-01). \\\\\"PaLM-E: An Embodied Multimodal Language Model\\\\\". arXiv:2303.03378 [cs.LG].\\\\n^ Liu, Haotian; Li, Chunyuan; Wu, Qingyang; Lee, Yong Jae (2023-04-01). \\\\\"Visual Instruction Tuning\\\\\". arXiv:2304.08485 [cs.CV].\\\\n^ Zhang, Hang; Li, Xin; Bing, Lidong (2023-06-01). \\\\\"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding\\\\\". arXiv:2306.02858 [cs.CL].\\\\n^ OpenAI (2023-03-27). \\\\\"GPT-4 Technical Report\\\\\". arXiv:2303.08774 [cs.CL].\\\\n^ OpenAI (September 25, 2023). \\\\\"GPT-4V(ision) System Card\\\\\" (PDF).\\\\n^ Pichai, Sundar (10 May 2023), Google Keynote (Google I/O \\'23), timestamp 15:31, retrieved 2023-07-02\\\\n^ Wiggers, Kyle (11 September 2024). \\\\\"Mistral releases Pixtral 12B, its first multimodal model\\\\\". TechCrunch. Retrieved 14 September 2024.\\\\n^ a b \\\\\"Introducing OpenAI o1-preview\\\\\". OpenAI. 2024-09-12. Retrieved 2025-02-03.\\\\n^ a b Metz, Cade (2024-12-20). \\\\\"OpenAI Unveils New A.I. That Can \\'Reason\\' Through Math and Science Problems\\\\\". The New York Times. Retrieved 2025-02-03.\\\\n^ Gibney, Elizabeth (2025-01-30). \\\\\"China\\'s cheap, open AI model DeepSeek thrills scientists\\\\\". Nature. Retrieved 2025-02-03.\\\\n^ Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; Buchatskaya, Elena; Cai, Trevor; Rutherford, Eliza; Casas, Diego de Las; Hendricks, Lisa Anne; Welbl, Johannes; Clark, Aidan; Hennigan, Tom; Noland, Eric; Millican, Katie; Driessche, George van den; Damoc, Bogdan (2022-03-29). \\\\\"Training Compute-Optimal Large Language Models\\\\\". arXiv:2203.15556 [cs.CL].\\\\n^ a b Caballero, Ethan; Gupta, Kshitij; Rish, Irina; Krueger, David (2022). \\\\\"Broken Neural Scaling Laws\\\\\". arXiv:2210.14891 [cs.LG].\\\\n^ \\\\\"137 emergent abilities of large language models\\\\\". Jason Wei. Retrieved 2023-06-24.\\\\n^ Bowman, Samuel R. (2023). \\\\\"Eight Things to Know about Large Language Models\\\\\". arXiv:2304.00612 [cs.CL].\\\\n^ Mukherjee, Anirban; Chang, Hannah (2024). \\\\\"Heuristic Reasoning in AI: Instrumental Use and Mimetic Absorption\\\\\". arXiv:2403.09404 [cs.AI].\\\\n^ Hahn, Michael; Goyal, Navin (2023-03-14). \\\\\"A Theory of Emergent In-Context Learning as Implicit Structure Induction\\\\\". arXiv:2303.07971 [cs.LG].\\\\n^ Pilehvar, Mohammad Taher; Camacho-Collados, Jose (June 2019). \\\\\"Proceedings of the 2019 Conference of the North\\\\\". Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Minneapolis, Minnesota: Association for Computational Linguistics: 1267–1273. doi:10.18653/v1/N19-1128. S2CID\\xa0102353817. Archived from the original on 2023-06-27. Retrieved 2023-06-27.\\\\n^ \\\\\"WiC: The Word-in-Context Dataset\\\\\". pilehvar.github.io. Archived from the original on 2023-06-27. Retrieved 2023-06-27.\\\\n^ Patel, Roma; Pavlick, Ellie (2021-10-06). \\\\\"Mapping Language Models to Grounded Conceptual Spaces\\\\\". ICLR. Archived from the original on 2023-06-24. Retrieved 2023-06-27.\\\\n^ A Closer Look at Large Language Models Emergent Abilities Archived 2023-06-24 at the Wayback Machine (Yao Fu, Nov 20, 2022)\\\\n^ Ornes, Stephen (March 16, 2023). \\\\\"The Unpredictable Abilities Emerging From Large AI Models\\\\\". Quanta Magazine. Archived from the original on March 16, 2023. Retrieved March 16, 2023.\\\\n^ Schaeffer, Rylan; Miranda, Brando; Koyejo, Sanmi (2023-04-01). \\\\\"Are Emergent Abilities of Large Language Models a Mirage?\\\\\". arXiv:2304.15004 [cs.AI].\\\\n^ Li, Kenneth; Hopkins, Aspen K.; Bau, David; Viégas, Fernanda; Pfister, Hanspeter; Wattenberg, Martin (2022-10-01). \\\\\"Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task\\\\\". arXiv:2210.13382 [cs.LG].\\\\n^ \\\\\"Large Language Model: world models or surface statistics?\\\\\". The Gradient. 2023-01-21. Retrieved 2023-06-12.\\\\n^ Jin, Charles; Rinard, Martin (2023-05-01). \\\\\"Evidence of Meaning in Language Models Trained on Programs\\\\\". arXiv:2305.11169 [cs.LG].\\\\n^ Nanda, Neel; Chan, Lawrence; Lieberum, Tom; Smith, Jess; Steinhardt, Jacob (2023-01-01). \\\\\"Progress measures for grokking via mechanistic interpretability\\\\\". arXiv:2301.05217 [cs.LG].\\\\n^ a b c d e Mitchell, Melanie; Krakauer, David C. (28 March 2023). \\\\\"The debate over understanding in AI\\'s large language models\\\\\". Proceedings of the National Academy of Sciences. 120 (13): e2215907120. arXiv:2210.13966. Bibcode:2023PNAS..12015907M. doi:10.1073/pnas.2215907120. PMC\\xa010068812. PMID\\xa036943882.\\\\n^ Metz, Cade (16 May 2023). \\\\\"Microsoft Says New A.I. Shows Signs of Human Reasoning\\\\\". The New York Times.\\\\n^ a b Bubeck, Sébastien; Chandrasekaran, Varun; Eldan, Ronen; Gehrke, Johannes; Horvitz, Eric; Kamar, Ece; Lee, Peter; Lee, Yin Tat; Li, Yuanzhi; Lundberg, Scott; Nori, Harsha; Palangi, Hamid; Ribeiro, Marco Tulio; Zhang, Yi (2023). \\\\\"Sparks of Artificial General Intelligence: Early experiments with GPT-4\\\\\". arXiv:2303.12712 [cs.CL].\\\\n^ \\\\\"Anthropic CEO Dario Amodei pens a smart look at our AI future\\\\\". Fast Company. October 17, 2024.\\\\n^ \\\\\"ChatGPT is more like an \\'alien intelligence\\' than a human brain, says futurist\\\\\". ZDNET. 2023. Archived from the original on 12 June 2023. Retrieved 12 June 2023.\\\\n^ a b Newport, Cal (13 April 2023). \\\\\"What Kind of Mind Does ChatGPT Have?\\\\\". The New Yorker. Archived from the original on 12 June 2023. Retrieved 12 June 2023.\\\\n^ Roose, Kevin (30 May 2023). \\\\\"Why an Octopus-like Creature Has Come to Symbolize the State of A.I.\\\\\" The New York Times. Archived from the original on 30 May 2023. Retrieved 12 June 2023.\\\\n^ \\\\\"The A to Z of Artificial Intelligence\\\\\". Time Magazine. 13 April 2023. Archived from the original on 16 June 2023. Retrieved 12 June 2023.\\\\n^ Ji, Ziwei; Lee, Nayeon; Frieske, Rita; Yu, Tiezheng; Su, Dan; Xu, Yan; Ishii, Etsuko; Bang, Yejin; Dai, Wenliang; Madotto, Andrea; Fung, Pascale (November 2022). \\\\\"Survey of Hallucination in Natural Language Generation\\\\\" (pdf). ACM Computing Surveys. 55 (12). Association for Computing Machinery: 1–38. arXiv:2202.03629. doi:10.1145/3571730. S2CID\\xa0246652372. Archived from the original on 26 March 2023. Retrieved 15 January 2023.\\\\n^ Varshney, Neeraj; Yao, Wenlin; Zhang, Hongming; Chen, Jianshu; Yu, Dong (2023). \\\\\"A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation\\\\\". arXiv:2307.03987 [cs.CL].\\\\n^ Lakoff, George (1999). Philosophy in the Flesh: The Embodied Mind and Its Challenge to Western Philosophy; Appendix: The Neural Theory of Language Paradigm. New York Basic Books. pp.\\xa0569–583. ISBN\\xa0978-0-465-05674-3.\\\\n^ Evans, Vyvyan. (2014). The Language Myth. Cambridge University Press. ISBN\\xa0978-1-107-04396-1.\\\\n^ Friston, Karl J. (2022). Active Inference: The Free Energy Principle in Mind, Brain, and Behavior; Chapter 4 The Generative Models of Active Inference. The MIT Press. ISBN\\xa0978-0-262-36997-8.\\\\n^ a b Huyen, Chip (October 18, 2019). \\\\\"Evaluation Metrics for Language Modeling\\\\\". The Gradient. Retrieved January 14, 2024.\\\\n^ a b Clark, Christopher; Lee, Kenton; Chang, Ming-Wei; Kwiatkowski, Tom; Collins, Michael; Toutanova, Kristina (2019). \\\\\"BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions\\\\\". arXiv:1905.10044 [cs.CL].\\\\n^ a b c Wayne Xin Zhao; Zhou, Kun; Li, Junyi; Tang, Tianyi; Wang, Xiaolei; Hou, Yupeng; Min, Yingqian; Zhang, Beichen; Zhang, Junjie; Dong, Zican; Du, Yifan; Yang, Chen; Chen, Yushuo; Chen, Zhipeng; Jiang, Jinhao; Ren, Ruiyang; Li, Yifan; Tang, Xinyu; Liu, Zikang; Liu, Peiyu; Nie, Jian-Yun; Wen, Ji-Rong (2023). \\\\\"A Survey of Large Language Models\\\\\". arXiv:2303.18223 [cs.CL].\\\\n^ openai/simple-evals, OpenAI, 2024-05-28, retrieved 2024-05-28\\\\n^ openai/evals, OpenAI, 2024-05-28, archived from the original on 2024-05-08, retrieved 2024-05-28\\\\n^ \\\\\"Sanitized open-source datasets for natural language and code understanding: how we evaluated our 70B model\\\\\". imbue.com. Archived from the original on 2024-07-26. Retrieved 2024-07-24.\\\\n^ Srivastava, Aarohi; et\\xa0al. (2022). \\\\\"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models\\\\\". arXiv:2206.04615 [cs.CL].\\\\n^ Lin, Stephanie; Hilton, Jacob; Evans, Owain (2021). \\\\\"TruthfulQA: Measuring How Models Mimic Human Falsehoods\\\\\". arXiv:2109.07958 [cs.CL].\\\\n^ a b Zellers, Rowan; Holtzman, Ari; Bisk, Yonatan; Farhadi, Ali; Choi, Yejin (2019). \\\\\"HellaSwag: Can a Machine Really Finish Your Sentence?\\\\\". arXiv:1905.07830 [cs.CL].\\\\n^ \\\\\"Prepare for truly useful large language models\\\\\". Nature Biomedical Engineering. 7 (2): 85–86. 7 March 2023. doi:10.1038/s41551-023-01012-6. PMID\\xa036882584. S2CID\\xa0257403466.\\\\n^ \\\\\"Your job is (probably) safe from artificial intelligence\\\\\". The Economist. 7 May 2023. Archived from the original on 17 June 2023. Retrieved 18 June 2023.\\\\n^ \\\\\"Generative AI Could Raise Global GDP by 7%\\\\\". Goldman Sachs. Archived from the original on 18 June 2023. Retrieved 18 June 2023.\\\\n^ Peng, Zhencan; Wang, Zhizhi; Deng, Dong (13 June 2023). \\\\\"Near-Duplicate Sequence Search at Scale for Large Language Model Memorization Evaluation\\\\\" (PDF). Proceedings of the ACM on Management of Data. 1 (2): 1–18. doi:10.1145/3589324. S2CID\\xa0259213212. Archived (PDF) from the original on 2024-08-27. Retrieved 2024-01-20. Citing Lee et al 2022.\\\\n^ Peng, Wang & Deng 2023, p.\\xa08.\\\\n^ Stephen Council (1 Dec 2023). \\\\\"How Googlers cracked an SF rival\\'s tech model with a single word\\\\\". SFGATE. Archived from the original on 16 December 2023.\\\\n^ Alba, Davey (1 May 2023). \\\\\"AI chatbots have been used to create dozens of news content farms\\\\\". The Japan Times. Retrieved 18 June 2023.\\\\n^ \\\\\"Could chatbots help devise the next pandemic virus?\\\\\". Science. 14 June 2023. doi:10.1126/science.adj2463. Archived from the original on 18 June 2023. Retrieved 18 June 2023.\\\\n^ Hubinger, Evan (10 January 2024). \\\\\"Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training\\\\\". arXiv:2401.05566 [cs.CR].\\\\n^ Kang, Daniel (2023). \\\\\"Exploiting programmatic behavior of LLMs: Dual-use through standard security attacks\\\\\". arXiv:2302.05733 [cs.CR].\\\\n^ Wang, Yongge (20 June 2024). \\\\\"Encryption Based Covert Channel for Large Language Models\\\\\" (PDF). IACR ePrint 2024/586. Archived (PDF) from the original on 24 June 2024. Retrieved 24 June 2024.\\\\n^ a b Stokel-Walker, Chris (November 22, 2023). \\\\\"ChatGPT Replicates Gender Bias in Recommendation Letters\\\\\". Scientific American. Archived from the original on 2023-12-29. Retrieved 2023-12-29.\\\\n^ Luo, Queenie; Puett, Michael J.; Smith, Michael D. (2023-03-28). \\\\\"A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, Wikipedia, and YouTube\\\\\". arXiv:2303.16281v2 [cs.CY].\\\\n^ Cheng, Myra; Durmus, Esin; Jurafsky, Dan (2023-05-29), Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models, arXiv:2305.18189\\\\n^ Kotek, Hadas; Dockum, Rikker; Sun, David (2023-11-05). \\\\\"Gender bias and stereotypes in Large Language Models\\\\\". Proceedings of the ACM Collective Intelligence Conference. CI \\'23. New York, NY, USA: Association for Computing Machinery. pp.\\xa012–24. doi:10.1145/3582269.3615599. ISBN\\xa0979-8-4007-0113-9.\\\\n^ Heikkilä, Melissa (August 7, 2023). \\\\\"AI language models are rife with different political biases\\\\\". MIT Technology Review. Retrieved 2023-12-29.\\\\n^ Mehta, Sourabh (2024-07-03). \\\\\"How Much Energy Do LLMs Consume? Unveiling the Power Behind AI\\\\\". Association of Data Scientists. Retrieved 2025-01-27.\\\\n^ \\\\\"Artificial Intelligence wants to go nuclear. Will it work?\\\\\". NPR. Retrieved 2025-01-27.\\\\n^ Roy, Dareen (December 19, 2024). \\\\\"AI\\'s energy hunger fuels geothermal startups but natgas rivalry clouds future\\\\\". Reuters.\\\\nFurther reading[edit]\\\\nJurafsky, Dan, Martin, James. H. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition, 3rd Edition draft, 2023.\\\\nZhao, Wayne Xin; et\\xa0al. (2023). \\\\\"A Survey of Large Language Models\\\\\". arXiv:2303.18223 [cs.CL].\\\\nKaddour, Jean; et\\xa0al. (2023). \\\\\"Challenges and Applications of Large Language Models\\\\\". arXiv:2307.10169 [cs.CL].\\\\nYin, Shukang; Fu, Chaoyou; Zhao, Sirui; Li, Ke; Sun, Xing; Xu, Tong; Chen, Enhong (2024). \\\\\"A Survey on Multimodal Large Language Models\\\\\". National Science Review. 11 (12): nwae403. arXiv:2306.13549. doi:10.1093/nsr/nwae403. PMC\\xa011645129. PMID\\xa039679213.\\\\n\\\\\"AI Index Report 2024 – Artificial Intelligence Index\\\\\". aiindex.stanford.edu. Retrieved 2024-05-05.\\\\nFrank, Michael C. (27 June 2023). \\\\\"Baby steps in evaluating the capacities of large language models\\\\\". Nature Reviews Psychology. 2 (8): 451–452. doi:10.1038/s44159-023-00211-x. ISSN\\xa02731-0574. S2CID\\xa0259713140. Retrieved 2 July 2023.\\\\nvte\\\\nNatural language processing\\\\nGeneral terms \\\\nAI-completeBag-of-wordsn-gram BigramTrigramComputational linguisticsNatural language understandingStop wordsText processing\\\\nText analysis \\\\nArgument miningCollocation extractionConcept miningCoreference resolutionDeep linguistic processingDistant readingInformation extractionNamed-entity recognitionOntology learningParsing Semantic parsingSyntactic parsingPart-of-speech taggingSemantic analysisSemantic role labelingSemantic decompositionSemantic similaritySentiment analysis\\\\nTerminology extractionText miningTextual entailmentTruecasingWord-sense disambiguationWord-sense induction\\\\nText segmentation \\\\nCompound-term processingLemmatisationLexical analysisText chunkingStemmingSentence segmentationWord segmentation\\\\nAutomatic summarization \\\\nMulti-document summarizationSentence extractionText simplification\\\\nMachine translation \\\\nComputer-assistedExample-basedRule-basedStatisticalTransfer-basedNeural\\\\nDistributional semantics models \\\\nBERTDocument-term matrixExplicit semantic analysisfastTextGloVeLanguage model (large)Latent semantic analysisSeq2seqWord embeddingWord2vec\\\\nLanguage resources,\\\\ndatasets and corpora  \\\\nTypes and\\\\nstandards \\\\nCorpus linguisticsLexical resourceLinguistic Linked Open DataMachine-readable dictionaryParallel textPropBankSemantic networkSimple Knowledge Organization SystemSpeech corpusText corpusThesaurus (information retrieval)TreebankUniversal Dependencies\\\\nData  \\\\nBabelNetBank of EnglishDBpediaFrameNetGoogle Ngram ViewerUBYWordNetWikidata\\\\nAutomatic identification\\\\nand data capture  \\\\nSpeech recognitionSpeech segmentationSpeech synthesisNatural language generationOptical character recognition\\\\nTopic model \\\\nDocument classificationLatent Dirichlet allocationPachinko allocation\\\\nComputer-assisted\\\\nreviewing \\\\nAutomated essay scoringConcordancerGrammar checkerPredictive textPronunciation assessmentSpell checker\\\\nNatural language\\\\nuser interface\\\\nChatbotInteractive fiction (c.f. Syntax guessing)Question answeringVirtual assistantVoice user interface\\\\nRelated \\\\nFormal semanticsHallucinationNatural Language ToolkitspaCy\\\\nvte\\\\nArtificial intelligence (AI)\\\\nHistory (timeline)\\\\nConcepts  \\\\nParameter HyperparameterLoss functionsRegression Bias–variance tradeoffDouble descentOverfittingClusteringGradient descent SGDQuasi-Newton methodConjugate gradient methodBackpropagationAttentionConvolutionNormalization BatchnormActivation SoftmaxSigmoidRectifierGatingWeight initializationRegularizationDatasets AugmentationPrompt engineeringReinforcement learning Q-learningSARSAImitationPolicy gradientDiffusionLatent diffusion modelAutoregressionAdversaryRAGUncanny valleyRLHFSelf-supervised learningRecursive self-improvementWord embeddingHallucination\\\\nApplications  \\\\nMachine learning In-context learningArtificial neural network Deep learningLanguage model Large language modelNMTArtificial general intelligence\\\\nImplementations \\\\nAudio–visual  \\\\nAlexNetWaveNetHuman image synthesisHWROCRSpeech synthesis 15.aiElevenLabsSpeech recognition WhisperFacial recognitionAlphaFoldText-to-image models AuroraDALL-EFireflyFluxIdeogramImagenMidjourneyStable DiffusionText-to-video models Dream MachineGen-3 AlphaHailuo AIKlingSoraVeoMusic generation Suno AIUdio\\\\nText  \\\\nWord2vecSeq2seqGloVeBERTT5LlamaChinchilla AIPaLMGPT 123JChatGPT44oo1o3ClaudeGemini chatbotGrokLaMDABLOOMProject DebaterIBM WatsonIBM WatsonxGranitePanGu-ΣDeepSeekQwen\\\\nDecisional\\\\nAlphaGoAlphaZeroOpenAI FiveSelf-driving carMuZeroAction selection AutoGPTRobot control\\\\nPeople\\\\nAlan TuringWarren Sturgis McCullochWalter PittsJohn von NeumannClaude ShannonMarvin MinskyJohn McCarthyNathaniel RochesterAllen NewellCliff ShawHerbert A. SimonOliver SelfridgeFrank RosenblattBernard WidrowJoseph WeizenbaumSeymour PapertSeppo LinnainmaaPaul WerbosJürgen SchmidhuberYann LeCunGeoffrey HintonJohn HopfieldYoshua BengioLotfi A. ZadehStephen GrossbergAlex GravesAndrew NgFei-Fei LiAlex KrizhevskyIlya SutskeverDemis HassabisDavid SilverIan GoodfellowAndrej Karpathy\\\\nArchitectures \\\\nNeural Turing machineDifferentiable neural computerTransformer Vision transformer (ViT)Recurrent neural network (RNN)Long short-term memory (LSTM)Gated recurrent unit (GRU)Echo state networkMultilayer perceptron (MLP)Convolutional neural network (CNN)Residual neural network (RNN)Highway networkMambaAutoencoderVariational autoencoder (VAE)Generative adversarial network (GAN)Graph neural network (GNN)\\\\nPortals Technology Category Artificial neural networksMachine learning List CompaniesProjects\\\\nCategories: Large language modelsDeep learningNatural language processing\\\\nThis page was last edited on 7 February 2025, at 09:16\\xa0(UTC).\\\\nText is available under the Creative Commons Attribution-ShareAlike 4.0 License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\\\\nPrivacy policy\\\\nAbout Wikipedia\\\\nDisclaimers\\\\nContact Wikipedia\\\\nCode of Conduct\\\\nDevelopers\\\\nStatistics\\\\nCookie statement\\\\nMobile view\"}], \"response_time\": 1.53}', name='search_web', id='129d4cd3-8654-4c5a-b0b5-6386fb06b7e3', tool_call_id='6248d479-40b6-49d9-a65e-123ccf1e478f'),\n",
              "  AIMessage(content='Based on the web search results, here are some of the latest LLMs released, keeping in mind that the information has publication dates up to February 2025:\\n\\n*   **DeepSeek R1:** Released in January 2025 by DeepSeek, this open-source reasoning model excels in math and coding.\\n*   **GPT-4o:** Launched in May 2024 by OpenAI, this multimodal model integrates text, image, video, and voice capabilities.\\n*   **Claude 3.5 Sonnet:** Released in June 2024 by Anthropic as an upgrade to Claude 3, this model is known for its coding and text reasoning capabilities.\\n*   **Llama 3.1:** Released in June 2024 by Meta AI, this is the latest version of Llama 3 with 405 billion parameters.\\n*   **Nemotron-4:** Released in June 2024 by NVIDIA, this family of large language models is designed for synthetic data generation and AI model training.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-1275f258-dbc7-4c17-9266-d9e1de7bc624-0', usage_metadata={'input_tokens': 64884, 'output_tokens': 222, 'total_tokens': 65106})]}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response['messages'][-1].content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "oEdjrx5NQhi7",
        "outputId": "635fe9c8-ac54-4092-b83c-ed663f13780b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on the web search results, here are some of the latest LLMs released, keeping in mind that the information has publication dates up to February 2025:\n\n*   **DeepSeek R1:** Released in January 2025 by DeepSeek, this open-source reasoning model excels in math and coding.\n*   **GPT-4o:** Launched in May 2024 by OpenAI, this multimodal model integrates text, image, video, and voice capabilities.\n*   **Claude 3.5 Sonnet:** Released in June 2024 by Anthropic as an upgrade to Claude 3, this model is known for its coding and text reasoning capabilities.\n*   **Llama 3.1:** Released in June 2024 by Meta AI, this is the latest version of Llama 3 with 405 billion parameters.\n*   **Nemotron-4:** Released in June 2024 by NVIDIA, this family of large language models is designed for synthetic data generation and AI model training."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"What are its key features\"\"\"\n",
        "response = agent.invoke({\"messages\": (\"user\", prompt)})\n",
        "display(Markdown(response['messages'][-1].content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "id": "d31YzMp0QePA",
        "outputId": "321f14ea-93e0-46a9-8dd3-d7e2e3895d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Could you please specify what you are referring to?\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def call_agent(agent, prompt):\n",
        "    events = agent.stream(\n",
        "        {\"messages\": [{\"role\": \"user\", \"content\": prompt}]},\n",
        "        {\"configurable\": {\"thread_id\": \"any\"}},\n",
        "        stream_mode=\"values\",\n",
        "    )\n",
        "\n",
        "    for event in events:\n",
        "        event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "    print()\n",
        "    print('Final Response:\\n')\n",
        "    display(Markdown(event[\"messages\"][-1].content))"
      ],
      "metadata": {
        "id": "cuDtE-G76JNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"What is the weather in bangalore?\n",
        "show detailed statistics\"\"\"\n",
        "call_agent(agent, prompt)"
      ],
      "metadata": {
        "id": "2UgxjeIrwN1h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "outputId": "cf7dd762-1ef1-4680-dc5d-146421db9d58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is the weather in bangalore?\n",
            "show detailed statistics\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  get_weather (f96500c0-04aa-4385-9304-6bf0fdefe59b)\n",
            " Call ID: f96500c0-04aa-4385-9304-6bf0fdefe59b\n",
            "  Args:\n",
            "    query: bangalore\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: get_weather\n",
            "\n",
            "{\"location\": {\"name\": \"Bangalore\", \"region\": \"Karnataka\", \"country\": \"India\", \"lat\": 12.9833, \"lon\": 77.5833, \"tz_id\": \"Asia/Kolkata\", \"localtime_epoch\": 1740081009, \"localtime\": \"2025-02-21 01:20\"}, \"current\": {\"last_updated_epoch\": 1740080700, \"last_updated\": \"2025-02-21 01:15\", \"temp_c\": 20.3, \"temp_f\": 68.5, \"is_day\": 0, \"condition\": {\"text\": \"Clear\", \"icon\": \"//cdn.weatherapi.com/weather/64x64/night/113.png\", \"code\": 1000}, \"wind_mph\": 8.7, \"wind_kph\": 14.0, \"wind_degree\": 102, \"wind_dir\": \"ESE\", \"pressure_mb\": 1018.0, \"pressure_in\": 30.06, \"precip_mm\": 0.0, \"precip_in\": 0.0, \"humidity\": 73, \"cloud\": 0, \"feelslike_c\": 20.3, \"feelslike_f\": 68.5, \"windchill_c\": 21.1, \"windchill_f\": 70.0, \"heatindex_c\": 24.4, \"heatindex_f\": 76.0, \"dewpoint_c\": 13.1, \"dewpoint_f\": 55.5, \"vis_km\": 6.0, \"vis_miles\": 3.0, \"uv\": 0.0, \"gust_mph\": 15.1, \"gust_kph\": 24.3}}\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The weather in Bangalore, India is clear with a temperature of 20.3 degrees Celsius (68.5 degrees Fahrenheit). The humidity is 73%, and the wind is coming from the ESE at 14 kph (8.7 mph). The dewpoint is 13.1 degrees Celsius (55.5 degrees Fahrenheit), and it feels like 20.3 degrees Celsius (68.5 degrees Fahrenheit). The pressure is 1018 mb, and visibility is 6 km. Last updated at 2025-02-21 01:15.\n",
            "\n",
            "Final Response:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The weather in Bangalore, India is clear with a temperature of 20.3 degrees Celsius (68.5 degrees Fahrenheit). The humidity is 73%, and the wind is coming from the ESE at 14 kph (8.7 mph). The dewpoint is 13.1 degrees Celsius (55.5 degrees Fahrenheit), and it feels like 20.3 degrees Celsius (68.5 degrees Fahrenheit). The pressure is 1018 mb, and visibility is 6 km. Last updated at 2025-02-21 01:15."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"What is the weather in hyderabad?\n",
        "show detailed statistics\"\"\"\n",
        "call_agent(agent, prompt)"
      ],
      "metadata": {
        "id": "hLqZiU6exnN5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "outputId": "c8402307-475c-4df1-d887-6e3f9fa1d7ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is the weather in hyderabad?\n",
            "show detailed statistics\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  get_weather (09081b35-d7f4-442c-813f-1d61c7d71c52)\n",
            " Call ID: 09081b35-d7f4-442c-813f-1d61c7d71c52\n",
            "  Args:\n",
            "    query: hyderabad\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: get_weather\n",
            "\n",
            "{\"location\": {\"name\": \"Hyderabad\", \"region\": \"Telangana\", \"country\": \"India\", \"lat\": 17.3753, \"lon\": 78.4744, \"tz_id\": \"Asia/Kolkata\", \"localtime_epoch\": 1740080981, \"localtime\": \"2025-02-21 01:19\"}, \"current\": {\"last_updated_epoch\": 1740080700, \"last_updated\": \"2025-02-21 01:15\", \"temp_c\": 22.3, \"temp_f\": 72.1, \"is_day\": 0, \"condition\": {\"text\": \"Mist\", \"icon\": \"//cdn.weatherapi.com/weather/64x64/night/143.png\", \"code\": 1030}, \"wind_mph\": 2.2, \"wind_kph\": 3.6, \"wind_degree\": 135, \"wind_dir\": \"SE\", \"pressure_mb\": 1017.0, \"pressure_in\": 30.03, \"precip_mm\": 0.0, \"precip_in\": 0.0, \"humidity\": 100, \"cloud\": 50, \"feelslike_c\": 24.4, \"feelslike_f\": 75.9, \"windchill_c\": 24.1, \"windchill_f\": 75.5, \"heatindex_c\": 25.2, \"heatindex_f\": 77.3, \"dewpoint_c\": 12.1, \"dewpoint_f\": 53.8, \"vis_km\": 4.0, \"vis_miles\": 2.0, \"uv\": 0.0, \"gust_mph\": 4.6, \"gust_kph\": 7.3}}\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The weather in Hyderabad, India is currently 22.3 degrees Celsius. It feels like 24.4 degrees Celsius. The humidity is 100%. There is mist. The wind is coming from the SE at 3.6 kph.\n",
            "\n",
            "Final Response:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The weather in Hyderabad, India is currently 22.3 degrees Celsius. It feels like 24.4 degrees Celsius. The humidity is 100%. There is mist. The wind is coming from the SE at 3.6 kph."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Which city is hotter?\"\"\"\n",
        "call_agent(agent, prompt)"
      ],
      "metadata": {
        "id": "2JRHxzdPxo8A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "outputId": "d8421ab0-121f-4b5a-cd22-8d208f64141f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Which city is hotter?\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Could you please provide the names of the cities you want me to compare?\n",
            "\n",
            "Final Response:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Could you please provide the names of the cities you want me to compare?\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have successfully built an AI Agent which can search the web, get weather for us but it is not yet conversational. We will add in the that functionality in Part III next."
      ],
      "metadata": {
        "id": "dtXAlOdBxt-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a multi-user conversational ReAct Agent with LangGraph\n",
        "\n",
        "Now, we will build a multi-user conversational ReAct agent with LangGraph which will use the web search or weather tool based on our input prompts to get relevant data which the LLM might not know by default and give relevant responses\n",
        "\n",
        "Our agentic chatbot from Part II can use tools to answer user questions, but it doesn't remember the context of previous interactions. This limits its ability to have coherent, multi-turn conversations.\n",
        "\n",
        "LangGraph solves this problem through **persistent checkpointing**. If you provide a `checkpointer` when compiling the graph and a `thread_id` when calling your graph, LangGraph automatically saves the state after each step. When you invoke the graph again using the same `thread_id`, the graph loads its saved state, allowing the chatbot to pick up where it left off.\n",
        "\n",
        "**checkpointing** is _much_ more powerful than simple chat memory - it lets you save and resume complex state at any time for error recovery, human-in-the-loop workflows, time travel interactions, and more\n",
        "\n",
        "While the legacy syntax uses `session_id`, in LangGraph, each user session is identified by `thread_id`\n",
        "\n",
        "We will use `SqliteSaver` which helps to store separate conversation histories per user or session.\n",
        "\n",
        "This will help us build a conversational Agentic Chatbot which will be accessed by many users at the same time.\n",
        "\n"
      ],
      "metadata": {
        "id": "rcfps7EJy570"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# removes the memory database file - usually not needed\n",
        "# you can run this only when you want to remove ALL conversation histories\n",
        "# ok if you get rm: cannot remove 'memory.db': No such file or directory  because initially no memory exists\n",
        "!rm memory.db*"
      ],
      "metadata": {
        "id": "Cph9LvyvIRrA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "883b73b0-66b4-4b04-b42b-dd4cad390ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'memory.db*': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "\n",
        "def chat_with_agent(agent_graph, prompt, session_id, verbose=False):\n",
        "    with SqliteSaver.from_conn_string(\"memory.db\") as memory:\n",
        "        agent = agent_graph.compile(checkpointer=memory)\n",
        "        events = agent.stream(\n",
        "            {\"messages\": [{\"role\": \"user\", \"content\": prompt}]},\n",
        "            {\"configurable\": {\"thread_id\": session_id}},\n",
        "            stream_mode=\"values\",\n",
        "        )\n",
        "\n",
        "        print('Running Agent, please wait...')\n",
        "        for event in events:\n",
        "            if verbose:\n",
        "                event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "    display(Markdown(event[\"messages\"][-1].content))"
      ],
      "metadata": {
        "id": "qzwnf7vfzTiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_builder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7_S8X7r78Ei",
        "outputId": "b81f345e-3199-4038-a267-1e46e76a4148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7bddb40feb90>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now simulate User 1 using the agent"
      ],
      "metadata": {
        "id": "vYTNL_iJ6ibC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = 'jack001'\n",
        "prompt = \"Tell me about recent LLMs released\"\n",
        "chat_with_agent(graph_builder, prompt, user_id, verbose=True)"
      ],
      "metadata": {
        "id": "irMU68Ds0NTm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "4cbc011b-50a7-4e4b-ecf6-3df0b420fca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Agent, please wait...\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Tell me about recent LLMs released\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  search_web (81611567-943c-4b92-bc02-96e4b4e5d192)\n",
            " Call ID: 81611567-943c-4b92-bc02-96e4b4e5d192\n",
            "  Args:\n",
            "    query: recent large language models released\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: search_web\n",
            "\n",
            "{\"query\": \"recent large language models released\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://explodingtopics.com/blog/list-of-llms\", \"title\": \"Best 22 Large Language Models (LLMs) (February 2025)\", \"content\": \"Inflection-2.5 is the latest large language model (LLM) developed by Inflection AI to power its conversational AI assistant, Pi. Significant upgrades have been made, as the model currently achieves over 94% of GPT-4’s average performance while only having 40% of the training FLOPs. In March 2024, the Microsoft-backed startup reached 1+ million daily active users on Pi. 13. Gemma is a series of lightweight open-source language models developed and released by Google DeepMind. Pythia is a series of 16 large language models developed and released by EleutherAI, a non-profit AI research lab. Alpaca is a 7 billion-parameter language model developed by a Stanford research team and fine-tuned from Meta's LLaMA 7B model.\", \"score\": 0.8815438, \"raw_content\": \"Best 22 Large Language Models (LLMs) (February 2025)\\n\\n\\n\\nAbout\\nNewsletter\\nBlog\\n\\n\\nBest 22 Large Language Models (LLMs) (February 2025)\\n\\nby Anthony Cardillo\\nFebruary 7, 2025\\nLarge language models are pre-trained on large datasets and use natural language processing to perform linguistic tasks such as text generation, code completion, paraphrasing, and more.\\nThe initial release of ChatGPT sparked the rapid adoption of generative AI, which has led to large language model innovations and industry growth.\\nIn fact, 92% of Fortune 500 firms have started using generative AI in their workflows.\\nAs adoption continues to grow, so does the LLM industry. The global large language model market is projected to grow from $6.5 billion in 2024 to $140.8 billion by 2033.\\nWith that, here is a list of the top 21 LLMs available in September 2024.\\nLLM NameDeveloperRelease DateAccessParametersDeepSeek R1DeepSeekJanuary 20, 2025Open-Source671 billionGPT-4oOpenAIMay 13, 2024APIUnknownClaude 3.5AnthropicJune 20, 2024APIUnknownGrok-1xAINovember 4, 2023Open-Source314 billionMistral 7BMistral AISeptember 27, 2023Open-Source7.3 billionPaLM 2GoogleMay 10, 2023Open-Source340 billionFalcon 180BTechnology Innovation InstituteSeptember 6, 2023Open-Source180 billionStable LM 2Stability AIJanuary 19, 2024Open-Source1.6 billion, 12 billionGemini 1.5Google DeepMindFebruary 2nd, 2024APIUnknownLlama 3.1Meta AIJune 23, 2024Open-Source405 billionMixtral 8x22BMistral AIApril 10, 2024Open-Source141 billionInflection-2.5Inflection AIMarch 10, 2024ProprietaryUnknownJambaAI21 LabsMarch 29, 2024Open-Source52 billionCommand RCohereMarch 11, 2024Both35 billionGemmaGoogle DeepMindFebruary 21, 2024Open-Source2 billion, 7 billionPhi-3MicrosoftApril 23, 2024Both3.8 billionXGen-7BSalesforceJuly 3, 2023Open-Source7 billionDBRXDatabricks' Mosaic MLMarch 27, 2024Open-Source132 billionPythiaEleutherAIFebruary 13, 2023Open-Source70 million to 12 billionSoraOpenAIFebruary 15, 2024 (announced)APIUnknownAlpaca 7BStanford CRFMMarch 13, 2023Open-Source7 billionNemotron-4NvidiaJune 14, 2024Open-Source340 billion\\n1. DeepSeek R1\\n\\nDeveloper: DeepSeek\\nRelease date: January 2025\\nNumber of Parameters: 671B total, 37B active\\nWhat is it? DeepSeek R1 is a reasoning model that excels in math and coding. It beats or matches OpenAI o1 in several benchmarks, including MATH-500 and AIME 2024.\\nOn its release, DeepSeek immediately hit headlines due to the low cost of training compared to most major LLMs.\\nDeepSeek R1 is free to use and open-source. It's accessible via the API, the DeepSeek website, and mobile apps.\\n2. GPT-4o\\n\\nDeveloper: OpenAI\\nRelease date: May 13, 2024\\nNumber of Parameters: Unknown\\nWhat is it? GPT-4o is the latest and most advanced OpenAI language model, succeeding GPT-4, GPT-3.5, and GPT-3. OpenAI claims that GPT-4o is 50% cheaper than GPT-4 despite being 2x faster at generating tokens. This multimodal model includes text, image, video, and voice capabilities packaged into one.\\nGPT-4o's biggest upgrade is the Voice-to-Voice function, which will improve input response times to an average of 320 milliseconds (compared to a few seconds with GPT-4). This feature is expected to be released in the coming weeks.\\n3. Claude 3.5\\n\\nDeveloper: Anthropic\\nRelease date: March 14, 2024\\nNumber of Parameters: Unknown\\nWhat is it? As a new upgrade from the highly rated Claude 3, Claude 3.5 Sonnet is the first release of the new Claude 3.5 model family. Similar to Claude 3, it'll also include the Haiku and Opus models. As debatably the biggest competitor to GPT-4 and ChatGPT, Claude made even bigger improvements to this model by maintaining the 200,000 token context window at a lower cost. This is much larger than GPT-4's 32,000 token capabilities.\\nAccording to Anthropic's report, Claude 3.5 Sonnet outperformed GPT-4o in major benchmarks like coding and text reasoning. Plus, this is Claude's most advanced vision model, with the ability to transcribe text from images or generate insights from charts.\\nAmazon has invested over $4 billion in Anthropic, bringing the startup's valuation to $15 billion. The Claude mobile app was also released in May 2024.\\n4. Grok-1\\n\\nDeveloper: xAI\\nRelease date: November 4, 2023\\nNumber of Parameters: 314 billion\\nWhat is it? Created by Elon Musk's artificial intelligence startup xAI, Grok-1 is currently the largest open-source LLM released to date at 314 billion parameters. Grok directly integrates with X (Twitter), and users must pay for an X Premium+ subscription to gain access.\\nBecause of the model’s size, Grok has a mixture-of-experts (MoE) architecture that only uses 25% of its weights for any given input token to maximize calculation efficiency.\\nIn August 2024, both Grok-2 and Grok-2 mini were released to X users in beta. According to xAI's reports, Grok-2 outperforms GPT-4o in numerous categories, such as GPQA, MMLU-Pro, and DocVQA.\\n5. Mistral 7B\\n\\nDeveloper: Mistral AI\\nRelease date: September 27, 2023\\nNumber of Parameters: 7.3 billion\\nWhat is it? Mistral 7B is an open-source language model with 32 layers, 32 attention heads, and eight key-value heads. Despite running with fewer parameters, they outperformed the Llama 2 family of models in nearly all metrics, including MMLU, reading comprehension, math, coding, etc.\\nMistral 7B is released under an Apache 2.0 license. Customers are free to download it locally, deploy it on the cloud, or run it on HuggingFace. The Paris-based startup is close to securing a new $600 million funding round that would value the company at $6 billion.\\n6. PaLM 2\\n\\nDeveloper: Google\\nRelease date: May 10, 2023\\nNumber of Parameters: 340 billion\\nWhat is it? PaLM 2 is an advanced large language model developed by Google. As the successor to the original Pathways Language Model (PaLM), it’s trained on 3.6 trillion tokens (compared to 780 billion) and 340 billion parameters (compared to 540 billion). PaLM 2 was originally used to power Google's first generative AI chatbot, Bard (rebranded to Gemini in February 2024).\\n7. Falcon 180B\\n\\nDeveloper: Technology Innovation Institute (TII)\\nRelease date: September 6, 2023\\nNumber of Parameters: 180 billion\\nWhat is it? Developed and funded by the Technology Innovation Institute, Falcon 180B is an upgraded version of the earlier Falcon 40B LLM. It has 180 billion parameters, which is 4.5 times larger than the 40 billion parameters of Falcon 40B.\\nIn addition to Falcon 40B, it also outperforms other large language models like GPT-3.5 and LLaMA 2 on tasks such as reasoning, question answering, and coding. In February 2024, the UAE-based Technology Innovation Institute (TII) committed $300 million in funding to the Falcon Foundation.\\n8. Stable LM 2\\n\\nDeveloper: Stability AI\\nRelease date: January 19, 2024\\nNumber of Parameters: 1.6 billion and 12 billion\\nWhat is it? Stability AI, the creators of the Stable Diffusion text-to-image model, are the developers behind Stable LM 2. This series of large language models includes Stable LM 2 12B (12 billion parameters) and Stable LM 2 1.6B (1.6 billion parameters). Released in April 2024, the larger 12B model outperforms models like LLaMA 2 70B on key benchmarks despite being much smaller.\\n9. Gemini 1.5\\n\\nDeveloper: Google DeepMind\\nRelease date: February 2nd, 2024\\nNumber of Parameters: Unknown\\nWhat is it? Gemini 1.5 is Google's next-generation large language model, offering a significant upgrade over its predecessor, Gemini 1.0. While it’s only available for early testing, Gemini 1.5 Pro provides a one million-token context window (1 hour of video, 700,000 words, or 30,000 lines of code), the largest to date compared to alternative LLMs and chatbots. This upgrade is 35 times larger than Gemini 1.0 Pro and surpasses the previous largest record of 200,000 tokens held by Anthropic’s Claude 2.1.\\n10. Llama 3.1\\n\\nDeveloper: Meta AI\\nRelease date: June 23, 2024\\nNumber of Parameters: 405 billion\\nWhat is it? Llama 3, the predecessor to Llama 3.1, was available in both 70B and 8B versions that outperformed other open-source models like Mistral 7B and Google's Gemma 7B on MMLU, reasoning, coding, and math benchmarks. Now, users will notice major upgrades to the latest version, including 405 billion parameters and an expended context length of 128,000.\\nUsers will also notice more accuracy because of the impressive knowledge base, which has been trained on over 15 trillion tokens. Plus, Meta added eight additional languages for this model. The increased size of this model makes it the largest open-source model released to date.\\nCustomers can still access its predecessor, Llama 2, which is available in three versions: 7 billion, 13 billion, and 70 billion parameters.\\n11. Mixtral 8x22B\\n\\nDeveloper: Mistral AI\\nRelease date: April 10, 2024\\nNumber of Parameters: 141 billion\\nWhat is it? Mixtral 8x22B is Mistral AI's latest and most advanced large language model. This sparse Mixture-of-Experts (SMoE) model has 141 billion total parameters but only uses 39B active parameters to focus on improving the model’s performance-to-cost ratio.\\nThe startup also recently released Mistral Large, a ChatGPT alternative that ranks second behind GPT-4 among API-based LLMs.\\n12. Inflection-2.5\\n\\nDeveloper: Inflection AI\\nRelease date: March 10, 2024\\nNumber of Parameters: Unknown\\nWhat is it? Inflection-2.5 is the latest large language model (LLM) developed by Inflection AI to power its conversational AI assistant, Pi. Significant upgrades have been made, as the model currently achieves over 94% of GPT-4’s average performance while only having 40% of the training FLOPs. In March 2024, the Microsoft-backed startup reached 1+ million daily active users on Pi.\\n13. Jamba\\n\\nDeveloper: AI21 Labs\\nRelease date: March 29, 2024\\nNumber of Parameters: 52 billion\\nWhat is it? AI21 Labs created Jamba, the world's first production-grade Mamba-style large language model. It integrates SSM technology with elements of a traditional transformer model to create a hybrid architecture. The model is efficient and highly scalable, with a context window of 256K and deployment support of 140K context on a single GPU.\\n14. Command R\\n\\nDeveloper: Cohere\\nRelease date: March 11, 2024\\nNumber of Parameters: 35 billion\\nWhat is it? Command R is a series of scalable LLMs from Cohere that support ten languages and 128,000-token context length (around 100 pages of text). This model primarily excels at retrieval-augmented generation, code-related tasks like explanations or rewrites, and reasoning. In April 2024, Command R+ was released to support larger workloads and provide real-world enterprise support.\\n15. Gemma\\n\\nDeveloper: Google DeepMind\\nRelease date: February 21, 2024\\nNumber of Parameters: 2 billion and 7 billion\\nWhat is it? Gemma is a series of lightweight open-source language models developed and released by Google DeepMind. The Gemma models are built with similar tech to the Gemini models, but Gemma is limited to text inputs and outputs only. The models have a context window of 8,000 tokens and are available in 2 billion and 7 billion parameter sizes.\\n16. Phi-3\\n\\nDeveloper: Microsoft\\nRelease date: April 23, 2024\\nNumber of Parameters: 3.8 billion\\nWhat is it? Classified as a small language model (SLM), Phi-3 is Microsoft's latest release with 3.8 billion parameters. Despite the smaller size, it's been trained on 3.3 trillion tokens of data to compete with Mistral 8x7B and GPT-3.5 performance on MT-bench and MMLU benchmarks.\\nTo date, Phi-3-mini is the only model available. However, Microsoft plans to release the Phi-3-small and Phi-3-medium models later this year.\\n17. XGen-7B\\n\\nDeveloper: Salesforce\\nRelease date: July 3, 2023\\nNumber of Parameters: 7 billion\\nWhat is it? XGen-7B is a large language model from Salesforce with 7 billion parameters and an 8k context window. The model was trained on 1.37 trillion tokens from various sources, such as RedPajama, Wikipedia, and Salesforce's own Starcoder dataset.\\nSalesforce has released two open-source versions, a 4,000 and 8,000 token context window base, hosted under an Apache 2.0 license.\\n18. DBRX\\n\\nDeveloper: Databricks' Mosaic ML\\nRelease date: March 27, 2024\\nNumber of Parameters: 132 billion\\nWhat is it? DBRX is an open-source LLM built by Databricks and the Mosaic ML research team. The mixture-of-experts architecture has 36 billion (of 132 billion total) active parameters on an input. DBRX has 16 experts and chooses 4 of them during inference, providing 65 times more expert combinations compared to similar models like Mixtral and Grok-1\\n19. Pythia\\n\\nDeveloper: EleutherAI\\nRelease date: February 13, 2023\\nNumber of Parameters: 70 million to 12 billion\\nWhat is it? Pythia is a series of 16 large language models developed and released by EleutherAI, a non-profit AI research lab. There are eight different model sizes: 70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B. Because of Pythia's open-source license, these LLMs serve as a base model for fine-tuned, instruction-following LLMs like Dolly 2.0 by Databricks.\\n20. Sora\\n\\nDeveloper: OpenAI\\nRelease date: February 15, 2024 (announced)\\nNumber of Parameters: Unknown\\nWhat is it? OpenAI's latest development is Sora, a text-to-video model that combines LLMs and generative AI to turn text prompts into realistic videos up to 60 seconds long. The model uses a transformer architecture that operates on \\\"spacetime patches\\\" of video and image data rather than text tokens like other LLMs. No official release date for Sora has been announced, but OpenAI expects it to open to the public in late 2024.\\n21. Alpaca 7B\\n\\nDeveloper: Stanford CRFM\\nRelease date: March 27, 2024\\nNumber of Parameters: 7 billion\\nWhat is it? Alpaca is a 7 billion-parameter language model developed by a Stanford research team and fine-tuned from Meta's LLaMA 7B model. Users will notice that although being much smaller, Alpaca performs similarly to text-DaVinci-003 (ChatGPT 3.5). However, Alpaca 7B is available for research purposes, and no commercial licenses are available.\\n22. Nemotron-4 340B\\n\\nDeveloper: NVIDIA\\nRelease date: June 14, 2024\\nNumber of Parameters: 340 billion\\nWhat is it? Nemotron-4 340B is a family of large language models for synthetic data generation and AI model training. These models help businesses create new LLMs without larger and more expensive datasets. Instead, Nemotron-4 can create high-quality synthetic data to train other AI models, which reduces the need for extensive human-annotated data.\\nThe model family includes Nemotron-4-340B-Base (foundation model), Nemotron-4-340B-Instruct (fine-tuned chatbot), and Nemotron-4-340B-Reward (quality assessment and preference ranking). Due to the 9 trillion tokens used in training, which includes English, multilingual, and coding language data, Nemotron-4 matches GPT-4's high-quality synthetic data generation capabilities.\\nConclusion\\nThe landscape of large language models is rapidly evolving, with new breakthroughs and innovations emerging at an unprecedented pace.\\nFrom compact models like Phi-2 and Alpaca 7B to cutting-edge architectures like Jamba and DBRX, the field of LLMs is pushing the boundaries of what's possible in natural language processing (NLP).\\nWe will keep this list regularly updated with new models. If you liked learning about these LLMs, check out our lists of generative AI startups and AI startups.\\nFind Thousands of Trending Topics With Our Platform\\nTry Exploding Topics Pro\\n\\nExploding Topics\\n\\nJoin Pro\\nNewsletter\\nTrending Topics\\nAdd a Topic\\nCustomer Login\\n\\nCompany\\n\\nAbout Us\\nContact\\nMethodology\\nCookie Settings\\n\\nFree Tools\\n\\nKeyword Research\\nBacklink Checker\\nSERP Checker\\nKeyword Rank Checker\\nFree SEO Tools\\n\\nConnect\\n\\nYouTube\\nInstagram\\nX (Twitter)\\n\\nResources\\n\\nBlog\\nMarketing Academy\\nFree Webinars\\n\\n\\n\\n© 2025  Exploding Topics is a Trademark of Semrush Inc\\n\\nPrivacy Policy\\nTerms of Service\\n\"}, {\"url\": \"https://hatchworks.com/blog/gen-ai/large-language-models-guide/\", \"title\": \"Large Language Models: What You Need to Know in 2025\", \"content\": \"Large language models (LLMs) are the unsung heroes of recent Generative AI advancements, quietly working behind the scenes to understand and generate language as we know it. OpenAI released GPT-4, an even more powerful and versatile model than its predecessors, with improvements in understanding, reasoning, and generating text across a broader range of contexts and languages. 2022: The emergence of GPT-4 and other advanced models such as Midjourney, continuing to push the boundaries of what’s possible with LLMs in terms of generating and understanding natural language across various domains and tasks, including image generation. These models are trained on massive data sets and can perform a broad range of tasks like generating text, translating languages, and more. Tags: AI, artificial intelligence, gen ai, Generative AI, large language models, LLMs\", \"score\": 0.83678174, \"raw_content\": \"Large Language Models: What You Need to Know in 2025 | HatchWorks AI\\nSkip to content\\n\\n\\nWhat We Do\\n\\n\\n\\n\\n\\nServices\\n\\n\\nAI Strategy & Roadmap\\n\\nData Engineering & Analytics\\nAI-Powered Software Development\\n\\nAI Engineering Teams\\n        *   *   Accelerators\\n\\n\\nGenerative Driven Development™\\n\\nAI Roadmap & ROI Workshop\\nAI Solution Accelerator\\nRAG\\n\\nGenIQ\\n        *   *   Industries\\n\\n\\nCommunications and IoT\\n\\nTechnology\\nHealthcare\\nFinance\\n\\nRetail\\n        *   *   Partnerships\\n\\n\\nDatabricks\\n\\nIndustries\\nCommunications and IoT Solutions\\nTechnology\\nHealthcare\\nFinance\\nRetail\\n\\n\\nAbout Us\\nAbout Us\\nCareers & Culture\\nHatchFutures\\nFAQ\\n\\n\\n\\nResources\\n\\n\\n\\n\\n\\nInsights\\n\\n\\n\\n\\n\\n\\n\\nBlog\\n\\nTalking AI Podcast\\n\\nTalking AI Newsletter\\n        *   *   Tools & Reports\\n\\n\\nState of AI Report 2025\\n\\nTech Talent Report 2024\\nNearshore Budget Calculator\\n\\nBuild your Own GPT\\n        *   *   Learn & Connect\\n\\n\\nEvents\\n        *   *   Media\\n\\n\\nNewsroom\\n\\nOur Work\\nCareers\\nContact\\n\\n\\n\\n\\n\\n\\n\\n\\nCareers\\nContact us\\nLarge Language Models: What You Need to Know in 2025\\n\\nMelissa Malec\\n\\nDecember 2, 2024\\n\\n\\nUpdated: January 16, 2025\\n\\n\\nLarge language models (LLMs) are the unsung heroes of recent Generative AI advancements, quietly working behind the scenes to understand and generate language as we know it.\\nBut how do they work? What are they capable of? And what should we look out for when using them?\\n\\nRead on and find out in this guide for LLMs in 2024. Jump ahead:\\n\\nUnderstanding Large Language Models\\nWhat is a Large Language Model?\\nHow Do Large Language Models Work?\\nKey Milestones in Large Language Model Development\\nCapabilities of Large Language Models\\nChallenges and Limitations of LLMs\\nThe Future of Language Models: What Comes Next?\\n\\nUnderstanding Large Language Models\\nLet’s get the basics out of the way. Here we’ll define the large language model (LLM), explain how they work, and provide a timeline of key milestones in LLM development.\\nWhat is a Large Language Model?\\nA large language model, often abbreviated to LLM, is a type of artificial intelligence model designed to understand natural language as well as generate it at a large scale.\\nWhen we say human language, we don’t just mean English, Spanish, or Cantonese. Those are certainly part of what LLMs are trained on but human language, in this context, also extends to:\\n\\nArt\\nDance\\nMorse code\\nGenetic code\\nHieroglyphics\\nCryptography\\nSign language\\nBody language\\nMusical notation\\nChemical signaling\\nEmojis and symbols\\nAnimal communication\\nHaptic communications\\nTraffic signs and signals\\nMathematical equations\\nProgramming languages\\n\\nLLMs are trained on billions of parameters and have the ability to learn from a wide range of data sources.\\nThis extensive training enables them to predict and produce text based on the input they receive so that they can engage in conversations, answer queries, or even write code.\\nSome of the leading very large models include giants like GPT, LLaMa, LaMDA, PaLM 2, BERT, and ERNIE.\\nThey’re at the heart of various applications, aiding in everything from customer service chatbots to content creation and software development.\\nSome companies even build their own LLMs but that requires significant time, investment, and tech knowledge. It’s much easier to integrate a pre-trained LLM into your own systems.\\nHow Do Large Language Models Work?\\nLarge Language Models use a blend of neural networks and machine learning (ML). It’s this blend that allows the technology to first process and then generate original text and imagery.\\nThink of neural networks as the LLM’s brain. It’s these networks that learn from vast amounts of data, improving over time as they’re exposed to more.\\nAs the model is trained on more data, it learns patterns, structures, and the nuances of language. It’s like teaching it the rules of grammar, the rhythm of poetry, and the jargon of technical manuals all at once.\\nMachine learning models then help the model to predict the next word in a sentence based on the words that come before it. This is done countless times, refining the model’s ability to generate coherent and contextually relevant text.\\nLLMs now also operate on a Transformer Architecture. This architecture allows the model to look at and weigh the importance of different words in a sentence. It’s the same as when we read a sentence and look for context clues to understand its meaning.\\n⚠️ While LLMs can generate original content, the quality, relevance, and innovativeness of their output can vary and require human oversight and refinement.\\nThe originality is also influenced by how the prompts are structured, the model’s training data, and the specific capabilities of the LLM in question.\\nKey Milestones in Large Language Model Development\\nLarge language models haven’t always been as useful as they are today. They’ve developed and been iterated upon significantly over time.\\nLet’s look at some of those key moments in LLM history. That way you can appreciate how far they’ve come and the rapid evolution in the last few years compared to decades of slow progress.\\n1966\\nELIZA\\n\\nThe first chatbot created by Joseph Weizenbaum, simulating a psychotherapist in conversation.\\n2013\\nword2vec\\n\\nA groundbreaking tool developed by a team led by Tomas Mikolov at Google, introducing efficient methods for learning word embeddings from raw text.\\n2018\\nGPT and BERT\\n\\nGPT (Generative Pretrained Transformer): OpenAI introduced GPT, showcasing a powerful model for understanding and generating human-like text.\\nBERT (Bidirectional Encoder Representations from Transformers): Developed by Google, BERT significantly advanced the state of the art in natural language understanding tasks.\\n\\n2020\\nGPT 3\\n\\nOpenAI released GPT-3, a model with 175 billion parameters, achieving unprecedented levels of language understanding and generation capabilities.\\nLate 2021\\nIntroduction of ChatGPT\\n\\nOpenAI introduced ChatGPT, a conversational agent based on the GPT-3.5 model, designed to provide more engaging and natural dialogue experiences. ChatGPT showcased the potential of GPT models in interactive applications.\\n2022\\nGPT-4\\nOpenAI released GPT-4, an even more powerful and versatile model than its predecessors, with improvements in understanding, reasoning, and generating text across a broader range of contexts and languages.\\n2022\\nMidjourney and Other Innovations\\n\\nThe launch of Midjourney, along with other models and platforms, reflected the growing diversity and application of AI in creative processes, design, and beyond, indicating a broader trend towards multimodal and specialized AI systems.\\nPre-2010: Early Foundations\\n\\n1950s-1970s: Early AI research lays the groundwork for natural language processing. Most famously, a tech called ‘Eliza’ was the world’s first chatbot.\\n1980s-1990s: Development of statistical methods for NLP, moving away from rule-based systems.\\n\\n2010: Initial Models\\n\\n2013: Introduction of word2vec, a tool for computing vector representations of words, which significantly improved the quality of NLP tasks by capturing semantic meanings of words.\\n\\n2014-2017: RNNs and Attention Mechanisms\\n\\n2014: Sequence to sequence (seq2seq) models and Recurrent Neural Networks (RNNs) become popular for tasks like machine translation.\\n2015: Introduction of Attention Mechanism, improving the performance of neural machine translation systems.\\n2017: The Transformer model is introduced in the paper “Attention is All You Need”, setting a new standard for NLP tasks with its efficient handling of sequences.\\n\\n2018: Emergence of GPT and BERT\\n\\nJune 2018: OpenAI introduces GPT (Generative Pretrained Transformer), a model that leverages unsupervised learning to generate coherent and diverse text.\\nOctober 2018: Google AI introduces BERT (Bidirectional Encoder Representations from Transformers), which uses bidirectional training of Transformer models to improve understanding of context in language.\\n\\n2019-2020: Larger and More Powerful Models\\n\\n2019: Introduction of GPT-2, an improved version of GPT with 1.5 billion parameters, showcasing the model’s ability to generate coherent and contextually relevant text over extended passages.\\n2020: OpenAI releases GPT-3, a much larger model with 175 billion parameters, demonstrating remarkable abilities in generating human-like text, translation, and answering questions.\\n\\n2021-2023: Specialization, Multimodality, and Democratization of LLMs\\n\\n2021-2022: Development of specialized models like Google’s LaMDA for conversational applications and Facebook’s OPT for open pre-trained transformers.\\n2021: Introduction of multimodal models like DALL·E by OpenAI, capable of generating images from textual descriptions, and CLIP, which can understand images in the context of natural language.\\n2022: The emergence of GPT-4 and other advanced models such as Midjourney, continuing to push the boundaries of what’s possible with LLMs in terms of generating and understanding natural language across various domains and tasks, including image generation. It’s also more accessible to larger numbers of people.\\n\\nCapabilities of Large Language Models\\nThe capabilities of Large Language Models are as vast as the datasets they’re trained on. Use cases range from generating code to suggesting strategy for a product launch and analyzing data points.\\nThis is because LLMs serve as foundation models that can be applied across multiple uses.\\nHere’s a list of LLM capabilities:\\n\\nText generation\\nLanguage translation\\nSummarization\\nQuestion answering\\nSentiment analysis\\nConversational agents\\nCode generation and explanation\\nNamed entity recognition\\nText classification\\nContent recommendation\\nLanguage modeling\\nSpell checking and grammar correction\\nParaphrasing and rewriting\\nKeyword and phrase extraction\\nDialogue systems\\n\\nAnd here’s a breakdown of some of the more common ones we see:\\nAutomated Code Generation\\nLLMs can generate code snippets, functions, or even entire modules based on natural language descriptions, reducing the time and effort required to implement common functionalities.\\nHere’s an example to illustrate how LLMs can be used for automated code generation:\\nPrompt:\\n“Write a Python function that takes a list of numbers as input and returns a list containing only the even numbers.”\\n\\nText Generation\\nLLMs can generate coherent, contextually relevant text based on prompts. This includes creating articles, stories, and even generating product descriptions.\\nHere’s an example to illustrate how LLMs can be used for text generation:\\nPrompt:\\n“Generate a product description for a cutting-edge smartwatch designed for fitness enthusiasts. The description should highlight its advanced health and fitness tracking, personalized coaching, long battery life, durability, connectivity features, and customizable design. Target the description to appeal to both seasoned athletes and beginners interested in tracking their fitness progress.”\\n\\nLanguage Translation\\nThey can translate text between different languages, often with a high degree of accuracy, depending on the languages involved and the model’s training data.\\nHere’s an example to illustrate how LLMs can be used for language translation:\\nPrompt:\\n“Translate the following English text into Spanish: ‘The quick brown fox jumps over the lazy dog.'”\\n\\nBug Detection and Correction\\nLLMs can help identify bugs in code by analyzing code patterns and suggesting fixes for common errors, potentially integrating with IDEs (Integrated Development Environments) to provide real-time assistance.\\nHere’s an example to illustrate how LLMs can be used for bug detection:\\nPrompt:\\n“The Python function below intends to return the nth Fibonacci number. Please identify and correct any bugs in the function.\\nPython Function:\\ndef fibonacci(n):\\nif n <\\\\= 1:\\nreturn n\\nelse:\\nreturn fibonacci(n – 1) + fibonacci(n – 2)”\\n\\nParaphrasing and Rewriting\\nThey can rephrase or rewrite text while maintaining the original meaning, useful for content creation and academic purposes.\\nHere’s an example to illustrate how LLMs can be used for paraphrasing:\\nPrompt:\\n“Rewrite the following sentence in a simpler and more concise way without losing its original meaning: ‘The comprehensive study on climate change incorporates a wide array of data, including historical weather patterns, satellite imagery, and computer model predictions, to provide a holistic view of the impacts of global warming.'”\\n\\nDialogue Systems\\nLLMs power sophisticated dialogue systems for customer service, interactive storytelling, and educational purposes, providing responses that can adapt to the user’s input.\\nThink of a chatbot on a software product you use where you can ask it questions and it generates insightful, helpful responses.\\nChallenges and Limitations of LLMs\\nLarge language models have come a long way since the early days of Eliza.\\nIn the last two years alone, we’ve seen LLMs power Generative AI and create high-quality text, music, video, and images.\\nBut with any technology, there will always be growing pains.\\nTechnical Limitations of Language Models\\nLarge Language Models sometimes face technical limitations impacting their accuracy and ability to understand context.\\nDomain Mismatch\\nModels trained on broad datasets may struggle with specific or niche subjects due to a lack of detailed data in those areas. This can lead to inaccuracies or overly generic responses when dealing with specialized knowledge.\\nWord Prediction\\nLLMs often falter with less common words or phrases, impacting their ability to fully understand or accurately generate text involving these terms. This limitation can affect the quality of translation, writing, and technical documentation tasks.\\nReal-time Translation Efficiency\\nWhile LLMs have made strides in translation accuracy, the computational demands of processing and generating translations in real-time can strain resources, especially for languages with complex grammatical structures or those less represented in training data.\\nHallucinations and Bias\\nOn occasion, LLM technology is too original. So original in fact that it’s making up information.\\nThis is a lesson Air Canada learned the hard way when its chatbot told a customer about a refund policy when no such policy exists, which they then had to honor.\\nFinally, LLMs can inadvertently propagate and amplify biases present in their training data, leading to outputs that may be discriminatory or offensive.\\nScalability and Environmental Impact\\nThe scalability of LLMs is tied to the impact it has on the environment. And that impact is turning out to be a big one.\\nTraining a system like GPT-3 took 1,287 Megawatt hours (MWh) of energy. To put that into perspective, 1 MWh could power about 330 homes for one hour in the United States.\\nThe image below shows the energy consumption of training four different LLMs.\\n\\nEnergy consumption doesn’t end at training—operating LLMs also uses a grotesque level of energy.\\nIn one report, Alex de Vries, founder of Digiconomist, has calculated that by 2027 the AI sector will consume between 85 to 134 Terawatt hours each year. That’s almost the same as the annual energy demand of the Netherlands.\\nWe can’t help but wonder how sustainable that is and what the long-term environmental impact will be on our energy sources. Especially when you consider LLMs are only going to become larger and more complex as we advance their capabilities.\\nAnd to maintain large language models, we’ll need to update them with new data and parameters as they arise. That will only expend more energy and resources.\\nThe Future of Language Models: What Comes Next?\\nNow that we’ve seen drastic and rapid improvement in the capabilities of LLMs through Generative AI, we expect users of AI to be fine-tuning prompts and discovering new use cases and applications.\\nIn the workplace especially, the focus will be on productivity hacks. It’s something we experiment with already through our Generative Driven Development™ offering, where our team has increased the productivity of software development by 30-50%.\\nHilary Ashton, Chief Product Officer at Teradata, shared her predictions for the future of LLMs and AI in AI Magazine:\\n\\nFirst, I foresee a massive productivity leap forward through GenAI, especially in technology and software. It’s getting more cost-effective to get into GenAI, and there are lots more solutions available that can help improve GenAI solutions. It will be the year when conversations gravitate to GenAI, ethics, and what it means to be human. In some cases, we’ll start to see the workforce shift and be reshaped, with the technology helping to usher in a four-day work week for some full-time employees.”\\nHilary Ashton\\n\\nAnd she’s right, especially when it comes to ethical considerations and where we humans add value AI can’t replicate.\\nWe’ll also see further democratization of AI with it infiltrating other areas of our life, much the same the computer has done since its invention.\\nWhat we know for certain is the development of LLMs and Generative AI is only getting started. And we want to be leading conversations on its use, ethics, scalability, and more as it evolves.\\nYou can be part of that conversation too:\\nListen or watch our Talking AI podcast where we interview AI experts and talk or sign up for our newsletter where we share insights and developments on LLMs, AI/ML, and Data governance, curated by our very own CTO, Omar Shanti.\\nFrequently Asked Questions About Large Language Models LLMs\\n1. What is a Large Language Model (LLM)?\\nA Large Language Model (LLM) is an artificial intelligence model that uses machine learning techniques, particularly deep learning and neural networks, to understand and generate human language. These models are trained on massive data sets and can perform a broad range of tasks like generating text, translating languages, and more.\\n2. How do Large Language Models work?\\nLarge Language Models work by leveraging transformer models, which utilize self-attention mechanisms to process input text. They are pre-trained on vast amounts of data and can perform in-context learning, allowing them to generate coherent and contextually relevant responses based on user inputs.\\n3. What is the significance of transformer models in LLMs?\\nTransformer models are crucial because they enable LLMs to handle long-range dependencies in text through self-attention. This mechanism allows the model to weigh the importance of different words in a sentence, improving the language model’s performance in understanding and generating language.\\n4. Why are Large Language Models important in AI technologies?\\nLarge Language Models are important because they serve as foundation models for various AI technologies like virtual assistants, conversational AI, and search engines. They enhance the ability of machines to understand and generate human language, making interactions with technology more natural.\\n5. What is fine-tuning in the context of LLMs?\\nFine-tuning involves taking a pre-trained language model and further training it on a specific task or dataset. This process adjusts the model to perform better on specific tasks like sentiment analysis, handling programming languages, or other specialized applications.\\n6. How does model size affect the performance of Large Language Models?\\nThe model size, often measured by the parameter count, affects an LLM’s ability to capture complex language patterns. Very large models with hundreds of billions of parameters generally perform better but require more computational resources during the training process.\\n7. Can LLMs generate code in programming languages?\\nYes, Large Language Models can generate code in various programming languages. They assist developers by providing code snippets, debugging help, and translating code, thanks to their training on diverse datasets that include programming code.\\n8. What is “in-context learning” in Large Language Models?\\nIn-context learning refers to an LLM’s ability to learn and perform specific tasks based solely on the input text provided during inference, without additional fine-tuning. This allows the model to adapt to new tasks or instructions on the fly, enhancing its versatility across a broad range of applications.\\n9. How do LLMs handle multiple tasks like text generation and sentiment analysis?\\nLLMs are versatile due to their training on diverse data. They can perform multiple tasks like text generation, sentiment analysis, and more by leveraging their learned knowledge. Through fine-tuning, they can be adapted to perform specific tasks more effectively.\\n10. What are “zero-shot” and “few-shot” learning in Large Language Models?\\nZero-shot learning allows an LLM to perform a specific task it wasn’t explicitly trained on by leveraging its general language understanding. Few-shot learning involves providing the model with a few examples of the task within the prompt to guide its response. Both methods showcase the model’s ability to generalize and adapt to new tasks with minimal or no additional training data.\\nInstantly access the power of AI and our team of AI-enabled practitioners\\nWe are ready to support you on your project!\\nContact us\\n\\nCategory: Gen AI\\nTags: AI, artificial intelligence, gen ai, Generative AI, large language models, LLMs\\n\\nGet the best of our content\\nstraight to your inbox!\\nDon’t worry, we don’t spam!\\nRelated Posts\\n\\nProprietary Patient Management System Unlocks 99% Faster Implementation and Client Onboarding\\n\\nAmazon Q Developer: The AWS Tool Revolutionizing Cloud Interaction\\n\\nPractical Data Governance Pillars: Safeguarding Your Digital Assets\\n\\nTesting Your RAG-Powered AI Chatbot\\nCategories\\n\\nAgile\\nCulture\\nModernization\\nNearshore Development\\nProduct + Design\\nSoftware Development\\nTalent\\n\\n\\nSubscribe to our newsletter and stay up to date on the latest in AI\\nServices\\n\\nAI Strategy Roadmap\\nData Engineering & Analytics\\nAI-Powered Software Development\\nAI Engineering Teams\\n\\nPartnerships\\n\\nDatabricks\\n\\nAccelerators\\n\\nGen AI Innovation Workshop\\nGen AI Solution Accelerator\\nRAG\\nGenIQ\\n\\nIndustries\\n\\nCommunications & IoT\\nTechnology\\nHealthcare\\nFinance\\nRetail\\n\\nResources\\n\\nBlog\\nTalking AI Podcast\\nTalking AI Newsletter\\nEvents\\nNearshore Budget Calculator\\n\\nGet in touch\\n\\nBook a call\\n1-800-621-7063\\n\\nFacebook Youtube \\n\\nAtlanta, GA [HQ]\\nChicago, IL\\nDallas, TX ​\\nSan Jose, Costa Rica [HQ]\\nBogota, Colombia\\nMedellin, Colombia\\nBarranquilla, Colombia\\nLima, Peru\\n\\n\\n\\n©2023 HatchWorks Inc. All rights reserved.\\nPrivacy Policy​\\nTerms and Conditions\\nRecruitment Fraud Disclaimer\\n\\nClose this module\\n\\nFREE E-BOOKState of AI 2025\\nA round-up of industry stats, research, and insights to understand where AI stands, how it got here, and where it’s going.\\nNameName\\nEmailEmail\\nCompany NameCompany Name\\nDownload E-book\\nNo thanks, I’m not interested!\"}, {\"url\": \"https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models\", \"title\": \"25 of the best large language models in 2025 - TechTarget\", \"content\": \"Large language models are the dynamite behind the generative AI boom. Some of the most well-known language models today are based on the transformer model, including the generative pre-trained transformer series of LLMs and bidirectional encoder representations from transformers (BERT). Gemma is a family of open-source language models from Google that were trained on the same resources as Gemini. GPT-3 is OpenAI's large language model with more than 175 billion parameters, released in 2020. Large Language Model Meta AI (Llama) is Meta's LLM which was first released in 2023. The Pathways Language Model is a 540 billion parameter transformer-based model from Google powering its AI chatbot Bard. StableLM is a series of open language models developed by Stability AI, the company behind image generator Stable Diffusion.\", \"score\": 0.82254606, \"raw_content\": \"25 of the best large language models in 2025\\nWhatIs\\nSearch the TechTarget Network \\nBrowse Definitions :\\n\\nA\\nB\\nC\\nD\\nE\\nF\\nG\\nH\\nI\\nJ\\nK\\nL\\nM\\nN\\nO\\nP\\nQ\\nR\\nS\\nT\\nU\\nV\\nW\\nX\\nY\\nZ\\n#\\n\\nLogin Register\\n\\nTechTarget Network\\nTech Accelerator\\nNews\\n2024 IT Salary Survey Results\\n\\nRSS\\n\\n\\nWhatIs\\n\\n\\nBrowse Definitions Data analytics and AI\\nTopics View All\\n\\nBusiness software\\nCloud computing\\nComputer science\\nData centers\\nIT management\\nNetworking\\nSecurity\\nSoftware development\\n\\nPlease select a category\\n\\nTopics\\n\\n\\n\\nBrowse Features Resources\\n\\nBusiness strategies\\nCareer resources\\nEmerging tech\\nTech explainers\\n\\n\\n\\nFollow:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\n\\nData analytics and AI\\n\\nTech Accelerator What is Gen AI? Generative AI explained\\nPrev Next Will AI replace jobs? 17 job types that might be affected Pros and cons of AI-generated content\\nDownload this guide1\\nFeature\\n25 of the best large language models in 2025\\nLarge language models have been affecting search for years and have been brought to the forefront by ChatGPT and other chatbots.\\n\\nShare this item with your network:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy\\n\\nSean Michael Kerner\\nBen Lutkevich, Site Editor\\n\\nPublished: 31 Jan 2025\\nLarge language models are the dynamite behind the generative AI boom. However, they've been around for a while.\\nLLMs are black box AI systems that use deep learning on extremely large datasets to understand and generate new text. Modern LLMs began taking shape in 2014 when the attention mechanism -- a machine learning technique designed to mimic human cognitive attention -- was introduced in a research paper titled \\\"Neural Machine Translation by Jointly Learning to Align and Translate.\\\" In 2017, that attention mechanism was honed with the introduction of the transformer model in another paper, \\\"Attention Is All You Need.\\\"\\nSome of the most well-known language models today are based on the transformer model, including the generative pre-trained transformer series of LLMs and bidirectional encoder representations from transformers (BERT).\\nChatGPT, which runs on a set of language models from OpenAI, attracted more than 100 million users just two months after its release in 2022. Since then, many competing models have been released. Some belong to big companies such as Google, Amazon and Microsoft; others are open source.\\nConstant developments in the field can be difficult to keep track of. Here are some of the most influential models, both past and present. Included in it are models that paved the way for today's leaders as well as those that could have a significant effect in the future.\\nThis article is part of\\nWhat is Gen AI? Generative AI explained\\n\\nWhich also includes:\\n8 top generative AI tool categories for 2025\\nWill AI replace jobs? 17 job types that might be affected\\n25 of the best large language models in 2025\\n\\nTop current LLMs\\nBelow are some of the most relevant large language models today. They do natural language processing and influence the architecture of future models.\\nBERT\\nBERT is a family of LLMs that Google introduced in 2018. BERT is a transformer-based model that can convert sequences of data to other sequences of data. BERT's architecture is a stack of transformer encoders and features 342 million parameters. BERT was pre-trained on a large corpus of data then fine-tuned to perform specific tasks along with natural language inference and sentence text similarity. It was used to improve query understanding in the 2019 iteration of Google search.\\nClaude\\nThe Claude LLM focuses on constitutional AI, which shapes AI outputs guided by a set of principles that help the AI assistant it powers helpful, harmless and accurate. Claude was created by the company Anthropic.\\nThere are three primary branches of Claude -- Opus, Haiku and Sonnet. The latest iteration of the Claude LLM is the Claude 3.5 Sonnet. It understands nuance, humor and complex instructions better than earlier versions of the LLM. It also has broad programming capabilities that make it well-suited for application development. In October 2024, Claude added a computer-use AI tool, that enables the LLM to use a computer like a human does. It's available via Claude.ai, the Claude iOS app and through an API.\\nCohere\\nCohere is an enterprise AI platform that provides several LLMs including Command, Rerank and Embed. These LLMs can be custom-trained and fine-tuned to a specific company's use case. The company that created the Cohere LLM was founded by one of the authors of Attention Is All You Need.\\nDeepSeek-R1\\nDeepSeek-R1 is an open-source reasoning model for tasks with complex reasoning, mathematical problem-solving and logical inference. The model uses reinforcement learning techniques to refine its reasoning ability and solve complex problems. DeepSeek-R1 can perform critical problem-solving through self-verification, chain-of-thought reasoning and reflection.\\nErnie\\nErnie is Baidu's large language model which powers the Ernie 4.0 chatbot. The bot was released in August 2023 and has garnered more than 45 million users. Ernie is rumored to have 10 trillion parameters. The bot works best in Mandarin but is capable in other languages.\\nFalcon\\nFalcon is a family of transformer-based models developed by the Technology Innovation Institute. It is open source and has multi-lingual capabilities. Falcon 2 is available in an 11 billion parameter version that provide multimodal capabilities for both text and vision.\\nThe Falcon 1 series includes a pair of larger models with Falcon 40B and Falcon 180B. Falcon models are available on GitHub as well as on cloud provider including Amazon.\\nGemini\\nGemini is Google's family of LLMs that power the company's chatbot of the same name. The model replaced Palm in powering the chatbot, which was rebranded from Bard to Gemini upon the model switch. Gemini models are multimodal, meaning they can handle images, audio and video as well as text. Gemini is also integrated in many Google applications and products. It comes in three sizes -- Ultra, Pro and Nano. Ultra is the largest and most capable model, Pro is the mid-tier model and Nano is the smallest model, designed for efficiency with on-device tasks.\\nAmong the most recent models is the Gemini 1.5 Pro update that debuted in May 2024 Gemini is available as a web chatbot, the Google Vertex AI service and via API. Early previews of Gemini 2.0 Flash became available in December 2024 with updated multimodal generation capabilities.\\nGemma\\nGemma is a family of open-source language models from Google that were trained on the same resources as Gemini. Gemma 2 was released in June 2024 in two sizes -- a 9 billion parameter model and a 27 billion parameter model. Gemma models can be run locally on a personal computer, and are also available in Google Vertex AI.\\nGPT-3\\nGPT-3 is OpenAI's large language model with more than 175 billion parameters, released in 2020. GPT-3 uses a decoder-only transformer architecture. In September 2022, Microsoft announced it had exclusive use of GPT-3's underlying model. GPT-3 is 10 times larger than its predecessor. GPT-3's training data includes Common Crawl, WebText2, Books1, Books2 and Wikipedia.\\nGPT-3 is the last of the GPT series of models in which OpenAI made the parameter counts publicly available. The GPT series was first introduced in 2018 with OpenAI's paper \\\"Improving Language Understanding by Generative Pre-Training.\\\"\\nGPT-3.5\\nGPT-3.5 is an upgraded version of GPT-3 with fewer parameters. GPT-3.5 was fine-tuned using reinforcement learning from human feedback. GPT-3.5 is the version of GPT that powers ChatGPT. There are several models, with GPT-3.5 turbo being the most capable, according to OpenAI. GPT-3.5's training data extends to September 2021.\\nIt was also integrated into the Bing search engine but has since been replaced with GPT-4.\\nGPT-4\\nGPT-4 , was released in 2023 and like the others in the OpenAI GPT family, it's a transformer-based model. Unlike the others, its parameter count has not been released to the public, though there are rumors that the model has more than 170 trillion. OpenAI describes GPT-4 as a multimodal model, meaning it can process and generate both language and images as opposed to being limited to only language. GPT-4 also introduced a system message, which lets users specify tone of voice and task.\\nGPT-4 demonstrated human-level performance in multiple academic exams. At the model's release, some speculated that GPT-4 came close to artificial general intelligence, which means it is as smart or smarter than a human. That speculation turned out to be unfounded.\\nGPT-4o\\nGPT-4 Omni (GPT-4o) is OpenAI's successor to GPT-4 and offers several improvements over the previous model. GPT-4o creates a more natural human interaction for ChatGPT and is a large multimodal model, accepting various inputs including audio, image and text. The conversations let users engage as they would in a normal human conversation, and the real-time interactivity can also pick up on emotions. GPT-4o can see photos or screens and ask questions about them during interaction.\\nGPT-4o can respond in 232 milliseconds, similar to human response time and faster than GPT-4 Turbo.\\nGranite\\nThe IBM Granite family of models are fully open source models under the Apache v.2 license. The first iteration of the open source model models debuted in May 2024, followed by Granite 3.0 in October and Granite 3.1 in December 2024.\\nThere are multiple variants in the Granite model family including General-purpose models (8B and 2B variants), guardrail model and Mixture-of-Experts models. While the model can be used for general purpose deployments, IBM itself is focusing deployment and optimization for enterprise use cases like customer service, IT automation and cybersecurity.\\nLamda\\nLamda (Language Model for Dialogue Applications) is a family of LLMs developed by Google Brain announced in 2021. Lamda used a decoder-only transformer language model and was pre-trained on a large corpus of text. In 2022, LaMDA gained widespread attention when then-Google engineer Blake Lemoine went public with claims that the program was sentient. It was built on the Seq2Seq architecture.\\nLlama\\nLarge Language Model Meta AI (Llama) is Meta's LLM which was first released in 2023. The Llama 3.1 models were released in July 2024, including both a 405 billion and 70 billion parameter model.\\nThe most recent version is Llama 3.2 which was released in September 2024, initially with smaller parameter counts of 11 billion and 90 billion.\\nLlama uses a transformer architecture and was trained on a variety of public data sources, including webpages from CommonCrawl, GitHub, Wikipedia and Project Gutenberg. Llama was effectively leaked and spawned many descendants, including Vicuna and Orca. Llama is available under an open license, allowing for free use of the models. Lllama models are available in many locations including llama.com and Hugging Face.\\nMistral\\nMistral is a family of a mixture of expert models from Mistral AI. Among the newest models is Mistral Large 2 which was first released in July 2024. The model operates with 123 billion parameters and a 128k context window, supporting dozens of languages including French, German, Spanish, Italian, and many others, along with more than 80 coding languages.\\nIn November 2024, Mistral released Pixtral Large, a 124-billion-parameter multimodal model that can handle text and visual data. Mistral models are available via Mistral's API on its Le Platforme-managed web service.\\no1\\nThe OpenAI o1 model family was first introduced in Sept. 2024. The o1 model's focus is to provide what OpenAI refers to as - reasoning models, that can reason through a problem or query before offering a response.\\nThe o1 models excel in STEM fields, with strong results in mathematical reasoning (scoring 83% on the International Mathematics Olympiad compared to GPT-4o's 13%), code generation and scientific research tasks. While they offer enhanced reasoning and improved safety features, they operate more slowly than previous models due to their thorough reasoning processes and come with certain limitations, such as restricted access features and higher API costs. The models are available to ChatGPT Plus and Team users, with varying access levels for different user categories.\\no3\\nOpenAI introduced the successor model, o3, in December 2024. According to OpenAI, o3 is designed to handle tasks with more analytical thinking, problem-solving and complex reasoning and will improve o1's capabilities and performance. The o3 model is in safety testing mode and is currently not available to the public.\\nOrca\\nOrca was developed by Microsoft and has 13 billion parameters, meaning it's small enough to run on a laptop. It aims to improve on advancements made by other open source models by imitating the reasoning procedures achieved by LLMs. Orca achieves the same performance as GPT-4 with significantly fewer parameters and is on par with GPT-3.5 for many tasks. Orca is built on top of the 13 billion parameter version of Llama.\\nPalm\\nThe Pathways Language Model is a 540 billion parameter transformer-based model from Google powering its AI chatbot Bard. It was trained across multiple TPU 4 Pods -- Google's custom hardware for machine learning. Palm specializes in reasoning tasks such as coding, math, classification and question answering. Palm also excels at decomposing complex tasks into simpler subtasks.\\nPaLM gets its name from a Google research initiative to build Pathways, ultimately creating a single model that serves as a foundation for multiple use cases. There are several fine-tuned versions of Palm, including Med-Palm 2 for life sciences and medical information as well as Sec-Palm for cybersecurity deployments to speed up threat analysis.\\nPhi\\nPhi is a transformer-based language model from Microsoft. The Phi 3.5 models were first released in August 2024.\\nThe series includes Phi-3.5-mini-instruct (3.82 billion parameters), Phi-3.5-MoE-instruct (41.9 billion parameters), and Phi-3.5-vision-instruct (4.15 billion parameters), each designed for specific tasks ranging from basic reasoning to vision analysis. All three models support a 128k token context length.\\nReleased under a Microsoft-branded MIT License, they are available for developers to download, use, and modify without restrictions, including for commercial purposes.\\nQwen\\nQwen is large family of open models developed by Chinese internet giant Alibaba Cloud. The newest set of models are the Qwen2.5 suite, which support 29 different languages and currently scale up to 72 billion parameters. These models are suitable for a wide range of tasks, including code generation, structured data understanding, mathematical problem-solving as well as general language understanding and generation.\\nStableLM\\nStableLM is a series of open language models developed by Stability AI, the company behind image generator Stable Diffusion.\\nStableLM 2 debuted in January 2024 initially with a 1.6 billion parameter model. In April 2024 that was expanded to also include a 12 billion parameter model. StableLM 2 supports seven languages: English, Spanish, German, Italian, French, Portuguese, and Dutch. Stability AI positions these models as offering different options for various use cases, with the 1.6B model suitable for specific, narrow tasks and faster processing while the 12B model provides more capability but requires more computational resources.\\nTülu 3\\nAllen Institute for AI's Tülu 3 is an open-source 405 billion-parameter LLM. The Tülu 3 405B model has post-training methods that combine supervised fine-tuning and reinforcement learning at a larger scale. Tülu 3 uses a \\\"reinforcement learning from verifiable rewards\\\" framework for fine-tuning tasks with verifiable outcomes -- such as solving mathematical problems and following instructions.\\nVicuna 33B\\nVicuna is another influential open source LLM derived from Llama. It was developed by LMSYS and was fine-tuned using data from sharegpt.com. It is smaller and less capable that GPT-4 according to several benchmarks, but does well for a model of its size. Vicuna has only 33 billion parameters, whereas GPT-4 has trillions.\\nLLM precursors\\nAlthough LLMs are a recent phenomenon, their precursors go back decades. Learn how recent precursor Seq2Seq and distant precursor ELIZA set the stage for modern LLMs.\\nSeq2Seq\\nSeq2Seq is a deep learning approach used for machine translation, image captioning and natural language processing. It was developed by Google and underlies some of their modern LLMs, including LaMDA. Seq2Seq also underlies AlexaTM 20B, Amazon's large language model. It uses a mix of encoders and decoders.\\nEliza\\nEliza was an early natural language processing program created in 1966. It is one of the earliest examples of a language model. Eliza simulated conversation using pattern matching and substitution. Eliza, running a certain script, could parody the interaction between a patient and therapist by applying weights to certain keywords and responding to the user accordingly. The creator of Eliza, Joshua Weizenbaum, wrote a book on the limits of computation and artificial intelligence.\\nNext Steps\\nGenerative AI challenges that businesses should consider\\nGenerative AI ethics: Biggest concerns\\nGenerative AI landscape: Potential future trends\\nGenerative models: VAEs, GANs, diffusion, transformers, NeRFs\\nAI content generators to explore\\nRelated Resources\\n\\nFive data quality trends to prepare for in the year ahead –Video\\nThe Digital Transformation And Innovation Landscape –Wipro\\nCloudera and NVIDIA Accelerate AI in the Financial Services Industry –Cloudera\\nImprove customer satisfaction or cut costs? Who says you have to choose? –Video\\n\\nDig Deeper on Data analytics and AI\\n\\n ##### What is GPT-3? Everything you need to know  By: Nick Barney\\n ##### What is a small language model (SLM)?  By: Sean Kerner\\n ##### GPT-4  By: Ben Lutkevich\\n ##### What are large language models (LLMs)?  By: Sean Kerner\\n\\nSponsored News\\n\\nSustainability, AI and Dell PowerEdge Servers –Dell Technologies and Intel\\nThree Innovative AI Use Cases for Natural Language Processing –Dell Technologies\\nAutonomous coding: The future of the revenue cycle –Solventum\\n\\nRelated Content\\n\\nExploring GPT-3 architecture – Search Enterprise AI\\nWhat is GPT-3? Everything you need to know – Search Enterprise AI\\nMicrosoft exclusively licenses OpenAI's GPT-3 ... – Search Enterprise AI\\n\\nLatest TechTarget resources\\n\\nNetworking\\nSecurity\\nCIO\\nHR Software\\nCustomer Experience\\n\\nSearch Networking\\n\\n\\nWhat is a thin client (lean client)?A thin client (lean client) is a virtual desktop computing model that runs on the resources stored on a central server instead of...\\n\\n\\nWhat is network monitoring?Network monitoring, also frequently called network management, is the practice of consistently overseeing a computer network for ...\\n\\n\\nWhat is network automation?Network automation is a process that uses intelligent software to automate the management, configuration, deployment, testing and...\\n\\n\\nSearch Security\\n\\n\\nWhat is Internet Key Exchange (IKE)?Internet Key Exchange (IKE) is a standard protocol used to set up a secure and authenticated communication channel between two ...\\n\\n\\nWhat is a certificate revocation list (CRL) and how is it used?A certificate revocation list (CRL) is a list of digital certificates that have been revoked by the issuing certificate authority...\\n\\n\\nWhat is cryptology?Cryptology is the mathematics, such as number theory and the application of formulas and algorithms, that underpin cryptography ...\\n\\n\\nSearch CIO\\n\\n\\nWhat is an IT project manager?An IT project manager is a professional charged with overseeing the process of planning, executing and delegating ...\\n\\n\\nWhat is a cyberthreat hunter (cybersecurity threat analyst)?A cyberthreat hunter, also called a cybersecurity threat analyst, proactively identifies security incidents that might go ...\\n\\n\\nWhat is blockchain? Definition, examples and how it worksBlockchain is a distributed ledger technology (DLT) that's shared across a network of computers to keep a digital record of ...\\n\\n\\nSearch HRSoftware\\n\\n\\nWhat is employee self-service (ESS)?Employee self-service (ESS) is a widely used human resources technology that enables employees to perform many job-related ...\\n\\n\\nWhat is DEI? Diversity, equity and inclusion explainedDiversity, equity and inclusion is a term used to describe policies and programs that promote the representation and ...\\n\\n\\nWhat is payroll software?Payroll software automates the process of paying salaried, hourly and contingent employees.\\n\\n\\nSearch Customer Experience\\n\\n\\nWhat is account-based selling? Everything you need to knowAccount-based selling (ABS) is a strategic sales approach in business-to-business sales and marketing that centers around ...\\n\\n\\nWhat is interactive voice response (IVR)?Interactive voice response (IVR) is an automated telephony system that interacts with callers, gathers information and routes ...\\n\\n\\nWhat is an AI assistant?An AI assistant, or digital assistant, is software that uses artificial intelligence to understand natural language voice ...\\n\\n\\nBrowse by Topic\\n\\n\\nBrowse Resources\\n\\n\\nAbout Us\\n\\nMeet The Editors\\nEditorial Ethics Policy\\nContact Us\\nAdvertisers\\nBusiness Partners\\nEvents\\nMedia Kit\\nCorporate Site\\nReprints\\n\\nAll Rights Reserved, Copyright 1999 - 2025, TechTarget  \\nPrivacy Policy\\nCookie Preferences\\nCookie Preferences\\nDo Not Sell or Share My Personal Information\\nClose\\n\\nX\\nFree Download What is generative AI? Everything you need to know\\nThe potential of AI technology has been percolating in the background for years. But when ChatGPT, the AI chatbot, began grabbing headlines in early 2023, it put generative AI in the spotlight. This guide is your go-to manual for generative AI, covering its benefits, limits, use cases, prospects and much more.\\n\"}, {\"url\": \"https://botpress.com/blog/best-large-language-models\", \"title\": \"Best Large Language Models in 2025 (Open Source + Hosted LLMs)\", \"content\": \"Discord Join thousands of peers and share ideas Docs Comprehensive guides and references API Reference material for use with external systems LLM Ranking Real-time data to help you choose the right models Videos Tutorials, demos, and product walkthroughs CLI Command-line tools for faster building Whether it's adding natural language processing capabilities to an existing app or building new AI-driven features, APIs allow developers to use LLMs for tasks like sentiment analysis, language translation, or content generation without needing to build or train models themselves. Developed by the Technology Innovation Institute and released on September 6, 2023, Falcon 180B features a staggering 180 billion parameters, making it one of the largest and most powerful open-source LLMs. It was designed to excel in tasks like translation, text generation, and research​.\", \"score\": 0.7409441, \"raw_content\": \"The Best Large Language Models in 2025 (Open Source + Hosted)\\n\\nPlatform\\nFeatures\\nAgent Studio Build and customize your agent rapidlyAutonomous Engine Use LLMs to guide conversations and tasksKnowledge Bases Train your bot with custom knowledge sourcesTables Store and manage conversation data\\nChannels\\n WhatsApp Instagram Messenger Slack\\nAll channels\\nIntegrations\\n HubSpot Notion Jira Calendly\\nAll integrations\\nLLM Providers\\n OpenAI Anthropic Groq Hugging Face\\nAll LLMs\\nSolutions\\nFor\\nEnterprise Automate mission-critical production workflowsAgencies Provide sophisticated agent servicesDevelopers Explore a robust API for agent development\\n Customer Stories Discover from successful customers how Botpress is transforming business worldwide.\\nBy Industry\\nEcommerce\\nEducation\\nFinance\\nHospitality\\nAll industries\\nBy Department\\nSales\\nEngineering\\nProduct\\nITSM\\nAll departments\\nBy Use Case\\nShopping Assistant\\nLead Generation\\nEmployee Experience\\nTicket Management\\nAll use cases\\nResources\\nEssential\\n Academy Learn to build through curated courses Library Resources to enhance your AI workflows Blog Insights and updates on Botpress and AI agents\\nbuilding\\n Discord Join thousands of peers and share ideas Docs Comprehensive guides and references API Reference material for use with external systems LLM Ranking Real-time data to help you choose the right models Videos Tutorials, demos, and product walkthroughs CLI Command-line tools for faster building\\nPartners\\n Become a Partner Join our network of certified experts Hire an Expert Connect with partners and consultants\\nDocs\\nPricing\\nContactGet started for free\\nback to blog\\nAI Basics\\nFor Builders\\nThe Best Large Language Models in 2025 (Open Source + Hosted)\\nOctober 19, 2024\\n·\\nUpdated on\\nLarge language models are increasing in power and popularity. Here are some of the best available for users today.\\nBotpress\\n\\nWith so many large language models (LLMs), it can be hard to decide which to use.\\nThe latest models are constantly pushing the boundaries of what's possible in artificial intelligence. As these models continue to shape the way we interact with technology, the possibilities for generative AI applications are limitless.\\nWe are now presented with a powerful toolset at our fingertips. It's suddenly easy to create AI agents and AI chatbots, or to use an LLM as a personal AI assistant in day-to-day tasks.\\nThe world of LLMs is only just beginning.\\nWhat are large language models?\\nA large language model (LLM) is an advanced type of artificial intelligence designed to understand and generate human-like text.\\nLLMs use deep learning algorithms that have been trained on vast amounts of data to recognize patterns and context in language.\\nAfter training, they use natural language processing perform tasks like translation, content creation, summarization, and answering questions.\\n\\nBuild LLM chatbots\\nBuild LLM-powered AI agents\\nStart now\\nNo credit card required\\nHow to use a large language model\\nThere are infinite ways to apply the power of an LLM. But most fall into one of 3 main categories:\\n1. AI agents and chatbots\\nLLMs are commonly integrated into chatbots and AI agents. These days, most conversational AI is powered by an LLM.\\nYou can see the most popular LLMs for chatbots and AI agents on our real-time ranking of the LLMs used by 500,000+ bot builders.\\nThese models can handle complex queries, generate contextual responses, and even manage dynamic conversations that evolve based on user input.\\nCommon AI agents include customer support chatbots and HR bots. But as the technology expands, so do use cases. Now businesses can build bespoke chatbots for hotels, sales chatbots, or even chatbots for real estate.\\nBy understanding the intent and context behind questions, LLM-powered chatbots can be used for customer support, virtual assistants, or even in business process automation.\\n2. Daily use\\nLLMs have increasingly made their way into everyday tasks. People use them for content generation, text summarization, language translation, and even creative projects, like writing poems or generating art descriptions.\\nThere are plenty of tools that use LLM APIs to help with daily tasks. Software like writing assistants or code completion tools are typically powered by LLMs these days.\\n3. API use\\nIf you're a developer, you can be the one using an API to build other software and tools.\\nLLMs can be accessed via APIs, which provide flexibility for integrating language models into various software applications.\\nWhether it's adding natural language processing capabilities to an existing app or building new AI-driven features, APIs allow developers to use LLMs for tasks like sentiment analysis, language translation, or content generation without needing to build or train models themselves.\\n\\nDeploying AI Agents?\\nRead our Blueprint for AI Agent Implementation\\nRead Now\\nAccess is always free\\nThe 5 best LLMs\\nMost LLM use is hosted software, which means it's maintained and run by a third-party provider on their servers, rather than on the user’s local system.\\nUsers access it over the internet, benefiting from simplified maintenance, updates, and infrastructure management handled by the host.\\nHere are the 5 best hosted LLMs available today:\\n1. GPT-4o\\nOpenAI’s latest multimodal model, GPT-4o, was released in May 2024 and integrates text, image, video, and voice capabilities.\\nThis model is 50% cheaper and twice as fast as GPT-4, making it highly efficient for a wide range of tasks. It stands out with its Voice-to-Voice function, allowing for audio responses in real-time, with a latency of just 320 milliseconds.\\nGPT-4o also improves performance in non-English languages and offers a more interactive experience​.\\n2. Claude 3.5\\nLaunched by Anthropic in June 2024, Claude 3.5 is known for its ethical design and strong performance across various benchmarks.\\nAvailable through an API, it continues Anthropic’s focus on safer AI interactions. While the number of parameters remains undisclosed, its advanced capabilities make it a strong competitor for tasks involving conversational AI and content generation​.\\n3. Grok-1\\nDeveloped by Elon Musk’s xAI, Grok-1 debuted in November 2023 with 314 billion parameters, focusing on generating responses with personality and real-time data from X (formerly Twitter).\\nIn August 2024, xAI released Grok-2 and Grok-2 mini, which have reportedly outperformed GPT-4o in several performance metrics​.\\n4. Gemini 1.5\\nGoogle's Gemini 1.5 focuses on improving multilingual capabilities and translation accuracy, making it particularly valuable for global businesses.\\nReleased in mid-2024, it is also designed to enhance tasks like text generation, customer interaction, and more​.\\n5. Inflection-2.5\\nInflection AI’s Inflection-2.5 powers the conversational AI assistant Pi, released in March 2024.\\nThis model achieves over 94% of GPT-4’s performance while using only 40% of the training computational resources.\\nIts efficiency has led to over a million daily active users on Pi, making it one of the most popular conversational models today​\\nThe 5 best open-source LLMs\\nIf you're a builder, open source LLMs are your friend. Open-source software refers to code that is publicly available for anyone to view, modify, and distribute.\\nIt fosters collaboration and transparency, allowing developers to adapt the software to their specific needs while contributing to its improvement.\\nHere are the top 5 open-source LLMs available today:\\n1. LLaMA 3.1\\nMeta’s latest open-source LLM, LLaMA 3, launched in April 2024, with sizes ranging from 8 billion to 70 billion parameters.\\nIt offers improved reasoning and coding abilities and is open-source for developers. LLaMA 3 is designed to outperform models like Claude 3 and Gemini 1.5, making it a top choice for a range of real-world tasks.\\n2. Mistral 7B\\nReleased by Mistral AI on September 27, 2023, this model has 7.3 billion parameters but manages to outperform larger models in many benchmarks.\\nIts smaller size makes it highly efficient, ideal for self-hosting, and versatile across NLP tasks​.\\n3. Falcon 180B\\nDeveloped by the Technology Innovation Institute and released on September 6, 2023, Falcon 180B features a staggering 180 billion parameters, making it one of the largest and most powerful open-source LLMs.\\nIt was designed to excel in tasks like translation, text generation, and research​.\\n4. OLMo\\nCreated by the Allen Institute for AI, OLMo focuses on transparency and reproducibility, making it highly valuable for research purposes.\\nIt’s particularly favored by researchers who need full insight into the data and training process​.\\n5. Qwen-1.5\\nAlibaba’s Qwen-1.5 is their open-source LLM, which competes with models from Meta and Google in both capability and cost-effectiveness.\\nIt's aimed at high-performance tasks in language processing and is designed to scale across various applications, from e-commerce to customer service​.\\nDeploy an LLM-powered AI agent\\nLeverage LLMs in your day-to-day with custom AI agents.\\nWith the plethora of chatbot platforms out there, it’s easy to set up an AI agent to fulfill your specific needs.\\nBotpress is an endlessly extensible AI automation platform. With a pre-built library of integrations, drag-and-drop workflows, and comprehensive tutorials, it's accessible for builders at all stages of expertise.\\nPlug in any LLM to power your AI project, across any use case.\\nStart building today. It's free.\\n\\nBuild LLM chatbots\\nBuild LLM-powered AI agents\\nStart now\\nNo credit card required\\nTable of Contents\\nStep 1. the title of the step goes here as expected\\nStep 1. the title of the step goes here as expected\\n\\nLearn how to build AI agents\\nShare this on:\\n\\n\\n##### What is RCS?\\nMarc Mercier\\nNov 29\\n\\n##### Ultimate Guide to Artificial Intelligence (AI) and Augmented Reality (AR)\\nBotpress\\nSep 23\\n\\n##### What is Agentic AI?\\nSarah Chudleigh\\nDec 11\\nBuild Better with Botpress\\nGet started today - it’s free!\\nStart building now \\n\\nAll Systems Operational\\nSOC 2\\nCertified\\nGDPR\\nCompliant\\n\\n© 2025\\n\\n\\nPlatform\\nPricingAgent StudioAutonomous EngineKnowledge BasesTables\\nHub\\nIntegrationsChannelsLLMs\\nResources\\nTalk to SalesDocumentationHire an ExpertVideosCustomer StoriesAPI ReferenceBlogStatusv12 Resources\\nCommunity\\nCommunity SupportBecome a PartnerBecome an AmbassadorBecome an Affiliate\\nCompany\\nAboutCareersNews & PressLegalPrivacy\\n© Botpress 2025\"}, {\"url\": \"https://en.wikipedia.org/wiki/List_of_large_language_models\", \"title\": \"List of large language models - Wikipedia\", \"content\": \"GLaM (Generalist Language Model)    December 2021   Google  1200[35]    1.6 trillion tokens[35] 5600[35]    Proprietary Sparse mixture of experts model, making it more expensive to train but cheaper to run inference compared to GPT-3. PaLM (Pathways Language Model)  April 2022  Google  540[43] 768 billion tokens[42]  29,250[38]  Proprietary Trained for ~60 days on ~6000 TPU v4 chips.[38] As of October 2024, it is the largest dense Transformer published. Mixtral 8x7B    December 2023   Mistral AI  46.7    Unknown Unknown Apache 2.0  Outperforms GPT-3.5 and Llama 2 70B on many benchmarks.[82] Mixture of experts model, with 12.9 billion parameters activated per token.[83] ^ a b c d Table 20 and page 66 of PaLM: Scaling Language Modeling with Pathways Archived 2023-06-10 at the Wayback Machine\", \"score\": 0.7251239, \"raw_content\": \"Jump to content\\nMain menu\\nSearch\\nDonate\\nCreate account\\nLog in\\nPersonal tools\\nToggle the table of contents\\nList of large language models\\nAdd languages\\nArticle\\nTalk\\nRead\\nEdit\\nView history\\nTools\\nFrom Wikipedia, the free encyclopedia\\nA large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThis page lists notable large language models.\\nFor the training cost column, 1 petaFLOP-day = 1 petaFLOP/sec × 1 day = 8.64E19 FLOP. Also, only the largest model's cost is written.\\nName    Release date[a] Developer   Number of parameters (billion) [b]  Corpus size Training cost (petaFLOP-day)    License[c]  Notes\\nGPT-1   June 2018   OpenAI  0.117       1[1]    MIT[2]  First GPT model, decoder-only transformer. Trained for 30 days on 8 P600 GPUs.\\nBERT    October 2018    Google  0.340[3]    3.3 billion words[3]    9[4]    Apache 2.0[5]   An early and influential language model.[6]Encoder-only and thus not built to be prompted or generative.[7] Training took 4 days on 64 TPUv2 chips.[8]\\nT5  October 2019    Google  11[9]   34 billion tokens[9]        Apache 2.0[10]  Base model for many Google projects, such as Imagen.[11]\\nXLNet   June 2019   Google  0.340[12]   33 billion words    330 Apache 2.0[13]  An alternative to BERT; designed as encoder-only. Trained on 512 TPU v3 chips for 5.5 days.[14]\\nGPT-2   February 2019   OpenAI  1.5[15] 40GB[16] (~10 billion tokens)[17]   28[18]  MIT[19] Trained on 32 TPUv3 chips for 1 week.[18]\\nGPT-3   May 2020    OpenAI  175[20] 300 billion tokens[17]  3640[21]    proprietary A fine-tuned variant of GPT-3, termed GPT-3.5, was made available to the public through a web interface called ChatGPT in 2022.[22]\\nGPT-Neo March 2021  EleutherAI  2.7[23] 825 GiB[24]     MIT[25] The first of a series of free GPT-3 alternatives released by EleutherAI. GPT-Neo outperformed an equivalent-size GPT-3 model on some benchmarks, but was significantly worse than the largest GPT-3.[25]\\nGPT-J   June 2021   EleutherAI  6[26]   825 GiB[24] 200[27] Apache 2.0  GPT-3-style language model\\nMegatron-Turing NLG October 2021[28]    Microsoft and Nvidia    530[29] 338.6 billion tokens[29]    38000[30]   Restricted web access   Trained for 3 months on over 2000 A100 GPUs on the NVIDIA Selene Supercomputer, for over 3 million GPU-hours.[30]\\nErnie 3.0 Titan December 2021   Baidu   260[31] 4 Tb        Proprietary Chinese-language LLM. Ernie Bot is based on this model.\\nClaude[32]  December 2021   Anthropic   52[33]  400 billion tokens[33]      beta    Fine-tuned for desirable behavior in conversations.[34]\\nGLaM (Generalist Language Model)    December 2021   Google  1200[35]    1.6 trillion tokens[35] 5600[35]    Proprietary Sparse mixture of experts model, making it more expensive to train but cheaper to run inference compared to GPT-3.\\nGopher  December 2021   DeepMind    280[36] 300 billion tokens[37]  5833[38]    Proprietary Later developed into the Chinchilla model.\\nLaMDA (Language Models for Dialog Applications) January 2022    Google  137[39] 1.56T words,[39] 168 billion tokens[37] 4110[40]    Proprietary Specialized for response generation in conversations.\\nGPT-NeoX    February 2022   EleutherAI  20[41]  825 GiB[24] 740[27] Apache 2.0  based on the Megatron architecture\\nChinchilla  March 2022  DeepMind    70[42]  1.4 trillion tokens[42][37] 6805[38]    Proprietary Reduced-parameter model trained on more data. Used in the Sparrow bot. Often cited for its neural scaling law.\\nPaLM (Pathways Language Model)  April 2022  Google  540[43] 768 billion tokens[42]  29,250[38]  Proprietary Trained for ~60 days on ~6000 TPU v4 chips.[38] As of October 2024, it is the largest dense Transformer published.\\nOPT (Open Pretrained Transformer)   May 2022    Meta    175[44] 180 billion tokens[45]  310[27] Non-commercial research[d]  GPT-3 architecture with some adaptations from Megatron. Uniquely, the training logbook written by the team was published.[46]\\nYaLM 100B   June 2022   Yandex  100[47] 1.7TB[47]       Apache 2.0  English-Russian model based on Microsoft's Megatron-LM.\\nMinerva June 2022   Google  540[48] 38.5B tokens from webpages filtered for mathematical content and from papers submitted to the arXiv preprint server[48]     Proprietary For solving \\\"mathematical and scientific questions using step-by-step reasoning\\\".[49] Initialized from PaLM models, then finetuned on mathematical and scientific data.\\nBLOOM   July 2022   Large collaboration led by Hugging Face 175[50] 350 billion tokens (1.6TB)[51]      Responsible AI  Essentially GPT-3 but trained on a multi-lingual corpus (30% English excluding programming languages)\\nGalactica   November 2022   Meta    120 106 billion tokens[52]  unknown CC-BY-NC-4.0    Trained on scientific text and modalities.\\nAlexaTM (Teacher Models)    November 2022   Amazon  20[53]  1.3 trillion[54]        proprietary[55] bidirectional sequence-to-sequence architecture\\nNeuro-sama  December 2022   Independent Unknown Unknown     privately-owned A language model designed for live-streaming on Twitch.\\nLLaMA (Large Language Model Meta AI)    February 2023   Meta AI 65[56]  1.4 trillion[56]    6300[57]    Non-commercial research[e]  Corpus has 20 languages. \\\"Overtrained\\\" (compared to Chinchilla scaling law) for better performance with fewer parameters.[56]\\nGPT-4   March 2023  OpenAI  Unknown[f] (According to rumors: 1760)[59]  Unknown Unknown proprietary Available for ChatGPT Plus users and used in several products.\\nChameleon   June 2024   Meta AI 34[60]  4.4 trillion      \\nCerebras-GPT    March 2023  Cerebras    13[61]      270[27] Apache 2.0  Trained with Chinchilla formula.\\nFalcon  March 2023  Technology Innovation Institute 40[62]  1 trillion tokens, from RefinedWeb (filtered web text corpus)[63] plus some \\\"curated corpora\\\".[64]  2800[57]    Apache 2.0[65]\\nBloombergGPT    March 2023  Bloomberg L.P.  50  363 billion token dataset based on Bloomberg's data sources, plus 345 billion tokens from general purpose datasets[66]      Proprietary Trained on financial data from proprietary sources, for financial tasks.\\nPanGu-Σ March 2023  Huawei  1085    329 billion tokens[67]      Proprietary \\nOpenAssistant[68]   March 2023  LAION   17  1.5 trillion tokens     Apache 2.0  Trained on crowdsourced open data\\nJurassic-2[69]  March 2023  AI21 Labs   Unknown Unknown     Proprietary Multilingual[70]\\nPaLM 2 (Pathways Language Model 2)  May 2023    Google  340[71] 3.6 trillion tokens[71] 85,000[57]  Proprietary Was used in Bard chatbot.[72]\\nLlama 2 July 2023   Meta AI 70[73]  2 trillion tokens[73]   21,000  Llama 2 license 1.7 million A100-hours.[74]\\nClaude 2    July 2023   Anthropic   Unknown Unknown Unknown Proprietary Used in Claude chatbot.[75]\\nGranite 13b July 2023   IBM Unknown Unknown Unknown Proprietary Used in IBM Watsonx.[76]\\nMistral 7B  September 2023  Mistral AI  7.3[77] Unknown     Apache 2.0\\nClaude 2.1  November 2023   Anthropic   Unknown Unknown Unknown Proprietary Used in Claude chatbot. Has a context window of 200,000 tokens, or ~500 pages.[78]\\nGrok-1[79]  November 2023   xAI 314 Unknown Unknown Apache 2.0  Used in Grok chatbot. Grok-1 has a context length of 8,192 tokens and has access to X (Twitter).[80]\\nGemini 1.0  December 2023   Google DeepMind Unknown Unknown Unknown Proprietary Multimodal model, comes in three sizes. Used in the chatbot of the same name.[81]\\nMixtral 8x7B    December 2023   Mistral AI  46.7    Unknown Unknown Apache 2.0  Outperforms GPT-3.5 and Llama 2 70B on many benchmarks.[82] Mixture of experts model, with 12.9 billion parameters activated per token.[83]\\nMixtral 8x22B   April 2024  Mistral AI  141 Unknown Unknown Apache 2.0  [84]\\nPhi-2   December 2023   Microsoft   2.7 1.4T tokens 419[85] MIT Trained on real and synthetic \\\"textbook-quality\\\" data, for 14 days on 96 A100 GPUs.[85]\\nGemini 1.5  February 2024   Google DeepMind Unknown Unknown Unknown Proprietary Multimodal model, based on a Mixture-of-Experts (MoE) architecture. Context window above 1 million tokens.[86]\\nGemini Ultra    February 2024   Google DeepMind Unknown Unknown Unknown   \\nGemma   February 2024   Google DeepMind 7   6T tokens   Unknown Gemma Terms of Use[87]\\nClaude 3    March 2024  Anthropic   Unknown Unknown Unknown Proprietary Includes three models, Haiku, Sonnet, and Opus.[88]\\nNova    October 2024    Rubik's AI  Unknown Unknown Unknown Proprietary Includes three models, Nova-Instant, Nova-Air, and Nova-Pro.\\nDBRX    March 2024  Databricks and Mosaic ML    136 12T Tokens      Databricks Open Model License   Training cost 10 million USD.\\nFugaku-LLM  May 2024    Fujitsu, Tokyo Institute of Technology, etc.    13  380B Tokens         The largest model ever trained on CPU-only, on the Fugaku.[89]\\nPhi-3   April 2024  Microsoft   14[90]  4.8T Tokens     MIT Microsoft markets them as \\\"small language model\\\".[91]\\nGranite Code Models May 2024    IBM Unknown Unknown Unknown Apache 2.0\\nQwen2   June 2024   Alibaba Cloud   72[92]  3T Tokens           Multiple sizes, the smallest being 0.5B.\\nNemotron-4  June 2024   Nvidia  340 9T Tokens   200,000 NVIDIA Open Model License   Trained for 1 epoch. Trained on 6144 H100 GPUs between December 2023 and May 2024.[93][94]\\nLlama 3.1   July 2024   Meta AI 405 15.6T tokens    440,000 Llama 3 license 405B version took 31 million hours on H100-80GB, at 3.8E25 FLOPs.[95][96]\\nDeepSeek V3 December 2024   DeepSeek    671 14.8T tokens    440,00  DeepSeek License    2.788M hours on H800 GPUs.[97]\\nAmazon Nova December 2024   Amazon  Unknown Unknown Unknown Proprietary Includes three models, Nova Micro, Nova Lite, and Nova Pro[98]\\nSee also[edit]\\nList of chatbots\\nNotes[edit]\\n^ This is the date that documentation describing the model's architecture was first released.\\n^ In many cases, researchers release or report on multiple versions of a model having different sizes. In these cases, the size of the largest model is listed here.\\n^ This is the license of the pre-trained model weights. In almost all cases the training code itself is open-source or can be easily replicated.\\n^ The smaller models including 66B are publicly available, while the 175B model is available on request.\\n^ Facebook's license and distribution scheme restricted access to approved researchers, but the model weights were leaked and became widely available.\\n^ As stated in Technical report: \\\"Given both the competitive landscape and the safety implications of large-scale models like GPT-4, this report contains no further details about the architecture (including model size), hardware, training compute, dataset construction, training method ...\\\"[58]\\nReferences[edit]\\n^ \\\"Improving language understanding with unsupervised learning\\\". openai.com. June 11, 2018. Archived from the original on 2023-03-18. Retrieved 2023-03-18.\\n^ \\\"finetune-transformer-lm\\\". GitHub. Archived from the original on 19 May 2023. Retrieved 2 January 2024.\\n^ a b Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (11 October 2018). \\\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\\". arXiv:1810.04805v2 [cs.CL].\\n^ Prickett, Nicole Hemsoth (2021-08-24). \\\"Cerebras Shifts Architecture To Meet Massive AI/ML Models\\\". The Next Platform. Archived from the original on 2023-06-20. Retrieved 2023-06-20.\\n^ \\\"BERT\\\". March 13, 2023. Archived from the original on January 13, 2021. Retrieved March 13, 2023 – via GitHub.\\n^ Manning, Christopher D. (2022). \\\"Human Language Understanding & Reasoning\\\". Daedalus. 151 (2): 127–138. doi:10.1162/daed_a_01905. S2CID 248377870. Archived from the original on 2023-11-17. Retrieved 2023-03-09.\\n^ Patel, Ajay; Li, Bryan; Rasooli, Mohammad Sadegh; Constant, Noah; Raffel, Colin; Callison-Burch, Chris (2022). \\\"Bidirectional Language Models Are Also Few-shot Learners\\\". arXiv:2209.14500 [cs.LG].\\n^ Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (11 October 2018). \\\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\\". arXiv:1810.04805v2 [cs.CL].\\n^ a b Raffel, Colin; Shazeer, Noam; Roberts, Adam; Lee, Katherine; Narang, Sharan; Matena, Michael; Zhou, Yanqi; Li, Wei; Liu, Peter J. (2020). \\\"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\\". Journal of Machine Learning Research. 21 (140): 1–67. arXiv:1910.10683. ISSN 1533-7928.\\n^ google-research/text-to-text-transfer-transformer, Google Research, 2024-04-02, archived from the original on 2024-03-29, retrieved 2024-04-04\\n^ \\\"Imagen: Text-to-Image Diffusion Models\\\". imagen.research.google. Archived from the original on 2024-03-27. Retrieved 2024-04-04.\\n^ \\\"Pretrained models — transformers 2.0.0 documentation\\\". huggingface.co. Archived from the original on 2024-08-05. Retrieved 2024-08-05.\\n^ \\\"xlnet\\\". GitHub. Archived from the original on 2 January 2024. Retrieved 2 January 2024.\\n^ Yang, Zhilin; Dai, Zihang; Yang, Yiming; Carbonell, Jaime; Salakhutdinov, Ruslan; Le, Quoc V. (2 January 2020). \\\"XLNet: Generalized Autoregressive Pretraining for Language Understanding\\\". arXiv:1906.08237 [cs.CL].\\n^ \\\"GPT-2: 1.5B Release\\\". OpenAI. 2019-11-05. Archived from the original on 2019-11-14. Retrieved 2019-11-14.\\n^ \\\"Better language models and their implications\\\". openai.com. Archived from the original on 2023-03-16. Retrieved 2023-03-13.\\n^ a b \\\"OpenAI's GPT-3 Language Model: A Technical Overview\\\". lambdalabs.com. 3 June 2020. Archived from the original on 27 March 2023. Retrieved 13 March 2023.\\n^ a b \\\"openai-community/gpt2-xl · Hugging Face\\\". huggingface.co. Archived from the original on 2024-07-24. Retrieved 2024-07-24.\\n^ \\\"gpt-2\\\". GitHub. Archived from the original on 11 March 2023. Retrieved 13 March 2023.\\n^ Wiggers, Kyle (28 April 2022). \\\"The emerging types of language models and why they matter\\\". TechCrunch. Archived from the original on 16 March 2023. Retrieved 9 March 2023.\\n^ Table D.1 in Brown, Tom B.; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Gretchen; Henighan, Tom; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel M.; Wu, Jeffrey; Winter, Clemens; Hesse, Christopher; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Scott; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (May 28, 2020). \\\"Language Models are Few-Shot Learners\\\". arXiv:2005.14165v4 [cs.CL].\\n^ \\\"ChatGPT: Optimizing Language Models for Dialogue\\\". OpenAI. 2022-11-30. Archived from the original on 2022-11-30. Retrieved 2023-01-13.\\n^ \\\"GPT Neo\\\". March 15, 2023. Archived from the original on March 12, 2023. Retrieved March 12, 2023 – via GitHub.\\n^ a b c Gao, Leo; Biderman, Stella; Black, Sid; Golding, Laurence; Hoppe, Travis; Foster, Charles; Phang, Jason; He, Horace; Thite, Anish; Nabeshima, Noa; Presser, Shawn; Leahy, Connor (31 December 2020). \\\"The Pile: An 800GB Dataset of Diverse Text for Language Modeling\\\". arXiv:2101.00027 [cs.CL].\\n^ a b Iyer, Abhishek (15 May 2021). \\\"GPT-3's free alternative GPT-Neo is something to be excited about\\\". VentureBeat. Archived from the original on 9 March 2023. Retrieved 13 March 2023.\\n^ \\\"GPT-J-6B: An Introduction to the Largest Open Source GPT Model | Forefront\\\". www.forefront.ai. Archived from the original on 2023-03-09. Retrieved 2023-02-28.\\n^ a b c d Dey, Nolan; Gosal, Gurpreet; Zhiming; Chen; Khachane, Hemant; Marshall, William; Pathria, Ribhu; Tom, Marvin; Hestness, Joel (2023-04-01). \\\"Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster\\\". arXiv:2304.03208 [cs.LG].\\n^ Alvi, Ali; Kharya, Paresh (11 October 2021). \\\"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World's Largest and Most Powerful Generative Language Model\\\". Microsoft Research. Archived from the original on 13 March 2023. Retrieved 13 March 2023.\\n^ a b Smith, Shaden; Patwary, Mostofa; Norick, Brandon; LeGresley, Patrick; Rajbhandari, Samyam; Casper, Jared; Liu, Zhun; Prabhumoye, Shrimai; Zerveas, George; Korthikanti, Vijay; Zhang, Elton; Child, Rewon; Aminabadi, Reza Yazdani; Bernauer, Julie; Song, Xia (2022-02-04). \\\"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model\\\". arXiv:2201.11990 [cs.CL].\\n^ a b Rajbhandari, Samyam; Li, Conglong; Yao, Zhewei; Zhang, Minjia; Aminabadi, Reza Yazdani; Awan, Ammar Ahmad; Rasley, Jeff; He, Yuxiong (2022-07-21), DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale, arXiv:2201.05596\\n^ Wang, Shuohuan; Sun, Yu; Xiang, Yang; Wu, Zhihua; Ding, Siyu; Gong, Weibao; Feng, Shikun; Shang, Junyuan; Zhao, Yanbin; Pang, Chao; Liu, Jiaxiang; Chen, Xuyi; Lu, Yuxiang; Liu, Weixin; Wang, Xi; Bai, Yangfan; Chen, Qiuliang; Zhao, Li; Li, Shiyong; Sun, Peng; Yu, Dianhai; Ma, Yanjun; Tian, Hao; Wu, Hua; Wu, Tian; Zeng, Wei; Li, Ge; Gao, Wen; Wang, Haifeng (December 23, 2021). \\\"ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation\\\". arXiv:2112.12731 [cs.CL].\\n^ \\\"Product\\\". Anthropic. Archived from the original on 16 March 2023. Retrieved 14 March 2023.\\n^ a b Askell, Amanda; Bai, Yuntao; Chen, Anna; et al. (9 December 2021). \\\"A General Language Assistant as a Laboratory for Alignment\\\". arXiv:2112.00861 [cs.CL].\\n^ Bai, Yuntao; Kadavath, Saurav; Kundu, Sandipan; et al. (15 December 2022). \\\"Constitutional AI: Harmlessness from AI Feedback\\\". arXiv:2212.08073 [cs.CL].\\n^ a b c Dai, Andrew M; Du, Nan (December 9, 2021). \\\"More Efficient In-Context Learning with GLaM\\\". ai.googleblog.com. Archived from the original on 2023-03-12. Retrieved 2023-03-09.\\n^ \\\"Language modelling at scale: Gopher, ethical considerations, and retrieval\\\". www.deepmind.com. 8 December 2021. Archived from the original on 20 March 2023. Retrieved 20 March 2023.\\n^ a b c Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; et al. (29 March 2022). \\\"Training Compute-Optimal Large Language Models\\\". arXiv:2203.15556 [cs.CL].\\n^ a b c d Table 20 and page 66 of PaLM: Scaling Language Modeling with Pathways Archived 2023-06-10 at the Wayback Machine\\n^ a b Cheng, Heng-Tze; Thoppilan, Romal (January 21, 2022). \\\"LaMDA: Towards Safe, Grounded, and High-Quality Dialog Models for Everything\\\". ai.googleblog.com. Archived from the original on 2022-03-25. Retrieved 2023-03-09.\\n^ Thoppilan, Romal; De Freitas, Daniel; Hall, Jamie; Shazeer, Noam; Kulshreshtha, Apoorv; Cheng, Heng-Tze; Jin, Alicia; Bos, Taylor; Baker, Leslie; Du, Yu; Li, YaGuang; Lee, Hongrae; Zheng, Huaixiu Steven; Ghafouri, Amin; Menegali, Marcelo (2022-01-01). \\\"LaMDA: Language Models for Dialog Applications\\\". arXiv:2201.08239 [cs.CL].\\n^ Black, Sidney; Biderman, Stella; Hallahan, Eric; et al. (2022-05-01). GPT-NeoX-20B: An Open-Source Autoregressive Language Model. Proceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models. Vol. Proceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models. pp. 95–136. Archived from the original on 2022-12-10. Retrieved 2022-12-19.\\n^ a b c Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; Sifre, Laurent (12 April 2022). \\\"An empirical analysis of compute-optimal large language model training\\\". Deepmind Blog. Archived from the original on 13 April 2022. Retrieved 9 March 2023.\\n^ Narang, Sharan; Chowdhery, Aakanksha (April 4, 2022). \\\"Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance\\\". ai.googleblog.com. Archived from the original on 2022-04-04. Retrieved 2023-03-09.\\n^ Susan Zhang; Mona Diab; Luke Zettlemoyer. \\\"Democratizing access to large-scale language models with OPT-175B\\\". ai.facebook.com. Archived from the original on 2023-03-12. Retrieved 2023-03-12.\\n^ Zhang, Susan; Roller, Stephen; Goyal, Naman; Artetxe, Mikel; Chen, Moya; Chen, Shuohui; Dewan, Christopher; Diab, Mona; Li, Xian; Lin, Xi Victoria; Mihaylov, Todor; Ott, Myle; Shleifer, Sam; Shuster, Kurt; Simig, Daniel; Koura, Punit Singh; Sridhar, Anjali; Wang, Tianlu; Zettlemoyer, Luke (21 June 2022). \\\"OPT: Open Pre-trained Transformer Language Models\\\". arXiv:2205.01068 [cs.CL].\\n^ \\\"metaseq/projects/OPT/chronicles at main · facebookresearch/metaseq\\\". GitHub. Retrieved 2024-10-18.\\n^ a b Khrushchev, Mikhail; Vasilev, Ruslan; Petrov, Alexey; Zinov, Nikolay (2022-06-22), YaLM 100B, archived from the original on 2023-06-16, retrieved 2023-03-18\\n^ a b Lewkowycz, Aitor; Andreassen, Anders; Dohan, David; Dyer, Ethan; Michalewski, Henryk; Ramasesh, Vinay; Slone, Ambrose; Anil, Cem; Schlag, Imanol; Gutman-Solo, Theo; Wu, Yuhuai; Neyshabur, Behnam; Gur-Ari, Guy; Misra, Vedant (30 June 2022). \\\"Solving Quantitative Reasoning Problems with Language Models\\\". arXiv:2206.14858 [cs.CL].\\n^ \\\"Minerva: Solving Quantitative Reasoning Problems with Language Models\\\". ai.googleblog.com. 30 June 2022. Retrieved 20 March 2023.\\n^ Ananthaswamy, Anil (8 March 2023). \\\"In AI, is bigger always better?\\\". Nature. 615 (7951): 202–205. Bibcode:2023Natur.615..202A. doi:10.1038/d41586-023-00641-w. PMID 36890378. S2CID 257380916. Archived from the original on 16 March 2023. Retrieved 9 March 2023.\\n^ \\\"bigscience/bloom · Hugging Face\\\". huggingface.co. Archived from the original on 2023-04-12. Retrieved 2023-03-13.\\n^ Taylor, Ross; Kardas, Marcin; Cucurull, Guillem; Scialom, Thomas; Hartshorn, Anthony; Saravia, Elvis; Poulton, Andrew; Kerkez, Viktor; Stojnic, Robert (16 November 2022). \\\"Galactica: A Large Language Model for Science\\\". arXiv:2211.09085 [cs.CL].\\n^ \\\"20B-parameter Alexa model sets new marks in few-shot learning\\\". Amazon Science. 2 August 2022. Archived from the original on 15 March 2023. Retrieved 12 March 2023.\\n^ Soltan, Saleh; Ananthakrishnan, Shankar; FitzGerald, Jack; et al. (3 August 2022). \\\"AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model\\\". arXiv:2208.01448 [cs.CL].\\n^ \\\"AlexaTM 20B is now available in Amazon SageMaker JumpStart | AWS Machine Learning Blog\\\". aws.amazon.com. 17 November 2022. Archived from the original on 13 March 2023. Retrieved 13 March 2023.\\n^ a b c \\\"Introducing LLaMA: A foundational, 65-billion-parameter large language model\\\". Meta AI. 24 February 2023. Archived from the original on 3 March 2023. Retrieved 9 March 2023.\\n^ a b c \\\"The Falcon has landed in the Hugging Face ecosystem\\\". huggingface.co. Archived from the original on 2023-06-20. Retrieved 2023-06-20.\\n^ \\\"GPT-4 Technical Report\\\" (PDF). OpenAI. 2023. Archived (PDF) from the original on March 14, 2023. Retrieved March 14, 2023.\\n^ Schreiner, Maximilian (2023-07-11). \\\"GPT-4 architecture, datasets, costs and more leaked\\\". THE DECODER. Archived from the original on 2023-07-12. Retrieved 2024-07-26.\\n^ Dickson, Ben (22 May 2024). \\\"Meta introduces Chameleon, a state-of-the-art multimodal model\\\". VentureBeat.\\n^ Dey, Nolan (March 28, 2023). \\\"Cerebras-GPT: A Family of Open, Compute-efficient, Large Language Models\\\". Cerebras. Archived from the original on March 28, 2023. Retrieved March 28, 2023.\\n^ \\\"Abu Dhabi-based TII launches its own version of ChatGPT\\\". tii.ae. Archived from the original on 2023-04-03. Retrieved 2023-04-03.\\n^ Penedo, Guilherme; Malartic, Quentin; Hesslow, Daniel; Cojocaru, Ruxandra; Cappelli, Alessandro; Alobeidli, Hamza; Pannier, Baptiste; Almazrouei, Ebtesam; Launay, Julien (2023-06-01). \\\"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only\\\". arXiv:2306.01116 [cs.CL].\\n^ \\\"tiiuae/falcon-40b · Hugging Face\\\". huggingface.co. 2023-06-09. Retrieved 2023-06-20.\\n^ UAE's Falcon 40B, World's Top-Ranked AI Model from Technology Innovation Institute, is Now Royalty-Free Archived 2024-02-08 at the Wayback Machine, 31 May 2023\\n^ Wu, Shijie; Irsoy, Ozan; Lu, Steven; Dabravolski, Vadim; Dredze, Mark; Gehrmann, Sebastian; Kambadur, Prabhanjan; Rosenberg, David; Mann, Gideon (March 30, 2023). \\\"BloombergGPT: A Large Language Model for Finance\\\". arXiv:2303.17564 [cs.LG].\\n^ Ren, Xiaozhe; Zhou, Pingyi; Meng, Xinfan; Huang, Xinjing; Wang, Yadao; Wang, Weichao; Li, Pengfei; Zhang, Xiaoda; Podolskiy, Alexander; Arshinov, Grigory; Bout, Andrey; Piontkovskaya, Irina; Wei, Jiansheng; Jiang, Xin; Su, Teng; Liu, Qun; Yao, Jun (March 19, 2023). \\\"PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing\\\". arXiv:2303.10845 [cs.CL].\\n^ Köpf, Andreas; Kilcher, Yannic; von Rütte, Dimitri; Anagnostidis, Sotiris; Tam, Zhi-Rui; Stevens, Keith; Barhoum, Abdullah; Duc, Nguyen Minh; Stanley, Oliver; Nagyfi, Richárd; ES, Shahul; Suri, Sameer; Glushkov, David; Dantuluri, Arnav; Maguire, Andrew (2023-04-14). \\\"OpenAssistant Conversations – Democratizing Large Language Model Alignment\\\". arXiv:2304.07327 [cs.CL].\\n^ Wrobel, Sharon. \\\"Tel Aviv startup rolls out new advanced AI language model to rival OpenAI\\\". www.timesofisrael.com. Archived from the original on 2023-07-24. Retrieved 2023-07-24.\\n^ Wiggers, Kyle (2023-04-13). \\\"With Bedrock, Amazon enters the generative AI race\\\". TechCrunch. Archived from the original on 2023-07-24. Retrieved 2023-07-24.\\n^ a b Elias, Jennifer (16 May 2023). \\\"Google's newest A.I. model uses nearly five times more text data for training than its predecessor\\\". CNBC. Archived from the original on 16 May 2023. Retrieved 18 May 2023.\\n^ \\\"Introducing PaLM 2\\\". Google. May 10, 2023. Archived from the original on May 18, 2023. Retrieved May 18, 2023.\\n^ a b \\\"Introducing Llama 2: The Next Generation of Our Open Source Large Language Model\\\". Meta AI. 2023. Archived from the original on 2024-01-05. Retrieved 2023-07-19.\\n^ \\\"llama/MODEL_CARD.md at main · meta-llama/llama\\\". GitHub. Archived from the original on 2024-05-28. Retrieved 2024-05-28.\\n^ \\\"Claude 2\\\". anthropic.com. Archived from the original on 15 December 2023. Retrieved 12 December 2023.\\n^ Nirmal, Dinesh (2023-09-07). \\\"Building AI for business: IBM's Granite foundation models\\\". IBM Blog. Archived from the original on 2024-07-22. Retrieved 2024-08-11.\\n^ \\\"Announcing Mistral 7B\\\". Mistral. 2023. Archived from the original on 2024-01-06. Retrieved 2023-10-06.\\n^ \\\"Introducing Claude 2.1\\\". anthropic.com. Archived from the original on 15 December 2023. Retrieved 12 December 2023.\\n^ xai-org/grok-1, xai-org, 2024-03-19, archived from the original on 2024-05-28, retrieved 2024-03-19\\n^ \\\"Grok-1 model card\\\". x.ai. Retrieved 12 December 2023.\\n^ \\\"Gemini – Google DeepMind\\\". deepmind.google. Archived from the original on 8 December 2023. Retrieved 12 December 2023.\\n^ Franzen, Carl (11 December 2023). \\\"Mistral shocks AI community as latest open source model eclipses GPT-3.5 performance\\\". VentureBeat. Archived from the original on 11 December 2023. Retrieved 12 December 2023.\\n^ \\\"Mixtral of experts\\\". mistral.ai. 11 December 2023. Archived from the original on 13 February 2024. Retrieved 12 December 2023.\\n^ AI, Mistral (2024-04-17). \\\"Cheaper, Better, Faster, Stronger\\\". mistral.ai. Archived from the original on 2024-05-05. Retrieved 2024-05-05.\\n^ a b Hughes, Alyssa (12 December 2023). \\\"Phi-2: The surprising power of small language models\\\". Microsoft Research. Archived from the original on 12 December 2023. Retrieved 13 December 2023.\\n^ \\\"Our next-generation model: Gemini 1.5\\\". Google. 15 February 2024. Archived from the original on 16 February 2024. Retrieved 16 February 2024. This means 1.5 Pro can process vast amounts of information in one go — including 1 hour of video, 11 hours of audio, codebases with over 30,000 lines of code or over 700,000 words. In our research, we've also successfully tested up to 10 million tokens.\\n^ \\\"Gemma\\\" – via GitHub.\\n^ \\\"Introducing the next generation of Claude\\\". www.anthropic.com. Archived from the original on 2024-03-04. Retrieved 2024-03-04.\\n^ \\\"Fugaku-LLM/Fugaku-LLM-13B · Hugging Face\\\". huggingface.co. Archived from the original on 2024-05-17. Retrieved 2024-05-17.\\n^ \\\"Phi-3\\\". azure.microsoft.com. 23 April 2024. Archived from the original on 2024-04-27. Retrieved 2024-04-28.\\n^ \\\"Phi-3 Model Documentation\\\". huggingface.co. Archived from the original on 2024-05-13. Retrieved 2024-04-28.\\n^ \\\"Qwen2\\\". GitHub. Archived from the original on 2024-06-17. Retrieved 2024-06-17.\\n^ \\\"nvidia/Nemotron-4-340B-Base · Hugging Face\\\". huggingface.co. 2024-06-14. Archived from the original on 2024-06-15. Retrieved 2024-06-15.\\n^ \\\"Nemotron-4 340B | Research\\\". research.nvidia.com. Archived from the original on 2024-06-15. Retrieved 2024-06-15.\\n^ \\\"The Llama 3 Herd of Models\\\" (July 23, 2024) Llama Team, AI @ Meta\\n^ \\\"llama-models/models/llama3_1/MODEL_CARD.md at main · meta-llama/llama-models\\\". GitHub. Archived from the original on 2024-07-23. Retrieved 2024-07-23.\\n^ deepseek-ai/DeepSeek-V3, DeepSeek, 2024-12-26, retrieved 2024-12-26\\n^ Amazon Nova Micro, Lite, and Pro - AWS AI Service Cards3, Amazon, 2024-12-27, retrieved 2024-12-27\\nvte\\nNatural language processing\\nGeneral terms \\nAI-completeBag-of-wordsn-gram BigramTrigramComputational linguisticsNatural language understandingStop wordsText processing\\nText analysis \\nArgument miningCollocation extractionConcept miningCoreference resolutionDeep linguistic processingDistant readingInformation extractionNamed-entity recognitionOntology learningParsing Semantic parsingSyntactic parsingPart-of-speech taggingSemantic analysisSemantic role labelingSemantic decompositionSemantic similaritySentiment analysis\\nTerminology extractionText miningTextual entailmentTruecasingWord-sense disambiguationWord-sense induction\\nText segmentation \\nCompound-term processingLemmatisationLexical analysisText chunkingStemmingSentence segmentationWord segmentation\\nAutomatic summarization \\nMulti-document summarizationSentence extractionText simplification\\nMachine translation \\nComputer-assistedExample-basedRule-basedStatisticalTransfer-basedNeural\\nDistributional semantics models \\nBERTDocument-term matrixExplicit semantic analysisfastTextGloVeLanguage model (large)Latent semantic analysisSeq2seqWord embeddingWord2vec\\nLanguage resources,\\ndatasets and corpora  \\nTypes and\\nstandards \\nCorpus linguisticsLexical resourceLinguistic Linked Open DataMachine-readable dictionaryParallel textPropBankSemantic networkSimple Knowledge Organization SystemSpeech corpusText corpusThesaurus (information retrieval)TreebankUniversal Dependencies\\nData  \\nBabelNetBank of EnglishDBpediaFrameNetGoogle Ngram ViewerUBYWordNetWikidata\\nAutomatic identification\\nand data capture  \\nSpeech recognitionSpeech segmentationSpeech synthesisNatural language generationOptical character recognition\\nTopic model \\nDocument classificationLatent Dirichlet allocationPachinko allocation\\nComputer-assisted\\nreviewing \\nAutomated essay scoringConcordancerGrammar checkerPredictive textPronunciation assessmentSpell checker\\nNatural language\\nuser interface\\nChatbotInteractive fiction (c.f. Syntax guessing)Question answeringVirtual assistantVoice user interface\\nRelated \\nFormal semanticsHallucinationNatural Language ToolkitspaCy\\nPortal:\\n Language\\nCategory: Software comparisons\\nThis page was last edited on 28 December 2024, at 21:13 (UTC).\\nText is available under the Creative Commons Attribution-ShareAlike 4.0 License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\\nPrivacy policy\\nAbout Wikipedia\\nDisclaimers\\nContact Wikipedia\\nCode of Conduct\\nDevelopers\\nStatistics\\nCookie statement\\nMobile view\"}, {\"url\": \"https://aibusiness.com/nlp/language-models\", \"title\": \"Language models recent news | AI Business\", \"content\": \"Language models recent news | AI Business AI Policy Recent in Responsible AI Generative AI Recent in Generative AI Language models are a type of artificial intelligence (AI) that are trained on massive amounts of text data. Generative AI Generative AI Generative AI Generative AI How Generative AI and Ambient IoT Can Make Products Talk How Generative AI and Ambient IoT Can Make Products Talk Generative AI Generative AI Most Read: OpenAI Releases DeepSeek Challenger Model; SoftBank, OpenAI Create Joint Venture to Advance AI in Japan Most Read: OpenAI Releases DeepSeek Challenger Model; SoftBank, OpenAI Create Joint Venture to Advance AI in Japan Generative AI Generative AI on the Edge: Implications for the Technology Industry Generative AI on the Edge: Implications for the Technology Industry\", \"score\": 0.7036111, \"raw_content\": \"Language models recent news | AI Business\\n\\nTechTarget and Informa Tech’s Digital Business Combine.TechTarget and Informa\\nTechTarget and Informa Tech’s Digital Business Combine.\\nTogether, we power an unparalleled network of 220+ online properties covering 10,000+ granular topics, serving an audience of 50+ million professionals with original, objective content from trusted sources. We help you gain critical insights and make more informed decisions across your business priorities.\\nIoT World TodayEventsPartner with usOmdia\\n\\nSTAY UPDATED\\n\\nSTAY UPDATED\\nML\\nRelated Topics\\n\\nDeep learning\\n\\nNeural networks\\n\\n\\nPredictive analytics\\n\\n\\nRecent in ML\\nSee All\\nthumbnailAutomation\\n7 Ways Emerging Technologies Power Super Bowl LIX7 Ways Emerging Technologies Power Super Bowl LIX\\nbyLiz Hughes\\nFeb 7, 2025\\n3 Min Read\\nThe Super Bowl trophy between helmets from the Kansas City Chiefs and the Philadelphia Eagles.ML\\nSuper Bowl AI-Predicted Winner: Philadelphia EaglesSuper Bowl AI-Predicted Winner: Philadelphia Eagles\\nbyChuck Martin\\nFeb 7, 2025\\n2 Min Read\\nNLP\\nRelated Topics\\n\\nLanguage models\\n\\nSpeech recognition\\n\\n\\nChatbots\\n\\n\\nRecent in NLP\\nSee All\\nA team of five people chatting in an officeHealth Care\\nAI and the Art of LeadershipAI and the Art of Leadership\\nbyChantel Cohen\\nFeb 6, 2025\\n5 Min Read\\nBookshelves filled with folders full of documentsComputer Vision\\nFocused AI Is the Future of Enterprise Document ProcessingFocused AI Is the Future of Enterprise Document Processing\\nbyBhavani Vangala\\nFeb 5, 2025\\n8 Min Read\\nData\\nRelated Topics\\n\\nData science\\n\\nData analytics\\n\\n\\nData management\\n\\nSynthetic data\\n\\nRecent in Data\\nSee All\\nA sleeping woman wearing a smart watch superimposed with images representing health trackingHealth Care\\nBringing AI to Bed With YouBringing AI to Bed With You\\nbyMikael Kågebäck\\nFeb 6, 2025\\n3 Min Read\\nQuarterback Patrick Mahomes #15 of the Kansas City Chiefs celebrates after defeating the Buffalo Bills during the AFC Championship game on Jan. 26.Data\\nSuper Bowl Ball Spotting Waits for New TechnologySuper Bowl Ball Spotting Waits for New Technology\\nbyJohn Yellig\\nFeb 6, 2025\\n3 Min Read\\nAutomation\\nRelated Topics\\n\\n\\nRobotic process automation\\n\\n\\nIntelligent automation\\n\\n\\nRecent in Automation\\nSee All\\nthumbnailAutomation\\n7 Ways Emerging Technologies Power Super Bowl LIX7 Ways Emerging Technologies Power Super Bowl LIX\\nbyLiz Hughes\\nFeb 7, 2025\\n3 Min Read\\nA white Waabi, Volvo self-driving truck on the roadAutomation\\nAI Company, Volvo Team to Develop Self-Driving TrucksAI Company, Volvo Team to Develop Self-Driving Trucks\\nbyGraham Hope\\nFeb 6, 2025\\n2 Min Read\\nVerticals\\nRelated Topics\\n\\nIT\\nRobotics\\nCloud Computing\\nCybersecurity\\nEdge Computing\\nMetaverse\\n\\nData Centers\\n\\n\\nIoT\\n\\nQuantum Computing\\nIndustrials / Manufacturing\\nConsumer Tech\\nHealth Care\\nFinance\\nEnergy\\n\\nRecent in Verticals\\nSee All\\nthumbnailAutomation\\n7 Ways Emerging Technologies Power Super Bowl LIX7 Ways Emerging Technologies Power Super Bowl LIX\\nbyLiz Hughes\\nFeb 7, 2025\\n3 Min Read\\nOpenAI CEO Sam AltmanGenerative AI\\nMost Read: OpenAI Releases DeepSeek Challenger Model; SoftBank, OpenAI Create Joint Venture to Advance AI in JapanMost Read: OpenAI Releases DeepSeek Challenger Model; SoftBank, OpenAI Create Joint Venture to Advance AI in Japan\\nbyBerenice Baker\\nFeb 7, 2025\\n3 Min Read\\nResponsible AI\\nRelated Topics\\n\\nAI Policy\\n\\nData Governance\\n\\n\\nExplainable AI\\n\\nAI Ethics\\n\\nRecent in Responsible AI\\nSee All\\nBuilding 10 on the campus of Massachusetts Institute of Technology in Cambridge, Massachusetts.Responsible AI\\nMIT Launches Generative AI Impact ConsortiumMIT Launches Generative AI Impact Consortium\\nbyHeidi Vella\\nFeb 6, 2025\\n2 Min Read\\nA hand pointing at a workflow diagram  Generative AI\\nOrchestrating AI Agents: The Key to Unlocking Enterprise Efficiency and GrowthOrchestrating AI Agents: The Key to Unlocking Enterprise Efficiency and Growth\\nbyDorit Zilbershot\\nFeb 4, 2025\\n6 Min Read\\nGenerative AI\\nRelated Topics\\n\\nFoundation Models\\n\\nRecent in Generative AI\\nSee All\\nOpenAI CEO Sam AltmanGenerative AI\\nMost Read: OpenAI Releases DeepSeek Challenger Model; SoftBank, OpenAI Create Joint Venture to Advance AI in JapanMost Read: OpenAI Releases DeepSeek Challenger Model; SoftBank, OpenAI Create Joint Venture to Advance AI in Japan\\nbyBerenice Baker\\nFeb 7, 2025\\n3 Min Read\\nThree executives stand around a table  Generative AI\\nFrom Hype to Mastery: The Path to AI MaturityFrom Hype to Mastery: The Path to AI Maturity\\nbyRobert Harrington\\nFeb 7, 2025\\n5 Min Read\\nMore\\nRelated Topics\\n\\nPodcasts\\nWebinars\\nEbooks\\nVideos\\nEvents\\n\\nWhite Papers\\n\\n\\nOmdia\\n\\nGenerate leads with us\\nAI Business TV London 2022\\nTech TV Austin 2022\\nAI Business TV New York 2022\\n\\nAI Business TV London 2023\\n\\nHome\\nNLP\\nLanguage models\\n\\nLanguage models\\nLanguage models are a type of artificial intelligence (AI) that are trained on massive amounts of text data. This allows them to generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way. In recent years, language models have become increasingly powerful and sophisticated. Get the latest information on LLM's here at AI Business\\nA team of five people chatting in an officeHealth Care\\nAI and the Art of LeadershipAI and the Art of Leadership4 tips for using AI alongside emotional intelligence to leverage efficiency and foster connection\\n\\nbyChantel Cohen, Founder and CEO of CWC Coaching & Therapy\\nFeb 6, 2025\\n1 Min Read\\nBookshelves filled with folders full of documentsComputer Vision\\nFocused AI Is the Future of Enterprise Document ProcessingFocused AI Is the Future of Enterprise Document Processing\\nbyBhavani Vangala\\nFeb 5, 2025\\n1 Min Read\\nthumbnailNLP\\nSoftBank, OpenAI Create Joint Venture to Advance AI in JapanSoftBank, OpenAI Create Joint Venture to Advance AI in Japan\\nbyLiz Hughes\\nFeb 3, 2025\\n1 Min Read\\nA small screen with the DeepSeek logo in front of a larger screen with the ChatGPT logoGenerative AI\\nOpenAI Takes Aim at DeepSeek With New, Free ModelOpenAI Takes Aim at DeepSeek With New, Free Model\\nbyYiannis Antoniou\\nFeb 3, 2025\\n1 Min Read\\nOpenAI CEO Sam AltmanGenerative AI\\nOpenAI Releases DeepSeek Challenger ModelOpenAI Releases DeepSeek Challenger Model\\nbyBerenice Baker\\nFeb 3, 2025\\n1 Min Read\\nThe White HouseNLP\\nOpenAI Launches ChatGPT Gov for US Government AgenciesOpenAI Launches ChatGPT Gov for US Government Agencies\\nbyBerenice Baker\\nJan 29, 2025\\n1 Min Read\\n\\nNLP\\nDeepSeek Hit by Cyberattack; Limits Registrations\\nDeepSeek Hit by Cyberattack; Limits Registrations\\nJan 28, 2025\\n|\\n1 Min Read\\n\\nbyLiz Hughes, Editor, IoT World Today and AI Business\\n\\nNLP\\nIndustry Weighs in on DeepSeek Launch\\nIndustry Weighs in on DeepSeek Launch\\nJan 28, 2025\\n|\\n1 Min Read\\n\\nbyBerenice Baker, Editor\\n\\nNLP\\nChinese Startup DeepSeek Launches; Competes With OpenAI\\nChinese Startup DeepSeek Launches; Competes With OpenAI\\nJan 27, 2025\\n|\\n1 Min Read\\n\\nbyLiz Hughes, Editor, IoT World Today and AI Business\\n\\nFinance\\nNative AI on Horizon for Finance, Accounting Teams\\nNative AI on Horizon for Finance, Accounting Teams\\nJan 27, 2025\\n|\\n1 Min Read\\n\\nbyCharis Thomas, Chris Tredwell\\n\\nGenerative AI\\nDo You Need a Personal AI Chatbot?\\nDo You Need a Personal AI Chatbot?\\nJan 23, 2025\\n|\\n1 Min Read\\n\\nbyZain Jaffer, CEO and founder, Zain Ventures\\n\\nAutomation\\nAI Set to Transform Legal Services in 2025\\nAI Set to Transform Legal Services in 2025\\nJan 22, 2025\\n|\\n1 Min Read\\n\\nbyPaul Gaskell, Chief technology officer, Avantia Law\\n\\nGenerative AI\\nArtificial General Intelligence: EY on the Short-Term Future\\nArtificial General Intelligence: EY on the Short-Term Future\\nJan 22, 2025\\n|\\n1 Min Read\\n\\nbyBerenice Baker, Editor\\n\\nLanguage models\\nAI Model Scaling Isn’t Over: It’s Entering a New Era\\nAI Model Scaling Isn’t Over: It’s Entering a New Era\\nJan 21, 2025\\n|\\n1 Min Read\\n\\nbyAkash Sharma, CEO, Vellum\\n\\nLanguage models\\nWhy Agnostic AI Is the Key To Cost-Efficient, Scalable AI Solutions\\nWhy Agnostic AI Is the Key To Cost-Efficient, Scalable AI Solutions\\nJan 16, 2025\\n|\\n1 Min Read\\n\\nbyKasia Borowska, Managing director at Brainpool AI\\n\\nLanguage models\\nFourth Industrial Revolution: How AI Agents Are Transforming the Future of Work\\nFourth Industrial Revolution: How AI Agents Are Transforming the Future of Work\\nJan 8, 2025\\n|\\n1 Min Read\\n\\nbyKarli Kalpala, Head of UK & Ireland and strategic transformation, Digital Workforce\\n\\nNLP\\nSamsung Harman’s AI Will Make Cars More Empathetic\\nSamsung Harman’s AI Will Make Cars More Empathetic\\nJan 8, 2025\\n|\\n1 Min Read\\n\\nbyGraham Hope, Contributing Writer\\n\\nResponsible AI\\nAI vs. AI: Technologies That Help Guard Against AI-Generated Scams\\nAI vs. AI: Technologies That Help Guard Against AI-Generated Scams\\nJan 2, 2025\\n|\\n1 Min Read\\n\\nbyJames Stokes, Head of enterprise, U.K. and Nordics at Infobip\\n\\nResponsible AI\\nA Sustainable AI Future Needs Community Data Protection\\nA Sustainable AI Future Needs Community Data Protection\\nDec 30, 2024\\n|\\n1 Min Read\\n\\nbyEllen Brandenberger, senior director of product innovation, Stack Overflow\\n\\nVerticals\\nAI Could Ease Record Holiday Travel Disruptions\\nAI Could Ease Record Holiday Travel Disruptions\\nDec 27, 2024\\n|\\n1 Min Read\\n\\nbyAnna Jaffe, CEO of Mobi.AI\\n\\nChatbots\\nSoftware Will Learn to Work With You in 2025\\nSoftware Will Learn to Work With You in 2025\\nDec 24, 2024\\n|\\n1 Min Read\\n\\nbyBurley Kawasaki, Global VP of product marketing and strategy at Creatio\\n\\nLanguage models\\nSandboxAQ Secures $300M to Drive Large Quantitative Model Innovation\\nSandboxAQ Secures $300M to Drive Large Quantitative Model Innovation\\nDec 23, 2024\\n|\\n1 Min Read\\n\\nbyBerenice Baker, Editor\\n\\nData Centers\\nMost Read: Meta to Open $10B AI Data Center in Louisiana; How IBM is Using AI to Disrupt Consultancy\\nMost Read: Meta to Open $10B AI Data Center in Louisiana; How IBM is Using AI to Disrupt Consultancy\\nDec 20, 2024\\n|\\n1 Min Read\\n\\nbyBerenice Baker, Editor\\n\\nNLP\\nAI-Santa Demonstrates New Conversational Video\\nAI-Santa Demonstrates New Conversational Video\\nDec 20, 2024\\n|\\n1 Min Read\\n\\nbyHeidi Vella, Contributing Writer\\n\\nNLP\\nDigitas’ Louis Vainquer on Large Language Models at AI Summit New York\\nDigitas’ Louis Vainquer on Large Language Models at AI Summit New York\\nDec 19, 2024\\n|\\n1 Min Read\\n\\nNLP\\nAmazon Launches New Generation of LLM Foundation Models\\nAmazon Launches New Generation of LLM Foundation Models\\nDec 16, 2024\\n|\\n1 Min Read\\n\\nbyHeidi Vella, Contributing Writer\\n\\nLanguage models\\nThe Role of Large Quantitative Models in Drug Discovery\\nThe Role of Large Quantitative Models in Drug Discovery\\nDec 12, 2024\\n|\\n1 Min Read\\n\\nbyBerenice Baker, Editor\\n\\nResponsible AI\\nLeaders Divided on Level of Governance Needed for Responsible AI\\nLeaders Divided on Level of Governance Needed for Responsible AI\\nDec 9, 2024\\n|\\n1 Min Read\\n\\nbyHeidi Vella, Contributing Writer\\n\\nLanguage models\\nMost Read: Agentic AI Set to Rise, With New Cybersecurity Risks: Gartner; Finance Operations Lean Heavily on AI: KPMG\\nMost Read: Agentic AI Set to Rise, With New Cybersecurity Risks: Gartner; Finance Operations Lean Heavily on AI: KPMG\\nDec 5, 2024\\n|\\n1 Min Read\\n\\nbyBerenice Baker, Editor\\n\\nGenerative AI\\nAI-Powered Holograms Converse at AWS re:Invent 2024\\nAI-Powered Holograms Converse at AWS re:Invent 2024\\nDec 4, 2024\\n|\\n1 Min Read\\n\\nbyBerenice Baker, Editor\\n\\nGenerative AI\\nHow Generative AI and Ambient IoT Can Make Products Talk\\nHow Generative AI and Ambient IoT Can Make Products Talk\\nDec 3, 2024\\n|\\n1 Min Read\\n\\nbyIlan Lifshitz, vice president of research and development at Wiliot\\n\\nAutomation\\nAgentic AI Set to Rise, With New Cybersecurity Risks: Gartner\\nAgentic AI Set to Rise, With New Cybersecurity Risks: Gartner\\nDec 2, 2024\\n|\\n1 Min Read\\n\\nbyHeidi Vella, Contributing Writer\\n\\nData\\nHow AI Can Lead to Operational Transformation in Smaller US Companies\\nHow AI Can Lead to Operational Transformation in Smaller US Companies\\nNov 25, 2024\\n|\\n1 Min Read\\n\\nbyMariano Jurich, Project and product manager at Making Sense\\n\\nNLP\\nXPeng Launches AI-Defined P7+ Electric SUV to Rival Tesla\\nXPeng Launches AI-Defined P7+ Electric SUV to Rival Tesla\\nNov 21, 2024\\n|\\n1 Min Read\\n\\nbyGraham Hope, Contributing Writer\\n\\nGenerative AI\\nUnlocking the True Potential of Mobile AI\\nUnlocking the True Potential of Mobile AI\\nNov 18, 2024\\n|\\n1 Min Read\\n\\nbyMohan Varthakavi, Vice president of software development, AI and edge, Couchbase\\n\\nPrevious\\n1\\n2\\n3\\n4\\n5\\n…\\n32\\nNext\\n\\nLatest News\\nthumbnailAutomation\\n7 Ways Emerging Technologies Power Super Bowl LIX7 Ways Emerging Technologies Power Super Bowl LIX\\nbyLiz Hughes\\nFeb 7, 2025\\n3 Min Read\\nOpenAI CEO Sam AltmanGenerative AI\\nMost Read: OpenAI Releases DeepSeek Challenger Model; SoftBank, OpenAI Create Joint Venture to Advance AI in JapanMost Read: OpenAI Releases DeepSeek Challenger Model; SoftBank, OpenAI Create Joint Venture to Advance AI in Japan\\nbyBerenice Baker\\nFeb 7, 2025\\n3 Min Read\\nThe Super Bowl trophy between helmets from the Kansas City Chiefs and the Philadelphia Eagles.ML\\nSuper Bowl AI-Predicted Winner: Philadelphia EaglesSuper Bowl AI-Predicted Winner: Philadelphia Eagles\\nbyChuck Martin\\nFeb 7, 2025\\n2 Min Read\\nSign Up for the Newsletter\\nThe most up-to-date AI news and insights delivered right to your inbox!\\nStay Updated!\\nTrending articles\\n\\nAutomation\\n7 Ways Emerging Technologies Power Super Bowl LIX\\n7 Ways Emerging Technologies Power Super Bowl LIX\\nFeb 7, 2025\\nOpenAI CEO Sam Altman\\nGenerative AI\\nMost Read: OpenAI Releases DeepSeek Challenger Model; SoftBank, OpenAI Create Joint Venture to Advance AI in Japan\\nMost Read: OpenAI Releases DeepSeek Challenger Model; SoftBank, OpenAI Create Joint Venture to Advance AI in Japan\\nFeb 7, 2025\\nThree executives stand around a table  \\nGenerative AI\\nFrom Hype to Mastery: The Path to AI Maturity\\nFrom Hype to Mastery: The Path to AI Maturity\\nFeb 7, 2025\\nRobots and a supervisor in a smart factory\\nEdge Computing\\nGenerative AI on the Edge: Implications for the Technology Industry\\nGenerative AI on the Edge: Implications for the Technology Industry\\nFeb 7, 2025\\nLatest podcasts\\n\\nUnlocking Networking Potential With Generative AIApr 17, 2024\\nEmbedding AI in the Enterprise With IBM's WatsonxApr 4, 2024\\nReshaping Customer Experiences with AIMar 7, 2024\\nGenerative AI Journeys with CDW UK's Chief TechnologistFeb 28, 2024\\n\\nSee all\\nSponsored Content\\n\\nSponsored by Google Cloud\\nChoosing Your First Generative AI Use Cases -------------------------------------------\\nTo get started with generative AI, first focus on areas that can improve human experiences with information.\\nRead More\\nSign Up for the Newsletter\\nThe most up-to-date AI news and insights delivered right to your inbox!\\nStay Updated!\\n\\nDiscover more\\nThe AI Summit SeriesApplied Intelligence Live!AI Research and ConsultingOur Solutions\\nWorking with us\\nAdvertise\\nCommunicate\\nContact usAbout us\\nJoin Us\\nSTAY UPDATED\\nFollow Us\\n\\n\\n\\nCopyright © 2025. This website is owned and operated by Informa TechTarget, part of a global network that informs, influences and connects the world’s technology buyers and sellers. All copyright resides with them. Informa PLC’s registered office is 5 Howick Place, London SW1P 1WG. Registered in England and Wales. TechTarget, Inc.’s registered office is 275 Grove St. Newton, MA 02466.\\nHome|About us|Contact|Cookie Policy|Terms of Use\"}, {\"url\": \"https://www.shakudo.io/blog/top-9-large-language-models\", \"title\": \"Top 9 Large Language Models as of Feburary 2025 - Shakudo\", \"content\": \"The latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. The latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. The latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data.\", \"score\": 0.6485337, \"raw_content\": \"Top 9 Large Language Models as of Feburary 2025 | Shakudo\\nLatest in Insights : When to Choose Deep Learning Over Machine Learning (And Vice Versa)\\n\\n\\nWhy SHakudo\\n\\n Data & AI OS Build your ideal data stack on one unified platform Learn more >\\nshakudo AI Applications\\n Text to SQL Workflow Automation Vector Database Reverse ETLSee all >\\nComponents\\nSolutions\\n\\nShakudo for Industries\\nAerospace\\nAutomotive & Transportation\\nClimate & Energy\\nFinancial Services\\nHealthcare & Life Sciences\\nHigher Education\\nManufacturing\\nReal Estate\\nRetail\\nSports\\nTechnology & Software\\nShakudo Use Cases\\nAutomate Custom Sustainability Report Population\\nChat with Enterprise Knowledge Base Using AI Assistants\\nGenerate Real-World Evidence for Healthcare Decisions\\nOptimize Ticket Pricing with Dynamic Demand Modeling\\nDetect Hidden Red Flags in Company Data\\nMonitor Market Sentiment Across Multiple Sources\\nSee all >\\nResources\\n\\n Case Studies Learn how leading companies leverage data & AI on Shakudo blog Read what's new at Shakudo and the data and AI world white papers Access in-depth reports and guides on data & AI solutions Docs Explore comprehensive guides on the Shakudo platform\\n  Case Study How CentralReach uses Shakudo to Cut Time-To-Deployment to Launch New AI- Powered Solutions\\n  Case Study How AI is Changing the Game for the Cleveland Cavaliers\\nCompany\\n\\n ABout Us Learn about our mission and values Careers Join us in building the next-gen data stack Partners Learn about the relationships that make it happen Contact us Have a question? We're here to help\\nAI WorkshopGet a Demo\\n\\n← Back to Blog\\nInsights\\nTop 9 Large Language Models as of Feburary 2025\\nAuthor(s):\\n\\nNo items found.\\nUpdated on:\\nFebruary 7, 2025\\n\\nTable of contents\\nExample H2\\nExample H3\\nMentioned Components\\nNo items found.\\n<>\\nGet the latest updates in Data & AI straight to your inboxWe’ll email you once per week—and never share your information.\\n🎉 Success! You're now signed up for the Shakudo newsletter.\\nOops! Something went wrong while submitting the form.\\nIntroduction\\nIf we had to choose one word to describe the rapid evolution of AI today, it would probably be something along the lines of explosive. As predicted by the Market Research Future report, the large language model (LLM) market in North America alone is expected to reach $105.5 billion by 2030. The exponential growth of AI tools combined with access to massive troves of text data has opened gates for better and more advanced content generation than we had ever hoped. Yet, such rapid expansion also makes it harder than ever to navigate and select the right tools among the diverse LLM models available.\\nThe goal of this post is to keep you, the AI enthusiast and professional, up-to-date with current trends and essential innovations in the field. Below, we highlighted the top 9 LLMs that we think are currently making waves in the industry, each with distinct capabilities and specialized strengths, excelling in areas such as natural language processing, code synthesis, few-shot learning, or scalability. While we believe there is no one-size-fits-all LLM for every use case, we hope that this list can help you identify the most current and well-suited LLM model that meets your business’s unique requirements.\\n1. GPT\\n\\nOur list kicks off with OpenAI's Generative Pre-trained Transformer (GPT) models, which have consistently exceeded their previous capabilities with each new release. Compared to its prior models, the latest ChatGPT-4o and ChatGPT-4o mini models offer significantly faster processing speeds and enhanced capabilities across text, voice, and vision.\\nThe latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. Both of these models are equipped with multimodal capabilities to handle images as well as audio data.\\nDespite having advanced conversational and reasoning capabilities, note that GPT is a proprietary model, meaning that the training data and parameters are kept confidential by OpenAI, and access to full functionality is restricted–a commercial license or subscription is often required to unlock the complete range of features. In this case, we recommend this model for businesses looking to adopt an LLM that excels in conversational dialogue, multi-step reasoning, efficient computation, and real-time interactions without the constraints of a budget.\\nFor companies who are curious to try out the proprietary models on the market before fully committing to one due to budget constraints or uncertainties about its long-term integration, Shakudo offers a compelling alternative. Our platform currently features a diverse selection of advanced LLMs with simplified deployment and scalability. With a simple subscription, you can access and assess the value of proprietary models, like GPT, before making a substantial investment.\\n2.DeepSeek\\n\\nDeepseek-R1 Benchmark. Source: deepseek.com\\nWith its latest R1 model, the Chinese AI company DeepSeek has once again set new benchmarks for innovation in the AI community. As of January 24th, the DeepSeek-R1 model is ranked fourth on Chatbot Arena, and top as the best open-source LM.\\nThe DeepSeek-R1 is a 671B parameter Mixture-of-Experts (MoE) model with 37B activated parameters per token, trained through large-scale reinforcement learning with a strong focus on reasoning capabilities. The model excels at understanding and handling long-form content and demonstrates superior performance in complex tasks such as mathematics and code generation. The model is approximately 30 times more cost-efficient than OpenAI-o1 and 5 times faster, offering groundbreaking performance at a fraction of the cost. Moreover, it has shown exceptional precision in tasks requiring complex pattern recognition, such as genomic data analysis, medical imaging, and large-scale scientific simulations.\\nDeepSeek-R1’s capabilities are transformative when it comes to integration with proprietary enterprise data such as PII and financial records. Leveraging retrieval-augmented generation (RAG), enterprises can connect the model to their internal data sources to enable highly personalized, context-aware interactions—all while maintaining stringent security and compliance standards. With Shakudo, you can streamline the deployment and integration of advanced AI models like DeepSeek by automating the setup, deployment, and management processes. This eliminates the need for businesses to invest in and maintain extensive computing infrastructure. By operating within your existing infrastructure, the platform ensures seamless integration, enhanced security, and optimal performance without requiring significant in-house resources or specialized expertise.\\n3. Qwen\\n\\n‍\\nAlibaba released Qwen2.5-Max earlier last month, the new model designed to deliver enhanced performance for large-scale natural language processing tasks. In instruct model evaluations, this new model outperforms DeepSeek V3 in benchmarks such as Arena-Hard, LiveBench, LiveCodeBench, and GPQA-Diamond, while also delivering strong performance in other assessments like MMLU-Pro.\\nQwen2.5-Max is pretrained on over 20 trillion tokens. While specific details about its parameter count and token window size are not publicly disclosed, Qwen2.5-Max is designed for low-latency, high-efficiency tasks, making it suitable for applications requiring quick, accurate responses with low latency. Its smaller size allows for deployment on devices with limited computational resources.\\nFor businesses and users looking for higher-performance models for natural language processing and AI-driven tasks, the Qwen 2.5 model is available on platforms like Hugging Face and ModelScope. This model boasts from 0.5 billion to 72 billion parameters, featuring context windows of up to 128,000 tokens, and is excellent for code generation, debugging, and automated forecasting.\\n4. LG AI\\n\\nsource\\nEXAONE 3.0 is a bilingual LLM developed by LG AI Research. The company released its latest model in December 2024 with excellent performance across various benchmarks and real-world applications.\\nWith 7.8 billion parameters, EXAONE 3.0 is capable of understanding and generating human-like text in multiple languages across complex domains, including coding, mathematics, patents, and chemistry. This model has also been optimized to reduce inference processing time by 56%, memory usage by 35%, and operating costs by 72%, ensuring that it remains cost-effective while maintaining high performance.\\nLG AI Research has open-sourced the 7.8B parameter instruction-tuned version of EXAONE 3.0 for non-commercial research purposes. This is a model we recommend to software companies or tech startups for generating Python code, assisting developers in troubleshooting, and creating APIs or other backend components.\\n5. LlaMA\\n\\nMeta is still leading the front with their state-of-the-art LlaMa models. The company released its latest LlaMA 3.3 model in December 2024, featuring multimodal capabilities that can process both text and image for in-depth analysis and response generation, such as interpreting charts, maps, or translating texts identified in an image.\\nLlaMA 3.3 improves on previous models with a longer context window of up to 128,000 tokens and an optimized transformer architecture. With a parameter of 70 billion, this model outperforms open-source and proprietary alternatives in areas such as multilingual dialogue, reasoning, and coding.\\nUnlike ChatGPT models, LlaMA 3 is open-source, giving users the flexibility to access and deploy freely on their cloud depending on the specific requirements of their infrastructure, security preferences, or customization needs. We recommend this model to businesses looking for advanced content generation and language understanding, such as those in customer service, education, marketing, and consumer markets. The openness of these models also allows for your greater control over the model’s performance, tuning, and integration into existing workflows.\\n6. Claude\\n\\nNext on our list is Claude, more specifically, the latest Claude 3.5 Sonnet model developed by Anthropic. We believe that Claude is arguably one of the most significant competitors to GPT since all of its current models–Claude 3 Haiku, Claude 3.5 Sonnet, and Claude 3 Opus–are designed with incredible contextual understanding capabilities that position themselves as the top conversational AI closely aligned with nuanced human interactions.\\nWhile the specific parameters of Claude 3.5 Sonnet remain undisclosed, the model boasts an impressive context window of 200,000 tokens, equivalent to approximately 150,000 words or 300 pages of text.\\nThe current Claude subscription service is credit-based, and the cost can go as high as $2,304/month for enterprise plans tailored to high-volume users. We recommend Claude to mature or mid-stage businesses looking not only to adopt an AI that facilitates human-like interactions but also to enhance their coding capabilities since the Claude 3.5 Sonnet model is currently reaching a 49.0% performance score on the SWE-bench Verified benchmark, placing it as third among all publicly available models, including reasoning models and systems specifically designed for agentic coding.\\n7. Mistral\\n\\nMistral's latest model – Mistral Small 3, a latency-optimized model was released under the Apache 2.0 license at the end of January. This 24-billion-parameter model is designed for low-latency, high-efficiency tasks. It processes approximately 150 tokens per second, making it over three times faster than Llama 3.3 70B on the same hardware.\\nThis new model is ideal for applications requiring quick, accurate responses with low latency, such as virtual assistants, real-time data processing, and on-device command and control. Its smaller size allows for deployment on devices with limited computational resources.\\nMistral Small 3 is currently open-source under the Apache 2.0 license. This means you can freely access and use the model for your own applications, provided you comply with the license terms. Since it is designed to be easily deployable, including on hardware with limited resources like a single GPU or even a MacBook with 32GB RAM, we'd recommend this to early-stage businesses looking to implement low-latency AI solutions without the need for extensive hardware infrastructure.\\n8. Gemini\\n\\nGemini is a family of closed-source LLM models developed by Google. The latest model—Gemini 2.0 Flash—operates at twice the speed of Gemini 1.5 Pro, offering substantial improvements in speed, reasoning, and multimodal processing capabilities.\\nWith that being said, Gemini remains a proprietary model; if your company deals with sensitive or confidential data regularly, you might be concerned about sending it to external servers due to security reasons. To address this concern, we recommend that you double-check vendor compliance regulations to ensure data privacy and security standards are met, such as adherence to GDPR, HIPAA, or other relevant data protection laws.\\nIf you’re looking for an open-source alternative that exhibits capabilities almost as good as Gemini, Google’s latest Gemma model, Gemma 2, offers three models available in 2 billion, 9 billion, and 27 billion parameters with a context window of 8,200. For businesses looking for a rather economic option, this is the optimal choice that interprets and understands messages with remarkable accuracy.\\n9. Command\\n\\nCommand R is a family of scalable models developed by Cohere with the goal of balancing high performance with strong accuracy, just like Claude. Both the Command R and Command R+ models offer APIs specifically optimized for Retrieval Augmented Generation (RAG). This means that these models can combine large-scale language generation with real-time information retrieval techniques for much more contextually aware outputs.\\nCurrently, the Command R+ model boasts 104 billion parameters and offers an industry-leading 128,000 token context window for enhanced long-form processing and multi-turn conversation capabilities.\\nOne of the perks of working with an open-source model is also to avoid vendor lock-in. Heavy reliance on a particular type of proprietary model may make it difficult for you to switch to alternative models when your business starts growing or the landscape changes. Cohere approaches this in a hybrid way, meaning that you can access and modify the model for personal usage but need a license for commercial use. In this case, we recommend this model for businesses that want flexibility in experimentation without a long-term commitment to a single vendor.\\n\\nWhitepaper\\nIntroduction\\nIf we had to choose one word to describe the rapid evolution of AI today, it would probably be something along the lines of explosive. As predicted by the Market Research Future report, the large language model (LLM) market in North America alone is expected to reach $105.5 billion by 2030. The exponential growth of AI tools combined with access to massive troves of text data has opened gates for better and more advanced content generation than we had ever hoped. Yet, such rapid expansion also makes it harder than ever to navigate and select the right tools among the diverse LLM models available.\\nThe goal of this post is to keep you, the AI enthusiast and professional, up-to-date with current trends and essential innovations in the field. Below, we highlighted the top 9 LLMs that we think are currently making waves in the industry, each with distinct capabilities and specialized strengths, excelling in areas such as natural language processing, code synthesis, few-shot learning, or scalability. While we believe there is no one-size-fits-all LLM for every use case, we hope that this list can help you identify the most current and well-suited LLM model that meets your business’s unique requirements.\\n1. GPT\\n\\nOur list kicks off with OpenAI's Generative Pre-trained Transformer (GPT) models, which have consistently exceeded their previous capabilities with each new release. Compared to its prior models, the latest ChatGPT-4o and ChatGPT-4o mini models offer significantly faster processing speeds and enhanced capabilities across text, voice, and vision.\\nThe latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. Both of these models are equipped with multimodal capabilities to handle images as well as audio data.\\nDespite having advanced conversational and reasoning capabilities, note that GPT is a proprietary model, meaning that the training data and parameters are kept confidential by OpenAI, and access to full functionality is restricted–a commercial license or subscription is often required to unlock the complete range of features. In this case, we recommend this model for businesses looking to adopt an LLM that excels in conversational dialogue, multi-step reasoning, efficient computation, and real-time interactions without the constraints of a budget.\\nFor companies who are curious to try out the proprietary models on the market before fully committing to one due to budget constraints or uncertainties about its long-term integration, Shakudo offers a compelling alternative. Our platform currently features a diverse selection of advanced LLMs with simplified deployment and scalability. With a simple subscription, you can access and assess the value of proprietary models, like GPT, before making a substantial investment.\\n2.DeepSeek\\n\\nDeepseek-R1 Benchmark. Source: deepseek.com\\nWith its latest R1 model, the Chinese AI company DeepSeek has once again set new benchmarks for innovation in the AI community. As of January 24th, the DeepSeek-R1 model is ranked fourth on Chatbot Arena, and top as the best open-source LM.\\nThe DeepSeek-R1 is a 671B parameter Mixture-of-Experts (MoE) model with 37B activated parameters per token, trained through large-scale reinforcement learning with a strong focus on reasoning capabilities. The model excels at understanding and handling long-form content and demonstrates superior performance in complex tasks such as mathematics and code generation. The model is approximately 30 times more cost-efficient than OpenAI-o1 and 5 times faster, offering groundbreaking performance at a fraction of the cost. Moreover, it has shown exceptional precision in tasks requiring complex pattern recognition, such as genomic data analysis, medical imaging, and large-scale scientific simulations.\\nDeepSeek-R1’s capabilities are transformative when it comes to integration with proprietary enterprise data such as PII and financial records. Leveraging retrieval-augmented generation (RAG), enterprises can connect the model to their internal data sources to enable highly personalized, context-aware interactions—all while maintaining stringent security and compliance standards. With Shakudo, you can streamline the deployment and integration of advanced AI models like DeepSeek by automating the setup, deployment, and management processes. This eliminates the need for businesses to invest in and maintain extensive computing infrastructure. By operating within your existing infrastructure, the platform ensures seamless integration, enhanced security, and optimal performance without requiring significant in-house resources or specialized expertise.\\n3. Qwen\\n\\n‍\\nAlibaba released Qwen2.5-Max earlier last month, the new model designed to deliver enhanced performance for large-scale natural language processing tasks. In instruct model evaluations, this new model outperforms DeepSeek V3 in benchmarks such as Arena-Hard, LiveBench, LiveCodeBench, and GPQA-Diamond, while also delivering strong performance in other assessments like MMLU-Pro.\\nQwen2.5-Max is pretrained on over 20 trillion tokens. While specific details about its parameter count and token window size are not publicly disclosed, Qwen2.5-Max is designed for low-latency, high-efficiency tasks, making it suitable for applications requiring quick, accurate responses with low latency. Its smaller size allows for deployment on devices with limited computational resources.\\nFor businesses and users looking for higher-performance models for natural language processing and AI-driven tasks, the Qwen 2.5 model is available on platforms like Hugging Face and ModelScope. This model boasts from 0.5 billion to 72 billion parameters, featuring context windows of up to 128,000 tokens, and is excellent for code generation, debugging, and automated forecasting.\\n4. LG AI\\n\\nsource\\nEXAONE 3.0 is a bilingual LLM developed by LG AI Research. The company released its latest model in December 2024 with excellent performance across various benchmarks and real-world applications.\\nWith 7.8 billion parameters, EXAONE 3.0 is capable of understanding and generating human-like text in multiple languages across complex domains, including coding, mathematics, patents, and chemistry. This model has also been optimized to reduce inference processing time by 56%, memory usage by 35%, and operating costs by 72%, ensuring that it remains cost-effective while maintaining high performance.\\nLG AI Research has open-sourced the 7.8B parameter instruction-tuned version of EXAONE 3.0 for non-commercial research purposes. This is a model we recommend to software companies or tech startups for generating Python code, assisting developers in troubleshooting, and creating APIs or other backend components.\\n5. LlaMA\\n\\nMeta is still leading the front with their state-of-the-art LlaMa models. The company released its latest LlaMA 3.3 model in December 2024, featuring multimodal capabilities that can process both text and image for in-depth analysis and response generation, such as interpreting charts, maps, or translating texts identified in an image.\\nLlaMA 3.3 improves on previous models with a longer context window of up to 128,000 tokens and an optimized transformer architecture. With a parameter of 70 billion, this model outperforms open-source and proprietary alternatives in areas such as multilingual dialogue, reasoning, and coding.\\nUnlike ChatGPT models, LlaMA 3 is open-source, giving users the flexibility to access and deploy freely on their cloud depending on the specific requirements of their infrastructure, security preferences, or customization needs. We recommend this model to businesses looking for advanced content generation and language understanding, such as those in customer service, education, marketing, and consumer markets. The openness of these models also allows for your greater control over the model’s performance, tuning, and integration into existing workflows.\\n6. Claude\\n\\nNext on our list is Claude, more specifically, the latest Claude 3.5 Sonnet model developed by Anthropic. We believe that Claude is arguably one of the most significant competitors to GPT since all of its current models–Claude 3 Haiku, Claude 3.5 Sonnet, and Claude 3 Opus–are designed with incredible contextual understanding capabilities that position themselves as the top conversational AI closely aligned with nuanced human interactions.\\nWhile the specific parameters of Claude 3.5 Sonnet remain undisclosed, the model boasts an impressive context window of 200,000 tokens, equivalent to approximately 150,000 words or 300 pages of text.\\nThe current Claude subscription service is credit-based, and the cost can go as high as $2,304/month for enterprise plans tailored to high-volume users. We recommend Claude to mature or mid-stage businesses looking not only to adopt an AI that facilitates human-like interactions but also to enhance their coding capabilities since the Claude 3.5 Sonnet model is currently reaching a 49.0% performance score on the SWE-bench Verified benchmark, placing it as third among all publicly available models, including reasoning models and systems specifically designed for agentic coding.\\n7. Mistral\\n\\nMistral's latest model – Mistral Small 3, a latency-optimized model was released under the Apache 2.0 license at the end of January. This 24-billion-parameter model is designed for low-latency, high-efficiency tasks. It processes approximately 150 tokens per second, making it over three times faster than Llama 3.3 70B on the same hardware.\\nThis new model is ideal for applications requiring quick, accurate responses with low latency, such as virtual assistants, real-time data processing, and on-device command and control. Its smaller size allows for deployment on devices with limited computational resources.\\nMistral Small 3 is currently open-source under the Apache 2.0 license. This means you can freely access and use the model for your own applications, provided you comply with the license terms. Since it is designed to be easily deployable, including on hardware with limited resources like a single GPU or even a MacBook with 32GB RAM, we'd recommend this to early-stage businesses looking to implement low-latency AI solutions without the need for extensive hardware infrastructure.\\n8. Gemini\\n\\nGemini is a family of closed-source LLM models developed by Google. The latest model—Gemini 2.0 Flash—operates at twice the speed of Gemini 1.5 Pro, offering substantial improvements in speed, reasoning, and multimodal processing capabilities.\\nWith that being said, Gemini remains a proprietary model; if your company deals with sensitive or confidential data regularly, you might be concerned about sending it to external servers due to security reasons. To address this concern, we recommend that you double-check vendor compliance regulations to ensure data privacy and security standards are met, such as adherence to GDPR, HIPAA, or other relevant data protection laws.\\nIf you’re looking for an open-source alternative that exhibits capabilities almost as good as Gemini, Google’s latest Gemma model, Gemma 2, offers three models available in 2 billion, 9 billion, and 27 billion parameters with a context window of 8,200. For businesses looking for a rather economic option, this is the optimal choice that interprets and understands messages with remarkable accuracy.\\n9. Command\\n\\nCommand R is a family of scalable models developed by Cohere with the goal of balancing high performance with strong accuracy, just like Claude. Both the Command R and Command R+ models offer APIs specifically optimized for Retrieval Augmented Generation (RAG). This means that these models can combine large-scale language generation with real-time information retrieval techniques for much more contextually aware outputs.\\nCurrently, the Command R+ model boasts 104 billion parameters and offers an industry-leading 128,000 token context window for enhanced long-form processing and multi-turn conversation capabilities.\\nOne of the perks of working with an open-source model is also to avoid vendor lock-in. Heavy reliance on a particular type of proprietary model may make it difficult for you to switch to alternative models when your business starts growing or the landscape changes. Cohere approaches this in a hybrid way, meaning that you can access and modify the model for personal usage but need a license for commercial use. In this case, we recommend this model for businesses that want flexibility in experimentation without a long-term commitment to a single vendor.\\nGet the whitepaper\\nTop 9 Large Language Models as of Feburary 2025\\nBy clicking \\\"Download,\\\" you agree to Shakudo processing your personal data in accordance with its Privacy Notice.\\nThank you for filling out the form. The whitepaper you have requested is available for download below.  \\nDownload White Paper\\nOops! Something went wrong while submitting the form.\\nGet the whitepaper\\nTop 9 Large Language Models as of Feburary 2025\\nThank you for your interest. Click the button below to download whitepaper you have requested.  \\nDownload White Paper\\n\\nTop 9 Large Language Models as of Feburary 2025\\nExplore the top 9 LLMs making waves in the AI world and what each of them excel at\\n\\n| Case Study\\nTop 9 Large Language Models as of Feburary 2025\\n\\nKey results\\nAbout\\nindustry\\nTech Stack\\nNo items found.\\n<>\\nIntroduction\\nIf we had to choose one word to describe the rapid evolution of AI today, it would probably be something along the lines of explosive. As predicted by the Market Research Future report, the large language model (LLM) market in North America alone is expected to reach $105.5 billion by 2030. The exponential growth of AI tools combined with access to massive troves of text data has opened gates for better and more advanced content generation than we had ever hoped. Yet, such rapid expansion also makes it harder than ever to navigate and select the right tools among the diverse LLM models available.\\nThe goal of this post is to keep you, the AI enthusiast and professional, up-to-date with current trends and essential innovations in the field. Below, we highlighted the top 9 LLMs that we think are currently making waves in the industry, each with distinct capabilities and specialized strengths, excelling in areas such as natural language processing, code synthesis, few-shot learning, or scalability. While we believe there is no one-size-fits-all LLM for every use case, we hope that this list can help you identify the most current and well-suited LLM model that meets your business’s unique requirements.\\n1. GPT\\n\\nOur list kicks off with OpenAI's Generative Pre-trained Transformer (GPT) models, which have consistently exceeded their previous capabilities with each new release. Compared to its prior models, the latest ChatGPT-4o and ChatGPT-4o mini models offer significantly faster processing speeds and enhanced capabilities across text, voice, and vision.\\nThe latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. Both of these models are equipped with multimodal capabilities to handle images as well as audio data.\\nDespite having advanced conversational and reasoning capabilities, note that GPT is a proprietary model, meaning that the training data and parameters are kept confidential by OpenAI, and access to full functionality is restricted–a commercial license or subscription is often required to unlock the complete range of features. In this case, we recommend this model for businesses looking to adopt an LLM that excels in conversational dialogue, multi-step reasoning, efficient computation, and real-time interactions without the constraints of a budget.\\nFor companies who are curious to try out the proprietary models on the market before fully committing to one due to budget constraints or uncertainties about its long-term integration, Shakudo offers a compelling alternative. Our platform currently features a diverse selection of advanced LLMs with simplified deployment and scalability. With a simple subscription, you can access and assess the value of proprietary models, like GPT, before making a substantial investment.\\n2.DeepSeek\\n\\nDeepseek-R1 Benchmark. Source: deepseek.com\\nWith its latest R1 model, the Chinese AI company DeepSeek has once again set new benchmarks for innovation in the AI community. As of January 24th, the DeepSeek-R1 model is ranked fourth on Chatbot Arena, and top as the best open-source LM.\\nThe DeepSeek-R1 is a 671B parameter Mixture-of-Experts (MoE) model with 37B activated parameters per token, trained through large-scale reinforcement learning with a strong focus on reasoning capabilities. The model excels at understanding and handling long-form content and demonstrates superior performance in complex tasks such as mathematics and code generation. The model is approximately 30 times more cost-efficient than OpenAI-o1 and 5 times faster, offering groundbreaking performance at a fraction of the cost. Moreover, it has shown exceptional precision in tasks requiring complex pattern recognition, such as genomic data analysis, medical imaging, and large-scale scientific simulations.\\nDeepSeek-R1’s capabilities are transformative when it comes to integration with proprietary enterprise data such as PII and financial records. Leveraging retrieval-augmented generation (RAG), enterprises can connect the model to their internal data sources to enable highly personalized, context-aware interactions—all while maintaining stringent security and compliance standards. With Shakudo, you can streamline the deployment and integration of advanced AI models like DeepSeek by automating the setup, deployment, and management processes. This eliminates the need for businesses to invest in and maintain extensive computing infrastructure. By operating within your existing infrastructure, the platform ensures seamless integration, enhanced security, and optimal performance without requiring significant in-house resources or specialized expertise.\\n3. Qwen\\n\\n‍\\nAlibaba released Qwen2.5-Max earlier last month, the new model designed to deliver enhanced performance for large-scale natural language processing tasks. In instruct model evaluations, this new model outperforms DeepSeek V3 in benchmarks such as Arena-Hard, LiveBench, LiveCodeBench, and GPQA-Diamond, while also delivering strong performance in other assessments like MMLU-Pro.\\nQwen2.5-Max is pretrained on over 20 trillion tokens. While specific details about its parameter count and token window size are not publicly disclosed, Qwen2.5-Max is designed for low-latency, high-efficiency tasks, making it suitable for applications requiring quick, accurate responses with low latency. Its smaller size allows for deployment on devices with limited computational resources.\\nFor businesses and users looking for higher-performance models for natural language processing and AI-driven tasks, the Qwen 2.5 model is available on platforms like Hugging Face and ModelScope. This model boasts from 0.5 billion to 72 billion parameters, featuring context windows of up to 128,000 tokens, and is excellent for code generation, debugging, and automated forecasting.\\n4. LG AI\\n\\nsource\\nEXAONE 3.0 is a bilingual LLM developed by LG AI Research. The company released its latest model in December 2024 with excellent performance across various benchmarks and real-world applications.\\nWith 7.8 billion parameters, EXAONE 3.0 is capable of understanding and generating human-like text in multiple languages across complex domains, including coding, mathematics, patents, and chemistry. This model has also been optimized to reduce inference processing time by 56%, memory usage by 35%, and operating costs by 72%, ensuring that it remains cost-effective while maintaining high performance.\\nLG AI Research has open-sourced the 7.8B parameter instruction-tuned version of EXAONE 3.0 for non-commercial research purposes. This is a model we recommend to software companies or tech startups for generating Python code, assisting developers in troubleshooting, and creating APIs or other backend components.\\n5. LlaMA\\n\\nMeta is still leading the front with their state-of-the-art LlaMa models. The company released its latest LlaMA 3.3 model in December 2024, featuring multimodal capabilities that can process both text and image for in-depth analysis and response generation, such as interpreting charts, maps, or translating texts identified in an image.\\nLlaMA 3.3 improves on previous models with a longer context window of up to 128,000 tokens and an optimized transformer architecture. With a parameter of 70 billion, this model outperforms open-source and proprietary alternatives in areas such as multilingual dialogue, reasoning, and coding.\\nUnlike ChatGPT models, LlaMA 3 is open-source, giving users the flexibility to access and deploy freely on their cloud depending on the specific requirements of their infrastructure, security preferences, or customization needs. We recommend this model to businesses looking for advanced content generation and language understanding, such as those in customer service, education, marketing, and consumer markets. The openness of these models also allows for your greater control over the model’s performance, tuning, and integration into existing workflows.\\n6. Claude\\n\\nNext on our list is Claude, more specifically, the latest Claude 3.5 Sonnet model developed by Anthropic. We believe that Claude is arguably one of the most significant competitors to GPT since all of its current models–Claude 3 Haiku, Claude 3.5 Sonnet, and Claude 3 Opus–are designed with incredible contextual understanding capabilities that position themselves as the top conversational AI closely aligned with nuanced human interactions.\\nWhile the specific parameters of Claude 3.5 Sonnet remain undisclosed, the model boasts an impressive context window of 200,000 tokens, equivalent to approximately 150,000 words or 300 pages of text.\\nThe current Claude subscription service is credit-based, and the cost can go as high as $2,304/month for enterprise plans tailored to high-volume users. We recommend Claude to mature or mid-stage businesses looking not only to adopt an AI that facilitates human-like interactions but also to enhance their coding capabilities since the Claude 3.5 Sonnet model is currently reaching a 49.0% performance score on the SWE-bench Verified benchmark, placing it as third among all publicly available models, including reasoning models and systems specifically designed for agentic coding.\\n7. Mistral\\n\\nMistral's latest model – Mistral Small 3, a latency-optimized model was released under the Apache 2.0 license at the end of January. This 24-billion-parameter model is designed for low-latency, high-efficiency tasks. It processes approximately 150 tokens per second, making it over three times faster than Llama 3.3 70B on the same hardware.\\nThis new model is ideal for applications requiring quick, accurate responses with low latency, such as virtual assistants, real-time data processing, and on-device command and control. Its smaller size allows for deployment on devices with limited computational resources.\\nMistral Small 3 is currently open-source under the Apache 2.0 license. This means you can freely access and use the model for your own applications, provided you comply with the license terms. Since it is designed to be easily deployable, including on hardware with limited resources like a single GPU or even a MacBook with 32GB RAM, we'd recommend this to early-stage businesses looking to implement low-latency AI solutions without the need for extensive hardware infrastructure.\\n8. Gemini\\n\\nGemini is a family of closed-source LLM models developed by Google. The latest model—Gemini 2.0 Flash—operates at twice the speed of Gemini 1.5 Pro, offering substantial improvements in speed, reasoning, and multimodal processing capabilities.\\nWith that being said, Gemini remains a proprietary model; if your company deals with sensitive or confidential data regularly, you might be concerned about sending it to external servers due to security reasons. To address this concern, we recommend that you double-check vendor compliance regulations to ensure data privacy and security standards are met, such as adherence to GDPR, HIPAA, or other relevant data protection laws.\\nIf you’re looking for an open-source alternative that exhibits capabilities almost as good as Gemini, Google’s latest Gemma model, Gemma 2, offers three models available in 2 billion, 9 billion, and 27 billion parameters with a context window of 8,200. For businesses looking for a rather economic option, this is the optimal choice that interprets and understands messages with remarkable accuracy.\\n9. Command\\n\\nCommand R is a family of scalable models developed by Cohere with the goal of balancing high performance with strong accuracy, just like Claude. Both the Command R and Command R+ models offer APIs specifically optimized for Retrieval Augmented Generation (RAG). This means that these models can combine large-scale language generation with real-time information retrieval techniques for much more contextually aware outputs.\\nCurrently, the Command R+ model boasts 104 billion parameters and offers an industry-leading 128,000 token context window for enhanced long-form processing and multi-turn conversation capabilities.\\nOne of the perks of working with an open-source model is also to avoid vendor lock-in. Heavy reliance on a particular type of proprietary model may make it difficult for you to switch to alternative models when your business starts growing or the landscape changes. Cohere approaches this in a hybrid way, meaning that you can access and modify the model for personal usage but need a license for commercial use. In this case, we recommend this model for businesses that want flexibility in experimentation without a long-term commitment to a single vendor.\\nExplore more from Shakudo\\n How VPCs Enable AI Deployments with a Modern Data Stack Insights January 28, 2025\\n The Power of Simple Questions: How to Choose the Right Natural Language to SQL Query Tool Insights May 15, 2024\\n Bring Data and AI tooling right to MongoDB Atlas with Shakudo News August 26, 2024\\nTake the next step\\n\\\"Shakudo gave us the flexibility to use the data stack components that fit our needs and evolve the stack to keep up with the industry.\\\"\\n\\nNeal Gilmore\\nSenior Vice President, Enterprise Data & Analytics\\nDiscover Shakudo\\n\\nShakudo brings the best data and AI products into your VPC and operates them for you automatically achieving a more reliable, performant, and cost effective data stack than ever before.\\n\\n Book Demo Email X (Twitter) Linkedin Youtube\\nNewsletter\\nSign up for the latest Shakudo news:\\n🎉 Success! You're now signed up for the Shakudo newsletter.\\nOops! Something went wrong while submitting the form.\\nApplications\\nData and AI OSStack ComponentsLanguage to SQLVector Database + LLMReverse ETLWorkflow Automation\\nIndustries\\nAutomotive & Transportation\\nAerospace\\nManufacturing\\nHigher Education\\nHealthcare & Life Sciences\\nClimate & Energy\\nTechnology & Software\\nSports\\nReal Estate\\nRetail\\nFinancial Services\\nResources\\nUse Cases\\nInsights\\nWhite Paper\\nCase Study\\nPress\\nProduct\\nTutorial\\nNews\\nWebinarGlossaryDocumentation\\nCompany\\nAboutPartnersDGX PartnerCareersMedia Kit\\nGet Started\\nSignupContact UsNewsletter\\n© 2025 Shakudo\\nToronto, CA\\nContact usPrivacy PolicyTerms/ConditionsSitemap\\nTrusted by industry leaders\\n\\n\\n\\n\\n\\n\\nSee Shakudo in Action  \\nWatch the 3 Minute Demo\\n\\nThis field is required\\n\\nFor information about how Shakudo handles your personal data, please see our Privacy Policy.\\nThank you for your submission. A Shakudo expert will be in touch with you shortly.  \\nIn the meantime, feel free to check out our data insights, case studies, and latest industry news that help data teams win.  \\n Live chat Live chat will provide the quickest answer to any of your questions.\\nOops! Something went wrong while submitting the form.\\n⨉\"}, {\"url\": \"https://ai.meta.com/blog/meta-llama-3/\", \"title\": \"Introducing Meta Llama 3: The most capable openly available LLM ...\", \"content\": \"Today, we’re introducing Meta Llama 3, the next generation of our state-of-the-art open source large language model. Today, we’re excited to share the first two models of the next generation of Llama, Meta Llama 3, available for broad use. We wanted to address developer feedback to increase the overall helpfulness of Llama 3 and are doing so while continuing to play a leading role on responsible use and deployment of LLMs. We are embracing the open source ethos of releasing early and often to enable the community to get access to these models while they are still in development. Please note that this data is based on an early checkpoint of Llama 3 that is still training and these capabilities are not supported as part of the models released today.\", \"score\": 0.6466616, \"raw_content\": \"Introducing Meta Llama 3: The most capable openly available LLM to date\\n\\n\\nOur approach\\nResearch\\nProduct experiences\\nLlama\\nBlog\\nTry Meta AI\\n\\n\\nLarge Language Model\\nIntroducing Meta Llama 3: The most capable openly available LLM to date\\nApril 18, 2024\\nTakeaways:\\nRECOMMENDED READS\\n\\n5 Steps to Getting Started with Llama 2\\nThe Llama Ecosystem: Past, Present, and Future\\nIntroducing Code Llama, a state-of-the-art large language model for coding\\n\\nMeta and Microsoft Introduce the Next Generation of Llama\\n\\n\\nToday, we’re introducing Meta Llama 3, the next generation of our state-of-the-art open source large language model.\\n\\nLlama 3 models will soon be available on AWS, Databricks, Google Cloud, Hugging Face, Kaggle, IBM WatsonX, Microsoft Azure, NVIDIA NIM, and Snowflake, and with support from hardware platforms offered by AMD, AWS, Dell, Intel, NVIDIA, and Qualcomm.\\nWe’re dedicated to developing Llama 3 in a responsible way, and we’re offering various resources to help others use it responsibly as well. This includes introducing new trust and safety tools with Llama Guard 2, Code Shield, and CyberSec Eval 2.\\nIn the coming months, we expect to introduce new capabilities, longer context windows, additional model sizes, and enhanced performance, and we’ll share the Llama 3 research paper.\\nMeta AI, built with Llama 3 technology, is now one of the world’s leading AI assistants that can boost your intelligence and lighten your load—helping you learn, get things done, create content, and connect to make the most out of every moment. You can try Meta AI here.\\n\\nToday, we’re excited to share the first two models of the next generation of Llama, Meta Llama 3, available for broad use. This release features pretrained and instruction-fine-tuned language models with 8B and 70B parameters that can support a broad range of use cases. This next generation of Llama demonstrates state-of-the-art performance on a wide range of industry benchmarks and offers new capabilities, including improved reasoning. We believe these are the best open source models of their class, period. In support of our longstanding open approach, we’re putting Llama 3 in the hands of the community. We want to kickstart the next wave of innovation in AI across the stack—from applications to developer tools to evals to inference optimizations and more. We can’t wait to see what you build and look forward to your feedback.\\nOur goals for Llama 3\\nWith Llama 3, we set out to build the best open models that are on par with the best proprietary models available today. We wanted to address developer feedback to increase the overall helpfulness of Llama 3 and are doing so while continuing to play a leading role on responsible use and deployment of LLMs. We are embracing the open source ethos of releasing early and often to enable the community to get access to these models while they are still in development. The text-based models we are releasing today are the first in the Llama 3 collection of models. Our goal in the near future is to make Llama 3 multilingual and multimodal, have longer context, and continue to improve overall performance across core LLM capabilities such as reasoning and coding.\\nState-of-the-art performance\\nOur new 8B and 70B parameter Llama 3 models are a major leap over Llama 2 and establish a new state-of-the-art for LLM models at those scales. Thanks to improvements in pretraining and post-training, our pretrained and instruction-fine-tuned models are the best models existing today at the 8B and 70B parameter scale. Improvements in our post-training procedures substantially reduced false refusal rates, improved alignment, and increased diversity in model responses. We also saw greatly improved capabilities like reasoning, code generation, and instruction following making Llama 3 more steerable.\\n*Please see evaluation details for setting and parameters with which these evaluations are calculated.\\nIn the development of Llama 3, we looked at model performance on standard benchmarks and also sought to optimize for performance for real-world scenarios. To this end, we developed a new high-quality human evaluation set. This evaluation set contains 1,800 prompts that cover 12 key use cases: asking for advice, brainstorming, classification, closed question answering, coding, creative writing, extraction, inhabiting a character/persona, open question answering, reasoning, rewriting, and summarization. To prevent accidental overfitting of our models on this evaluation set, even our own modeling teams do not have access to it. The chart below shows aggregated results of our human evaluations across of these categories and prompts against Claude Sonnet, Mistral Medium, and GPT-3.5.\\n\\nPreference rankings by human annotators based on this evaluation set highlight the strong performance of our 70B instruction-following model compared to competing models of comparable size in real-world scenarios.\\nOur pretrained model also establishes a new state-of-the-art for LLM models at those scales.\\n*Please see evaluation details for setting and parameters with which these evaluations are calculated.\\nTo develop a great language model, we believe it’s important to innovate, scale, and optimize for simplicity. We adopted this design philosophy throughout the Llama 3 project with a focus on four key ingredients: the model architecture, the pretraining data, scaling up pretraining, and instruction fine-tuning.\\nModel architecture\\nIn line with our design philosophy, we opted for a relatively standard decoder-only transformer architecture in Llama 3. Compared to Llama 2, we made several key improvements. Llama 3 uses a tokenizer with a vocabulary of 128K tokens that encodes language much more efficiently, which leads to substantially improved model performance. To improve the inference efficiency of Llama 3 models, we’ve adopted grouped query attention (GQA) across both the 8B and 70B sizes. We trained the models on sequences of 8,192 tokens, using a mask to ensure self-attention does not cross document boundaries.\\nTraining data\\nTo train the best language model, the curation of a large, high-quality training dataset is paramount. In line with our design principles, we invested heavily in pretraining data. Llama 3 is pretrained on over 15T tokens that were all collected from publicly available sources. Our training dataset is seven times larger than that used for Llama 2, and it includes four times more code. To prepare for upcoming multilingual use cases, over 5% of the Llama 3 pretraining dataset consists of high-quality non-English data that covers over 30 languages. However, we do not expect the same level of performance in these languages as in English.\\nTo ensure Llama 3 is trained on data of the highest quality, we developed a series of data-filtering pipelines. These pipelines include using heuristic filters, NSFW filters, semantic deduplication approaches, and text classifiers to predict data quality. We found that previous generations of Llama are surprisingly good at identifying high-quality data, hence we used Llama 2 to generate the training data for the text-quality classifiers that are powering Llama 3.\\nWe also performed extensive experiments to evaluate the best ways of mixing data from different sources in our final pretraining dataset. These experiments enabled us to select a data mix that ensures that Llama 3 performs well across use cases including trivia questions, STEM, coding, historical knowledge, etc.\\nScaling up pretraining\\nTo effectively leverage our pretraining data in Llama 3 models, we put substantial effort into scaling up pretraining. Specifically, we have developed a series of detailed scaling laws for downstream benchmark evaluations. These scaling laws enable us to select an optimal data mix and to make informed decisions on how to best use our training compute. Importantly, scaling laws allow us to predict the performance of our largest models on key tasks (for example, code generation as evaluated on the HumanEval benchmark—see above) before we actually train the models. This helps us ensure strong performance of our final models across a variety of use cases and capabilities.\\nWe made several new observations on scaling behavior during the development of Llama 3. For example, while the Chinchilla-optimal amount of training compute for an 8B parameter model corresponds to ~200B tokens, we found that model performance continues to improve even after the model is trained on two orders of magnitude more data. Both our 8B and 70B parameter models continued to improve log-linearly after we trained them on up to 15T tokens. Larger models can match the performance of these smaller models with less training compute, but smaller models are generally preferred because they are much more efficient during inference.\\nTo train our largest Llama 3 models, we combined three types of parallelization: data parallelization, model parallelization, and pipeline parallelization. Our most efficient implementation achieves a compute utilization of over 400 TFLOPS per GPU when trained on 16K GPUs simultaneously. We performed training runs on two custom-built 24K GPU clusters. To maximize GPU uptime, we developed an advanced new training stack that automates error detection, handling, and maintenance. We also greatly improved our hardware reliability and detection mechanisms for silent data corruption, and we developed new scalable storage systems that reduce overheads of checkpointing and rollback. Those improvements resulted in an overall effective training time of more than 95%. Combined, these improvements increased the efficiency of Llama 3 training by ~three times compared to Llama 2.\\nInstruction fine-tuning\\nTo fully unlock the potential of our pretrained models in chat use cases, we innovated on our approach to instruction-tuning as well. Our approach to post-training is a combination of supervised fine-tuning (SFT), rejection sampling, proximal policy optimization (PPO), and direct preference optimization (DPO). The quality of the prompts that are used in SFT and the preference rankings that are used in PPO and DPO has an outsized influence on the performance of aligned models. Some of our biggest improvements in model quality came from carefully curating this data and performing multiple rounds of quality assurance on annotations provided by human annotators.\\nLearning from preference rankings via PPO and DPO also greatly improved the performance of Llama 3 on reasoning and coding tasks. We found that if you ask a model a reasoning question that it struggles to answer, the model will sometimes produce the right reasoning trace: The model knows how to produce the right answer, but it does not know how to select it. Training on preference rankings enables the model to learn how to select it.\\nBuilding with Llama 3\\nOur vision is to enable developers to customize Llama 3 to support relevant use cases and to make it easier to adopt best practices and improve the open ecosystem. With this release, we’re providing new trust and safety tools including updated components with both Llama Guard 2 and Cybersec Eval 2, and the introduction of Code Shield—an inference time guardrail for filtering insecure code produced by LLMs.\\nWe’ve also co-developed Llama 3 with torchtune, the new PyTorch-native library for easily authoring, fine-tuning, and experimenting with LLMs. torchtune provides memory efficient and hackable training recipes written entirely in PyTorch. The library is integrated with popular platforms such as Hugging Face, Weights & Biases, and EleutherAI and even supports Executorch for enabling efficient inference to be run on a wide variety of mobile and edge devices. For everything from prompt engineering to using Llama 3 with LangChain we have a comprehensive getting started guide and takes you from downloading Llama 3 all the way to deployment at scale within your generative AI application.\\nA system-level approach to responsibility\\nWe have designed Llama 3 models to be maximally helpful while ensuring an industry leading approach to responsibly deploying them. To achieve this, we have adopted a new, system-level approach to the responsible development and deployment of Llama. We envision Llama models as part of a broader system that puts the developer in the driver’s seat. Llama models will serve as a foundational piece of a system that developers design with their unique end goals in mind.\\n\\nInstruction fine-tuning also plays a major role in ensuring the safety of our models. Our instruction-fine-tuned models have been red-teamed (tested) for safety through internal and external efforts. ​​Our red teaming approach leverages human experts and automation methods to generate adversarial prompts that try to elicit problematic responses. For instance, we apply comprehensive testing to assess risks of misuse related to Chemical, Biological, Cyber Security, and other risk areas. All of these efforts are iterative and used to inform safety fine-tuning of the models being released. You can read more about our efforts in the model card.\\nLlama Guard models are meant to be a foundation for prompt and response safety and can easily be fine-tuned to create a new taxonomy depending on application needs. As a starting point, the new Llama Guard 2 uses the recently announced MLCommons taxonomy, in an effort to support the emergence of industry standards in this important area. Additionally, CyberSecEval 2 expands on its predecessor by adding measures of an LLM’s propensity to allow for abuse of its code interpreter, offensive cybersecurity capabilities, and susceptibility to prompt injection attacks (learn more in our technical paper). Finally, we’re introducing Code Shield which adds support for inference-time filtering of insecure code produced by LLMs. This offers mitigation of risks around insecure code suggestions, code interpreter abuse prevention, and secure command execution.\\nWith the speed at which the generative AI space is moving, we believe an open approach is an important way to bring the ecosystem together and mitigate these potential harms. As part of that, we’re updating our Responsible Use Guide (RUG) that provides a comprehensive guide to responsible development with LLMs. As we outlined in the RUG, we recommend that all inputs and outputs be checked and filtered in accordance with content guidelines appropriate to the application. Additionally, many cloud service providers offer content moderation APIs and other tools for responsible deployment, and we encourage developers to also consider using these options.\\nDeploying Llama 3 at scale\\nLlama 3 will soon be available on all major platforms including cloud providers, model API providers, and much more. Llama 3 will be everywhere.\\nOur benchmarks show the tokenizer offers improved token efficiency, yielding up to 15% fewer tokens compared to Llama 2. Also, Group Query Attention (GQA) now has been added to Llama 3 8B as well. As a result, we observed that despite the model having 1B more parameters compared to Llama 2 7B, the improved tokenizer efficiency and GQA contribute to maintaining the inference efficiency on par with Llama 2 7B.\\nFor examples of how to leverage all of these capabilities, check out Llama Recipes which contains all of our open source code that can be leveraged for everything from fine-tuning to deployment to model evaluation.\\nWhat’s next for Llama 3?\\nThe Llama 3 8B and 70B models mark the beginning of what we plan to release for Llama 3. And there’s a lot more to come.\\nOur largest models are over 400B parameters and, while these models are still training, our team is excited about how they’re trending. Over the coming months, we’ll release multiple models with new capabilities including multimodality, the ability to converse in multiple languages, a much longer context window, and stronger overall capabilities. We will also publish a detailed research paper once we are done training Llama 3.\\nTo give you a sneak preview for where these models are today as they continue training, we thought we could share some snapshots of how our largest LLM model is trending. Please note that this data is based on an early checkpoint of Llama 3 that is still training and these capabilities are not supported as part of the models released today.\\n*Please see evaluation details for setting and parameters with which these evaluations are calculated.\\nWe’re committed to the continued growth and development of an open AI ecosystem for releasing our models responsibly. We have long believed that openness leads to better, safer products, faster innovation, and a healthier overall market. This is good for Meta, and it is good for society. We’re taking a community-first approach with Llama 3, and starting today, these models are available on the leading cloud, hosting, and hardware platforms with many more to come.\\nTry Meta Llama 3 today\\nWe’ve integrated our latest models into Meta AI, which we believe is the world’s leading AI assistant. It’s now built with Llama 3 technology and it’s available in more countries across our apps.\\nYou can use Meta AI on Facebook, Instagram, WhatsApp, Messenger, and the web to get things done, learn, create, and connect with the things that matter to you. You can read more about the Meta AI experience here.\\nVisit the Llama 3 website to download the models and reference the Getting Started Guide for the latest list of all available platforms.\\nYou’ll also soon be able to test multimodal Meta AI on our Ray-Ban Meta smart glasses.\\nAs always, we look forward to seeing all the amazing products and experiences you will build with Meta Llama 3.\\n\\nShare:\\n\\nOur latest updates delivered to your inbox\\nSubscribe to our newsletter to keep up with Meta AI news, events, research breakthroughs, and more.\\nJoin us in the pursuit of what’s possible with AI.\\nSee all open positions\\nRelated Posts\\n\\nComputer Vision\\nIntroducing Segment Anything: Working toward the first foundation model for image segmentation\\nApril 5, 2023\\nRead post\\nFEATURED\\n\\nResearch\\nMultiRay: Optimizing efficiency for large-scale AI models\\nNovember 18, 2022\\nRead post\\nFEATURED\\n\\nML Applications\\nMuAViC: The first audio-video speech translation benchmark\\nMarch 8, 2023\\nRead post\\nOur approach\\nAbout AI at Meta\\nPeople\\nCareers\\nResearch\\nInfrastructure\\nResources\\nDemos\\nProduct experiences\\nMeta AI\\nAI Studio\\nLatest news\\nBlog\\nNewsletter\\nFoundational models\\nLlama\\n\\n \\n \\n \\n \\nOur approach\\nOur approachAbout AI at MetaPeopleCareers\\nResearch\\nResearchInfrastructureResourcesDemos\\nProduct experiences\\nMeta AIAI Studio\\nLatest news\\nLatest newsBlogNewsletter\\nFoundational models\\nLlama\\n \\n \\n \\n \\nPrivacy Policy\\nTerms\\nCookies\\nMeta © 2025\\n \\n \\n \\n \"}], \"response_time\": 1.39}\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Based on the web search results, here are some of the recent LLMs that have been released:\n",
            "\n",
            "*   **GPT-4o:** OpenAI's latest model, released in May 2024, integrates text, image, video, and voice capabilities. Claims to be 50% cheaper and twice as fast as GPT-4.\n",
            "*   **Claude 3.5 Sonnet:** Launched by Anthropic in June 2024, known for its ethical design and strong performance.\n",
            "*   **Llama 3.1:** Meta AI's Llama 3.1 models were released in July 2024, including both a 405 billion and 70 billion parameter model\n",
            "*   **Nemotron-4 340B:** Nvidia released the Nemotron-4 340B family of models in June 2024 for synthetic data generation and AI model training.\n",
            "*   **Gemma 2:** Google DeepMind released Gemma 2 in June 2024, a family of open-source language models.\n",
            "*   **DeepSeek R1:** DeepSeek released the DeepSeek-R1 model in January 2025, an open-source reasoning model excelling in math and coding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on the web search results, here are some of the recent LLMs that have been released:\n\n*   **GPT-4o:** OpenAI's latest model, released in May 2024, integrates text, image, video, and voice capabilities. Claims to be 50% cheaper and twice as fast as GPT-4.\n*   **Claude 3.5 Sonnet:** Launched by Anthropic in June 2024, known for its ethical design and strong performance.\n*   **Llama 3.1:** Meta AI's Llama 3.1 models were released in July 2024, including both a 405 billion and 70 billion parameter model\n*   **Nemotron-4 340B:** Nvidia released the Nemotron-4 340B family of models in June 2024 for synthetic data generation and AI model training.\n*   **Gemma 2:** Google DeepMind released Gemma 2 in June 2024, a family of open-source language models.\n*   **DeepSeek R1:** DeepSeek released the DeepSeek-R1 model in January 2025, an open-source reasoning model excelling in math and coding."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What about reasoning LLMs?\"\n",
        "chat_with_agent(graph_builder, prompt, user_id, verbose=True)"
      ],
      "metadata": {
        "id": "XT4vtvku0TBW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "a163d67a-c079-472a-a278-854e8b230518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Agent, please wait...\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What about reasoning LLMs?\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  search_web (26e0185f-c201-4bcc-959a-fdfcb2119258)\n",
            " Call ID: 26e0185f-c201-4bcc-959a-fdfcb2119258\n",
            "  Args:\n",
            "    query: reasoning large language models released\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: search_web\n",
            "\n",
            "{\"query\": \"reasoning large language models released\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://openreview.net/forum?id=dCPF1wlqj8\", \"title\": \"Unlocking Reasoning Potential in Large Language Models by ...\", \"content\": \"The tested models (Mistral/llama-2) are released more than a year ago and are known for limited reasoning abilities. For example, fine-tuned llama-2 is\", \"score\": 0.79369855, \"raw_content\": null}, {\"url\": \"https://medium.com/@med.el.harchaoui/the-next-generation-of-language-model-large-reasoning-models-lrms-dc740b0c441f\", \"title\": \"The Next Generation of Language Model: Large Reasoning Models ...\", \"content\": \"The Next Generation of Language Model: Large Reasoning Models (LRMs) | by Mohamed EL HARCHAOUI | Dec, 2024 | Medium LRMs operate on ideas — fundamental units of thought and common sense — rather than tokens, aiming to emulate human reasoning more effectively. Reasoning Attention Blocks: Processes these ideas, enabling the model to engage in hierarchical planning, solve complex problems, and anticipate outcomes with greater accuracy. By operating on ideas, LRMs can engage in higher-order reasoning, making deductions and inferences that are challenging for token-based models. By focusing on ideas rather than tokens, LRMs aim to overcome the limitations of current LLMs, enabling machines to engage in genuine reasoning and understanding.\", \"score\": 0.77792513, \"raw_content\": null}, {\"url\": \"https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models\", \"title\": \"25 of the best large language models in 2025 - TechTarget\", \"content\": \"Large language models are the dynamite behind the generative AI boom. Some of the most well-known language models today are based on the transformer model, including the generative pre-trained transformer series of LLMs and bidirectional encoder representations from transformers (BERT). Gemma is a family of open-source language models from Google that were trained on the same resources as Gemini. GPT-3 is OpenAI's large language model with more than 175 billion parameters, released in 2020. Large Language Model Meta AI (Llama) is Meta's LLM which was first released in 2023. The Pathways Language Model is a 540 billion parameter transformer-based model from Google powering its AI chatbot Bard. StableLM is a series of open language models developed by Stability AI, the company behind image generator Stable Diffusion.\", \"score\": 0.725824, \"raw_content\": \"25 of the best large language models in 2025\\nWhatIs\\nSearch the TechTarget Network \\nBrowse Definitions :\\n\\nA\\nB\\nC\\nD\\nE\\nF\\nG\\nH\\nI\\nJ\\nK\\nL\\nM\\nN\\nO\\nP\\nQ\\nR\\nS\\nT\\nU\\nV\\nW\\nX\\nY\\nZ\\n#\\n\\nLogin Register\\n\\nTechTarget Network\\nTech Accelerator\\nNews\\n2024 IT Salary Survey Results\\n\\nRSS\\n\\n\\nWhatIs\\n\\n\\nBrowse Definitions Data analytics and AI\\nTopics View All\\n\\nBusiness software\\nCloud computing\\nComputer science\\nData centers\\nIT management\\nNetworking\\nSecurity\\nSoftware development\\n\\nPlease select a category\\n\\nTopics\\n\\n\\n\\nBrowse Features Resources\\n\\nBusiness strategies\\nCareer resources\\nEmerging tech\\nTech explainers\\n\\n\\n\\nFollow:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\n\\nData analytics and AI\\n\\nTech Accelerator What is Gen AI? Generative AI explained\\nPrev Next Will AI replace jobs? 17 job types that might be affected Pros and cons of AI-generated content\\nDownload this guide1\\nFeature\\n25 of the best large language models in 2025\\nLarge language models have been affecting search for years and have been brought to the forefront by ChatGPT and other chatbots.\\n\\nShare this item with your network:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy\\n\\nSean Michael Kerner\\nBen Lutkevich, Site Editor\\n\\nPublished: 31 Jan 2025\\nLarge language models are the dynamite behind the generative AI boom. However, they've been around for a while.\\nLLMs are black box AI systems that use deep learning on extremely large datasets to understand and generate new text. Modern LLMs began taking shape in 2014 when the attention mechanism -- a machine learning technique designed to mimic human cognitive attention -- was introduced in a research paper titled \\\"Neural Machine Translation by Jointly Learning to Align and Translate.\\\" In 2017, that attention mechanism was honed with the introduction of the transformer model in another paper, \\\"Attention Is All You Need.\\\"\\nSome of the most well-known language models today are based on the transformer model, including the generative pre-trained transformer series of LLMs and bidirectional encoder representations from transformers (BERT).\\nChatGPT, which runs on a set of language models from OpenAI, attracted more than 100 million users just two months after its release in 2022. Since then, many competing models have been released. Some belong to big companies such as Google, Amazon and Microsoft; others are open source.\\nConstant developments in the field can be difficult to keep track of. Here are some of the most influential models, both past and present. Included in it are models that paved the way for today's leaders as well as those that could have a significant effect in the future.\\nThis article is part of\\nWhat is Gen AI? Generative AI explained\\n\\nWhich also includes:\\n8 top generative AI tool categories for 2025\\nWill AI replace jobs? 17 job types that might be affected\\n25 of the best large language models in 2025\\n\\nTop current LLMs\\nBelow are some of the most relevant large language models today. They do natural language processing and influence the architecture of future models.\\nBERT\\nBERT is a family of LLMs that Google introduced in 2018. BERT is a transformer-based model that can convert sequences of data to other sequences of data. BERT's architecture is a stack of transformer encoders and features 342 million parameters. BERT was pre-trained on a large corpus of data then fine-tuned to perform specific tasks along with natural language inference and sentence text similarity. It was used to improve query understanding in the 2019 iteration of Google search.\\nClaude\\nThe Claude LLM focuses on constitutional AI, which shapes AI outputs guided by a set of principles that help the AI assistant it powers helpful, harmless and accurate. Claude was created by the company Anthropic.\\nThere are three primary branches of Claude -- Opus, Haiku and Sonnet. The latest iteration of the Claude LLM is the Claude 3.5 Sonnet. It understands nuance, humor and complex instructions better than earlier versions of the LLM. It also has broad programming capabilities that make it well-suited for application development. In October 2024, Claude added a computer-use AI tool, that enables the LLM to use a computer like a human does. It's available via Claude.ai, the Claude iOS app and through an API.\\nCohere\\nCohere is an enterprise AI platform that provides several LLMs including Command, Rerank and Embed. These LLMs can be custom-trained and fine-tuned to a specific company's use case. The company that created the Cohere LLM was founded by one of the authors of Attention Is All You Need.\\nDeepSeek-R1\\nDeepSeek-R1 is an open-source reasoning model for tasks with complex reasoning, mathematical problem-solving and logical inference. The model uses reinforcement learning techniques to refine its reasoning ability and solve complex problems. DeepSeek-R1 can perform critical problem-solving through self-verification, chain-of-thought reasoning and reflection.\\nErnie\\nErnie is Baidu's large language model which powers the Ernie 4.0 chatbot. The bot was released in August 2023 and has garnered more than 45 million users. Ernie is rumored to have 10 trillion parameters. The bot works best in Mandarin but is capable in other languages.\\nFalcon\\nFalcon is a family of transformer-based models developed by the Technology Innovation Institute. It is open source and has multi-lingual capabilities. Falcon 2 is available in an 11 billion parameter version that provide multimodal capabilities for both text and vision.\\nThe Falcon 1 series includes a pair of larger models with Falcon 40B and Falcon 180B. Falcon models are available on GitHub as well as on cloud provider including Amazon.\\nGemini\\nGemini is Google's family of LLMs that power the company's chatbot of the same name. The model replaced Palm in powering the chatbot, which was rebranded from Bard to Gemini upon the model switch. Gemini models are multimodal, meaning they can handle images, audio and video as well as text. Gemini is also integrated in many Google applications and products. It comes in three sizes -- Ultra, Pro and Nano. Ultra is the largest and most capable model, Pro is the mid-tier model and Nano is the smallest model, designed for efficiency with on-device tasks.\\nAmong the most recent models is the Gemini 1.5 Pro update that debuted in May 2024 Gemini is available as a web chatbot, the Google Vertex AI service and via API. Early previews of Gemini 2.0 Flash became available in December 2024 with updated multimodal generation capabilities.\\nGemma\\nGemma is a family of open-source language models from Google that were trained on the same resources as Gemini. Gemma 2 was released in June 2024 in two sizes -- a 9 billion parameter model and a 27 billion parameter model. Gemma models can be run locally on a personal computer, and are also available in Google Vertex AI.\\nGPT-3\\nGPT-3 is OpenAI's large language model with more than 175 billion parameters, released in 2020. GPT-3 uses a decoder-only transformer architecture. In September 2022, Microsoft announced it had exclusive use of GPT-3's underlying model. GPT-3 is 10 times larger than its predecessor. GPT-3's training data includes Common Crawl, WebText2, Books1, Books2 and Wikipedia.\\nGPT-3 is the last of the GPT series of models in which OpenAI made the parameter counts publicly available. The GPT series was first introduced in 2018 with OpenAI's paper \\\"Improving Language Understanding by Generative Pre-Training.\\\"\\nGPT-3.5\\nGPT-3.5 is an upgraded version of GPT-3 with fewer parameters. GPT-3.5 was fine-tuned using reinforcement learning from human feedback. GPT-3.5 is the version of GPT that powers ChatGPT. There are several models, with GPT-3.5 turbo being the most capable, according to OpenAI. GPT-3.5's training data extends to September 2021.\\nIt was also integrated into the Bing search engine but has since been replaced with GPT-4.\\nGPT-4\\nGPT-4 , was released in 2023 and like the others in the OpenAI GPT family, it's a transformer-based model. Unlike the others, its parameter count has not been released to the public, though there are rumors that the model has more than 170 trillion. OpenAI describes GPT-4 as a multimodal model, meaning it can process and generate both language and images as opposed to being limited to only language. GPT-4 also introduced a system message, which lets users specify tone of voice and task.\\nGPT-4 demonstrated human-level performance in multiple academic exams. At the model's release, some speculated that GPT-4 came close to artificial general intelligence, which means it is as smart or smarter than a human. That speculation turned out to be unfounded.\\nGPT-4o\\nGPT-4 Omni (GPT-4o) is OpenAI's successor to GPT-4 and offers several improvements over the previous model. GPT-4o creates a more natural human interaction for ChatGPT and is a large multimodal model, accepting various inputs including audio, image and text. The conversations let users engage as they would in a normal human conversation, and the real-time interactivity can also pick up on emotions. GPT-4o can see photos or screens and ask questions about them during interaction.\\nGPT-4o can respond in 232 milliseconds, similar to human response time and faster than GPT-4 Turbo.\\nGranite\\nThe IBM Granite family of models are fully open source models under the Apache v.2 license. The first iteration of the open source model models debuted in May 2024, followed by Granite 3.0 in October and Granite 3.1 in December 2024.\\nThere are multiple variants in the Granite model family including General-purpose models (8B and 2B variants), guardrail model and Mixture-of-Experts models. While the model can be used for general purpose deployments, IBM itself is focusing deployment and optimization for enterprise use cases like customer service, IT automation and cybersecurity.\\nLamda\\nLamda (Language Model for Dialogue Applications) is a family of LLMs developed by Google Brain announced in 2021. Lamda used a decoder-only transformer language model and was pre-trained on a large corpus of text. In 2022, LaMDA gained widespread attention when then-Google engineer Blake Lemoine went public with claims that the program was sentient. It was built on the Seq2Seq architecture.\\nLlama\\nLarge Language Model Meta AI (Llama) is Meta's LLM which was first released in 2023. The Llama 3.1 models were released in July 2024, including both a 405 billion and 70 billion parameter model.\\nThe most recent version is Llama 3.2 which was released in September 2024, initially with smaller parameter counts of 11 billion and 90 billion.\\nLlama uses a transformer architecture and was trained on a variety of public data sources, including webpages from CommonCrawl, GitHub, Wikipedia and Project Gutenberg. Llama was effectively leaked and spawned many descendants, including Vicuna and Orca. Llama is available under an open license, allowing for free use of the models. Lllama models are available in many locations including llama.com and Hugging Face.\\nMistral\\nMistral is a family of a mixture of expert models from Mistral AI. Among the newest models is Mistral Large 2 which was first released in July 2024. The model operates with 123 billion parameters and a 128k context window, supporting dozens of languages including French, German, Spanish, Italian, and many others, along with more than 80 coding languages.\\nIn November 2024, Mistral released Pixtral Large, a 124-billion-parameter multimodal model that can handle text and visual data. Mistral models are available via Mistral's API on its Le Platforme-managed web service.\\no1\\nThe OpenAI o1 model family was first introduced in Sept. 2024. The o1 model's focus is to provide what OpenAI refers to as - reasoning models, that can reason through a problem or query before offering a response.\\nThe o1 models excel in STEM fields, with strong results in mathematical reasoning (scoring 83% on the International Mathematics Olympiad compared to GPT-4o's 13%), code generation and scientific research tasks. While they offer enhanced reasoning and improved safety features, they operate more slowly than previous models due to their thorough reasoning processes and come with certain limitations, such as restricted access features and higher API costs. The models are available to ChatGPT Plus and Team users, with varying access levels for different user categories.\\no3\\nOpenAI introduced the successor model, o3, in December 2024. According to OpenAI, o3 is designed to handle tasks with more analytical thinking, problem-solving and complex reasoning and will improve o1's capabilities and performance. The o3 model is in safety testing mode and is currently not available to the public.\\nOrca\\nOrca was developed by Microsoft and has 13 billion parameters, meaning it's small enough to run on a laptop. It aims to improve on advancements made by other open source models by imitating the reasoning procedures achieved by LLMs. Orca achieves the same performance as GPT-4 with significantly fewer parameters and is on par with GPT-3.5 for many tasks. Orca is built on top of the 13 billion parameter version of Llama.\\nPalm\\nThe Pathways Language Model is a 540 billion parameter transformer-based model from Google powering its AI chatbot Bard. It was trained across multiple TPU 4 Pods -- Google's custom hardware for machine learning. Palm specializes in reasoning tasks such as coding, math, classification and question answering. Palm also excels at decomposing complex tasks into simpler subtasks.\\nPaLM gets its name from a Google research initiative to build Pathways, ultimately creating a single model that serves as a foundation for multiple use cases. There are several fine-tuned versions of Palm, including Med-Palm 2 for life sciences and medical information as well as Sec-Palm for cybersecurity deployments to speed up threat analysis.\\nPhi\\nPhi is a transformer-based language model from Microsoft. The Phi 3.5 models were first released in August 2024.\\nThe series includes Phi-3.5-mini-instruct (3.82 billion parameters), Phi-3.5-MoE-instruct (41.9 billion parameters), and Phi-3.5-vision-instruct (4.15 billion parameters), each designed for specific tasks ranging from basic reasoning to vision analysis. All three models support a 128k token context length.\\nReleased under a Microsoft-branded MIT License, they are available for developers to download, use, and modify without restrictions, including for commercial purposes.\\nQwen\\nQwen is large family of open models developed by Chinese internet giant Alibaba Cloud. The newest set of models are the Qwen2.5 suite, which support 29 different languages and currently scale up to 72 billion parameters. These models are suitable for a wide range of tasks, including code generation, structured data understanding, mathematical problem-solving as well as general language understanding and generation.\\nStableLM\\nStableLM is a series of open language models developed by Stability AI, the company behind image generator Stable Diffusion.\\nStableLM 2 debuted in January 2024 initially with a 1.6 billion parameter model. In April 2024 that was expanded to also include a 12 billion parameter model. StableLM 2 supports seven languages: English, Spanish, German, Italian, French, Portuguese, and Dutch. Stability AI positions these models as offering different options for various use cases, with the 1.6B model suitable for specific, narrow tasks and faster processing while the 12B model provides more capability but requires more computational resources.\\nTülu 3\\nAllen Institute for AI's Tülu 3 is an open-source 405 billion-parameter LLM. The Tülu 3 405B model has post-training methods that combine supervised fine-tuning and reinforcement learning at a larger scale. Tülu 3 uses a \\\"reinforcement learning from verifiable rewards\\\" framework for fine-tuning tasks with verifiable outcomes -- such as solving mathematical problems and following instructions.\\nVicuna 33B\\nVicuna is another influential open source LLM derived from Llama. It was developed by LMSYS and was fine-tuned using data from sharegpt.com. It is smaller and less capable that GPT-4 according to several benchmarks, but does well for a model of its size. Vicuna has only 33 billion parameters, whereas GPT-4 has trillions.\\nLLM precursors\\nAlthough LLMs are a recent phenomenon, their precursors go back decades. Learn how recent precursor Seq2Seq and distant precursor ELIZA set the stage for modern LLMs.\\nSeq2Seq\\nSeq2Seq is a deep learning approach used for machine translation, image captioning and natural language processing. It was developed by Google and underlies some of their modern LLMs, including LaMDA. Seq2Seq also underlies AlexaTM 20B, Amazon's large language model. It uses a mix of encoders and decoders.\\nEliza\\nEliza was an early natural language processing program created in 1966. It is one of the earliest examples of a language model. Eliza simulated conversation using pattern matching and substitution. Eliza, running a certain script, could parody the interaction between a patient and therapist by applying weights to certain keywords and responding to the user accordingly. The creator of Eliza, Joshua Weizenbaum, wrote a book on the limits of computation and artificial intelligence.\\nNext Steps\\nGenerative AI challenges that businesses should consider\\nGenerative AI ethics: Biggest concerns\\nGenerative AI landscape: Potential future trends\\nGenerative models: VAEs, GANs, diffusion, transformers, NeRFs\\nAI content generators to explore\\nRelated Resources\\n\\nFive data quality trends to prepare for in the year ahead –Video\\nThe Digital Transformation And Innovation Landscape –Wipro\\nCloudera and NVIDIA Accelerate AI in the Financial Services Industry –Cloudera\\nImprove customer satisfaction or cut costs? Who says you have to choose? –Video\\n\\nDig Deeper on Data analytics and AI\\n\\n ##### What is GPT-3? Everything you need to know  By: Nick Barney\\n ##### What is a small language model (SLM)?  By: Sean Kerner\\n ##### GPT-4  By: Ben Lutkevich\\n ##### What are large language models (LLMs)?  By: Sean Kerner\\n\\nSponsored News\\n\\nSustainability, AI and Dell PowerEdge Servers –Dell Technologies and Intel\\nThree Innovative AI Use Cases for Natural Language Processing –Dell Technologies\\nAutonomous coding: The future of the revenue cycle –Solventum\\n\\nRelated Content\\n\\nExploring GPT-3 architecture – Search Enterprise AI\\nWhat is GPT-3? Everything you need to know – Search Enterprise AI\\nMicrosoft exclusively licenses OpenAI's GPT-3 ... – Search Enterprise AI\\n\\nLatest TechTarget resources\\n\\nNetworking\\nSecurity\\nCIO\\nHR Software\\nCustomer Experience\\n\\nSearch Networking\\n\\n\\nWhat is a thin client (lean client)?A thin client (lean client) is a virtual desktop computing model that runs on the resources stored on a central server instead of...\\n\\n\\nWhat is network monitoring?Network monitoring, also frequently called network management, is the practice of consistently overseeing a computer network for ...\\n\\n\\nWhat is network automation?Network automation is a process that uses intelligent software to automate the management, configuration, deployment, testing and...\\n\\n\\nSearch Security\\n\\n\\nWhat is Internet Key Exchange (IKE)?Internet Key Exchange (IKE) is a standard protocol used to set up a secure and authenticated communication channel between two ...\\n\\n\\nWhat is a certificate revocation list (CRL) and how is it used?A certificate revocation list (CRL) is a list of digital certificates that have been revoked by the issuing certificate authority...\\n\\n\\nWhat is cryptology?Cryptology is the mathematics, such as number theory and the application of formulas and algorithms, that underpin cryptography ...\\n\\n\\nSearch CIO\\n\\n\\nWhat is an IT project manager?An IT project manager is a professional charged with overseeing the process of planning, executing and delegating ...\\n\\n\\nWhat is a cyberthreat hunter (cybersecurity threat analyst)?A cyberthreat hunter, also called a cybersecurity threat analyst, proactively identifies security incidents that might go ...\\n\\n\\nWhat is blockchain? Definition, examples and how it worksBlockchain is a distributed ledger technology (DLT) that's shared across a network of computers to keep a digital record of ...\\n\\n\\nSearch HRSoftware\\n\\n\\nWhat is employee self-service (ESS)?Employee self-service (ESS) is a widely used human resources technology that enables employees to perform many job-related ...\\n\\n\\nWhat is DEI? Diversity, equity and inclusion explainedDiversity, equity and inclusion is a term used to describe policies and programs that promote the representation and ...\\n\\n\\nWhat is payroll software?Payroll software automates the process of paying salaried, hourly and contingent employees.\\n\\n\\nSearch Customer Experience\\n\\n\\nWhat is account-based selling? Everything you need to knowAccount-based selling (ABS) is a strategic sales approach in business-to-business sales and marketing that centers around ...\\n\\n\\nWhat is interactive voice response (IVR)?Interactive voice response (IVR) is an automated telephony system that interacts with callers, gathers information and routes ...\\n\\n\\nWhat is an AI assistant?An AI assistant, or digital assistant, is software that uses artificial intelligence to understand natural language voice ...\\n\\n\\nBrowse by Topic\\n\\n\\nBrowse Resources\\n\\n\\nAbout Us\\n\\nMeet The Editors\\nEditorial Ethics Policy\\nContact Us\\nAdvertisers\\nBusiness Partners\\nEvents\\nMedia Kit\\nCorporate Site\\nReprints\\n\\nAll Rights Reserved, Copyright 1999 - 2025, TechTarget  \\nPrivacy Policy\\nCookie Preferences\\nCookie Preferences\\nDo Not Sell or Share My Personal Information\\nClose\\n\\nX\\nFree Download What is generative AI? Everything you need to know\\nThe potential of AI technology has been percolating in the background for years. But when ChatGPT, the AI chatbot, began grabbing headlines in early 2023, it put generative AI in the spotlight. This guide is your go-to manual for generative AI, covering its benefits, limits, use cases, prospects and much more.\\n\"}, {\"url\": \"https://www.reddit.com/r/LLMDevs/comments/1e4oc1l/reasoning_skills_of_large_language_models_are/\", \"title\": \"Reasoning skills of large language models are often overestimated\", \"content\": \"MIT's recent study reveals that while large language models (LLMs) like GPT-4 can churn out impressive text, their reasoning skills might not be as sharp as we\", \"score\": 0.7050753, \"raw_content\": null}, {\"url\": \"https://arxiv.org/abs/2502.09100\", \"title\": \"Logical Reasoning in Large Language Models: A Survey - arXiv\", \"content\": \"cs arXiv:2502.09100 Help | Advanced Search arXiv author ID Logical Reasoning in Large Language Models: A Survey This survey synthesizes recent advancements in logical reasoning within LLMs, a critical area of AI research. It outlines the scope of logical reasoning in LLMs, its theoretical foundations, and the benchmarks used to evaluate reasoning proficiency. The review concludes with future directions, emphasizing the need for further exploration to strengthen logical reasoning in AI systems. Subjects:   Artificial Intelligence (cs.AI); Computation and Language (cs.CL) Cite as:    arXiv:2502.09100 [cs.AI] (or arXiv:2502.09100v1 [cs.AI] for this version) From: Hanmeng Liu [view email] Access Paper: cs.AI cs Bibliographic and Citation Tools Bibliographic Explorer Toggle Connected Papers Toggle scite.ai Toggle Which authors of this paper are endorsers?\", \"score\": 0.6721816, \"raw_content\": \"Help | Advanced Search\\nquick links\\nComputer Science > Artificial Intelligence\\nTitle:Logical Reasoning in Large Language Models: A Survey\\nSubmission history\\nAccess Paper:\\nReferences & Citations\\nBibTeX formatted citation\\nBookmark\\nBibliographic and Citation Tools\\nCode, Data and Media Associated with this Article\\nDemos\\nRecommenders and Search Tools\\narXivLabs: experimental projects with community collaborators\\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\\n\\n\\narXiv Operational Status \\n                    Get status notifications via\\n                    email\\n                    or slack\\n\\n\"}, {\"url\": \"https://news.mit.edu/2024/technique-improves-reasoning-capabilities-large-language-models-0614\", \"title\": \"Technique improves the reasoning capabilities of large language ...\", \"content\": \"There is still a long way to go, but we have shown that combining the capabilities of programming and natural language in large language models is a very good potential first step toward a future where people can fully understand and trust what is going on inside their AI model,” says Hongyin Luo PhD ’22, an MIT postdoc and co-lead author of a paper on NLEPs. Luo is joined on the paper by co-lead authors Tianhua Zhang, a graduate student at the Chinese University of Hong Kong; and Jiaxin Ge, an undergraduate at Peking University; Yoon Kim, an assistant professor in MIT’s Department of Electrical Engineering and Computer Science and a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL); senior author James Glass, senior research scientist and head of the Spoken Language Systems Group in CSAIL; and others.\", \"score\": 0.6607204, \"raw_content\": \"Suggestions or feedback?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMIT News | Massachusetts Institute of Technology\\n\\nBrowse By\\nTopics\\nDepartments\\nCenters, Labs, & Programs\\nSchools\\nTechnique improves the reasoning capabilities of large language models \\n\\n Press Contact:\\n\\nMedia Download\\n\\n    Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a \\n    Creative Commons Attribution Non-Commercial No Derivatives license.\\n    You may not alter the images provided, other than to crop them to size. A credit line must be used when reproducing images; if one is not provided \\n    below, credit the images to \\\"MIT.\\\" \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPrevious image\\nNext image\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLarge language models like those that power ChatGPT have shown impressive performance on tasks like drafting legal briefs, analyzing the sentiment of customer reviews, or translating documents into different languages.\\nThese machine-learning models typically use only natural language to process information and answer queries, which can make it difficult for them to perform tasks that require numerical or symbolic reasoning.\\nFor instance, a large language model might be able to memorize and recite a list of recent U.S. presidents and their birthdays, but that same model could fail if asked the question “Which U.S. presidents elected after 1950 were born on a Wednesday?” (The answer is Jimmy Carter.)\\nResearchers from MIT and elsewhere have proposed a new technique that enables large language models to solve natural language, math and data analysis, and symbolic reasoning tasks by generating programs.\\nTheir approach, called natural language embedded programs (NLEPs), involves prompting a language model to create and execute a Python program to solve a user’s query, and then output the solution as natural language.\\nThey found that NLEPs enabled large language models to achieve higher accuracy on a wide range of reasoning tasks. The approach is also generalizable, which means one NLEP prompt can be reused for multiple tasks.\\nNLEPs also improve transparency, since a user could check the program to see exactly how the model reasoned about the query and fix the program if the model gave a wrong answer.\\n“We want AI to perform complex reasoning in a way that is transparent and trustworthy. There is still a long way to go, but we have shown that combining the capabilities of programming and natural language in large language models is a very good potential first step toward a future where people can fully understand and trust what is going on inside their AI model,” says Hongyin Luo PhD ’22, an MIT postdoc and co-lead author of a paper on NLEPs.\\nLuo is joined on the paper by co-lead authors Tianhua Zhang, a graduate student at the Chinese University of Hong Kong; and Jiaxin Ge, an undergraduate at Peking University; Yoon Kim, an assistant professor in MIT’s Department of Electrical Engineering and Computer Science and a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL); senior author James Glass, senior research scientist and head of the Spoken Language Systems Group in CSAIL; and others. The research will be presented at the Annual Conference of the North American Chapter of the Association for Computational Linguistics.\\nProblem-solving with programs\\nMany popular large language models work by predicting the next word, or token, given some natural language input. While models like GPT-4 can be used to write programs, they embed those programs within natural language, which can lead to errors in the program reasoning or results.\\nWith NLEPs, the MIT researchers took the opposite approach. They prompt the model to generate a step-by-step program entirely in Python code, and then embed the necessary natural language inside the program.\\nAn NLEP is a problem-solving template with four steps. First, the model calls the necessary packages, or functions, it will need to solve the task. Step two involves importing natural language representations of the knowledge the task requires (like a list of U.S. presidents’ birthdays). For step three, the model implements a function that calculates the answer. And for the final step, the model outputs the result as a line of natural language with an automatic data visualization, if needed.\\n“It is like a digital calculator that always gives you the correct computation result as long as the program is correct,” Luo says.\\nThe user can easily investigate the program and fix any errors in the code directly rather than needing to rerun the entire model to troubleshoot.\\nThe approach also offers greater efficiency than some other methods. If a user has many similar questions, they can generate one core program and then replace certain variables without needing to run the model repeatedly.\\nTo prompt the model to generate an NLEP, the researchers give it an overall instruction to write a Python program, provide two NLEP examples (one with math and one with natural language), and one test question.\\n“Usually, when people do this kind of few-shot prompting, they still have to design prompts for every task. We found that we can have one prompt for many tasks because it is not a prompt that teaches LLMs to solve one problem, but a prompt that teaches LLMs to solve many problems by writing a program,” says Luo.\\n“Having language models reason with code unlocks many opportunities for tool use, output validation, more structured understanding into model's capabilities and way of thinking, and more,” says Leonid Karlinsky, principal scientist at the MIT-IBM Watson AI Lab.\\n“No magic here”\\nNLEPs achieved greater than 90 percent accuracy when prompting GPT-4 to solve a range of symbolic reasoning tasks, like tracking shuffled objects or playing a game of 24, as well as instruction-following and text classification tasks. The researchers found that NLEPs even exhibited 30 percent greater accuracy than task-specific prompting methods. The method also showed improvements over open-source LLMs. \\nAlong with boosting the accuracy of large language models, NLEPs could also improve data privacy. Since NLEP programs are run locally, sensitive user data do not need to be sent to a company like OpenAI or Google to be processed by a model.\\nIn addition, NLEPs can enable small language models to perform better without the need to retrain a model for a certain task, which can be a costly process.\\n“There is no magic here. We do not have a more expensive or fancy language model. All we do is use program generation instead of natural language generation, and we can make it perform significantly better,” Luo says.\\nHowever, an NLEP relies on the program generation capability of the model, so the technique does not work as well for smaller models which have been trained on limited datasets. In the future, the researchers plan to study methods that could make smaller language models generate more effective NLEPs. In addition, they want to investigate the impact of prompt variations on NLEPs to enhance the robustness of the model’s reasoning processes.\\nThis research was supported, in part, by the Center for Perceptual and Interactive Intelligence of Hong Kong. \\nShare this news article on:\\nPaper\\nRelated Links\\nRelated Topics\\nRelated Articles\\n\\nNatural language boosts LLM performance in coding, planning, and robotics \\n\\n\\n\\nLarge language models use a surprisingly simple mechanism to retrieve some stored knowledge \\n\\n\\n\\nExplained: Generative AI \\n\\n\\n\\nLearning to grow machine-learning models \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPrevious item\\nNext item\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMore MIT News\\n\\nAI system predicts protein fragments that can bind to or inhibit a target\\n\\n\\nRead full story →\\n            \\n\\nMIT faculty, alumni named 2025 Sloan Research Fellows\\n\\n\\nRead full story →\\n            \\n\\nMIT biologists discover a new type of control over RNA splicing\\n\\n\\nRead full story →\\n            \\n\\nRooftop panels, EV chargers, and smart thermostats could chip in to boost power grid resilience\\n\\n\\nRead full story →\\n            \\n\\nChip-based system for terahertz waves could enable more efficient, sensitive electronics\\n\\n\\nRead full story →\\n            \\n\\nReducing carbon emissions from residential heating: A pathway forward\\n\\n\\nRead full story →\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMore about MIT News at Massachusetts Institute of Technology\\n\\nThis website is managed by the MIT News Office, part of the Institute Office of Communications.\\nNews by Schools/College:\\nResources:\\nTools:\\nMassachusetts Institute of Technology77 Massachusetts Avenue, Cambridge, MA, USA\\n\"}, {\"url\": \"https://aclanthology.org/2023.findings-acl.67/\", \"title\": \"Towards Reasoning in Large Language Models: A Survey\", \"content\": \"Towards Reasoning in Large Language Models: A Survey - ACL Anthology 2023.findings-acl.67 Findings of the Association for Computational Linguistics: ACL 2023 https://aclanthology.org/2023.findings-acl.67/ In Findings of the Association for Computational Linguistics: ACL 2023, pages 1049–1065, Toronto, Canada. Towards Reasoning in Large Language Models: A Survey (Huang & Chang, Findings 2023) https://aclanthology.org/2023.findings-acl.67.pdf url = \\\"https://aclanthology.org/2023.findings-acl.67/\\\", <title>Findings of the Association for Computational Linguistics: ACL 2023</title> <identifier type=\\\"doi\\\">10.18653/v1/2023.findings-acl.67</identifier> %S Findings of the Association for Computational Linguistics: ACL 2023 %U https://doi.org/10.18653/v1/2023.findings-acl.67 [Towards Reasoning in Large Language Models: A Survey](https://aclanthology.org/2023.findings-acl.67/) (Huang & Chang, Findings 2023) Towards Reasoning in Large Language Models: A Survey (Huang & Chang, Findings 2023) In Findings of the Association for Computational Linguistics: ACL 2023, pages 1049–1065, Toronto, Canada.\", \"score\": 0.6016175, \"raw_content\": \"Towards Reasoning in Large Language Models: A Survey\\nJie Huang,\\nKevin Chen-Chuan Chang\\n[Towards Reasoning in Large Language Models: A Survey](https://aclanthology.org/2023.findings-acl.67/) (Huang & Chang, Findings 2023)\\n\\nACL materials are Copyright © 1963–2025 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a Creative Commons Attribution 4.0 International License.\\nThe ACL Anthology is managed and built by the ACL Anthology team of volunteers.\\nSite last built on 19 February 2025 at 23:06 UTC with commit 88259be.\\n\"}, {\"url\": \"https://github.com/jeffhj/LM-reasoning\", \"title\": \"Reasoning in Large Language Models - GitHub\", \"content\": \"This repository contains a collection of papers and resources on Reasoning in Large Language Models. For more details, please refer to Towards Reasoning in\", \"score\": 0.5079833, \"raw_content\": \"Navigation Menu\\nSearch code, repositories, users, issues, pull requests...\\n\\n        Provide feedback\\n      \\nWe read every piece of feedback, and take your input very seriously.\\n\\n        Saved searches\\n      \\nUse saved searches to filter your results more quickly\\n\\n            To see all available qualifiers, see our documentation.\\n          \\n\\n        This repository contains a collection of papers and resources on Reasoning in Large Language Models.\\n      \\nLicense\\njeffhj/LM-reasoning\\nFolders and files\\nLatest commit\\nHistory\\nRepository files navigation\\nReasoning in Large Language Models\\n\\n\\n\\nThis repository contains a collection of papers and resources on Reasoning in Large Language Models.\\nFor more details, please refer to Towards Reasoning in Large Language Models: A Survey\\nFeel free to let me know the missing papers (issue or pull request).\\nContributor: Jie Huang @UIUC\\nThank Kevin Chen-Chuan Chang @UIUC, Jason Wei @Google Brain, Denny Zhou @Google Brain for insightful discussions and suggestions.\\nContents\\nSurvey\\nJie Huang, Kevin Chen-Chuan Chang\\nRelevant Survey and Position Paper and Blog\\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus\\nDavid Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Sohl-dickstein, Kevin Murphy, Charles Sutton\\nYao Fu, Hao Peng, Tushar Shot\\nShuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen\\nPan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, Kai-Wei Chang\\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, Zhifang Sui\\nZonglin Yang, Xinya Du, Rui Mao, Jinjie Ni, Erik Cambria\\nFei Yu, Hongbo Zhang, Benyou Wang\\nTechnique\\nFully Supervised Finetuning\\nWe mainly focus on techniques that are applicable to improving or eliciting \\\"reasoning\\\" in large language models like GPT-3 (175B)\\nPapers in this paradigm vary a lot and are usually based on small models trained on specific datasets. We list several papers here for reference (that is, the list is not complete). Please refer to our survey for some discussion.\\nNazneen Fatema Rajani, Bryan McCann, Caiming Xiong, Richard Socher\\nAlon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, Jonathan Berant\\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt\\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, Augustus Odena\\nSoumya Sanyal, Harman Singh, Xiang Ren\\nPrompting and In-Context Learning\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou\\nBoshi Wang, Xiang Deng, Huan Sun\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa\\nBen Prystawski, Paul Thibodeau, Noah Goodman\\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, Jason Wei\\nWenhu Chen\\nAman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, Graham Neubig\\nLuyu Gao*, Aman Madaan*, Shuyan Zhou*, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig\\nWenhu Chen, Xueguang Ma, Xinyi Wang, William W. Cohen\\nHangfeng He, Hongming Zhang, Dan Roth\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou\\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen\\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot\\nZhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola\\nHattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, Hanie Sedghi\\nYixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, Jun Zhao\\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, Ed Chi\\nAndrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, Denny Zhou\\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal\\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, Mike Lewis\\nDheeru Dua, Shivanshu Gupta, Sameer Singh, Matt Gardner\\nYunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, Yongbin Li\\nWenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch\\nAntonia Creswell, Murray Shanahan, Irina Higgins\\nJaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, Yejin Choi\\nAntonia Creswell, Murray Shanahan\\nPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan\\nShiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, Wenhu Chen, Xifeng Yan\\nKumar Shridhar, Alessandro Stolfo, Mrinmaya Sachan\\nLucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, Aliaksei Severyn\\nSeyed Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, Deepak Ramachandran\\nShibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu\\nHybrid Method\\nXinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Qiang Fu, Yan Gao, Jian-Guang Lou, Weizhu Chen\\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, Vedant Misra\\nCem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, Behnam Neyshabur\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, Jason Wei\\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic\\nPing Yu, Tianlu Wang, Olga Golovneva, Badr Alkhamissy, Gargi Ghosh, Mona Diab, Asli Celikyilmaz\\nEric Zelikman, Yuhuai Wu, Jesse Mu, Noah D. Goodman\\nPatrick Haluptzok, Matthew Bowers, Adam Tauman Kalai\\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han\\nEvaluation and Analysis\\nArkil Patel, Satwik Bhattamishra, Navin Goyal\\nYasaman Razeghi, Robert L. Logan IV, Matt Gardner, Sameer Singh\\nJie Huang, Hanyin Shao, Kevin Chen-Chuan Chang\\nKarthik Valmeekam, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati\\nCem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, Behnam Neyshabur\\nIshita Dasgupta, Andrew K. Lampinen, Stephanie C. Y. Chan, Antonia Creswell, Dharshan Kumaran, James L. McClelland, Felix Hill\\nSimeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq Joty, Alexander R. Fabbri, Wojciech Kryscinski, Xi Victoria Lin, Caiming Xiong, Dragomir Radev\\nAbulhair Saparov, He He\\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, Jason Wei\\nLaura Ruis, Akbir Khan, Stella Biderman, Sara Hooker, Tim Rocktäschel, Edward Grefenstette\\nOlga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz\\nBoshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, Huan Sun\\nCitation\\nIf you find this repo useful, please kindly cite our survey:\\nAbout\\n\\n        This repository contains a collection of papers and resources on Reasoning in Large Language Models.\\n      \\nTopics\\nResources\\nLicense\\nStars\\nWatchers\\nForks\\n\\nReleases\\n\\nPackages\\n      0\\n\\nContributors\\n      6\\nFooter\\nFooter navigation\\n\"}], \"response_time\": 2.01}\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Based on the search results, here's what I found about reasoning LLMs:\n",
            "\n",
            "*   **DeepSeek-R1:** This model is explicitly mentioned as a reasoning model excelling in tasks with complex reasoning, mathematical problem-solving, and logical inference. It utilizes reinforcement learning techniques.\n",
            "*   **OpenAI o1 model family:** Introduced in September 2024, these models focus on reasoning through a problem or query before offering a response. They reportedly excel in STEM fields, particularly mathematical reasoning, code generation, and scientific research.\n",
            "*   There's a trend towards Large Reasoning Models (LRMs) that operate on \"ideas\" rather than tokens, aiming to emulate human reasoning more effectively.\n",
            "*   Techniques like Natural Language Embedded Programs (NLEPs) are being developed to improve the reasoning capabilities of LLMs by generating programs to solve tasks.\n",
            "*   The paper \"Logical Reasoning in Large Language Models: A Survey\" synthesizes recent advancements in logical reasoning within LLMs.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on the search results, here's what I found about reasoning LLMs:\n\n*   **DeepSeek-R1:** This model is explicitly mentioned as a reasoning model excelling in tasks with complex reasoning, mathematical problem-solving, and logical inference. It utilizes reinforcement learning techniques.\n*   **OpenAI o1 model family:** Introduced in September 2024, these models focus on reasoning through a problem or query before offering a response. They reportedly excel in STEM fields, particularly mathematical reasoning, code generation, and scientific research.\n*   There's a trend towards Large Reasoning Models (LRMs) that operate on \"ideas\" rather than tokens, aiming to emulate human reasoning more effectively.\n*   Techniques like Natural Language Embedded Programs (NLEPs) are being developed to improve the reasoning capabilities of LLMs by generating programs to solve tasks.\n*   The paper \"Logical Reasoning in Large Language Models: A Survey\" synthesizes recent advancements in logical reasoning within LLMs."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What are the four major design patterns for creating agentic ai systems?\"\n",
        "chat_with_agent(graph_builder, prompt, user_id, verbose=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "c2WYu2_AUJ9F",
        "outputId": "40e9c127-7d35-4e25-eec5-154ce9b5ae12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Agent, please wait...\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, based on the search results, here are four major design patterns for creating agentic AI systems, as highlighted by Andrew Ng and other sources:\n\n1.  **Reflection:** This involves the AI agent evaluating and refining its own outputs iteratively. The agent analyzes its performance, identifies errors or areas for improvement, and adjusts its strategy accordingly. It promotes self-correction and can significantly improve the quality of results.\n\n2.  **Tool Use:** This pattern equips AI agents with the ability to access and utilize external tools, APIs, or resources to enhance their problem-solving capabilities. Instead of relying solely on internal knowledge, the agent can interact with the real world, retrieve data, perform computations, and more.\n\n3.  **Planning:** This enables AI agents to break down complex tasks into smaller, more manageable subtasks. The agent creates a roadmap of actions and determines the most efficient path to achieve the overall goal. This strategic thinking helps to solve tasks more effectively and avoid confusion.\n\n4.  **Multi-Agent Collaboration:** This pattern involves orchestrating multiple AI agents with different roles or functions to work together on a complex task. The agents collaborate, communicate, and delegate subtasks to achieve a unified outcome. This approach mirrors how humans collaborate across specialties.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now simulate User 2 using the agent"
      ],
      "metadata": {
        "id": "B4zIlISy6m-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = 'bond007'\n",
        "prompt = \"how is the weather in Hyderabad today? Show detailed statistics\"\n",
        "chat_with_agent(graph_builder, prompt, user_id, verbose=False)"
      ],
      "metadata": {
        "id": "ta1RUF_81RxX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "befaa50a-d765-4a98-91e5-39dfd4101767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Agent, please wait...\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The weather in Hyderabad, Telangana, India is currently:\n- Temperature: 22.3°C (72.1°F)\n- Feels like: 24.4°C (75.9°F)\n- Humidity: 100%\n- Cloud cover: 50%\n- Condition: Mist\n- Wind: 3.6 kph from SE\n- Pressure: 1017 mb\n- Visibility: 4 km\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = 'bond007'\n",
        "prompt = \"what about Dubai?\"\n",
        "chat_with_agent(graph_builder, prompt, user_id, verbose=False)"
      ],
      "metadata": {
        "id": "vfn_Wxxg42rZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "d1148436-1c7a-417d-c11e-9d4976558546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Agent, please wait...\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The weather in Dubai, United Arab Emirates is currently:\n- Temperature: 23.2°C (73.8°F)\n- Feels like: 25°C (77°F)\n- Humidity: 65%\n- Cloud cover: 0%\n- Condition: Clear\n- Wind: 13.3 kph from NE\n- Pressure: 1011 mb\n- Visibility: 10 km\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = 'bond007'\n",
        "prompt = \"which city is hotter?\"\n",
        "chat_with_agent(graph_builder, prompt, user_id, verbose=True)"
      ],
      "metadata": {
        "id": "I7jjxtmz6Qzi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "18eee76d-a97a-422a-c842-605e33f5fa65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Agent, please wait...\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "which city is hotter?\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Dubai is hotter than Hyderabad. Dubai feels like 25°C (77°F) while Hyderabad feels like 24.4°C (75.9°F).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Dubai is hotter than Hyderabad. Dubai feels like 25°C (77°F) while Hyderabad feels like 24.4°C (75.9°F).\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = 'jack001'\n",
        "prompt = \"what are the top frameworks to build such systems?\"\n",
        "chat_with_agent(graph_builder, prompt, user_id, verbose=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "Ixll7BlRVvm9",
        "outputId": "8cf6ae31-e1dd-49ff-b97d-f4a5ea1ff06f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Agent, please wait...\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "From the search results, here are some top frameworks for building agentic AI systems:\n\n*   **LangChain:** A widely adopted framework known for its modularity, extensive set of tools and abstractions, and integrations with various LLMs and external resources. It's suitable for enterprise development and building foundational blocks for GenAI applications.\n*   **LangGraph:** An open-source framework by the LangChain team designed specifically for agentic architectures. It focuses on stateful design, graph-based workflows, and multi-agent capabilities.\n*   **AutoGen:** A Microsoft framework for building multi-agent conversational AI systems. It supports asynchronous messaging, is modular and extensible, and offers tools for observability and debugging.\n*   **Semantic Kernel:** Another Microsoft framework, this one as an SDK designed for creating enterprise-ready applications with strong integration capabilities. It supports multiple programming languages and offers agent and process frameworks.\n*   **LlamaIndex:** While starting as a data framework, LlamaIndex has evolved to cover AI agents, offering document parsing & indexing, workflows, and connectors. It's a compelling alternative to LangChain, particularly for data-intensive LLM applications.\n*   **CrewAI:** A framework that simplifies the orchestration of autonomous agents, enabling the creation of \"crews\" of AI agents with specific roles to work together.\n*   **Haystack:** An open-source framework for building production-ready LLM applications, RAG pipelines, and complex search applications.\n\nThese frameworks provide various tools and abstractions to simplify the development of complex agentic systems, including pre-built components, communication protocols, task management systems, and integration tools.\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}