{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aa1reXo/Agentic-AI/blob/main/Ax_QLC_02_Build_a_multi_user_conversational_Tool_use_Agentic_AI_System_with_Langgraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a ReAct AI Agent from scratch with LangGraph"
      ],
      "metadata": {
        "id": "8pdRDlVs1MJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This demo will cover building AI Agents with LangGraph from scratch.\n",
        "\n",
        "Here we'll create a simple ReAct agent app that can search the web and check the weather. The app consists of an agent (LLM) and tools. As we interact with the app, we will first call the agent (LLM) to decide if we should use tools. Then we will run a loop:\n",
        "\n",
        "- If the agent said to take an action (i.e. call tool), we'll run the tools and pass the results back to the agent\n",
        "- If the agent did not ask to run tools, we will finish (respond to the user)\n",
        "\n",
        "We will implement this in LangGraph completely from scratch step by step.\n"
      ],
      "metadata": {
        "id": "NYBpZTjLnEXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There will be three parts to this hands-on demo:\n",
        "\n",
        "- Part I: Build a Basic Chatbot with LangGraph\n",
        "- Part II: Build a simple ReAct Agent with LangGraph - LLM + Tools\n",
        "- Part III: Build a multi-user conversational ReAct Agent with LangGraph"
      ],
      "metadata": {
        "id": "dZ0q3K8Y1TCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install OpenAI, and LangChain dependencies"
      ],
      "metadata": {
        "id": "L1KvMtf54l0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.3.12\n",
        "#!pip install langchain-openai==0.2.12\n",
        "!pip install langchain-community==0.3.12\n",
        "!pip install langgraph==0.2.56\n",
        "!pip install langgraph-checkpoint-sqlite==2.0.1\n",
        "!pip install beautifulsoup4\n",
        "!pip install langchain-google-genai==2.0.1"
      ],
      "metadata": {
        "id": "2evPp14fy258",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e71854ae-32a3-4930-fe86-ef23b2f2401f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.3.12\n",
            "  Downloading langchain-0.3.12-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.12) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.12) (2.0.38)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.12) (3.11.12)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.12) (0.3.35)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.12) (0.3.6)\n",
            "Collecting langsmith<0.3,>=0.1.17 (from langchain==0.3.12)\n",
            "  Downloading langsmith-0.2.11-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.12) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.12) (2.10.6)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.12) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.12) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.12) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.12) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.12) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.12) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.12) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.12) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.12) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain==0.3.12) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain==0.3.12) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain==0.3.12) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain==0.3.12) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain==0.3.12) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain==0.3.12) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.12) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.12) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.12) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.12) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.12) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.12) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.12) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain==0.3.12) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain==0.3.12) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain==0.3.12) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain==0.3.12) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain==0.3.12) (1.3.1)\n",
            "Downloading langchain-0.3.12-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.2.11-py3-none-any.whl (326 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.9/326.9 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langsmith, langchain\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.3.8\n",
            "    Uninstalling langsmith-0.3.8:\n",
            "      Successfully uninstalled langsmith-0.3.8\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.18\n",
            "    Uninstalling langchain-0.3.18:\n",
            "      Successfully uninstalled langchain-0.3.18\n",
            "Successfully installed langchain-0.3.12 langsmith-0.2.11\n",
            "Collecting langchain-community==0.3.12\n",
            "  Downloading langchain_community-0.3.12-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.12) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.12) (2.0.38)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.12) (3.11.12)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.3.12)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community==0.3.12)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.12 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.12) (0.3.12)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.12) (0.3.35)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.12) (0.2.11)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.12) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community==0.3.12)\n",
            "  Downloading pydantic_settings-2.8.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.12) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.12) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.12) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.12) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.12) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.12) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.12) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.12) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.12) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.12)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.12)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.12->langchain-community==0.3.12) (0.3.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.12->langchain-community==0.3.12) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain-community==0.3.12) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain-community==0.3.12) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain-community==0.3.12) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.12) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.12) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.12) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.12)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.12) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.12) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.12) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.12) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.3.12) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.12) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.12) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.12) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain-community==0.3.12) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.12->langchain-community==0.3.12) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.12->langchain-community==0.3.12) (2.27.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.12)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.12) (1.3.1)\n",
            "Downloading langchain_community-0.3.12-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.8.0-py3-none-any.whl (30 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.12 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.0 python-dotenv-1.0.1 typing-inspect-0.9.0\n",
            "Collecting langgraph==0.2.56\n",
            "  Downloading langgraph-0.2.56-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43 in /usr/local/lib/python3.11/dist-packages (from langgraph==0.2.56) (0.3.35)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.4 (from langgraph==0.2.56)\n",
            "  Downloading langgraph_checkpoint-2.0.16-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph==0.2.56)\n",
            "  Downloading langgraph_sdk-0.1.53-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (0.2.11)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (4.12.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (2.10.6)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.4->langgraph==0.2.56) (1.1.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.2.56) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.2.56) (3.10.15)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.2.56) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.2.56) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.2.56) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.2.56) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.2.56) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (3.0.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph==0.2.56) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.2.56) (1.3.1)\n",
            "Downloading langgraph-0.2.56-py3-none-any.whl (126 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.16-py3-none-any.whl (38 kB)\n",
            "Downloading langgraph_sdk-0.1.53-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langgraph-sdk, langgraph-checkpoint, langgraph\n",
            "Successfully installed langgraph-0.2.56 langgraph-checkpoint-2.0.16 langgraph-sdk-0.1.53\n",
            "Collecting langgraph-checkpoint-sqlite==2.0.1\n",
            "  Downloading langgraph_checkpoint_sqlite-2.0.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting aiosqlite<0.21.0,>=0.20.0 (from langgraph-checkpoint-sqlite==2.0.1)\n",
            "  Downloading aiosqlite-0.20.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint-sqlite==2.0.1) (2.0.16)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.11/dist-packages (from aiosqlite<0.21.0,>=0.20.0->langgraph-checkpoint-sqlite==2.0.1) (4.12.2)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.2.38 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (0.3.35)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (1.1.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (0.2.11)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (2.10.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (3.10.15)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (2.27.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.2->langgraph-checkpoint-sqlite==2.0.1) (1.3.1)\n",
            "Downloading langgraph_checkpoint_sqlite-2.0.1-py3-none-any.whl (12 kB)\n",
            "Downloading aiosqlite-0.20.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: aiosqlite, langgraph-checkpoint-sqlite\n",
            "Successfully installed aiosqlite-0.20.0 langgraph-checkpoint-sqlite-2.0.1\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.12.2)\n",
            "Collecting langchain-google-genai==2.0.1\n",
            "  Downloading langchain_google_genai-2.0.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: google-generativeai<0.9.0,>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai==2.0.1) (0.8.4)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai==2.0.1) (0.3.35)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai==2.0.1) (2.10.6)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (2.160.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (4.25.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (1.26.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (0.2.11)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai==2.0.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai==2.0.1) (2.27.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (1.67.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (1.0.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (3.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (0.14.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai==2.0.1) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-google-genai==2.0.1) (1.3.1)\n",
            "Downloading langchain_google_genai-2.0.1-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-google-genai\n",
            "Successfully installed langchain-google-genai-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter Open AI API Key"
      ],
      "metadata": {
        "id": "H9c37cLnSrbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "# OPENAI_KEY = getpass('Enter your OpenAI Key: ')\n",
        "GEMINI_API_KEY = getpass('Enter your Google Gemini API Key: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e30999ce-6b8e-4872-b403-e62213521567",
        "id": "gdKxKv-g0sgN"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Google Gemini API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter WeatherAPI API Key\n",
        "\n",
        "Get a free API key from [here](https://www.weatherapi.com/signup.aspx)"
      ],
      "metadata": {
        "id": "djj1pH6A04BX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WEATHER_API_KEY = getpass('Enter WeatherAPI API Key: ')"
      ],
      "metadata": {
        "id": "XpAMz1XgEEov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39370eff-d481-4f2d-8c3c-cd1954119a89"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter WeatherAPI API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter Tavily Search API Key\n",
        "\n",
        "Get a free API key from [here](https://tavily.com/#api)"
      ],
      "metadata": {
        "id": "ucWRRI3QztL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TAVILY_API_KEY = getpass('Enter Tavily Search API Key: ')"
      ],
      "metadata": {
        "id": "mK-1WLzOrJdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2f9d077-6157-4fe7-9986-f9aac13b8bfe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Tavily Search API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Environment Variables"
      ],
      "metadata": {
        "id": "1T0s0um5Svfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# os.environ['OPENAI_API_KEY'] = OPENAI_KEY\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
        "os.environ['TAVILY_API_KEY'] = TAVILY_API_KEY"
      ],
      "metadata": {
        "id": "x1YSuHNF_lbh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
        "from langchain_core.tools import tool\n",
        "import json\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "tavily_search = TavilySearchAPIWrapper()\n",
        "\n",
        "@tool\n",
        "def search_web(query: str) -> list:\n",
        "    \"\"\"Search the web for a query. Userful for general information or general news\"\"\"\n",
        "    results = tavily_search.raw_results(query=query,\n",
        "                                        max_results=8,\n",
        "                                        search_depth='advanced',\n",
        "                                        include_answer=False,\n",
        "                                        include_raw_content=True)\n",
        "    return results\n",
        "\n",
        "@tool\n",
        "def get_weather(query: str) -> list:\n",
        "    \"\"\"Search weatherapi to get the current weather.\"\"\"\n",
        "    base_url = \"http://api.weatherapi.com/v1/current.json\"\n",
        "    complete_url = f\"{base_url}?key={WEATHER_API_KEY}&q={query}\"\n",
        "\n",
        "    response = requests.get(complete_url)\n",
        "    data = response.json()\n",
        "    if data.get(\"location\"):\n",
        "        return data\n",
        "    else:\n",
        "        return \"Weather Data Not Found\""
      ],
      "metadata": {
        "id": "Ue8xgu9WpuPi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_web.invoke('Latest LLMs released in 2025')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryEb_pHP10br",
        "outputId": "d967a3d4-583a-43f7-b05c-5e4341756239"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'Latest LLMs released in 2025',\n",
              " 'follow_up_questions': None,\n",
              " 'answer': None,\n",
              " 'images': [],\n",
              " 'results': [{'url': 'https://www.koyeb.com/blog/best-open-source-llms-in-2025',\n",
              "   'title': 'Best Open Source LLMs in 2025 - Koyeb',\n",
              "   'content': 'DeepSeek-R1 Qwen 32BMistral Small 3Qwen 2.5 Coder 7B InstructBest Open Source Models for Reasoning, Code Generation, and MoreFine-Tuning and Deploying Open LLMs with Serverless GPUs Open source LLMs continue to compete with proprietary models on performance benchmarks for natural language tasks like text generation, code completion, and reasoning. In this blog post, we’ll explore the best open LLMs available at the start of 2025, including: DeepSeek-R1, Mistral Small 3, and Qwen 2.5 Coder. Qwen 2.5 7B Coder Instruct stands out for its high performance in code tasks, including generation, reasoning, and code fixing. Open-source AI models like DeepSeek-R1, Mistral Small 3, and Qwen 2.5 Coder provide powerful alternatives to proprietary options, offering flexibility and cost-effectiveness.',\n",
              "   'score': 0.854509,\n",
              "   'raw_content': 'Best Open Source LLMs in 2025\\nOpen source LLMs continue to compete with proprietary models on performance benchmarks for natural language tasks like text generation, code completion, and reasoning.\\nDespite having fewer resources than closed models, these open LLMs offer cutting-edge AI without the high costs and restrictions of proprietary models.\\nHowever, running these open-source models in production and at scale remains a challenge. Enter Serverless GPUs: a cost-effective, scalable way to deploy and fine-tune LLMs without managing complex infrastructure.\\nIn this blog post, we’ll explore the best open LLMs available at the start of 2025, including: DeepSeek-R1, Mistral Small 3, and Qwen 2.5 Coder. After comparing their capabilities and ideal use cases for real-world AI applications, we’ll also share how to fine-tune and deploy them using serverless GPUs for optimized inference and training.\\nDeepSeek-R1 Qwen 32B\\nDeepSeek released two first-generation reasoning models: DeepSeek-R1-Zero and DeepSeek-R1.\\nDeepSeek-R1-Zero was trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT), allowing it to explore chain-of-thought (CoT) reasoning for complex problem-solving.\\nAlthough this approach led to impressive advancements, DeepSeek-R1-Zero faced challenges, such as: repetition, poor readability, and language mixing. To improve performance, DeepSeek developed DeepSeek-R1, with cold-start data incorporated before RL.\\nIn addition to these two models, DeepSeek released six models of varying sizes based on Llama and Qwen, including DeepSeek-R1-Distill-Qwen-32B.\\nDistilled models are smaller models that have been trained with the reasoning patterns of larger, more complex models.\\nDeploy DeepSeek R1 on Koyeb →\\nMistral Small 3\\nMistral AI is a leading provider for AI models, including multimodal models like Pixtral 12B and Large, edge models such as Ministral 3B and 8B, LLMs like Nemo Instruct, Codestral for code generation, Mathstral for mathematics, and more.\\nReleased in January 2025, Mistral Small 3 Instruct is a 24-billion-parameter model that achieves state-of-the-art capabilities comparable to larger models. It is ideal for various text generation tasks, including fast-response conversational agents, low-latency function calling, and any other applications requiring robust language understanding and instruction-following performance.\\nThis model is an instruction-fine-tuned version of the base model:\\xa0Mistral-Small-24B-Base-2501.\\nDeploy Mistral Small 3 on Koyeb →\\nQwen 2.5 Coder 7B Instruct\\nQwen2.5 is a new family of models from Qwen that includes Qwen2.5 LLMs, and specialized models Qwen2.5-Math for mathematics and Qwen2.5 Coder for coding.\\nThe open-source Qwen2.5 models available with an Apache 2.0 license include:\\nThere are also 3B and 72B variants, not available with an open-source license.\\nAmong all the advancements in AI, code generation has been significant. Qwen 2.5 7B Coder Instruct stands out for its high performance in code tasks, including generation, reasoning, and code fixing.\\nDeploy Qwen 2.5 Coder 7B Instruct on Koyeb →\\nBest Open Source Models for Reasoning, Code Generation, and More\\nFine-Tuning and Deploying Open LLMs with Serverless GPUs\\nOpen-source AI models like DeepSeek-R1, Mistral Small 3, and Qwen 2.5 Coder provide powerful alternatives to proprietary options, offering flexibility and cost-effectiveness.\\nWith Koyeb’s serverless GPUs, you can fine-tune and deploy these models with a single click. Get a dedicated inference endpoint running on high performance GPUs without managing any infrastructure.\\nDeploy AI apps to production in minutes\\n'},\n",
              "  {'url': 'https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models',\n",
              "   'title': '25 of the best large language models in 2025 - TechTarget',\n",
              "   'content': \"Large language models are the dynamite behind the\\xa0generative AI\\xa0boom. Some of the most well-known language models today are based on the transformer model, including the\\xa0generative pre-trained transformer series\\xa0of LLMs and bidirectional encoder representations from transformers (BERT). Gemma\\xa0is a family of open-source language models from Google that were trained on the same resources as Gemini. GPT-3\\xa0is OpenAI's large language model with more than 175 billion parameters, released in 2020. Large Language Model Meta AI (Llama) is Meta's LLM which was first released in 2023. The\\xa0Pathways Language Model\\xa0is a 540 billion parameter transformer-based model from Google powering its AI chatbot\\xa0Bard. StableLM is a series of open language models developed by Stability AI, the company behind image generator Stable Diffusion.\",\n",
              "   'score': 0.7380092,\n",
              "   'raw_content': '25 of the best large language models in 2025\\nWhatIs\\nSearch the TechTarget Network \\nBrowse Definitions :\\n\\nA\\nB\\nC\\nD\\nE\\nF\\nG\\nH\\nI\\nJ\\nK\\nL\\nM\\nN\\nO\\nP\\nQ\\nR\\nS\\nT\\nU\\nV\\nW\\nX\\nY\\nZ\\n#\\n\\nLogin Register\\n\\nTechTarget Network\\nTech Accelerator\\nNews\\n2024 IT Salary Survey Results\\n\\nRSS\\n\\n\\nWhatIs\\n\\n\\nBrowse Definitions Data analytics and AI\\nTopics View All\\n\\nBusiness software\\nCloud computing\\nComputer science\\nData centers\\nIT management\\nNetworking\\nSecurity\\nSoftware development\\n\\nPlease select a category\\n\\nTopics\\n\\n\\n\\nBrowse Features Resources\\n\\nBusiness strategies\\nCareer resources\\nEmerging tech\\nTech explainers\\n\\n\\n\\nFollow:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\n\\nData analytics and AI\\n\\nTech Accelerator What is Gen AI? Generative AI explained\\nPrev Next Will AI replace jobs? 17 job types that might be affected Pros and cons of AI-generated content\\nDownload this guide1\\nFeature\\n25 of the best large language models in 2025\\nLarge language models have been affecting search for years and have been brought to the forefront by ChatGPT and other chatbots.\\n\\nShare this item with your network:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy\\n\\nSean Michael Kerner\\nBen Lutkevich, Site Editor\\n\\nPublished: 31 Jan 2025\\nLarge language models are the dynamite behind the\\xa0generative AI\\xa0boom. However, they\\'ve been around for a while.\\nLLMs\\xa0are black box AI systems that use deep learning on extremely large datasets to understand and generate new text. Modern LLMs began taking shape in 2014 when the attention mechanism -- a machine learning technique designed to mimic human cognitive attention -- was introduced in a\\xa0research paper\\xa0titled \"Neural Machine Translation by Jointly Learning to Align and Translate.\" In 2017, that attention mechanism was honed with the introduction of the transformer model in another\\xa0paper, \"Attention Is All You Need.\"\\nSome of the most well-known language models today are based on the transformer model, including the\\xa0generative pre-trained transformer series\\xa0of LLMs and bidirectional encoder representations from transformers (BERT).\\nChatGPT, which runs on a set of language models from OpenAI, attracted more than 100 million users just two months after its release in 2022. Since then, many competing models have been released. Some belong to big companies such as Google, Amazon and Microsoft; others are open source.\\nConstant developments in the field can be difficult to keep track of. Here are some of the most influential models, both past and present. Included in it are models that paved the way for today\\'s leaders as well as those that could have a significant effect in the future.\\nThis article is part of\\nWhat is Gen AI? Generative AI explained\\n\\nWhich also includes:\\n8 top generative AI tool categories for 2025\\nWill AI replace jobs? 17 job types that might be affected\\n25 of the best large language models in 2025\\n\\nTop current LLMs\\nBelow are some of the most relevant large language models today. They do natural language processing and influence the architecture of future models.\\nBERT\\nBERT\\xa0is a family of LLMs that Google introduced in 2018. BERT is a\\xa0transformer-based\\xa0model that can convert sequences of data to other sequences of data. BERT\\'s architecture is a stack of transformer encoders and features 342 million parameters. BERT was pre-trained on a large corpus of data then fine-tuned to perform specific tasks along with natural language inference and sentence text similarity. It was used to improve query understanding in the 2019 iteration of Google search.\\nClaude\\nThe\\xa0Claude LLM\\xa0focuses on constitutional AI, which shapes AI outputs guided by a set of principles that help the AI assistant it powers helpful, harmless and accurate. Claude was created by the company Anthropic.\\nThere are three primary branches of Claude -- Opus, Haiku and Sonnet. The latest iteration of the Claude LLM is the Claude 3.5 Sonnet. It understands nuance, humor and complex instructions better than earlier versions of the LLM. It also has broad programming capabilities that make it well-suited for application development. In October 2024, Claude added a computer-use AI tool, that enables the LLM to use a computer like a human does. It\\'s available via Claude.ai, the Claude iOS app and through an API.\\nCohere\\nCohere is an enterprise AI platform that provides several LLMs including Command, Rerank and Embed. These\\xa0LLMs can be custom-trained\\xa0and fine-tuned to a specific company\\'s use case. The company that created the Cohere LLM was founded by one of the authors of Attention Is All You Need.\\nDeepSeek-R1\\nDeepSeek-R1 is an open-source reasoning model for tasks with complex reasoning, mathematical problem-solving and logical inference. The model uses reinforcement learning techniques to refine its reasoning ability and solve complex problems. DeepSeek-R1 can perform critical problem-solving through self-verification, chain-of-thought reasoning and reflection.\\nErnie\\nErnie is Baidu\\'s large language model which powers the Ernie 4.0 chatbot. The bot was released in August 2023 and has garnered more than 45 million users. Ernie is rumored to have 10 trillion parameters. The bot works best in Mandarin but is capable in other languages.\\nFalcon\\nFalcon is a family of transformer-based models developed by the Technology Innovation Institute. It is open source and has multi-lingual capabilities. Falcon 2 is available in an 11 billion parameter version that provide multimodal capabilities for both text and vision.\\nThe Falcon 1 series includes a pair of larger models with Falcon 40B and Falcon 180B. Falcon models are available on GitHub as well as on cloud provider including Amazon.\\nGemini\\nGemini\\xa0is Google\\'s family of LLMs that power the company\\'s chatbot of the same name. The model replaced Palm in powering the chatbot, which was rebranded from Bard to Gemini upon the model switch. Gemini models are multimodal, meaning they can handle images, audio and video as well as text. Gemini is also integrated in many Google applications and products. It comes in three sizes -- Ultra, Pro and Nano. Ultra is the largest and most capable model, Pro is the mid-tier model and Nano is the smallest model, designed for efficiency with on-device tasks.\\nAmong the most recent models is the Gemini 1.5 Pro update that debuted in May 2024 Gemini is available as a web chatbot, the Google Vertex AI service and via API. Early previews of Gemini 2.0 Flash became available in December 2024 with updated multimodal generation capabilities.\\nGemma\\nGemma\\xa0is a family of open-source language models from Google that were trained on the same resources as Gemini. Gemma 2 was released in June 2024 in two sizes -- a 9 billion parameter model and a 27 billion parameter model. Gemma models can be\\xa0run locally\\xa0on a personal computer, and are also available in Google Vertex AI.\\nGPT-3\\nGPT-3\\xa0is OpenAI\\'s large language model with more than 175 billion parameters, released in 2020. GPT-3 uses a decoder-only transformer architecture. In September 2022, Microsoft announced it had exclusive use of GPT-3\\'s underlying model. GPT-3 is 10 times larger than its predecessor. GPT-3\\'s training data includes Common Crawl, WebText2, Books1, Books2 and Wikipedia.\\nGPT-3 is the last of the GPT series of models in which OpenAI made the parameter counts publicly available. The GPT series was first introduced in 2018 with OpenAI\\'s paper \"Improving Language Understanding by Generative Pre-Training.\"\\nGPT-3.5\\nGPT-3.5 is an upgraded version of GPT-3 with fewer parameters. GPT-3.5 was fine-tuned using\\xa0reinforcement learning from human feedback. GPT-3.5 is the version of GPT that powers ChatGPT. There are several models, with GPT-3.5 turbo being the most capable, according to OpenAI. GPT-3.5\\'s training data extends to September 2021.\\nIt was also integrated into the Bing search engine but has since been replaced with GPT-4.\\nGPT-4\\nGPT-4\\xa0, was released in 2023 and like the others in the OpenAI GPT family, it\\'s a\\xa0transformer-based model. Unlike the others, its parameter count has not been released to the public, though there are rumors that the model has more than 170 trillion. OpenAI describes GPT-4 as a multimodal model, meaning it can\\xa0process and generate both language and images\\xa0as opposed to being limited to only language. GPT-4 also introduced a system message, which lets users specify tone of voice and task.\\nGPT-4 demonstrated human-level performance in multiple academic exams. At the model\\'s release, some speculated that GPT-4 came close to\\xa0artificial general intelligence, which means it is as smart or smarter than a human. That speculation turned out to be unfounded.\\nGPT-4o\\nGPT-4 Omni (GPT-4o) is OpenAI\\'s successor to GPT-4 and offers several improvements over the previous model. GPT-4o creates a more natural human interaction for ChatGPT and is a large multimodal model, accepting various inputs including audio, image and text. The conversations let users engage as they would in a normal human conversation, and the real-time interactivity can also pick up on emotions. GPT-4o can see photos or screens and ask questions about them during interaction.\\nGPT-4o can respond in 232 milliseconds, similar to human response time and faster than GPT-4 Turbo.\\nGranite\\nThe IBM Granite family of models are fully open source models under the Apache v.2 license. The first iteration of the open source model models debuted in May 2024, followed by Granite 3.0 in October and Granite 3.1 in December 2024.\\nThere are multiple variants in the Granite model family including General-purpose models (8B and 2B variants), guardrail model and Mixture-of-Experts models. While the model can be used for general purpose deployments, IBM itself is focusing deployment and optimization for enterprise use cases like customer service, IT automation and cybersecurity.\\nLamda\\nLamda (Language Model for Dialogue Applications) is a family of LLMs developed by Google Brain announced in 2021. Lamda used a decoder-only transformer language model and was pre-trained on a large corpus of text. In 2022, LaMDA gained widespread attention when then-Google engineer Blake Lemoine went public with claims that the\\xa0program was sentient. It was built on the Seq2Seq architecture.\\nLlama\\nLarge Language Model Meta AI (Llama) is Meta\\'s LLM which was first released in 2023. The Llama 3.1 models were released in July 2024, including both a 405 billion and 70 billion parameter model.\\nThe most recent version is Llama 3.2 which was released in September 2024, initially with smaller parameter counts of 11 billion and 90 billion.\\nLlama uses a transformer architecture and was trained on a variety of public data sources, including webpages from CommonCrawl, GitHub, Wikipedia and Project Gutenberg. Llama was effectively leaked and spawned many descendants, including Vicuna and Orca. Llama is available under an open license, allowing for free use of the models. Lllama models are available in many locations including llama.com and Hugging Face.\\nMistral\\nMistral is a family of a mixture of expert models from Mistral AI. Among the newest models is Mistral Large 2 which was first released in July 2024. The model operates with 123 billion parameters and a 128k context window, supporting dozens of languages including French, German, Spanish, Italian, and many others, along with more than 80 coding languages.\\nIn November 2024, Mistral released Pixtral Large, a 124-billion-parameter multimodal model that can handle text and visual data. Mistral models are available via Mistral\\'s API on its Le Platforme-managed web service.\\no1\\nThe OpenAI o1 model family was first introduced in Sept. 2024. The o1 model\\'s focus is to provide what OpenAI refers to as - reasoning models, that can reason through a problem or query before offering a response.\\nThe o1 models excel in STEM fields, with strong results in mathematical reasoning (scoring 83% on the International Mathematics Olympiad compared to GPT-4o\\'s 13%), code generation and scientific research tasks. While they offer enhanced reasoning and improved safety features, they operate more slowly than previous models due to their thorough reasoning processes and come with certain limitations, such as restricted access features and higher API costs. The models are available to ChatGPT Plus and Team users, with varying access levels for different user categories.\\no3\\nOpenAI introduced the successor model, o3, in December 2024. According to OpenAI, o3 is designed to handle tasks with more analytical thinking, problem-solving and complex reasoning and will improve o1\\'s capabilities and performance. The o3 model is in safety testing mode and is currently not available to the public.\\nOrca\\nOrca was developed by Microsoft and has 13 billion parameters, meaning it\\'s small enough to run on a laptop. It aims to improve on advancements made by other open source models by imitating the reasoning procedures achieved by LLMs. Orca achieves the same performance as GPT-4 with significantly fewer parameters and is on par with GPT-3.5 for many tasks. Orca is built on top of the 13 billion parameter version of Llama.\\nPalm\\nThe\\xa0Pathways Language Model\\xa0is a 540 billion parameter transformer-based model from Google powering its AI chatbot\\xa0Bard. It was trained across multiple\\xa0TPU\\xa04 Pods -- Google\\'s custom hardware for machine learning. Palm specializes in reasoning tasks such as coding, math, classification and question answering. Palm also excels at decomposing complex tasks into simpler subtasks.\\nPaLM gets its name from a Google research initiative to build Pathways, ultimately creating a single model that serves as a foundation for multiple use cases. There are\\xa0several fine-tuned versions\\xa0of Palm, including Med-Palm 2 for life sciences and medical information as well as Sec-Palm for cybersecurity deployments to speed up threat analysis.\\nPhi\\nPhi is a transformer-based language model from Microsoft. The Phi 3.5 models were first released in August 2024.\\nThe series includes Phi-3.5-mini-instruct (3.82 billion parameters), Phi-3.5-MoE-instruct (41.9 billion parameters), and Phi-3.5-vision-instruct (4.15 billion parameters), each designed for specific tasks ranging from basic reasoning to vision analysis. All three models support a 128k token context length.\\nReleased under a Microsoft-branded MIT License, they are available for developers to download, use, and modify without restrictions, including for commercial purposes.\\nQwen\\nQwen is large family of open models developed by Chinese internet giant Alibaba Cloud. The newest set of models are the Qwen2.5 suite, which support 29 different languages and currently scale up to 72 billion parameters. These models are suitable for a wide range of tasks, including code generation, structured data understanding, mathematical problem-solving as well as general language understanding and generation.\\nStableLM\\nStableLM is a series of open language models developed by Stability AI, the company behind image generator Stable Diffusion.\\nStableLM 2 debuted in January 2024 initially with a 1.6 billion parameter model. In April 2024 that was expanded to also include a 12 billion parameter model. StableLM 2 supports seven languages: English, Spanish, German, Italian, French, Portuguese, and Dutch. Stability AI positions these models as offering different options for various use cases, with the 1.6B model suitable for specific, narrow tasks and faster processing while the 12B model provides more capability but requires more computational resources.\\nTülu 3\\nAllen Institute for AI\\'s Tülu 3 is an open-source 405 billion-parameter LLM. The Tülu 3 405B model has post-training methods that combine supervised fine-tuning and reinforcement learning at a larger scale. Tülu 3 uses a \"reinforcement learning from verifiable rewards\" framework for fine-tuning tasks with verifiable outcomes -- such as solving mathematical problems and following instructions.\\nVicuna 33B\\nVicuna is another influential open source LLM derived from Llama. It was developed by LMSYS and was fine-tuned using data from sharegpt.com. It is smaller and less capable that GPT-4 according to several benchmarks, but does well for a model of its size. Vicuna has only 33 billion parameters, whereas GPT-4 has trillions.\\nLLM precursors\\nAlthough LLMs are a recent phenomenon, their precursors go back decades. Learn how recent precursor Seq2Seq and distant precursor ELIZA set the stage for modern LLMs.\\nSeq2Seq\\nSeq2Seq is a deep learning approach used for machine translation, image captioning and natural language processing. It was developed by Google and underlies some of their modern LLMs, including LaMDA. Seq2Seq also underlies AlexaTM 20B, Amazon\\'s large language model. It uses a mix of encoders and decoders.\\nEliza\\nEliza was an\\xa0early natural language processing program\\xa0created in 1966. It is one of the earliest examples of a language model. Eliza simulated conversation using pattern matching and substitution. Eliza, running a certain script, could parody the interaction between a patient and therapist by applying weights to certain keywords and responding to the user accordingly. The creator of Eliza, Joshua Weizenbaum, wrote a book on the limits of computation and artificial intelligence.\\nNext Steps\\nGenerative AI challenges that businesses should consider\\nGenerative AI ethics: Biggest concerns\\nGenerative AI landscape: Potential future trends\\nGenerative models: VAEs, GANs, diffusion, transformers, NeRFs\\nAI content generators to explore\\nRelated Resources\\n\\nFive data quality trends to prepare for in the year ahead –Video\\nThe Digital Transformation And Innovation Landscape –Wipro\\nCloudera and NVIDIA Accelerate AI in the Financial Services Industry –Cloudera\\nImprove customer satisfaction or cut costs? Who says you have to choose? –Video\\n\\nDig Deeper on Data analytics and AI\\n\\n ##### What is GPT-3? Everything you need to know  By: Nick Barney\\n ##### What is a small language model (SLM)?  By: Sean Kerner\\n ##### GPT-4  By: Ben Lutkevich\\n ##### What are large language models (LLMs)?  By: Sean Kerner\\n\\nSponsored News\\n\\nSustainability, AI and Dell PowerEdge Servers –Dell Technologies and Intel\\nThree Innovative AI Use Cases for Natural Language Processing –Dell Technologies\\nAutonomous coding: The future of the revenue cycle –Solventum\\n\\nRelated Content\\n\\nExploring GPT-3 architecture – Search Enterprise AI\\nWhat is GPT-3? Everything you need to know – Search Enterprise AI\\nMicrosoft exclusively licenses OpenAI\\'s GPT-3 ... – Search Enterprise AI\\n\\nLatest TechTarget resources\\n\\nNetworking\\nSecurity\\nCIO\\nHR Software\\nCustomer Experience\\n\\nSearch Networking\\n\\n\\nWhat is a thin client (lean client)?A thin client (lean client) is a virtual desktop computing model that runs on the resources stored on a central server instead of...\\n\\n\\nWhat is network monitoring?Network monitoring, also frequently called network management, is the practice of consistently overseeing a computer network for ...\\n\\n\\nWhat is network automation?Network automation is a process that uses intelligent software to automate the management, configuration, deployment, testing and...\\n\\n\\nSearch Security\\n\\n\\nWhat is Internet Key Exchange (IKE)?Internet Key Exchange (IKE) is a standard protocol used to set up a secure and authenticated communication channel between two ...\\n\\n\\nWhat is a certificate revocation list (CRL) and how is it used?A certificate revocation list (CRL) is a list of digital certificates that have been revoked by the issuing certificate authority...\\n\\n\\nWhat is cryptology?Cryptology is the mathematics, such as number theory and the application of formulas and algorithms, that underpin cryptography ...\\n\\n\\nSearch CIO\\n\\n\\nWhat is an IT project manager?An IT project manager is a professional charged with overseeing the process of planning, executing and delegating ...\\n\\n\\nWhat is a cyberthreat hunter (cybersecurity threat analyst)?A cyberthreat hunter, also called a cybersecurity threat analyst, proactively identifies security incidents that might go ...\\n\\n\\nWhat is blockchain? Definition, examples and how it worksBlockchain is a distributed ledger technology (DLT) that\\'s shared across a network of computers to keep a digital record of ...\\n\\n\\nSearch HRSoftware\\n\\n\\nWhat is employee self-service (ESS)?Employee self-service (ESS) is a widely used human resources technology that enables employees to perform many job-related ...\\n\\n\\nWhat is DEI? Diversity, equity and inclusion explainedDiversity, equity and inclusion is a term used to describe policies and programs that promote the representation and ...\\n\\n\\nWhat is payroll software?Payroll software automates the process of paying salaried, hourly and contingent employees.\\n\\n\\nSearch Customer Experience\\n\\n\\nWhat is account-based selling? Everything you need to knowAccount-based selling (ABS) is a strategic sales approach in business-to-business sales and marketing that centers around ...\\n\\n\\nWhat is interactive voice response (IVR)?Interactive voice response (IVR) is an automated telephony system that interacts with callers, gathers information and routes ...\\n\\n\\nWhat is an AI assistant?An AI assistant, or digital assistant, is software that uses artificial intelligence to understand natural language voice ...\\n\\n\\nBrowse by Topic\\n\\n\\nBrowse Resources\\n\\n\\nAbout Us\\n\\nMeet The Editors\\nEditorial Ethics Policy\\nContact Us\\nAdvertisers\\nBusiness Partners\\nEvents\\nMedia Kit\\nCorporate Site\\nReprints\\n\\nAll Rights Reserved, Copyright 1999 - 2025, TechTarget  \\nPrivacy Policy\\nCookie Preferences\\nCookie Preferences\\nDo Not Sell or Share My Personal Information\\nClose\\n\\nX\\nFree Download What is generative AI? Everything you need to know\\nThe potential of AI technology has been percolating in the background for years. But when ChatGPT, the AI chatbot, began grabbing headlines in early 2023, it put generative AI in the spotlight. This guide is your go-to manual for generative AI, covering its benefits, limits, use cases, prospects and much more.\\n'},\n",
              "  {'url': 'https://www.ki-company.ai/en/blog-beitraege/the-5-best-open-source-ai-models-in-2025',\n",
              "   'title': 'The 5 best open-source AI models 2025 — DeepSeek leads',\n",
              "   'content': 'The 5 best open-source AI models 2025 — DeepSeek leads The 5 best open-source AI models in 2025 Open-source large language models (LLMs) are driving the democratization of artificial intelligence and have established themselves as an essential part of technological development. that Falcon 2 model, developed by the Technology Innovation Institute (TII) in Abu Dhabi, is setting new standards in the open-source AI world. Open-source AI models promote innovation, transparency, and accessibility. As Meta, Hugging Face, and others continue to develop impressive models, DeepSeek is sending a strong signal for the future of open-source AI. Die KI Company helps you to tap into the potential of modern AI models — contact us today for a non-binding consultation!',\n",
              "   'score': 0.7023877,\n",
              "   'raw_content': \"By clicking â\\x80\\x9cAcceptâ\\x80\\x9d, you agree to the storage of cookies on your device to improve navigation on the website and ensure the maximum user experience. For more information, please see our privacy policy and cookie policy.\\nSave time and improve quality with AI.\\nSave time and improve quality with AI.\\nIndividual AI training for your company.\\nTailored AI advice for your company.\\nAutomated completion of simple repetitive tasks.\\nDiscover our vision and the people behind it.\\nInterested in becoming part of our team?\\nTauchenÂ\\xa0Sie ein in die Welt der kÃ¼nstlichen Intelligenz als Videopodcast.\\nDiscover details of the â\\x80\\x9cSuccess through AIâ\\x80\\x9d event series.\\nDiscover the possibilities of artificial intelligence.\\nFind out about the latest topics related to AI.\\nFind out about the latest topics related to AI.\\nContent & scope of process optimization.\\nContent & scope of AI consulting.\\nContent & scope of automations.\\nThe 5 best open-source AI models in 2025 \\nOpen-source large language models (LLMs) are driving the democratization of artificial intelligence and have established themselves as an essential part of technological development. They enable researchers, developers, and enthusiasts to drive innovation by accessing AI in an open and transparent way.\\nIn 2025, there are a number of impressive open-source LLMs. This article presents the five best models that not only stand out technically but are also making waves in the AI community. The list is led by DeepSeek, the Chinese model that has caused a stir all over the world.\\nâ\\x80\\x8d\\n1. DeepSeek R1: China's open-source AI as a game changer\\nDeepSeek R1, the flagship model from Chinese startup DeepSeek, established itself as the most notable open-source LLM in 2025. With its combination of efficiency, performance and cost-effective operation, it stands out from the competition.\\nDeepSeek R1 shows weaknesses in politically sensitive issues, particularly when they concern China. Yet it remains a milestone in AI development.\\n2. Llama 3: Meta's powerful open-source GPT\\nLlama 3, developed by Meta, continues the success story of the Llama series and is considered one of the most versatile open source models of the year.\\nMeta is already planning larger, multimodal versions with over 400 billion parameters, which should further push the limits of open-source AI.\\nâ\\x80\\x8d\\n3. BLOOM: Multilingual open-source AI from Hugging Face\\nThe BLOOM project, initiated by Hugging Face, remains a milestone in AI development. BLOOM was developed in collaboration with over 1,000 researchers from 70 countries and focuses on maximum openness.\\nBLOOM focuses on ethical AI development and is licensed to restrict malicious applications.\\nâ\\x80\\x8d\\n4. Falcon 2: Multimodal excellence from Abu Dhabi\\nthat Falcon 2 model, developed by the Technology Innovation Institute (TII) in Abu Dhabi, is setting new standards in the open-source AI world.\\nâ\\x80\\x8d\\n5. Vicuna-13B: Open Source Champion for Dialogues\\nVicuna-13B, developed by LMSYS ORG, was specifically optimized for dialog-based applications and is considered one of the best open-source models in this area.\\nWhy are open source models so important?\\nOpen-source AI models promote innovation, transparency, and accessibility. They enable researchers and developers to:\\nIn 2025, it becomes apparent that open-source LLMs play a key role in democratizing AI and driving competition with proprietary systems.\\nâ\\x80\\x8d\\nConclusion: DeepSeek sets new standards\\nThe world of open-source AI is diverse and dynamic, yet DeepSeek R1 Clearly stands out. With its efficiency, performance and accessibility, the model shows how artificial intelligence can be made accessible not only to a few but to many.\\nAs Meta, Hugging Face, and others continue to develop impressive models, DeepSeek is sending a strong signal for the future of open-source AI.\\nDie KI Company helps you to tap into the potential of modern AI models â\\x80\\x94 contact us today for a non-binding consultation!\\nBereit bessere Ergebnisse mit ChatGPTÂ\\xa0&Â\\xa0Co. zu erzielen?Â\\xa0Jetzt Prompting-Guide herunterladen und QualitÃ¤t der KI-Ergebnisse steigern.\\n\\nMore articles from our AI blog\\nDiscover more insights into the fascinating world of artificial intelligence.\\nThe 5 best open-source AI models in 2025 \\nWhich open-source AI models will dominate 2025? Find out why DeepSeek R1 tops the list and which other models are convincing.\\nStargate: How Donald Trump's AI project is changing the tech world\\nâ\\x80\\x9cStargateâ\\x80\\x9d: Trump's AI project with 500 billion dollars of investment is causing debate. Find out what it means for the AI world.\\nWhat is DeepSeek? Everything you need to know about the Chinese AI startup\\nDeepSeek R1 is an AI model from China that is revolutionizing the industry with low costs and high efficiency. Find out everything here!\\n\"},\n",
              "  {'url': 'https://www.shakudo.io/blog/top-9-large-language-models',\n",
              "   'title': 'Top 9 Large Language Models as of Feburary 2025 - Shakudo',\n",
              "   'content': 'The latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. The latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. The latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data.',\n",
              "   'score': 0.69274646,\n",
              "   'raw_content': 'Top 9 Large Language Models as of Feburary 2025 | Shakudo\\nLatest in Insights : When to Choose Deep Learning Over Machine Learning (And Vice Versa)\\n\\n\\nWhy SHakudo\\n\\n Data & AI OS Build your ideal data stack on one unified platform Learn more >\\nshakudo AI Applications\\n Text to SQL Workflow Automation Vector Database Reverse ETLSee all >\\nComponents\\nSolutions\\n\\nShakudo for Industries\\nAerospace\\nAutomotive & Transportation\\nClimate & Energy\\nFinancial Services\\nHealthcare & Life Sciences\\nHigher Education\\nManufacturing\\nReal Estate\\nRetail\\nSports\\nTechnology & Software\\nShakudo Use Cases\\nAutomate Custom Sustainability Report Population\\nChat with Enterprise Knowledge Base Using AI Assistants\\nGenerate Real-World Evidence for Healthcare Decisions\\nOptimize Ticket Pricing with Dynamic Demand Modeling\\nDetect Hidden Red Flags in Company Data\\nMonitor Market Sentiment Across Multiple Sources\\nSee all >\\nResources\\n\\n Case Studies Learn how leading companies leverage data & AI on Shakudo blog Read what\\'s new at Shakudo and the data and AI world white papers Access in-depth reports and guides on data & AI solutions Docs Explore comprehensive guides on the Shakudo platform\\n  Case Study How CentralReach uses Shakudo to Cut Time-To-Deployment to Launch New AI- Powered Solutions\\n  Case Study How AI is Changing the Game for the Cleveland Cavaliers\\nCompany\\n\\n ABout Us Learn about our mission and values Careers Join us in building the next-gen data stack Partners Learn about the relationships that make it happen Contact us Have a question? We\\'re here to help\\nAI WorkshopGet a Demo\\n\\n← Back to Blog\\nInsights\\nTop 9 Large Language Models as of Feburary 2025\\nAuthor(s):\\n\\nNo items found.\\nUpdated on:\\nFebruary 7, 2025\\n\\nTable of contents\\nExample H2\\nExample H3\\nMentioned Components\\nNo items found.\\n<>\\nGet the latest updates in Data & AI straight to your inboxWe’ll email you once per week—and never share your information.\\n🎉 Success! You\\'re now signed up for the Shakudo newsletter.\\nOops! Something went wrong while submitting the form.\\nIntroduction\\nIf we had to choose one word to describe the rapid evolution of AI today, it would probably be something along the lines of explosive. As predicted by the Market Research Future report, the large language model (LLM) market in North America alone is expected to reach $105.5 billion by 2030. The exponential growth of AI tools combined with access to massive troves of text data has opened gates for better and more advanced content generation than we had ever hoped. Yet, such rapid expansion also makes it harder than ever to navigate and select the right tools among the diverse LLM models available.\\nThe goal of this post is to keep you, the AI enthusiast and professional, up-to-date with current trends and essential innovations in the field. Below, we highlighted the top 9 LLMs that we think are currently making waves in the industry, each with distinct capabilities and specialized strengths, excelling in areas such as natural language processing, code synthesis, few-shot learning, or scalability. While we believe there is no one-size-fits-all LLM for every use case, we hope that this list can help you identify the most current and well-suited LLM model that meets your business’s unique requirements.\\n1. GPT\\n\\nOur list kicks off with OpenAI\\'s Generative Pre-trained Transformer (GPT) models, which have consistently exceeded their previous capabilities with each new release. Compared to its prior models, the latest ChatGPT-4o and ChatGPT-4o mini models offer significantly faster processing speeds and enhanced capabilities across text, voice, and vision.\\nThe latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. Both of these models are equipped with multimodal capabilities to handle images as well as audio data.\\nDespite having advanced conversational and reasoning capabilities, note that GPT is a proprietary model, meaning that the training data and parameters are kept confidential by OpenAI, and access to full functionality is restricted–a commercial license or subscription is often required to unlock the complete range of features. In this case, we recommend this model for businesses looking to adopt an LLM that excels in conversational dialogue, multi-step reasoning, efficient computation, and real-time interactions without the constraints of a budget.\\nFor companies who are curious to try out the proprietary models on the market before fully committing to one due to budget constraints or uncertainties about its long-term integration, Shakudo offers a compelling alternative. Our platform currently features a diverse selection of advanced LLMs with simplified deployment and scalability. With a simple subscription, you can access and assess the value of proprietary models, like GPT, before making a substantial investment.\\n2.DeepSeek\\n\\nDeepseek-R1 Benchmark. Source: deepseek.com\\nWith its latest R1 model, the Chinese AI company DeepSeek has once again set new benchmarks for innovation in the AI community. As of January 24th, the DeepSeek-R1 model is ranked fourth on Chatbot Arena, and top as the best open-source LM.\\nThe DeepSeek-R1 is a 671B parameter Mixture-of-Experts (MoE) model with 37B activated parameters per token, trained through large-scale reinforcement learning with a strong focus on reasoning capabilities. The model excels at understanding and handling long-form content and demonstrates superior performance in complex tasks such as mathematics and code generation. The model is approximately 30 times more cost-efficient than OpenAI-o1 and 5 times faster, offering groundbreaking performance at a fraction of the cost. Moreover, it has shown exceptional precision in tasks requiring complex pattern recognition, such as genomic data analysis, medical imaging, and large-scale scientific simulations.\\nDeepSeek-R1’s capabilities are transformative when it comes to integration with proprietary enterprise data such as PII and financial records. Leveraging retrieval-augmented generation (RAG), enterprises can connect the model to their internal data sources to enable highly personalized, context-aware interactions—all while maintaining stringent security and compliance standards. With Shakudo, you can streamline the deployment and integration of advanced AI models like DeepSeek by automating the setup, deployment, and management processes. This eliminates the need for businesses to invest in and maintain extensive computing infrastructure. By operating within your existing infrastructure, the platform ensures seamless integration, enhanced security, and optimal performance without requiring significant in-house resources or specialized expertise.\\n3. Qwen\\n\\n\\u200d\\nAlibaba released Qwen2.5-Max earlier last month, the new model designed to deliver enhanced performance for large-scale natural language processing tasks. In instruct model evaluations, this new model outperforms DeepSeek V3 in benchmarks such as Arena-Hard, LiveBench, LiveCodeBench, and GPQA-Diamond, while also delivering strong performance in other assessments like MMLU-Pro.\\nQwen2.5-Max is pretrained on over 20 trillion tokens. While specific details about its parameter count and token window size are not publicly disclosed, Qwen2.5-Max is designed for low-latency, high-efficiency tasks, making it suitable for applications requiring quick, accurate responses with low latency. Its smaller size allows for deployment on devices with limited computational resources.\\nFor businesses and users looking for higher-performance models for natural language processing and AI-driven tasks, the Qwen 2.5 model is available on platforms like Hugging Face and ModelScope. This model boasts from 0.5 billion to 72 billion parameters, featuring context windows of up to 128,000 tokens, and is excellent for code generation, debugging, and automated forecasting.\\n4. LG AI\\n\\nsource\\nEXAONE 3.0 is a bilingual LLM developed by LG AI Research. The company released its latest model in December 2024 with excellent performance across various benchmarks and real-world applications.\\nWith 7.8 billion parameters, EXAONE 3.0 is capable of understanding and generating human-like text in multiple languages across complex domains, including coding, mathematics, patents, and chemistry. This model has also been optimized to reduce inference processing time by 56%, memory usage by 35%, and operating costs by 72%, ensuring that it remains cost-effective while maintaining high performance.\\nLG AI Research has open-sourced the 7.8B parameter instruction-tuned version of EXAONE 3.0 for non-commercial research purposes. This is a model we recommend to software companies or tech startups for generating Python code, assisting developers in troubleshooting, and creating APIs or other backend components.\\n5. LlaMA\\n\\nMeta is still leading the front with their state-of-the-art LlaMa models. The company released its latest LlaMA 3.3 model in December 2024, featuring multimodal capabilities that can process both text and image for in-depth analysis and response generation, such as interpreting charts, maps, or translating texts identified in an image.\\nLlaMA 3.3 improves on previous models with a longer context window of up to 128,000 tokens and an optimized transformer architecture. With a parameter of 70 billion, this model outperforms open-source and proprietary alternatives in areas such as multilingual dialogue, reasoning, and coding.\\nUnlike ChatGPT models, LlaMA 3 is open-source, giving users the flexibility to access and deploy freely on their cloud depending on the specific requirements of their infrastructure, security preferences, or customization needs. We recommend this model to businesses looking for advanced content generation and language understanding, such as those in customer service, education, marketing, and consumer markets. The openness of these models also allows for your greater control over the model’s performance, tuning, and integration into existing workflows.\\n6. Claude\\n\\nNext on our list is Claude, more specifically, the latest Claude 3.5 Sonnet model developed by Anthropic. We believe that Claude is arguably one of the most significant competitors to GPT since all of its current models–Claude 3 Haiku, Claude 3.5 Sonnet, and Claude 3 Opus–are designed with incredible contextual understanding capabilities that position themselves as the top conversational AI closely aligned with nuanced human interactions.\\nWhile the specific parameters of Claude 3.5 Sonnet remain undisclosed, the model boasts an impressive context window of 200,000 tokens, equivalent to approximately 150,000 words or 300 pages of text.\\nThe current Claude subscription service is credit-based, and the cost can go as high as $2,304/month for enterprise plans tailored to high-volume users. We recommend Claude to mature or mid-stage businesses looking not only to adopt an AI that facilitates human-like interactions but also to enhance their coding capabilities since the Claude 3.5 Sonnet model is currently reaching a 49.0% performance score on the SWE-bench Verified benchmark, placing it as third among all publicly available models, including reasoning models and systems specifically designed for agentic coding.\\n7. Mistral\\n\\nMistral\\'s latest model – Mistral Small 3, a latency-optimized model was released under the Apache 2.0 license at the end of January. This 24-billion-parameter model is designed for low-latency, high-efficiency tasks. It processes approximately 150 tokens per second, making it over three times faster than Llama 3.3 70B on the same hardware.\\nThis new model is ideal for applications requiring quick, accurate responses with low latency, such as virtual assistants, real-time data processing, and on-device command and control. Its smaller size allows for deployment on devices with limited computational resources.\\nMistral Small 3 is currently open-source under the Apache 2.0 license. This means you can freely access and use the model for your own applications, provided you comply with the license terms. Since it is designed to be easily deployable, including on hardware with limited resources like a single GPU or even a MacBook with 32GB RAM, we\\'d recommend this to early-stage businesses looking to implement low-latency AI solutions without the need for extensive hardware infrastructure.\\n8. Gemini\\n\\nGemini is a family of closed-source LLM models developed by Google. The latest model—Gemini 2.0 Flash—operates at twice the speed of Gemini 1.5 Pro, offering substantial improvements in speed, reasoning, and multimodal processing capabilities.\\nWith that being said, Gemini remains a proprietary model; if your company deals with sensitive or confidential data regularly, you might be concerned about sending it to external servers due to security reasons. To address this concern, we recommend that you double-check vendor compliance regulations to ensure data privacy and security standards are met, such as adherence to GDPR, HIPAA, or other relevant data protection laws.\\nIf you’re looking for an open-source alternative that exhibits capabilities almost as good as Gemini, Google’s latest Gemma model, Gemma 2, offers three models available in 2 billion, 9 billion, and 27 billion parameters with a context window of 8,200. For businesses looking for a rather economic option, this is the optimal choice that interprets and understands messages with remarkable accuracy.\\n9. Command\\n\\nCommand R is a family of scalable models developed by Cohere with the goal of balancing high performance with strong accuracy, just like Claude. Both the Command R and Command R+ models offer APIs specifically optimized for Retrieval Augmented Generation (RAG). This means that these models can combine large-scale language generation with real-time information retrieval techniques for much more contextually aware outputs.\\nCurrently, the Command R+ model boasts 104 billion parameters and offers an industry-leading 128,000 token context window for enhanced long-form processing and multi-turn conversation capabilities.\\nOne of the perks of working with an open-source model is also to avoid vendor lock-in. Heavy reliance on a particular type of proprietary model may make it difficult for you to switch to alternative models when your business starts growing or the landscape changes. Cohere approaches this in a hybrid way, meaning that you can access and modify the model for personal usage but need a license for commercial use. In this case, we recommend this model for businesses that want flexibility in experimentation without a long-term commitment to a single vendor.\\n\\nWhitepaper\\nIntroduction\\nIf we had to choose one word to describe the rapid evolution of AI today, it would probably be something along the lines of explosive. As predicted by the Market Research Future report, the large language model (LLM) market in North America alone is expected to reach $105.5 billion by 2030. The exponential growth of AI tools combined with access to massive troves of text data has opened gates for better and more advanced content generation than we had ever hoped. Yet, such rapid expansion also makes it harder than ever to navigate and select the right tools among the diverse LLM models available.\\nThe goal of this post is to keep you, the AI enthusiast and professional, up-to-date with current trends and essential innovations in the field. Below, we highlighted the top 9 LLMs that we think are currently making waves in the industry, each with distinct capabilities and specialized strengths, excelling in areas such as natural language processing, code synthesis, few-shot learning, or scalability. While we believe there is no one-size-fits-all LLM for every use case, we hope that this list can help you identify the most current and well-suited LLM model that meets your business’s unique requirements.\\n1. GPT\\n\\nOur list kicks off with OpenAI\\'s Generative Pre-trained Transformer (GPT) models, which have consistently exceeded their previous capabilities with each new release. Compared to its prior models, the latest ChatGPT-4o and ChatGPT-4o mini models offer significantly faster processing speeds and enhanced capabilities across text, voice, and vision.\\nThe latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. Both of these models are equipped with multimodal capabilities to handle images as well as audio data.\\nDespite having advanced conversational and reasoning capabilities, note that GPT is a proprietary model, meaning that the training data and parameters are kept confidential by OpenAI, and access to full functionality is restricted–a commercial license or subscription is often required to unlock the complete range of features. In this case, we recommend this model for businesses looking to adopt an LLM that excels in conversational dialogue, multi-step reasoning, efficient computation, and real-time interactions without the constraints of a budget.\\nFor companies who are curious to try out the proprietary models on the market before fully committing to one due to budget constraints or uncertainties about its long-term integration, Shakudo offers a compelling alternative. Our platform currently features a diverse selection of advanced LLMs with simplified deployment and scalability. With a simple subscription, you can access and assess the value of proprietary models, like GPT, before making a substantial investment.\\n2.DeepSeek\\n\\nDeepseek-R1 Benchmark. Source: deepseek.com\\nWith its latest R1 model, the Chinese AI company DeepSeek has once again set new benchmarks for innovation in the AI community. As of January 24th, the DeepSeek-R1 model is ranked fourth on Chatbot Arena, and top as the best open-source LM.\\nThe DeepSeek-R1 is a 671B parameter Mixture-of-Experts (MoE) model with 37B activated parameters per token, trained through large-scale reinforcement learning with a strong focus on reasoning capabilities. The model excels at understanding and handling long-form content and demonstrates superior performance in complex tasks such as mathematics and code generation. The model is approximately 30 times more cost-efficient than OpenAI-o1 and 5 times faster, offering groundbreaking performance at a fraction of the cost. Moreover, it has shown exceptional precision in tasks requiring complex pattern recognition, such as genomic data analysis, medical imaging, and large-scale scientific simulations.\\nDeepSeek-R1’s capabilities are transformative when it comes to integration with proprietary enterprise data such as PII and financial records. Leveraging retrieval-augmented generation (RAG), enterprises can connect the model to their internal data sources to enable highly personalized, context-aware interactions—all while maintaining stringent security and compliance standards. With Shakudo, you can streamline the deployment and integration of advanced AI models like DeepSeek by automating the setup, deployment, and management processes. This eliminates the need for businesses to invest in and maintain extensive computing infrastructure. By operating within your existing infrastructure, the platform ensures seamless integration, enhanced security, and optimal performance without requiring significant in-house resources or specialized expertise.\\n3. Qwen\\n\\n\\u200d\\nAlibaba released Qwen2.5-Max earlier last month, the new model designed to deliver enhanced performance for large-scale natural language processing tasks. In instruct model evaluations, this new model outperforms DeepSeek V3 in benchmarks such as Arena-Hard, LiveBench, LiveCodeBench, and GPQA-Diamond, while also delivering strong performance in other assessments like MMLU-Pro.\\nQwen2.5-Max is pretrained on over 20 trillion tokens. While specific details about its parameter count and token window size are not publicly disclosed, Qwen2.5-Max is designed for low-latency, high-efficiency tasks, making it suitable for applications requiring quick, accurate responses with low latency. Its smaller size allows for deployment on devices with limited computational resources.\\nFor businesses and users looking for higher-performance models for natural language processing and AI-driven tasks, the Qwen 2.5 model is available on platforms like Hugging Face and ModelScope. This model boasts from 0.5 billion to 72 billion parameters, featuring context windows of up to 128,000 tokens, and is excellent for code generation, debugging, and automated forecasting.\\n4. LG AI\\n\\nsource\\nEXAONE 3.0 is a bilingual LLM developed by LG AI Research. The company released its latest model in December 2024 with excellent performance across various benchmarks and real-world applications.\\nWith 7.8 billion parameters, EXAONE 3.0 is capable of understanding and generating human-like text in multiple languages across complex domains, including coding, mathematics, patents, and chemistry. This model has also been optimized to reduce inference processing time by 56%, memory usage by 35%, and operating costs by 72%, ensuring that it remains cost-effective while maintaining high performance.\\nLG AI Research has open-sourced the 7.8B parameter instruction-tuned version of EXAONE 3.0 for non-commercial research purposes. This is a model we recommend to software companies or tech startups for generating Python code, assisting developers in troubleshooting, and creating APIs or other backend components.\\n5. LlaMA\\n\\nMeta is still leading the front with their state-of-the-art LlaMa models. The company released its latest LlaMA 3.3 model in December 2024, featuring multimodal capabilities that can process both text and image for in-depth analysis and response generation, such as interpreting charts, maps, or translating texts identified in an image.\\nLlaMA 3.3 improves on previous models with a longer context window of up to 128,000 tokens and an optimized transformer architecture. With a parameter of 70 billion, this model outperforms open-source and proprietary alternatives in areas such as multilingual dialogue, reasoning, and coding.\\nUnlike ChatGPT models, LlaMA 3 is open-source, giving users the flexibility to access and deploy freely on their cloud depending on the specific requirements of their infrastructure, security preferences, or customization needs. We recommend this model to businesses looking for advanced content generation and language understanding, such as those in customer service, education, marketing, and consumer markets. The openness of these models also allows for your greater control over the model’s performance, tuning, and integration into existing workflows.\\n6. Claude\\n\\nNext on our list is Claude, more specifically, the latest Claude 3.5 Sonnet model developed by Anthropic. We believe that Claude is arguably one of the most significant competitors to GPT since all of its current models–Claude 3 Haiku, Claude 3.5 Sonnet, and Claude 3 Opus–are designed with incredible contextual understanding capabilities that position themselves as the top conversational AI closely aligned with nuanced human interactions.\\nWhile the specific parameters of Claude 3.5 Sonnet remain undisclosed, the model boasts an impressive context window of 200,000 tokens, equivalent to approximately 150,000 words or 300 pages of text.\\nThe current Claude subscription service is credit-based, and the cost can go as high as $2,304/month for enterprise plans tailored to high-volume users. We recommend Claude to mature or mid-stage businesses looking not only to adopt an AI that facilitates human-like interactions but also to enhance their coding capabilities since the Claude 3.5 Sonnet model is currently reaching a 49.0% performance score on the SWE-bench Verified benchmark, placing it as third among all publicly available models, including reasoning models and systems specifically designed for agentic coding.\\n7. Mistral\\n\\nMistral\\'s latest model – Mistral Small 3, a latency-optimized model was released under the Apache 2.0 license at the end of January. This 24-billion-parameter model is designed for low-latency, high-efficiency tasks. It processes approximately 150 tokens per second, making it over three times faster than Llama 3.3 70B on the same hardware.\\nThis new model is ideal for applications requiring quick, accurate responses with low latency, such as virtual assistants, real-time data processing, and on-device command and control. Its smaller size allows for deployment on devices with limited computational resources.\\nMistral Small 3 is currently open-source under the Apache 2.0 license. This means you can freely access and use the model for your own applications, provided you comply with the license terms. Since it is designed to be easily deployable, including on hardware with limited resources like a single GPU or even a MacBook with 32GB RAM, we\\'d recommend this to early-stage businesses looking to implement low-latency AI solutions without the need for extensive hardware infrastructure.\\n8. Gemini\\n\\nGemini is a family of closed-source LLM models developed by Google. The latest model—Gemini 2.0 Flash—operates at twice the speed of Gemini 1.5 Pro, offering substantial improvements in speed, reasoning, and multimodal processing capabilities.\\nWith that being said, Gemini remains a proprietary model; if your company deals with sensitive or confidential data regularly, you might be concerned about sending it to external servers due to security reasons. To address this concern, we recommend that you double-check vendor compliance regulations to ensure data privacy and security standards are met, such as adherence to GDPR, HIPAA, or other relevant data protection laws.\\nIf you’re looking for an open-source alternative that exhibits capabilities almost as good as Gemini, Google’s latest Gemma model, Gemma 2, offers three models available in 2 billion, 9 billion, and 27 billion parameters with a context window of 8,200. For businesses looking for a rather economic option, this is the optimal choice that interprets and understands messages with remarkable accuracy.\\n9. Command\\n\\nCommand R is a family of scalable models developed by Cohere with the goal of balancing high performance with strong accuracy, just like Claude. Both the Command R and Command R+ models offer APIs specifically optimized for Retrieval Augmented Generation (RAG). This means that these models can combine large-scale language generation with real-time information retrieval techniques for much more contextually aware outputs.\\nCurrently, the Command R+ model boasts 104 billion parameters and offers an industry-leading 128,000 token context window for enhanced long-form processing and multi-turn conversation capabilities.\\nOne of the perks of working with an open-source model is also to avoid vendor lock-in. Heavy reliance on a particular type of proprietary model may make it difficult for you to switch to alternative models when your business starts growing or the landscape changes. Cohere approaches this in a hybrid way, meaning that you can access and modify the model for personal usage but need a license for commercial use. In this case, we recommend this model for businesses that want flexibility in experimentation without a long-term commitment to a single vendor.\\nGet the whitepaper\\nTop 9 Large Language Models as of Feburary 2025\\nBy clicking \"Download,\" you agree to Shakudo processing your personal data in accordance with its Privacy Notice.\\nThank you for filling out the form. The whitepaper you have requested is available for download below.  \\nDownload White Paper\\nOops! Something went wrong while submitting the form.\\nGet the whitepaper\\nTop 9 Large Language Models as of Feburary 2025\\nThank you for your interest. Click the button below to download whitepaper you have requested.  \\nDownload White Paper\\n\\nTop 9 Large Language Models as of Feburary 2025\\nExplore the top 9 LLMs making waves in the AI world and what each of them excel at\\n\\n| Case Study\\nTop 9 Large Language Models as of Feburary 2025\\n\\nKey results\\nAbout\\nindustry\\nTech Stack\\nNo items found.\\n<>\\nIntroduction\\nIf we had to choose one word to describe the rapid evolution of AI today, it would probably be something along the lines of explosive. As predicted by the Market Research Future report, the large language model (LLM) market in North America alone is expected to reach $105.5 billion by 2030. The exponential growth of AI tools combined with access to massive troves of text data has opened gates for better and more advanced content generation than we had ever hoped. Yet, such rapid expansion also makes it harder than ever to navigate and select the right tools among the diverse LLM models available.\\nThe goal of this post is to keep you, the AI enthusiast and professional, up-to-date with current trends and essential innovations in the field. Below, we highlighted the top 9 LLMs that we think are currently making waves in the industry, each with distinct capabilities and specialized strengths, excelling in areas such as natural language processing, code synthesis, few-shot learning, or scalability. While we believe there is no one-size-fits-all LLM for every use case, we hope that this list can help you identify the most current and well-suited LLM model that meets your business’s unique requirements.\\n1. GPT\\n\\nOur list kicks off with OpenAI\\'s Generative Pre-trained Transformer (GPT) models, which have consistently exceeded their previous capabilities with each new release. Compared to its prior models, the latest ChatGPT-4o and ChatGPT-4o mini models offer significantly faster processing speeds and enhanced capabilities across text, voice, and vision.\\nThe latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. Both of these models are equipped with multimodal capabilities to handle images as well as audio data.\\nDespite having advanced conversational and reasoning capabilities, note that GPT is a proprietary model, meaning that the training data and parameters are kept confidential by OpenAI, and access to full functionality is restricted–a commercial license or subscription is often required to unlock the complete range of features. In this case, we recommend this model for businesses looking to adopt an LLM that excels in conversational dialogue, multi-step reasoning, efficient computation, and real-time interactions without the constraints of a budget.\\nFor companies who are curious to try out the proprietary models on the market before fully committing to one due to budget constraints or uncertainties about its long-term integration, Shakudo offers a compelling alternative. Our platform currently features a diverse selection of advanced LLMs with simplified deployment and scalability. With a simple subscription, you can access and assess the value of proprietary models, like GPT, before making a substantial investment.\\n2.DeepSeek\\n\\nDeepseek-R1 Benchmark. Source: deepseek.com\\nWith its latest R1 model, the Chinese AI company DeepSeek has once again set new benchmarks for innovation in the AI community. As of January 24th, the DeepSeek-R1 model is ranked fourth on Chatbot Arena, and top as the best open-source LM.\\nThe DeepSeek-R1 is a 671B parameter Mixture-of-Experts (MoE) model with 37B activated parameters per token, trained through large-scale reinforcement learning with a strong focus on reasoning capabilities. The model excels at understanding and handling long-form content and demonstrates superior performance in complex tasks such as mathematics and code generation. The model is approximately 30 times more cost-efficient than OpenAI-o1 and 5 times faster, offering groundbreaking performance at a fraction of the cost. Moreover, it has shown exceptional precision in tasks requiring complex pattern recognition, such as genomic data analysis, medical imaging, and large-scale scientific simulations.\\nDeepSeek-R1’s capabilities are transformative when it comes to integration with proprietary enterprise data such as PII and financial records. Leveraging retrieval-augmented generation (RAG), enterprises can connect the model to their internal data sources to enable highly personalized, context-aware interactions—all while maintaining stringent security and compliance standards. With Shakudo, you can streamline the deployment and integration of advanced AI models like DeepSeek by automating the setup, deployment, and management processes. This eliminates the need for businesses to invest in and maintain extensive computing infrastructure. By operating within your existing infrastructure, the platform ensures seamless integration, enhanced security, and optimal performance without requiring significant in-house resources or specialized expertise.\\n3. Qwen\\n\\n\\u200d\\nAlibaba released Qwen2.5-Max earlier last month, the new model designed to deliver enhanced performance for large-scale natural language processing tasks. In instruct model evaluations, this new model outperforms DeepSeek V3 in benchmarks such as Arena-Hard, LiveBench, LiveCodeBench, and GPQA-Diamond, while also delivering strong performance in other assessments like MMLU-Pro.\\nQwen2.5-Max is pretrained on over 20 trillion tokens. While specific details about its parameter count and token window size are not publicly disclosed, Qwen2.5-Max is designed for low-latency, high-efficiency tasks, making it suitable for applications requiring quick, accurate responses with low latency. Its smaller size allows for deployment on devices with limited computational resources.\\nFor businesses and users looking for higher-performance models for natural language processing and AI-driven tasks, the Qwen 2.5 model is available on platforms like Hugging Face and ModelScope. This model boasts from 0.5 billion to 72 billion parameters, featuring context windows of up to 128,000 tokens, and is excellent for code generation, debugging, and automated forecasting.\\n4. LG AI\\n\\nsource\\nEXAONE 3.0 is a bilingual LLM developed by LG AI Research. The company released its latest model in December 2024 with excellent performance across various benchmarks and real-world applications.\\nWith 7.8 billion parameters, EXAONE 3.0 is capable of understanding and generating human-like text in multiple languages across complex domains, including coding, mathematics, patents, and chemistry. This model has also been optimized to reduce inference processing time by 56%, memory usage by 35%, and operating costs by 72%, ensuring that it remains cost-effective while maintaining high performance.\\nLG AI Research has open-sourced the 7.8B parameter instruction-tuned version of EXAONE 3.0 for non-commercial research purposes. This is a model we recommend to software companies or tech startups for generating Python code, assisting developers in troubleshooting, and creating APIs or other backend components.\\n5. LlaMA\\n\\nMeta is still leading the front with their state-of-the-art LlaMa models. The company released its latest LlaMA 3.3 model in December 2024, featuring multimodal capabilities that can process both text and image for in-depth analysis and response generation, such as interpreting charts, maps, or translating texts identified in an image.\\nLlaMA 3.3 improves on previous models with a longer context window of up to 128,000 tokens and an optimized transformer architecture. With a parameter of 70 billion, this model outperforms open-source and proprietary alternatives in areas such as multilingual dialogue, reasoning, and coding.\\nUnlike ChatGPT models, LlaMA 3 is open-source, giving users the flexibility to access and deploy freely on their cloud depending on the specific requirements of their infrastructure, security preferences, or customization needs. We recommend this model to businesses looking for advanced content generation and language understanding, such as those in customer service, education, marketing, and consumer markets. The openness of these models also allows for your greater control over the model’s performance, tuning, and integration into existing workflows.\\n6. Claude\\n\\nNext on our list is Claude, more specifically, the latest Claude 3.5 Sonnet model developed by Anthropic. We believe that Claude is arguably one of the most significant competitors to GPT since all of its current models–Claude 3 Haiku, Claude 3.5 Sonnet, and Claude 3 Opus–are designed with incredible contextual understanding capabilities that position themselves as the top conversational AI closely aligned with nuanced human interactions.\\nWhile the specific parameters of Claude 3.5 Sonnet remain undisclosed, the model boasts an impressive context window of 200,000 tokens, equivalent to approximately 150,000 words or 300 pages of text.\\nThe current Claude subscription service is credit-based, and the cost can go as high as $2,304/month for enterprise plans tailored to high-volume users. We recommend Claude to mature or mid-stage businesses looking not only to adopt an AI that facilitates human-like interactions but also to enhance their coding capabilities since the Claude 3.5 Sonnet model is currently reaching a 49.0% performance score on the SWE-bench Verified benchmark, placing it as third among all publicly available models, including reasoning models and systems specifically designed for agentic coding.\\n7. Mistral\\n\\nMistral\\'s latest model – Mistral Small 3, a latency-optimized model was released under the Apache 2.0 license at the end of January. This 24-billion-parameter model is designed for low-latency, high-efficiency tasks. It processes approximately 150 tokens per second, making it over three times faster than Llama 3.3 70B on the same hardware.\\nThis new model is ideal for applications requiring quick, accurate responses with low latency, such as virtual assistants, real-time data processing, and on-device command and control. Its smaller size allows for deployment on devices with limited computational resources.\\nMistral Small 3 is currently open-source under the Apache 2.0 license. This means you can freely access and use the model for your own applications, provided you comply with the license terms. Since it is designed to be easily deployable, including on hardware with limited resources like a single GPU or even a MacBook with 32GB RAM, we\\'d recommend this to early-stage businesses looking to implement low-latency AI solutions without the need for extensive hardware infrastructure.\\n8. Gemini\\n\\nGemini is a family of closed-source LLM models developed by Google. The latest model—Gemini 2.0 Flash—operates at twice the speed of Gemini 1.5 Pro, offering substantial improvements in speed, reasoning, and multimodal processing capabilities.\\nWith that being said, Gemini remains a proprietary model; if your company deals with sensitive or confidential data regularly, you might be concerned about sending it to external servers due to security reasons. To address this concern, we recommend that you double-check vendor compliance regulations to ensure data privacy and security standards are met, such as adherence to GDPR, HIPAA, or other relevant data protection laws.\\nIf you’re looking for an open-source alternative that exhibits capabilities almost as good as Gemini, Google’s latest Gemma model, Gemma 2, offers three models available in 2 billion, 9 billion, and 27 billion parameters with a context window of 8,200. For businesses looking for a rather economic option, this is the optimal choice that interprets and understands messages with remarkable accuracy.\\n9. Command\\n\\nCommand R is a family of scalable models developed by Cohere with the goal of balancing high performance with strong accuracy, just like Claude. Both the Command R and Command R+ models offer APIs specifically optimized for Retrieval Augmented Generation (RAG). This means that these models can combine large-scale language generation with real-time information retrieval techniques for much more contextually aware outputs.\\nCurrently, the Command R+ model boasts 104 billion parameters and offers an industry-leading 128,000 token context window for enhanced long-form processing and multi-turn conversation capabilities.\\nOne of the perks of working with an open-source model is also to avoid vendor lock-in. Heavy reliance on a particular type of proprietary model may make it difficult for you to switch to alternative models when your business starts growing or the landscape changes. Cohere approaches this in a hybrid way, meaning that you can access and modify the model for personal usage but need a license for commercial use. In this case, we recommend this model for businesses that want flexibility in experimentation without a long-term commitment to a single vendor.\\nExplore more from Shakudo\\n How VPCs Enable AI Deployments with a Modern Data Stack Insights January 28, 2025\\n The Power of Simple Questions: How to Choose the Right Natural Language to SQL Query Tool Insights May 15, 2024\\n Bring Data and AI tooling right to MongoDB Atlas with Shakudo News August 26, 2024\\nTake the next step\\n\"Shakudo gave us the flexibility to use the data stack components that fit our needs and evolve the stack to keep up with the industry.\"\\n\\nNeal Gilmore\\nSenior Vice President, Enterprise Data & Analytics\\nDiscover Shakudo\\n\\nShakudo brings the best data and AI products into your VPC and operates them for you automatically achieving a more reliable, performant, and cost effective data stack than ever before.\\n\\n Book Demo Email X (Twitter) Linkedin Youtube\\nNewsletter\\nSign up for the latest Shakudo news:\\n🎉 Success! You\\'re now signed up for the Shakudo newsletter.\\nOops! Something went wrong while submitting the form.\\nApplications\\nData and AI OSStack ComponentsLanguage to SQLVector Database + LLMReverse ETLWorkflow Automation\\nIndustries\\nAutomotive & Transportation\\nAerospace\\nManufacturing\\nHigher Education\\nHealthcare & Life Sciences\\nClimate & Energy\\nTechnology & Software\\nSports\\nReal Estate\\nRetail\\nFinancial Services\\nResources\\nUse Cases\\nInsights\\nWhite Paper\\nCase Study\\nPress\\nProduct\\nTutorial\\nNews\\nWebinarGlossaryDocumentation\\nCompany\\nAboutPartnersDGX PartnerCareersMedia Kit\\nGet Started\\nSignupContact UsNewsletter\\n© 2025 Shakudo\\nToronto, CA\\nContact usPrivacy PolicyTerms/ConditionsSitemap\\nTrusted by industry leaders\\n\\n\\n\\n\\n\\n\\nSee Shakudo in Action  \\nWatch the 3 Minute Demo\\n\\nThis field is required\\n\\nFor information about how Shakudo handles your personal data, please see our Privacy Policy.\\nThank you for your submission. A Shakudo expert will be in touch with you shortly.  \\nIn the meantime, feel free to check out our data insights, case studies, and latest industry news that help data teams win.  \\n Live chat Live chat will provide the quickest answer to any of your questions.\\nOops! Something went wrong while submitting the form.\\n⨉'},\n",
              "  {'url': 'https://medium.com/@ayeshasidhikha188/explore-the-leading-llms-of-2025-2428f7068d40',\n",
              "   'title': 'Explore the Leading LLMs of 2025 - Medium',\n",
              "   'content': 'Must Know the Leading LLMs of 2025 | by Ayesha sidhikha | Feb, 2025 | Medium Must Know the Leading LLMs of 2025 By 2025, an estimated 90% of companies will integrate language models into their daily operations. In this post, we’ll dive into the evolution of large language models (LLMs), highlight the best available options in early 2025, and explore how these powerful tools can transform the way you work. Today’s LLMs are far more than text generators — they are comprehensive systems that help you achieve goals, enhance productivity, and even analyze complex legal documents. Unparalleled Capabilities: Modern LLMs offer advanced natural language processing, increased accuracy in text generation, and the ability to tackle intricate tasks.',\n",
              "   'score': 0.68697983,\n",
              "   'raw_content': 'Must Know the Leading LLMs of 2025 | by Ayesha sidhikha | Feb, 2025 | Medium\\nOpen in app\\nSign up\\nSign in\\n\\nWrite\\n\\nSign up\\nSign in\\n\\nMember-only story\\nMust Know the Leading LLMs of 2025\\n\\nAyesha sidhikha\\n·Follow\\n4 min read\\n·\\n7 hours ago\\n\\n--\\n\\nShare\\nBy 2025, an estimated 90% of companies will integrate language models into their daily operations. These advanced tools aren’t just futuristic fantasies — they’re the driving force behind smarter communication, streamlined workflows, and innovative solutions across industries. In this post, we’ll dive into the evolution of large language models (LLMs), highlight the best available options in early 2025, and explore how these powerful tools can transform the way you work.\\n\\nIntroduction to LLMs\\nLanguage models have come a long way in a short period. Today’s LLMs are far more than text generators — they are comprehensive systems that help you achieve goals, enhance productivity, and even analyze complex legal documents. With the rapid pace of technological updates, staying informed about these models isn’t just an option; it’s a necessity.\\nKey Takeaways\\n\\nWidespread Adoption: By the end of 2025, 90% of organizations are expected to use language models.\\nUnparalleled Capabilities: Modern LLMs offer advanced natural language processing, increased accuracy in text generation, and the ability to tackle intricate tasks.\\nBusiness Revolution: Top LLM programs are set to transform operations, providing tools for enhanced efficiency and creativity.\\n\\n\\n--\\n\\n--\\n\\n\\n\\nFollow\\nWritten by Ayesha sidhikha --------------------------\\n63 Followers\\n·1 Following\\nEnthusiastic learner passionate about data science and generative AI. Committed to tackling challenges and continuous growth in technology.\\nFollow\\nNo responses yet\\n\\nWhat are your thoughts?\\nAlso publish to my profile\\nRespond\\nRespond\\nHelp\\nStatus\\nAbout\\nCareers\\nPress\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams'},\n",
              "  {'url': 'https://eleks.com/blog/best-llms-for-language-processing/',\n",
              "   'title': 'The Best LLMs for Enhanced Language Processing in 2025 - ELEKS',\n",
              "   'content': 'LLM apps are applications that use large language models (LLMs) and AI models to perform various tasks, including language translation, content generation, and other language processing tasks. Article Nuclear Power Plants for AI Data Centres: a Solution to Growing Energy Challenge View article Article Enhancing Patient Experience in Healthcare: The Role of Experience Platforms View article Article Edge Computing for Industry 5.0: Enabling Next-Generation Industrial Intelligence View article R&D Generative AI in Healthcare: Solving Medical Staff Performance Issues View article Whitepaper Winning More Deals with AI: How ELEKS’ Delphi AI Enhances Presale Process View article Article AI in Clinical Trials: Improving Patient Recruitment and Retention View article R&D AI Code Review: Can Technologies Really Ensure Code Quality?',\n",
              "   'score': 0.64639384,\n",
              "   'raw_content': 'The Best LLMs for Enhanced Language Processing in 2025\\nSkip to main content\\n\\n\\n\\nServices\\n\\n\\nEngineering\\nEnd-to-end engineering services for seamless software delivery.\\n\\n Application development Bring your software vision to life with a tailored solution and deliver an industry-leading user experience.\\n PoC development Safely explore business-boosting concepts with robust testing and expert road mapping.\\n Product-oriented delivery Ensure timely and cost-effective product delivery while prioritizing your business objectives.\\n\\n Enterprise applications\\n\\nERP consulting\\nCRM consulting\\n\\n\\n\\n Application re‑engineering Transform your core legacy systems to elevate performance, agility, scalability, and UX.\\n\\n Cloud migration Boost agility, scalability and cost effectiveness by integrating your IT infrastructure with the cloud.\\n\\n\\n\\nAdvisory\\nStrategic guidance for top-notch products and services.\\n\\n Product and service design Validate niche ideas and create innovative products and services that scale as your business does.\\n Cyber security Proactively identify threats to your digital infrastructure to futureproof your IT ecosystem.\\n Technical feasibility study Explore new technologies and their potential for your business before making an investment.\\n Sustainability consulting Reach your net-zero goals and seize new, sustainable business growth opportunities.\\n Agile transformation Transform your organization to achieve agility, resilience, and sustainable business growth.\\n AI consulting Get strategic guidance on implementing AI solutions for scalable business growth.\\n\\n\\n\\nData & AI\\nCustom solutions to maximize the value of your data.\\n\\n Data science Boost your business performance and achieve optimization through tailored data-driven solutions.\\n\\n Artificial intelligence Transform your industry with AI-driven innovations and custom data-centric solutions.\\n\\nGenerative AI\\nMachine learning\\nConversational AI\\n\\n\\n\\n MLOps Achieve seamless integration and maximum ROI for your machine learning models.\\n\\n Data platforms Enable actionable insights and unmatched strategic impact with a custom data platform from ELEKS.\\n Data strategy Enable smart, insight-driven business decisions with our end-to-end data strategy consulting.\\n Business intelligence Maximize your data potential, make smarter decisions, and strategize for long-term success.\\n\\n Intelligent automation Maximize efficiency, cut costs, and streamline processes with intelligent automation solutions.\\n\\nIntelligent document processing\\n\\n\\n\\n\\n\\nOptimisation\\nExpert help for flawless performance of your products and services.\\n\\n Software audit Assess your software products and processes to mitigate risks and minimize revenue loss.\\n Quality assurance Delver flawless products and seamless user experiences with our expert QA services.\\n Support Efficiently handle technical issues and system changes with our IT support services.\\n FinOps Get expert guidance to maximise your cloud infrastructure value, optimise costs, and boost ROI.\\n\\n\\n\\nExpertise\\nLatest technologies and innovative approaches to drive your business growth forward.\\n\\n DevOps Full-cycle DevOps solutions to optimise your SDLC for greater agility and cost-efficiency.\\n VR/AR/MR Utilize virtual reality technologies to deliver brand-defining, immersive user experiences.\\n Internet of Things Embrace the potential of IoT for better efficiency, smoother collaboration and powerful data insights.\\n Market research Evaluate your business landscape to capitalise on promising niches and get ahead of the competition.\\n Customer Experience Refine every customer interaction to enhance satisfaction and drive sustainable revenue growth.\\n Digital enterprise Introduce digital transformation to your enterprise to drive efficiency, productivity, and revenue.\\n UX consulting Launch successful products and deliver services that your customers will truly want to use.\\n Nearshore development Partner with a trusted nearshore software development company to deliver your software project.\\n\\n\\n\\n\\n\\nIndustries\\nInnovative solutions across industriesExpert software services tailored to meet the unique needs of every sector.\\n\\n\\n Fintech FintechEffectively manage risks, protect your assets against fraud and maximize the potential of your data. \\n Healthcare HealthcareLeverage new technologies to provide outstanding patient care focused on improving clinical results. \\n Energy EnergyElevate your power system’s productivity, safety and sustainability with energy management software. \\n Government GovernmentDigitise your public services to create a transparent, efficient, and data-centric government agency. \\n Insurance InsuranceAdopt data-driven insurance software solutions boosting efficiency, profitability, and security in your sector. \\n Retail RetailTransform customer journeys and engage shoppers in new ways, increasing sales and enhancing profitability. \\n Logistics LogisticsStreamline your supply chain, fleets, and warehousing for industry-leading logistics services. \\n Automotive AutomotiveIntroduce intelligent features and process optimizations to deliver a new level of driver experience. \\n Agriculture AgricultureAdopt data-driven innovations to balance supply and optimise production under increasing pressures. \\n Media & Entertainment Media & EntertainmentReach wider audiences with unparalleled digital experiences crafted for maximum engagement. \\n\\n\\n\\nClients\\n\\nOur clients We are proud of contributing to the success of the world’s leading brands. \\nCase studies See how ELEKS has helped its clients achieve their vision of digital innovation. \\n\\n20+\\nyears of partnership with clients\\n\\n\\n120+\\nactive client accounts\\n        *   1000+\\nend-to-end projects\\n\\n\\n\\n\\n\\n\\nAbout us\\n\\nAbout us Meet ELEKS, a trusted partner for software engineering and technology consulting services. \\nHow we work Learn how we help our clients address complex business problems with technology solutions. \\nAwards and partners The recognition our solutions receive from prestigious associations and award programs. \\n Leadership team\\n\\nCSR\\n\\nCareers\\nPress kit\\n\\n\\n\\n Our products\\n\\n\\n\\n\\nBlog\\n\\n\\nContact us\\nSearch request  Search \\nContact us\\n\\n\\nArticle\\nThe Best LLMs for Enhanced Language Processing in 2025\\nHome Article AI development\\nLarge Language Models (LLMs) have emerged as advanced artificial intelligence systems that can process and generate text with logical communication.\\nAs a cornerstone of modern generative AI software development, LLMs often approach human-level proficiency across a variety of language-related tasks.\\nIn this article, we\\'ll overview top LLMs and their features, explore challenges and trends, and consider industry-specific applications of LLMs.\\nHow LLMs work\\nData collection\\nIt starts with collecting a wide range of text from global sources, including books, research papers, news, and websites. Depending on the industry, the model can also train on various types of data organisations own, such as financial reports, customer behaviour data, patient records, equipment data, and even weather data. The more diverse the data, the better the model can learn.\\nGenerally, LLMs have anywhere from 8 billion to 70 billion parameters and are trained on vast amounts of data. For example, Crawl, one of the largest datasets, includes web pages and information from the past decade, holding several petabytes of data.\\nTokenisation\\nAt this step, data is broken into tokens, words, or parts of words. In this way, the model processes and analyses the text.\\nPre-training or knowledge distillation\\nIn pre-training, the model learns by predicting the next token in a sequence and grasping language patterns, grammar, and word relationships. For example, given \"The sky is,\" it predicts \"blue.\" Using a transformer architecture, it processes tokens and applies self-attention to focus on the most important words in a sentence. This approach boosts the model\\'s language skills and lets intelligent automation handle tasks with less human input.\\nOn the other hand, Knowledge Distillation allows smaller models (like LLaMA or Mistral) to learn from larger and more complex models (like GPT-4). KD helps smaller models perform well with fewer resources. The smaller model is essentially \"taught\" by the larger one, which improves the smaller model\\'s efficiency and performance while reducing its computational cost.\\nFine-tuning\\nAfter pre-training, the model is fine-tuned for specific tasks like question answering or summarising text. This involves training the model on smaller, task-specific datasets. Fine-tuning helps the model specialise in particular tasks and improve its performance.\\nInference\\nThe model processes input, such as a question or prompt, and gives a relevant response. It understands language and context to provide accurate answers or generate text. Conversational AI systems, such as chatbots, use this process to interact meaningfully with users.\\nResponse generation\\nThe model creates text one token at a time, predicting each next token based on the input and its acquired knowledge. The output layer creates tokens and forms them into sentences. Methods like beam search are used to find the best and most coherent response.\\nFor more insights into how generative AI is shaping the future of software development, check out this article: Expert Insights on Generative AI: Evolution, Challenges, and Future Trends\\nTop LLM models\\n1. GPT\\nGPT Models (OpenAI): OpenAI created the GPT series, which includes some of the most widely known and used language models.\\nGPT-3.5, the engine behind ChatGPT, builds on GPT-3 with improved learning from human feedback.\\nThe latest GPT o3 processes both text and images. It has over 170 billion parameters, which makes it incredibly powerful for many tasks.\\n2. Gemini\\nGemini is Google\\'s family of large language models that can process text, images, and other media. The Gemini family includes different versions: Ultra (the largest and most capable), Pro (mid-tier), and Nano (efficient for on-device processing). Gemini 2.0 Flash builds on the success of 1.5 Flash, offering faster performance and even outperforming 1.5 Pro in key benchmarks.\\nIn addition to handling multimodal inputs like images, video, and audio, 2.0 Flash supports new features like generating pictures mixed with text, steerable text-to-speech (TTS) multilingual audio and calling tools like Google Search, code execution, and third-party functions.\\n3. Claude\\nClaude is an LLM developed by Anthropic. It is built to focus on ethical and safe AI through constitutional AI principles. Claude 3.5 Sonnet is the latest iteration. It\\'s designed to offer safer, more reliable interactions, especially for enterprise applications, and is available through platforms like Claude.ai and its iOS app.\\n4. Command\\nCommand by Cohere blends real-time data with natural language generation to provide accurate, up-to-date responses. The Command R is built to scale, delivering fast, reliable results for complex tasks like customer support or content creation. Cohere\\'s open-source approach lets users easily customise the models to fit their needs without being tied to a specific vendor. Command easily integrates with existing systems, helping businesses quickly innovate and stay competitive.\\n5. LLaMA\\nLLaMA (Large Language Model Meta AI) is Meta\\'s series of open-source large language models. The latest version, LLaMA 3.1, was released in July 2024 and introduces an expanded context length of up to 128,000 tokens, multilingual support across eight languages, and improved reasoning and coding capabilities. LLaMA models range from 8 billion to 405 billion parameters. Meta emphasises accessibility and innovation, allowing developers to fine-tune these models for diverse applications while fostering collaboration in the AI community.\\n\\nRole of LLM APIs in application development\\nLLM APIs act as a communication channel between applications and the LLM models. With the help of APIs, developers don\\'t need to understand the complexities of LLMs. Instead, developers interact with the API. They send text-based inputs and receive responses.\\nHow LLM API works\\n\\nData transmission: The user provides a text input like a question or command. The application formats this input and transmits it to the LLM API.\\nNatural language processing by the LLM: Upon receiving the input, the API forwards it to the LLM model. The model processes the language.\\nAPI response generation: LLM generates an appropriate response, from simple facts to creative content.\\nApplication integration: The response is returned to the app. It then integrates it into the user experience. This could mean showing the response on the screen, playing it as audio, or triggering actions in the app.\\n\\nKey considerations for choosing the right LLM API\\nBefore exploring the different language model providers, understand your project\\'s needs.\\n\\nWhat do you want the LLM to do? Think about the specific tasks it will handle.\\nWho will use it, and what do they need? Consider your audience and what they expect.\\nHow much will you use it? Estimate how often you\\'ll send requests to the API.\\nWhat\\'s your budget? Decide how much money you can spend on monthly or yearly LLM services.\\n\\nNarrow down your choices and focus on models that suit your needs. Then, you can compare the features and abilities of different LLMs to find the best fit.\\nThe factors influencing the selection of the right large language model (LLM) begin with a clear understanding of the domain and the specific task. Beyond that, considerations such as the intended usage, the organisation\\'s FinOps strategy, and the model\\'s positioning within competitive arenas—like the Chatbot Arena or Language Model Arena—play a critical role. Choosing the right model is about its capabilities and aligning it with business goals, operational requirements, and cost-efficiency strategies to ensure optimal performance and scalability.\\n\\nVolodymyr Getmanskyi\\nHead of Data Science at ELEKS\\nThe tables below list large language models, their API providers, and key metrics for evaluating them for different use cases.\\nQuality overview\\n| Model | API Providers | Arena Score | Latency (s) | Context Window |\\n| --- | --- | --- | --- | --- |\\n| o1-preview | OpenAI | 1334 | 23.57 | 128k |\\n| o1-mini | OpenAI | 1306 | 9.44 | 128k |\\n| GPT-4o-2024-08-06 | Microsoft Azure | 1265 | 0.83 | 128k |\\n| Claude 3.5 Sonnet (20241022) | AWS | 1283 | 1.01 | 200k |\\n| Claude 3 Opus | AWS | 1248 | 1.61 | 200k |\\n| Claude 3 Haiku | Anthropic | 1179 | 0.51 | 200k |\\n| Command R+ (04-2024) | Cohere | 1190 | 0.32 | 128k |\\n| Llama-3.1-Nemotron-70B-Instruct | Nebius | 1269 | 0.33 | 128k |\\n| Llama-3.3-70B-Instruct | Microsoft Azure | 1256 | 0.44 | 128k |\\n| Gemini-1.5-Flash-002 | Google (AI Studio) | 1271 | 0.35 | 1m |\\nCost and volumes overview\\n| Model | API Providers | Blended Price (USD/1m tokens) | Input Price (USD/1m tokens) | Output Price (USD/1m tokens) | Latency (s) |\\n| --- | --- | --- | --- | --- | --- |\\n| o1-preview | OpenAI | $26.25 | $15.00 | $60.00 | 23.57 |\\n| o1-mini | OpenAI | $5.25 | $3.00 | $12.00 | 9.44 |\\n| GPT-4o-2024-08-06 | Microsoft Azure | $4.38 | $2.50 | $10.00 | 0.83 |\\n| Claude 3.5 Sonnet (20241022) | AWS | $6.00 | $3.00 | $15.00 | 1.01 |\\n| Claude 3 Opus | AWS | $30.00 | $15.00 | $75.00 | 1.61 |\\n| Claude 3 Haiku | Anthropic | $0.50 | $0.25 | $1.25 | 0.51 |\\n| Command R+ (04-2024) | Cohere | $6.00 | $3.00 | $15.00 | 0.32 |\\n| Llama-3.1-Nemotron-70B-Instruct | Nebius | $0.20 | $0.13 | $0.40 | 0.33 |\\n| Llama-3.3-70B-Instruct | Microsoft Azure | $0.71 | $0.71 | $0.71 | 0.44 |\\n| Gemini-1.5-Flash-002 | Google (AI Studio) | $0.13 | $0.13 | ($0.30) | 0.35 |\\n\\nArena score is a performance metric used to evaluate and rank models based on their effectiveness in a competitive or benchmark setting.\\nContext window represents the number of tokens the model can handle in a single session.\\nBlended price is the average cost per million tokens.\\nInput price is the cost of processing one million tokens sent as input to the model.\\nOutput price is the cost of generating one million tokens as a response from the model.\\nLatency is the average time (in seconds) it takes for the model to process input and deliver output.\\n\\nIt\\'s important to note that the models and providers listed in the tables are just a selection, and many more options are available in the market. For a more extended comparison, check the LLM API Providers Leaderboard and Chatbot Arena LLM Leaderboard\\nWe understand that navigating these metrics can be complex, so you can contact our team for assistance in selecting the best model for your use case.\\nChallenges and limitations of LLM tools\\nModel bias and hallucinations\\nOne important issue with LLMs is their tendency to \"hallucinate.\" LLMs predict the next word in a sequence. This can make them sound believable, but they may generate false or nonsensical responses. This can be especially problematic in applications where accuracy is crucial. To avoid misinformation, users should verify LLMs\\' output with other sources.\\n\\nFor instance, our data science engineers have encountered cases where models sometimes confused financial data from different companies. Even with instructions to admit uncertainty or missing data, the models still gave wrong answers. It shows how hard it is to ensure models provide accurate results in complex situations.\\n\\nInput and output length limitations\\nLarge language models are limited by the number of tokens they can process in a single instance. It restricts both the length of the input and the output. This limitation can be a challenge for processing long documents or generating detailed responses.\\n\\nResearchers are working on optimising models to process longer text sequences. In the meantime, users can break up lengthy inputs into smaller ones.\\n\\nLimited multimodal capabilities\\nMost LLMs are focused on text and do not yet handle other forms of media effectively. Full integration across modalities is still developing.\\n\\nLarge language models are being updated to handle both text and other media, like images or audio. Models like GPT-4 and Google Gemini are already starting to process multiple types of data, with plans for more advanced media handling in the future.\\n\\nVulnerability to misuse and ethical risks\\nLLM tools are also vulnerable to misuse. There are concerns about generated code vulnerabilities, contradictory suggestions from models, and unethical usage, such as using AI to cheat on exams or gain instructions on illegal activities. These issues highlight the need for careful oversight and regulation to prevent harmful or unintended uses of AI technologies.\\nIndustry-specific applications of LLM tools\\nHealthcare\\nLLM-driven AI chatbot assistants in the healthcare industry help facilitate patient-doctor communication. These chatbots are being created for different fields, from helping patients and doctors communicate to improving internal processes. AI chatbots boost patient engagement, offer quick 24/7 assessments, reduce administrative tasks, and improve planning, thus making the work of healthcare providers more efficient and patient-centric.\\nRetail\\nLLMs analyse consumer behaviour in retail to improve marketing strategies and campaign precision. Building a chain of LLM-based agents that automates internal processes, from ordering and communication to hiring, significantly decreases operating costs.\\nFinance\\nLLMs act as financial advisors, tailoring investment recommendations and strategies based on customer preferences and historical trends. They also gather market data and expert opinions to generate actionable insights, helping financial institutions make informed investment decisions in the finance industry.\\nMedia and entertainment\\nIn the media and entertainment industry, SOTA (State-of-the-Art Large Language Models) LLMs are used to create personalised advertising and dynamically adjust the appearance of websites, apps, and marketing materials such as tailored ads and content for specific audiences. It leads to higher click-through rates (CTR) and improved engagement metrics.\\nInsurance\\nPersonalised insurance products involve creating an LLM-based recommender system that combines underwriting policies with recognised consumption patterns and customer needs. This system analyses the limitations and possibilities of available policies and tailors recommendations to individual customers.\\nAutomotive\\nLLM-based agents are used in the automotive industry for automated contractors\\' information search, filtering, and ranking based on usefulness and predefined conditions. This helps businesses find suppliers more efficiently and improve their internal processes. The automation allows for smoother negotiations and quicker RFQ preparation, ultimately leading to higher efficiency in operations.\\nTesting an LLM for healthcare: ELEKS case study\\nAt ELEKS, we have developed a generative AI-powered solution for medical document summarisation. This solution aims to organise and manage large volumes of unstructured healthcare data.\\nOur team began by researching and selecting the task\\'s best large language models (LLMs). We compared general-purpose models like GPT-3.5 and GPT-4 with specialised medical LLMs such as DHEIVER and MedLlama2.\\nWe strictly adhered to HIPAA and GDPR regulations. We also implemented Optical Character Recognition (OCR) to convert unstructured medical documents into searchable text and a classification module to identify document types for targeted summarisation.\\nOur solution is built on a flexible tech stack. It uses Microsoft Azure and .NET to manage workflows and scalability. We refined the tool based on testing and feedback. We switched to GPT-4o to handle larger data volumes. Future upgrades include integrating the solution with electronic medical records (EMR) systems.\\nTo learn more about our experience developing this innovative solution, read our full article: Generative AI in Healthcare: Solving Medical Staff Performance Issue\\nFuture of LLMs\\nGPT-4 and Google\\'s Gemini models are among the first LMMs to be widely deployed. Their full capabilities are still being rolled out.\\nHowever, in the near future, we will see more large language models (LLMs), especially from tech giants like Apple, Amazon, IBM, Intel, and NVIDIA. These models may be less known than some popular ones. Large companies will likely use them for internal tasks and customer support.\\nWe may also see more efficient LLMs for smartphones and other lightweight devices. Google has already started this trend with Gemini Nano, which operates some features on the Google Pixel Pro 8. Similarly, Apple introduced Apple Intelligence.\\nAnother trend is the rise of multimodal models that combine text generation with other media, including images and audio. These models will allow users to ask a chatbot about an image or receive an audio response.\\nSummary and final thoughts\\nLarge Language Models (LLMs) are at the forefront of artificial intelligence. These models are changing how businesses and individuals interact with a language.\\nLLM APIs help organisations stay ahead in today\\'s competitive landscape, improve user experiences, and automate routine tasks.\\nThe future of LLMs looks bright as research continues to overcome their limitations. As we improve knowledge cutoffs, hallucinations, and multimodal skills, LLMs will evolve and help organisations be more productive and creative.\\n\\nLooking forward to applying LLM solution in your business?\\nContact an expert\\nAI development\\nPartner with ELEKS to implement AI-powered strategies that drive breakthrough performance.\\nView service\\nData science\\nDeep-dive into your data and boost business performance by understanding what your users really want.\\nView expertise\\nFAQs\\nIs ChatGPT is LLM?\\nYes, ChatGPT is an AI-powered large language model. It uses deep learning and neural networks to let you have human-like conversations with a chatbot.\\nIs LLM free?\\nYes, there are free options available! While many advanced large language models require payment, there are open-source models trained on extensive training data that can be used for free.\\nWhat are LLM apps?\\nLLM apps are applications that use large language models (LLMs) and AI models to perform various tasks, including language translation, content generation, and other language processing tasks. These apps are often built on the latest breakthroughs in AI research.\\nWhat are LLM model tools?\\nLLM model tools are software applications powered by advanced artificial intelligence models. These tools can understand and generate human-like text, as well as perform language processing tasks, and are the result of ongoing AI research.\\nIs Bert an LLM?\\nYes, BERT was one of the first modern LLMs. It uses neural networks and deep learning, making it widely used and very successful.\\nWhat are the three features of a smart grid?\\nLLMs are a type of generative AI that focuses on creating text. Generative AI, however, can produce many types of outputs, including text, images, audio, and code.\\nWhat is conversational AI?\\nConversational AI is a technology that allows computers to understand and respond to human language in real-time, often through chatbots or voice assistants, leveraging deep learning and extensive training data.\\nHow do AI LLM models and machine learning work?\\nAI LLM models and machine learning use deep learning and neural networks to process language and perform language processing tasks, enabling accurate and natural conversations.\\xa0\\n Olha Zhydik Content Marketing Manager\\nPublished:\\nJanuary 10, 2025\\nUpdated:\\nJanuary 10, 2025\\nAI artificial intelligence data science machine learning\\n\\n\\n\\n\\n\\nSkip the section\\nRelated Insights\\n Article Nuclear Power Plants for AI Data Centres: a Solution to Growing Energy Challenge View article Article Enhancing Patient Experience in Healthcare: The Role of Experience Platforms View article Article Edge Computing for Industry 5.0: Enabling Next-Generation Industrial Intelligence View article R&D Generative AI in Healthcare: Solving Medical Staff Performance Issues View article Whitepaper Winning More Deals with AI: How ELEKS’ Delphi AI Enhances Presale Process View article Article AI in Clinical Trials: Improving Patient Recruitment and Retention View article R&D AI Code Review: Can Technologies Really Ensure Code Quality? View article R&D Tackling Power Grid Challenges: ELEKS’ AI-Supported Solution for Power Infrastructure Mapping View article\\nDiscover more insights\\nHave a question?\\nSpeak to an expert\\n Explore AI development services Explore AI development services\\nContact Us\\n\\n\\nFull name*\\n\\n\\nWe need your name to know how to address you\\n\\n\\nEmail*\\n\\n\\nWe need your email to respond to your request\\n\\n\\nPhone number*\\n\\n\\nWe need your phone number to reach you with response to your request\\n\\n\\nCountry*\\n\\n\\nWe need your country of business to know from what office to contact you\\n\\n\\nCompany*\\n\\n\\nWe need your company name to know your background and how we can use our experience to help you\\n\\n\\nMessage*\\n\\n\\nAttach file\\nAccepted file types: jpg, gif, png, pdf, doc, docx, xls, xlsx, ppt, pptx, Max. file size: 10 MB.\\n\\n\\nAdd an attachment\\n(jpg, gif, png, pdf, doc, docx, xls, xlsx, ppt, pptx, PNG)\\n\\n\\n\\n[ ]  I want to receive news and updates once in a while\\n\\n\\n\\nWe will add your info to our CRM for contacting you regarding your request. For more info please consult our privacy policy\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nComments\\nThis field is for validation purposes and should be left unchanged.\\n\\n\\nΔ\\nWhat our customers say\\nThe breadth of knowledge and understanding that ELEKS has within its walls allows us to leverage that expertise to make superior deliverables for our customers. When you work with ELEKS, you are working with the top 1% of the aptitude and engineering excellence of the whole country.\\n\\nSam Fleming\\nPresident, Fleming-AOD\\nRight from the start, we really liked ELEKS’ commitment and engagement. They came to us with their best people to try to understand our context, our business idea, and developed the first prototype with us. They were very professional and very customer oriented. I think, without ELEKS it probably would not have been possible to have such a successful product in such a short period of time.\\n\\nCaroline Aumeran\\nHead of Product Development, appygas\\nELEKS has been involved in the development of a number of our consumer-facing websites and mobile applications that allow our customers to easily track their shipments, get the information they need as well as stay in touch with us. We’ve appreciated the level of ELEKS’ expertise, responsiveness and attention to details.\\n\\nSamer Awajan\\nCTO, Aramex\\n\\nAddress:\\nViru väljak 2, Tallinn, Harju maakond, 10111\\nEleks, Inc.\\nCAGE/NCAGE: 7W6F0\\nSAM Unique Entity ID: NQ9PRQMMSJG4\\n\\n\\nServices\\n\\n\\nEngineering\\n\\nPoC development\\nApplication development\\nProduct-oriented delivery\\nEnterprise applications\\nApplication re‑engineering\\nCloud migration\\n\\n\\n\\nData & AI\\n\\nData science\\nData strategy\\nArtificial intelligence\\nGenerative AI\\nMachine learning\\nConversational AI\\nIntelligent automation\\nMLOps\\nBusiness intelligence\\nData platforms\\n\\n\\n\\nAdvisory\\n\\nProduct and service design\\nCyber security\\nTechnical feasibility study\\nSustainability consulting\\nAgile transformation\\n\\n\\n\\nOptimisation\\n\\nFinOps\\nSoftware audit\\nQuality assurance\\nSupport\\n\\n\\n\\n\\n\\nExpertise\\n\\nDevOps\\nVR/AR/MR\\nInternet of Things\\nMarket research\\nCustomer experience\\nDigital enterprise\\nNearshore development\\nUX consulting\\nSoftware development\\n\\n\\n\\nIndustries\\n\\nFintech\\nHealthcare\\nEnergy\\nGovernment\\nInsurance\\nRetail\\nLogistics\\nAutomotive\\nAgriculture\\nMedia & Entertainment\\n\\n\\n\\nCompany\\n\\nAbout us\\nServices\\nHow we work\\nAwards and partners\\nOur clients\\nCase studies\\nBlog\\nCareers\\nContact us\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTerms of Use\\n\\nPrivacy policy\\nSite Map\\n\\n© 1991-2025 ELEKS, All rights reserved'},\n",
              "  {'url': 'https://www.instaclustr.com/education/top-10-open-source-llms-for-2025/',\n",
              "   'title': 'Top 10 open source LLMs for 2025 - Instaclustr',\n",
              "   'content': 'NetApp Instaclustr steps in to support open source large language models, providing a robust infrastructure and managed services that simplify the process. In this article, we will explore how NetApp Instaclustr empowers organizations to leverage the full potential of open source large language models. By leveraging the distributed computing capabilities and storage capacity provided by NetApp Instaclustr, organizations can efficiently train large language models, reducing the time and resources required for the training process. NetApp Instaclustr leverages its scalable infrastructure to support the deployment of open source large language models. NetApp Instaclustr offers comprehensive monitoring and support services for open source large language models. Managing the infrastructure for open source large language models can be costly.',\n",
              "   'score': 0.6337056,\n",
              "   'raw_content': None},\n",
              "  {'url': 'https://hatchworks.com/blog/gen-ai/large-language-models-guide/',\n",
              "   'title': 'Large Language Models: What You Need to Know in 2025',\n",
              "   'content': 'Large language models (LLMs) are the unsung heroes of recent Generative AI advancements, quietly working behind the scenes to understand and generate language as we know it. OpenAI released GPT-4, an even more powerful and versatile model than its predecessors, with improvements in understanding, reasoning, and generating text across a broader range of contexts and languages. 2022: The emergence of GPT-4 and other advanced models such as Midjourney, continuing to push the boundaries of what’s possible with LLMs in terms of generating and understanding natural language across various domains and tasks, including image generation. These models are trained on massive data sets and can perform a broad range of tasks like generating text, translating languages, and more. Tags: AI, artificial intelligence, gen ai, Generative AI, large language models, LLMs',\n",
              "   'score': 0.56489134,\n",
              "   'raw_content': \"Large Language Models: What You Need to Know in 2025 | HatchWorks AI\\nSkip to content\\n\\n\\nWhat We Do\\n\\n\\n\\n\\n\\nServices\\n\\n\\nAI Strategy & Roadmap\\n\\nData Engineering & Analytics\\nAI-Powered Software Development\\n\\nAI Engineering Teams\\n        *   *   Accelerators\\n\\n\\nGenerative Driven Development™\\n\\nAI Roadmap & ROI Workshop\\nAI Solution Accelerator\\nRAG\\n\\nGenIQ\\n        *   *   Industries\\n\\n\\nCommunications and IoT\\n\\nTechnology\\nHealthcare\\nFinance\\n\\nRetail\\n        *   *   Partnerships\\n\\n\\nDatabricks\\n\\nIndustries\\nCommunications and IoT Solutions\\nTechnology\\nHealthcare\\nFinance\\nRetail\\n\\n\\nAbout Us\\nAbout Us\\nCareers & Culture\\nHatchFutures\\nFAQ\\n\\n\\n\\nResources\\n\\n\\n\\n\\n\\nInsights\\n\\n\\n\\n\\n\\n\\n\\nBlog\\n\\nTalking AI Podcast\\n\\nTalking AI Newsletter\\n        *   *   Tools & Reports\\n\\n\\nState of AI Report 2025\\n\\nTech Talent Report 2024\\nNearshore Budget Calculator\\n\\nBuild your Own GPT\\n        *   *   Learn & Connect\\n\\n\\nEvents\\n        *   *   Media\\n\\n\\nNewsroom\\n\\nOur Work\\nCareers\\nContact\\n\\n\\n\\n\\n\\n\\n\\n\\nCareers\\nContact us\\nLarge Language Models: What You Need to Know in 2025\\n\\nMelissa Malec\\n\\nDecember 2, 2024\\n\\n\\nUpdated: January 16, 2025\\n\\n\\nLarge language models (LLMs) are the unsung heroes of recent Generative AI advancements, quietly working behind the scenes to understand and generate language as we know it.\\nBut how do they work? What are they capable of? And what should we look out for when using them?\\n\\nRead on and find out in this guide for LLMs in 2024. Jump ahead:\\n\\nUnderstanding Large Language Models\\nWhat is a Large Language Model?\\nHow Do Large Language Models Work?\\nKey Milestones in Large Language Model Development\\nCapabilities of Large Language Models\\nChallenges and Limitations of LLMs\\nThe Future of Language Models: What Comes Next?\\n\\nUnderstanding Large Language Models\\nLet’s get the basics out of the way. Here we’ll define the large language model (LLM), explain how they work, and provide a timeline of key milestones in LLM development.\\nWhat is a Large Language Model?\\nA large language model, often abbreviated to LLM, is a type of artificial intelligence model designed to understand natural language as well as generate it at a large scale.\\nWhen we say human language, we don’t just mean English, Spanish, or Cantonese. Those are certainly part of what LLMs are trained on but human language, in this context, also extends to:\\n\\nArt\\nDance\\nMorse code\\nGenetic code\\nHieroglyphics\\nCryptography\\nSign language\\nBody language\\nMusical notation\\nChemical signaling\\nEmojis and symbols\\nAnimal communication\\nHaptic communications\\nTraffic signs and signals\\nMathematical equations\\nProgramming languages\\n\\nLLMs are trained on billions of parameters and have the ability to learn from a wide range of data sources.\\nThis extensive training enables them to predict and produce text based on the input they receive so that they can engage in conversations, answer queries, or even write code.\\nSome of the leading very large models include giants like GPT, LLaMa, LaMDA, PaLM 2, BERT, and ERNIE.\\nThey’re at the heart of various applications, aiding in everything from customer service chatbots to content creation and software development.\\nSome companies even build their own LLMs but that requires significant time, investment, and tech knowledge. It’s much easier to integrate a pre-trained LLM into your own systems.\\nHow Do Large Language Models Work?\\nLarge Language Models use a blend of neural networks and machine learning (ML). It’s this blend that allows the technology to first process and then generate original text and imagery.\\nThink of neural networks as the LLM’s brain. It’s these networks that learn from vast amounts of data, improving over time as they’re exposed to more.\\nAs the model is trained on more data, it learns patterns, structures, and the nuances of language. It’s like teaching it the rules of grammar, the rhythm of poetry, and the jargon of technical manuals all at once.\\nMachine learning models then help the model to predict the next word in a sentence based on the words that come before it. This is done countless times, refining the model’s ability to generate coherent and contextually relevant text.\\nLLMs now also operate on a Transformer Architecture. This architecture allows the model to look at and weigh the importance of different words in a sentence. It’s the same as when we read a sentence and look for context clues to understand its meaning.\\n⚠️ While LLMs can generate original content, the quality, relevance, and innovativeness of their output can vary and require human oversight and refinement.\\nThe originality is also influenced by how the prompts are structured, the model’s training data, and the specific capabilities of the LLM in question.\\nKey Milestones in Large Language Model Development\\nLarge language models haven’t always been as useful as they are today. They’ve developed and been iterated upon significantly over time.\\nLet’s look at some of those key moments in LLM history. That way you can appreciate how far they’ve come and the rapid evolution in the last few years compared to decades of slow progress.\\n1966\\nELIZA\\n\\nThe first chatbot created by Joseph Weizenbaum, simulating a psychotherapist in conversation.\\n2013\\nword2vec\\n\\nA groundbreaking tool developed by a team led by Tomas Mikolov at Google, introducing efficient methods for learning word embeddings from raw text.\\n2018\\nGPT and BERT\\n\\nGPT (Generative Pretrained Transformer): OpenAI introduced GPT, showcasing a powerful model for understanding and generating human-like text.\\nBERT (Bidirectional Encoder Representations from Transformers): Developed by Google, BERT significantly advanced the state of the art in natural language understanding tasks.\\n\\n2020\\nGPT 3\\n\\nOpenAI released GPT-3, a model with 175 billion parameters, achieving unprecedented levels of language understanding and generation capabilities.\\nLate 2021\\nIntroduction of ChatGPT\\n\\nOpenAI introduced ChatGPT, a conversational agent based on the GPT-3.5 model, designed to provide more engaging and natural dialogue experiences. ChatGPT showcased the potential of GPT models in interactive applications.\\n2022\\nGPT-4\\nOpenAI released GPT-4, an even more powerful and versatile model than its predecessors, with improvements in understanding, reasoning, and generating text across a broader range of contexts and languages.\\n2022\\nMidjourney and Other Innovations\\n\\nThe launch of Midjourney, along with other models and platforms, reflected the growing diversity and application of AI in creative processes, design, and beyond, indicating a broader trend towards multimodal and specialized AI systems.\\nPre-2010: Early Foundations\\n\\n1950s-1970s: Early AI research lays the groundwork for natural language processing. Most famously, a tech called ‘Eliza’ was the world’s first chatbot.\\n1980s-1990s: Development of statistical methods for NLP, moving away from rule-based systems.\\n\\n2010: Initial Models\\n\\n2013: Introduction of word2vec, a tool for computing vector representations of words, which significantly improved the quality of NLP tasks by capturing semantic meanings of words.\\n\\n2014-2017: RNNs and Attention Mechanisms\\n\\n2014: Sequence to sequence (seq2seq) models and Recurrent Neural Networks (RNNs) become popular for tasks like machine translation.\\n2015: Introduction of Attention Mechanism, improving the performance of neural machine translation systems.\\n2017: The Transformer model is introduced in the paper “Attention is All You Need”, setting a new standard for NLP tasks with its efficient handling of sequences.\\n\\n2018: Emergence of GPT and BERT\\n\\nJune 2018: OpenAI introduces GPT (Generative Pretrained Transformer), a model that leverages unsupervised learning to generate coherent and diverse text.\\nOctober 2018: Google AI introduces BERT (Bidirectional Encoder Representations from Transformers), which uses bidirectional training of Transformer models to improve understanding of context in language.\\n\\n2019-2020: Larger and More Powerful Models\\n\\n2019: Introduction of GPT-2, an improved version of GPT with 1.5 billion parameters, showcasing the model’s ability to generate coherent and contextually relevant text over extended passages.\\n2020: OpenAI releases GPT-3, a much larger model with 175 billion parameters, demonstrating remarkable abilities in generating human-like text, translation, and answering questions.\\n\\n2021-2023: Specialization, Multimodality, and Democratization of LLMs\\n\\n2021-2022: Development of specialized models like Google’s LaMDA for conversational applications and Facebook’s OPT for open pre-trained transformers.\\n2021: Introduction of multimodal models like DALL·E by OpenAI, capable of generating images from textual descriptions, and CLIP, which can understand images in the context of natural language.\\n2022: The emergence of GPT-4 and other advanced models such as Midjourney, continuing to push the boundaries of what’s possible with LLMs in terms of generating and understanding natural language across various domains and tasks, including image generation. It’s also more accessible to larger numbers of people.\\n\\nCapabilities of Large Language Models\\nThe capabilities of Large Language Models are as vast as the datasets they’re trained on. Use cases range from generating code to suggesting strategy for a product launch and analyzing data points.\\nThis is because LLMs serve as foundation models that can be applied across multiple uses.\\nHere’s a list of LLM capabilities:\\n\\nText generation\\nLanguage translation\\nSummarization\\nQuestion answering\\nSentiment analysis\\nConversational agents\\nCode generation and explanation\\nNamed entity recognition\\nText classification\\nContent recommendation\\nLanguage modeling\\nSpell checking and grammar correction\\nParaphrasing and rewriting\\nKeyword and phrase extraction\\nDialogue systems\\n\\nAnd here’s a breakdown of some of the more common ones we see:\\nAutomated Code Generation\\nLLMs can generate code snippets, functions, or even entire modules based on natural language descriptions, reducing the time and effort required to implement common functionalities.\\nHere’s an example to illustrate how LLMs can be used for automated code generation:\\nPrompt:\\n“Write a Python function that takes a list of numbers as input and returns a list containing only the even numbers.”\\n\\nText Generation\\nLLMs can generate coherent, contextually relevant text based on prompts. This includes creating articles, stories, and even generating product descriptions.\\nHere’s an example to illustrate how LLMs can be used for text generation:\\nPrompt:\\n“Generate a product description for a cutting-edge smartwatch designed for fitness enthusiasts. The description should highlight its advanced health and fitness tracking, personalized coaching, long battery life, durability, connectivity features, and customizable design. Target the description to appeal to both seasoned athletes and beginners interested in tracking their fitness progress.”\\n\\nLanguage Translation\\nThey can translate text between different languages, often with a high degree of accuracy, depending on the languages involved and the model’s training data.\\nHere’s an example to illustrate how LLMs can be used for language translation:\\nPrompt:\\n“Translate the following English text into Spanish: ‘The quick brown fox jumps over the lazy dog.'”\\n\\nBug Detection and Correction\\nLLMs can help identify bugs in code by analyzing code patterns and suggesting fixes for common errors, potentially integrating with IDEs (Integrated Development Environments) to provide real-time assistance.\\nHere’s an example to illustrate how LLMs can be used for bug detection:\\nPrompt:\\n“The Python function below intends to return the nth Fibonacci number. Please identify and correct any bugs in the function.\\nPython Function:\\ndef fibonacci(n):\\nif n <\\\\= 1:\\nreturn n\\nelse:\\nreturn fibonacci(n – 1) + fibonacci(n – 2)”\\n\\nParaphrasing and Rewriting\\nThey can rephrase or rewrite text while maintaining the original meaning, useful for content creation and academic purposes.\\nHere’s an example to illustrate how LLMs can be used for paraphrasing:\\nPrompt:\\n“Rewrite the following sentence in a simpler and more concise way without losing its original meaning: ‘The comprehensive study on climate change incorporates a wide array of data, including historical weather patterns, satellite imagery, and computer model predictions, to provide a holistic view of the impacts of global warming.'”\\n\\nDialogue Systems\\nLLMs power sophisticated dialogue systems for customer service, interactive storytelling, and educational purposes, providing responses that can adapt to the user’s input.\\nThink of a chatbot on a software product you use where you can ask it questions and it generates insightful, helpful responses.\\nChallenges and Limitations of LLMs\\nLarge language models have come a long way since the early days of Eliza.\\nIn the last two years alone, we’ve seen LLMs power Generative AI and create high-quality text, music, video, and images.\\nBut with any technology, there will always be growing pains.\\nTechnical Limitations of Language Models\\nLarge Language Models sometimes face technical limitations impacting their accuracy and ability to understand context.\\nDomain Mismatch\\nModels trained on broad datasets may struggle with specific or niche subjects due to a lack of detailed data in those areas. This can lead to inaccuracies or overly generic responses when dealing with specialized knowledge.\\nWord Prediction\\nLLMs often falter with less common words or phrases, impacting their ability to fully understand or accurately generate text involving these terms. This limitation can affect the quality of translation, writing, and technical documentation tasks.\\nReal-time Translation Efficiency\\nWhile LLMs have made strides in translation accuracy, the computational demands of processing and generating translations in real-time can strain resources, especially for languages with complex grammatical structures or those less represented in training data.\\nHallucinations and Bias\\nOn occasion, LLM technology is too original. So original in fact that it’s making up information.\\nThis is a lesson Air Canada learned the hard way when its chatbot told a customer about a refund policy when no such policy exists, which they then had to honor.\\nFinally, LLMs can inadvertently propagate and amplify biases present in their training data, leading to outputs that may be discriminatory or offensive.\\nScalability and Environmental Impact\\nThe scalability of LLMs is tied to the impact it has on the environment. And that impact is turning out to be a big one.\\nTraining a system like GPT-3 took 1,287 Megawatt hours (MWh) of energy. To put that into perspective, 1 MWh could power about 330 homes for one hour in the United States.\\nThe image below shows the energy consumption of training four different LLMs.\\n\\nEnergy consumption doesn’t end at training—operating LLMs also uses a grotesque level of energy.\\nIn one report, Alex de Vries, founder of Digiconomist, has calculated that by 2027 the AI sector will consume between 85 to 134 Terawatt hours each year. That’s almost the same as the annual energy demand of the Netherlands.\\nWe can’t help but wonder how sustainable that is and what the long-term environmental impact will be on our energy sources. Especially when you consider LLMs are only going to become larger and more complex as we advance their capabilities.\\nAnd to maintain large language models, we’ll need to update them with new data and parameters as they arise. That will only expend more energy and resources.\\nThe Future of Language Models: What Comes Next?\\nNow that we’ve seen drastic and rapid improvement in the capabilities of LLMs through Generative AI, we expect users of AI to be fine-tuning prompts and discovering new use cases and applications.\\nIn the workplace especially, the focus will be on productivity hacks. It’s something we experiment with already through our Generative Driven Development™ offering, where our team has increased the productivity of software development by 30-50%.\\nHilary Ashton, Chief Product Officer at Teradata, shared her predictions for the future of LLMs and AI in AI Magazine:\\n\\nFirst, I foresee a massive productivity leap forward through GenAI, especially in technology and software. It’s getting more cost-effective to get into GenAI, and there are lots more solutions available that can help improve GenAI solutions. It will be the year when conversations gravitate to GenAI, ethics, and what it means to be human. In some cases, we’ll start to see the workforce shift and be reshaped, with the technology helping to usher in a four-day work week for some full-time employees.”\\nHilary Ashton\\n\\nAnd she’s right, especially when it comes to ethical considerations and where we humans add value AI can’t replicate.\\nWe’ll also see further democratization of AI with it infiltrating other areas of our life, much the same the computer has done since its invention.\\nWhat we know for certain is the development of LLMs and Generative AI is only getting started. And we want to be leading conversations on its use, ethics, scalability, and more as it evolves.\\nYou can be part of that conversation too:\\nListen or watch our Talking AI podcast where we interview AI experts and talk or sign up for our newsletter where we share insights and developments on LLMs, AI/ML, and Data governance, curated by our very own CTO, Omar Shanti.\\nFrequently Asked Questions About Large Language Models LLMs\\n1. What is a Large Language Model (LLM)?\\nA Large Language Model (LLM) is an artificial intelligence model that uses machine learning techniques, particularly deep learning and neural networks, to understand and generate human language. These models are trained on massive data sets and can perform a broad range of tasks like generating text, translating languages, and more.\\n2. How do Large Language Models work?\\nLarge Language Models work by leveraging transformer models, which utilize self-attention mechanisms to process input text. They are pre-trained on vast amounts of data and can perform in-context learning, allowing them to generate coherent and contextually relevant responses based on user inputs.\\n3. What is the significance of transformer models in LLMs?\\nTransformer models are crucial because they enable LLMs to handle long-range dependencies in text through self-attention. This mechanism allows the model to weigh the importance of different words in a sentence, improving the language model’s performance in understanding and generating language.\\n4. Why are Large Language Models important in AI technologies?\\nLarge Language Models are important because they serve as foundation models for various AI technologies like virtual assistants, conversational AI, and search engines. They enhance the ability of machines to understand and generate human language, making interactions with technology more natural.\\n5. What is fine-tuning in the context of LLMs?\\nFine-tuning involves taking a pre-trained language model and further training it on a specific task or dataset. This process adjusts the model to perform better on specific tasks like sentiment analysis, handling programming languages, or other specialized applications.\\n6. How does model size affect the performance of Large Language Models?\\nThe model size, often measured by the parameter count, affects an LLM’s ability to capture complex language patterns. Very large models with hundreds of billions of parameters generally perform better but require more computational resources during the training process.\\n7. Can LLMs generate code in programming languages?\\nYes, Large Language Models can generate code in various programming languages. They assist developers by providing code snippets, debugging help, and translating code, thanks to their training on diverse datasets that include programming code.\\n8. What is “in-context learning” in Large Language Models?\\nIn-context learning refers to an LLM’s ability to learn and perform specific tasks based solely on the input text provided during inference, without additional fine-tuning. This allows the model to adapt to new tasks or instructions on the fly, enhancing its versatility across a broad range of applications.\\n9. How do LLMs handle multiple tasks like text generation and sentiment analysis?\\nLLMs are versatile due to their training on diverse data. They can perform multiple tasks like text generation, sentiment analysis, and more by leveraging their learned knowledge. Through fine-tuning, they can be adapted to perform specific tasks more effectively.\\n10. What are “zero-shot” and “few-shot” learning in Large Language Models?\\nZero-shot learning allows an LLM to perform a specific task it wasn’t explicitly trained on by leveraging its general language understanding. Few-shot learning involves providing the model with a few examples of the task within the prompt to guide its response. Both methods showcase the model’s ability to generalize and adapt to new tasks with minimal or no additional training data.\\nInstantly access the power of AI and our team of AI-enabled practitioners\\nWe are ready to support you on your project!\\nContact us\\n\\nCategory: Gen AI\\nTags: AI, artificial intelligence, gen ai, Generative AI, large language models, LLMs\\n\\nGet the best of our content\\nstraight to your inbox!\\nDon’t worry, we don’t spam!\\nRelated Posts\\n\\nProprietary Patient Management System Unlocks 99% Faster Implementation and Client Onboarding\\n\\nAmazon Q Developer: The AWS Tool Revolutionizing Cloud Interaction\\n\\nPractical Data Governance Pillars: Safeguarding Your Digital Assets\\n\\nTesting Your RAG-Powered AI Chatbot\\nCategories\\n\\nAgile\\nCulture\\nModernization\\nNearshore Development\\nProduct + Design\\nSoftware Development\\nTalent\\n\\n\\nSubscribe to our newsletter and stay up to date on the latest in AI\\nServices\\n\\nAI Strategy Roadmap\\nData Engineering & Analytics\\nAI-Powered Software Development\\nAI Engineering Teams\\n\\nPartnerships\\n\\nDatabricks\\n\\nAccelerators\\n\\nGen AI Innovation Workshop\\nGen AI Solution Accelerator\\nRAG\\nGenIQ\\n\\nIndustries\\n\\nCommunications & IoT\\nTechnology\\nHealthcare\\nFinance\\nRetail\\n\\nResources\\n\\nBlog\\nTalking AI Podcast\\nTalking AI Newsletter\\nEvents\\nNearshore Budget Calculator\\n\\nGet in touch\\n\\nBook a call\\n1-800-621-7063\\n\\nFacebook Youtube \\n\\nAtlanta, GA [HQ]\\nChicago, IL\\nDallas, TX \\u200b\\nSan Jose, Costa Rica [HQ]\\nBogota, Colombia\\nMedellin, Colombia\\nBarranquilla, Colombia\\nLima, Peru\\n\\n\\n\\n©2023 HatchWorks Inc. All rights reserved.\\nPrivacy Policy\\u200b\\nTerms and Conditions\\nRecruitment Fraud Disclaimer\\n\\nClose this module\\n\\nFREE E-BOOKState of AI 2025\\nA round-up of industry stats, research, and insights to understand where AI stands, how it got here, and where it’s going.\\nNameName\\nEmailEmail\\nCompany NameCompany Name\\nDownload E-book\\nNo thanks, I’m not interested!\"}],\n",
              " 'response_time': 1.64}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_weather.invoke('Hyderabad')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_rGY_Tr2T6F",
        "outputId": "90e8d6ee-e0ad-4cfd-afdf-172475c519b9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'location': {'name': 'Hyderabad',\n",
              "  'region': 'Telangana',\n",
              "  'country': 'India',\n",
              "  'lat': 17.3753,\n",
              "  'lon': 78.4744,\n",
              "  'tz_id': 'Asia/Kolkata',\n",
              "  'localtime_epoch': 1740130758,\n",
              "  'localtime': '2025-02-21 15:09'},\n",
              " 'current': {'last_updated_epoch': 1740130200,\n",
              "  'last_updated': '2025-02-21 15:00',\n",
              "  'temp_c': 33.1,\n",
              "  'temp_f': 91.6,\n",
              "  'is_day': 1,\n",
              "  'condition': {'text': 'Partly cloudy',\n",
              "   'icon': '//cdn.weatherapi.com/weather/64x64/day/116.png',\n",
              "   'code': 1003},\n",
              "  'wind_mph': 7.6,\n",
              "  'wind_kph': 12.2,\n",
              "  'wind_degree': 348,\n",
              "  'wind_dir': 'NNW',\n",
              "  'pressure_mb': 1016.0,\n",
              "  'pressure_in': 30.0,\n",
              "  'precip_mm': 0.0,\n",
              "  'precip_in': 0.0,\n",
              "  'humidity': 30,\n",
              "  'cloud': 50,\n",
              "  'feelslike_c': 31.0,\n",
              "  'feelslike_f': 87.7,\n",
              "  'windchill_c': 33.8,\n",
              "  'windchill_f': 92.9,\n",
              "  'heatindex_c': 31.8,\n",
              "  'heatindex_f': 89.2,\n",
              "  'dewpoint_c': 5.9,\n",
              "  'dewpoint_f': 42.6,\n",
              "  'vis_km': 6.0,\n",
              "  'vis_miles': 3.0,\n",
              "  'uv': 3.3,\n",
              "  'gust_mph': 8.7,\n",
              "  'gust_kph': 14.1}}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "tools = [search_web, get_weather]\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
        "llm_with_tools = llm.bind_tools(tools)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucF0fYXm1zgq",
        "outputId": "5e737974-35d9-447b-e4c7-e06bbcdf575e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_with_tools.invoke('What is AI in 1 line')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_Z-6p5Q2pOC",
        "outputId": "e283bdb2-c302-4ada-a779-2b9e2958c9d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='AI, or artificial intelligence, is the ability of a computer to perform tasks that normally require human intelligence.\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-5b7abb29-029e-440d-ba87-49e0853cf42c-0', usage_metadata={'input_tokens': 65, 'output_tokens': 22, 'total_tokens': 87})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_with_tools.invoke('What are some of the latest LLMs released')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJtvE9Id4Rui",
        "outputId": "3b0d5318-4dff-4999-864d-423198651811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'name': 'search_web', 'arguments': '{\"query\": \"latest large language models released\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-9caca380-9768-483d-a19f-9c1eedbb0d51-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'latest large language models released'}, 'id': 'de89a4c8-e22a-402c-b66a-2ec4b1975228', 'type': 'tool_call'}], usage_metadata={'input_tokens': 67, 'output_tokens': 9, 'total_tokens': 76})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the Agentic Graph with LangGraph built-ins - Recommended Flow\n",
        "\n",
        "Here we will use LangGraph to build the full graph which will have the Agentic workflow.\n",
        "\n",
        "Each functionality will be implemented as we would in the real-world.\n",
        "\n",
        "Here we will be replacing our `BasicToolNode` for the prebuilt [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#toolnode),\n",
        "and our `route_tools` condition with the prebuilt [tools_condition](https://langchain-ai.github.io/langgraph/reference/prebuilt/#tools_condition)"
      ],
      "metadata": {
        "id": "JPcJl2m-vYMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "\n",
        "#from langchain_openai import ChatOpenAI\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.messages import BaseMessage\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "# define function which will be used to store all agent messages\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "# start the graph building\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "# add tools and bind to LLM\n",
        "tools = [search_web, get_weather]\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "# add the LLM to graph\n",
        "def chatbot(state: State):\n",
        "    current_state= state['messages']\n",
        "    response = llm_with_tools.invoke(current_state)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "# Define entry point to the graph\n",
        "graph_builder.set_entry_point(\"chatbot\")"
      ],
      "metadata": {
        "id": "akYszF3ivXiS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da2ecf3b-b23c-46f0-8e4e-c63b64f5a08b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x79beac40db50>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the graph\n",
        "agent = graph_builder.compile()\n",
        "display(Image(agent.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "yQA0K2Q_5Eyg",
        "outputId": "5b956461-8363-4af2-e558-224729c8c417",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Add tools to a node\n",
        "tool_node = ToolNode(tools=tools)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n"
      ],
      "metadata": {
        "id": "1wy2bFFX4LkU",
        "outputId": "0e5ed600-f4d3-48cf-ebcb-f1cfb8b869fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langgraph.graph.state:Adding a node to a graph that has already been compiled. This will not be reflected in the compiled graph.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x79beac40db50>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the graph\n",
        "agent = graph_builder.compile()\n",
        "display(Image(agent.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "a3msp5cS5K5W",
        "outputId": "0338cf2e-3e26-442d-8d42-50bafad46e19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add conditional edges\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        "    ['tools', '__end__']\n",
        ")"
      ],
      "metadata": {
        "id": "ARNLl4_a4mAn",
        "outputId": "2258943c-dcbf-43a3-f8b8-6a7ed32edcc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langgraph.graph.graph:Adding an edge to a graph that has already been compiled. This will not be reflected in the compiled graph.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x79beac40db50>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the graph\n",
        "agent = graph_builder.compile()\n",
        "display(Image(agent.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "vj2HVlWF5Mwv",
        "outputId": "05473220-ab9d-4521-bb02-6ea37710dd22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Any time a tool is called, we return to the chatbot to decide the next step\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")"
      ],
      "metadata": {
        "id": "ZvDtNoYr4dUy",
        "outputId": "4caa7663-9736-448d-8db0-029e800d580a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langgraph.graph.graph:Adding an edge to a graph that has already been compiled. This will not be reflected in the compiled graph.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x79beac40db50>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the graph\n",
        "agent = graph_builder.compile()\n",
        "display(Image(agent.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "jcxBlI_d5Q_n",
        "outputId": "3db53aa0-0be9-44b6-f8a8-fe02eb64ed80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# compile the graph\n",
        "agent = graph_builder.compile()"
      ],
      "metadata": {
        "id": "klvGqNPS4QmW",
        "outputId": "6b50d3cc-8c42-4c8c-a333-20ddf4d92f5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langgraph.graph.graph:Adding an edge to a graph that has already been compiled. This will not be reflected in the compiled graph.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display, Markdown\n",
        "\n",
        "display(Image(agent.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "rdBRSLehwMm_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "458b25be-2112-4e52-dcd1-4b305a33b142"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"What are some of the latest LLMs released?\"\"\"\n",
        "response = agent.invoke({\"messages\": (\"user\", prompt)})\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dabt40FjQXyK",
        "outputId": "2eb2ff06-69ff-409e-9d7e-220f648da69d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='What are some of the latest LLMs released?', additional_kwargs={}, response_metadata={}, id='f9b93af9-e9d4-4274-bd0d-f0b98109892e'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'search_web', 'arguments': '{\"query\": \"latest large language models released\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-1534e6b1-7c4d-4347-b4b8-85e61e94a83b-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'latest large language models released'}, 'id': '04859d0a-76ba-49db-9670-5c6fc0cf187a', 'type': 'tool_call'}], usage_metadata={'input_tokens': 68, 'output_tokens': 9, 'total_tokens': 77}),\n",
              "  ToolMessage(content='{\"query\": \"latest large language models released\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://explodingtopics.com/blog/list-of-llms\", \"title\": \"Best 22 Large Language Models (LLMs) (February 2025)\", \"content\": \"Inflection-2.5 is the latest large language model (LLM) developed by Inflection AI to power its conversational AI assistant, Pi. Significant upgrades have been made, as the model currently achieves over 94% of GPT-4’s average performance while only having 40% of the training FLOPs. In March 2024, the Microsoft-backed startup reached 1+ million daily active users on Pi. 13. Gemma is a series of lightweight open-source language models developed and released by Google DeepMind. Pythia is a series of 16 large language models developed and released by EleutherAI, a non-profit AI research lab. Alpaca is a 7 billion-parameter language model developed by a Stanford research team and fine-tuned from Meta\\'s LLaMA 7B model.\", \"score\": 0.9220975, \"raw_content\": \"Best 22 Large Language Models (LLMs) (February 2025)\\\\n\\\\n\\\\n\\\\nAbout\\\\nNewsletter\\\\nBlog\\\\n\\\\n\\\\nBest 22 Large Language Models (LLMs) (February 2025)\\\\n\\\\nby Anthony Cardillo\\\\nFebruary 7, 2025\\\\nLarge language models are pre-trained on large datasets and use natural language processing to perform linguistic tasks such as text generation, code completion, paraphrasing, and more.\\\\nThe initial release of ChatGPT sparked the rapid adoption of generative AI, which has led to large language model innovations and industry growth.\\\\nIn fact, 92% of Fortune 500 firms have started using generative AI in their workflows.\\\\nAs adoption continues to grow, so does the LLM industry. The global large language model market is projected to grow from $6.5 billion in 2024 to $140.8 billion by 2033.\\\\nWith that, here is a list of the top 21 LLMs available in September 2024.\\\\nLLM NameDeveloperRelease DateAccessParametersDeepSeek R1DeepSeekJanuary 20, 2025Open-Source671 billionGPT-4oOpenAIMay 13, 2024APIUnknownClaude 3.5AnthropicJune 20, 2024APIUnknownGrok-1xAINovember 4, 2023Open-Source314 billionMistral 7BMistral AISeptember 27, 2023Open-Source7.3 billionPaLM 2GoogleMay 10, 2023Open-Source340 billionFalcon 180BTechnology Innovation InstituteSeptember 6, 2023Open-Source180 billionStable LM 2Stability AIJanuary 19, 2024Open-Source1.6 billion, 12 billionGemini 1.5Google DeepMindFebruary 2nd, 2024APIUnknownLlama 3.1Meta AIJune 23, 2024Open-Source405 billionMixtral 8x22BMistral AIApril 10, 2024Open-Source141 billionInflection-2.5Inflection AIMarch 10, 2024ProprietaryUnknownJambaAI21 LabsMarch 29, 2024Open-Source52 billionCommand RCohereMarch 11, 2024Both35 billionGemmaGoogle DeepMindFebruary 21, 2024Open-Source2 billion, 7 billionPhi-3MicrosoftApril 23, 2024Both3.8 billionXGen-7BSalesforceJuly 3, 2023Open-Source7 billionDBRXDatabricks\\' Mosaic MLMarch 27, 2024Open-Source132 billionPythiaEleutherAIFebruary 13, 2023Open-Source70 million to 12 billionSoraOpenAIFebruary 15, 2024 (announced)APIUnknownAlpaca 7BStanford CRFMMarch 13, 2023Open-Source7 billionNemotron-4NvidiaJune 14, 2024Open-Source340 billion\\\\n1. DeepSeek R1\\\\n\\\\nDeveloper: DeepSeek\\\\nRelease date: January 2025\\\\nNumber of Parameters: 671B total, 37B active\\\\nWhat is it? DeepSeek R1 is a reasoning model that excels in math and coding. It beats or matches OpenAI o1 in several benchmarks, including MATH-500 and AIME 2024.\\\\nOn its release, DeepSeek immediately hit headlines due to the low cost of training compared to most major LLMs.\\\\nDeepSeek R1 is free to use and open-source. It\\'s accessible via the API, the DeepSeek website, and mobile apps.\\\\n2. GPT-4o\\\\n\\\\nDeveloper: OpenAI\\\\nRelease date: May 13, 2024\\\\nNumber of Parameters: Unknown\\\\nWhat is it? GPT-4o is the latest and most advanced OpenAI language model, succeeding GPT-4, GPT-3.5, and GPT-3. OpenAI claims that GPT-4o is 50% cheaper than GPT-4 despite being 2x faster at generating tokens. This multimodal model includes text, image, video, and voice capabilities packaged into one.\\\\nGPT-4o\\'s biggest upgrade is the Voice-to-Voice function, which will improve input response times to an average of 320 milliseconds (compared to a few seconds with GPT-4). This feature is expected to be released in the coming weeks.\\\\n3. Claude 3.5\\\\n\\\\nDeveloper: Anthropic\\\\nRelease date: March 14, 2024\\\\nNumber of Parameters: Unknown\\\\nWhat is it?\\xa0As a new upgrade from the highly rated\\xa0Claude 3, Claude 3.5 Sonnet is the first release of the new Claude 3.5 model family. Similar to Claude 3, it\\'ll also include the Haiku and Opus models. As debatably the biggest competitor to GPT-4 and ChatGPT, Claude made even bigger improvements to this model by maintaining the 200,000 token context window at a lower cost. This is much larger than GPT-4\\'s 32,000 token capabilities.\\\\nAccording to Anthropic\\'s report, Claude 3.5 Sonnet outperformed GPT-4o in major benchmarks like coding and text reasoning. Plus, this is Claude\\'s most advanced vision model, with the ability to transcribe text from images or generate insights from charts.\\\\nAmazon has invested over $4 billion in Anthropic, bringing the startup\\'s valuation to $15 billion. The Claude mobile app was also released in May 2024.\\\\n4. Grok-1\\\\n\\\\nDeveloper: xAI\\\\nRelease date: November 4, 2023\\\\nNumber of Parameters: 314 billion\\\\nWhat is it? Created by Elon Musk\\'s artificial intelligence startup xAI, Grok-1 is currently the largest open-source LLM released to date at 314 billion parameters. Grok directly integrates with X (Twitter), and users must pay for an X Premium+ subscription to gain access.\\\\nBecause of the model’s size, Grok has a mixture-of-experts (MoE) architecture that only uses 25% of its weights for any given input token to maximize calculation efficiency.\\\\nIn August 2024, both Grok-2 and Grok-2 mini were released to X users in beta. According to xAI\\'s reports, Grok-2 outperforms GPT-4o in numerous categories, such as GPQA, MMLU-Pro, and DocVQA.\\\\n5. Mistral 7B\\\\n\\\\nDeveloper: Mistral AI\\\\nRelease date: September 27, 2023\\\\nNumber of Parameters: 7.3 billion\\\\nWhat is it? Mistral 7B is an open-source language model with 32 layers, 32 attention heads, and eight key-value heads. Despite running with fewer parameters, they outperformed the Llama 2 family of models in nearly all metrics, including MMLU, reading comprehension, math, coding, etc.\\\\nMistral 7B is released under an Apache 2.0 license. Customers are free to download it locally, deploy it on the cloud, or run it on HuggingFace. The Paris-based startup is close to securing a new $600 million funding round that would value the company at $6 billion.\\\\n6. PaLM 2\\\\n\\\\nDeveloper: Google\\\\nRelease date: May 10, 2023\\\\nNumber of Parameters: 340 billion\\\\nWhat is it? PaLM 2 is an advanced large language model developed by Google. As the successor to the original Pathways Language Model (PaLM), it’s trained on 3.6 trillion tokens (compared to 780 billion) and 340 billion parameters (compared to 540 billion). PaLM 2 was originally used to power Google\\'s first generative AI chatbot, Bard (rebranded to Gemini in February 2024).\\\\n7. Falcon 180B\\\\n\\\\nDeveloper: Technology Innovation Institute (TII)\\\\nRelease date: September 6, 2023\\\\nNumber of Parameters: 180 billion\\\\nWhat is it? Developed and funded by the Technology Innovation Institute, Falcon 180B is an upgraded version of the earlier Falcon 40B LLM. It has 180 billion parameters, which is 4.5 times larger than the 40 billion parameters of Falcon 40B.\\\\nIn addition to Falcon 40B, it also outperforms other large language models like GPT-3.5 and LLaMA 2 on tasks such as reasoning, question answering, and coding. In February 2024, the UAE-based Technology Innovation Institute (TII) committed $300 million in funding to the Falcon Foundation.\\\\n8. Stable LM 2\\\\n\\\\nDeveloper: Stability AI\\\\nRelease date: January 19, 2024\\\\nNumber of Parameters: 1.6 billion and 12 billion\\\\nWhat is it? Stability AI, the creators of the Stable Diffusion text-to-image model, are the developers behind Stable LM 2. This series of large language models includes Stable LM 2 12B (12 billion parameters) and Stable LM 2 1.6B (1.6 billion parameters). Released in April 2024, the larger 12B model outperforms models like LLaMA 2 70B on key benchmarks despite being much smaller.\\\\n9. Gemini 1.5\\\\n\\\\nDeveloper: Google DeepMind\\\\nRelease date: February 2nd, 2024\\\\nNumber of Parameters: Unknown\\\\nWhat is it? Gemini 1.5 is Google\\'s next-generation large language model, offering a significant upgrade over its predecessor, Gemini 1.0. While it’s only available for early testing, Gemini 1.5 Pro provides a one million-token context window (1 hour of video, 700,000 words, or 30,000 lines of code), the largest to date compared to alternative LLMs and chatbots. This upgrade is 35 times larger than Gemini 1.0 Pro and surpasses the previous largest record of 200,000 tokens held by Anthropic’s Claude 2.1.\\\\n10. Llama 3.1\\\\n\\\\nDeveloper: Meta AI\\\\nRelease date: June 23, 2024\\\\nNumber of Parameters: 405 billion\\\\nWhat is it? Llama 3, the predecessor to Llama 3.1, was available in both 70B and 8B versions that outperformed other open-source models like Mistral 7B and Google\\'s Gemma 7B on MMLU, reasoning, coding, and math benchmarks. Now, users will notice major upgrades to the latest version, including 405 billion parameters and an expended context length of 128,000.\\\\nUsers will also notice more accuracy because of the impressive knowledge base, which has been trained on over 15 trillion tokens. Plus, Meta added eight additional languages for this model. The increased size of this model makes it the largest open-source model released to date.\\\\nCustomers can still access its predecessor, Llama 2, which is available in three versions: 7 billion, 13 billion, and 70 billion parameters.\\\\n11. Mixtral 8x22B\\\\n\\\\nDeveloper: Mistral AI\\\\nRelease date: April 10, 2024\\\\nNumber of Parameters: 141 billion\\\\nWhat is it? Mixtral 8x22B is Mistral AI\\'s latest and most advanced large language model. This sparse Mixture-of-Experts (SMoE) model has 141 billion total parameters but only uses 39B active parameters to focus on improving the model’s performance-to-cost ratio.\\\\nThe startup also recently released Mistral Large, a ChatGPT alternative that ranks second behind GPT-4 among API-based LLMs.\\\\n12. Inflection-2.5\\\\n\\\\nDeveloper: Inflection AI\\\\nRelease date: March 10, 2024\\\\nNumber of Parameters: Unknown\\\\nWhat is it? Inflection-2.5 is the latest large language model (LLM) developed by Inflection AI to power its conversational AI assistant, Pi. Significant upgrades have been made, as the model currently achieves over 94% of GPT-4’s average performance while only having 40% of the training FLOPs. In March 2024, the Microsoft-backed startup reached 1+ million daily active users on Pi.\\\\n13. Jamba\\\\n\\\\nDeveloper: AI21 Labs\\\\nRelease date: March 29, 2024\\\\nNumber of Parameters: 52 billion\\\\nWhat is it? AI21 Labs created Jamba, the world\\'s first production-grade Mamba-style large language model. It integrates SSM technology with elements of a traditional transformer model to create a hybrid architecture. The model is efficient and highly scalable, with a context window of 256K and deployment support of 140K context on a single GPU.\\\\n14. Command R\\\\n\\\\nDeveloper: Cohere\\\\nRelease date: March 11, 2024\\\\nNumber of Parameters: 35 billion\\\\nWhat is it? Command R is a series of scalable LLMs from Cohere that support ten languages and 128,000-token context length (around 100 pages of text). This model primarily excels at retrieval-augmented generation, code-related tasks like explanations or rewrites, and reasoning. In April 2024, Command R+ was released to support larger workloads and provide real-world enterprise support.\\\\n15. Gemma\\\\n\\\\nDeveloper: Google DeepMind\\\\nRelease date: February 21, 2024\\\\nNumber of Parameters: 2 billion and 7 billion\\\\nWhat is it? Gemma is a series of lightweight open-source language models developed and released by Google DeepMind. The Gemma models are built with similar tech to the Gemini models, but Gemma is limited to text inputs and outputs only. The models have a context window of 8,000 tokens and are available in 2 billion and 7 billion parameter sizes.\\\\n16. Phi-3\\\\n\\\\nDeveloper: Microsoft\\\\nRelease date: April 23, 2024\\\\nNumber of Parameters: 3.8 billion\\\\nWhat is it? Classified as a small language model (SLM), Phi-3 is Microsoft\\'s latest release with 3.8 billion parameters. Despite the smaller size, it\\'s been trained on 3.3 trillion tokens of data to compete with Mistral 8x7B and GPT-3.5 performance on MT-bench and MMLU benchmarks.\\\\nTo date, Phi-3-mini is the only model available. However, Microsoft plans to release the Phi-3-small and Phi-3-medium models later this year.\\\\n17. XGen-7B\\\\n\\\\nDeveloper: Salesforce\\\\nRelease date: July 3, 2023\\\\nNumber of Parameters: 7 billion\\\\nWhat is it? XGen-7B is a large language model from Salesforce with 7 billion parameters and an 8k context window. The model was trained on 1.37 trillion tokens from various sources, such as RedPajama, Wikipedia, and Salesforce\\'s own Starcoder dataset.\\\\nSalesforce has released two open-source versions, a 4,000 and 8,000 token context window base, hosted under an Apache 2.0 license.\\\\n18. DBRX\\\\n\\\\nDeveloper: Databricks\\' Mosaic ML\\\\nRelease date: March 27, 2024\\\\nNumber of Parameters: 132 billion\\\\nWhat is it? DBRX is an open-source LLM built by Databricks and the Mosaic ML research team. The mixture-of-experts architecture has 36 billion (of 132 billion total) active parameters on an input. DBRX has 16 experts and chooses 4 of them during inference, providing 65 times more expert combinations compared to similar models like Mixtral and Grok-1\\\\n19. Pythia\\\\n\\\\nDeveloper: EleutherAI\\\\nRelease date: February 13, 2023\\\\nNumber of Parameters: 70 million to 12 billion\\\\nWhat is it? Pythia is a series of 16 large language models developed and released by EleutherAI, a non-profit AI research lab. There are eight different model sizes: 70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B. Because of Pythia\\'s open-source license, these LLMs serve as a base model for fine-tuned, instruction-following LLMs like Dolly 2.0 by Databricks.\\\\n20. Sora\\\\n\\\\nDeveloper: OpenAI\\\\nRelease date: February 15, 2024 (announced)\\\\nNumber of Parameters: Unknown\\\\nWhat is it? OpenAI\\'s latest development is Sora, a text-to-video model that combines LLMs and generative AI to turn text prompts into realistic videos up to 60 seconds long. The model uses a transformer architecture that operates on \\\\\"spacetime patches\\\\\" of video and image data rather than text tokens like other LLMs. No official release date for Sora has been announced, but OpenAI expects it to open to the public in late 2024.\\\\n21. Alpaca 7B\\\\n\\\\nDeveloper: Stanford CRFM\\\\nRelease date: March 27, 2024\\\\nNumber of Parameters: 7 billion\\\\nWhat is it? Alpaca is a 7 billion-parameter language model developed by a Stanford research team and fine-tuned from Meta\\'s LLaMA 7B model. Users will notice that although being much smaller, Alpaca performs similarly to text-DaVinci-003 (ChatGPT 3.5). However, Alpaca 7B is available for research purposes, and no commercial licenses are available.\\\\n22. Nemotron-4 340B\\\\n\\\\nDeveloper: NVIDIA\\\\nRelease date: June 14, 2024\\\\nNumber of Parameters: 340 billion\\\\nWhat is it? Nemotron-4 340B\\xa0is a family of large language models for synthetic data generation and AI model training. These models help businesses create new LLMs without larger and more expensive datasets. Instead, Nemotron-4 can create high-quality synthetic data to train other AI models, which reduces the need for extensive human-annotated data.\\\\nThe model family includes Nemotron-4-340B-Base (foundation model), Nemotron-4-340B-Instruct (fine-tuned chatbot), and Nemotron-4-340B-Reward (quality assessment and preference ranking). Due to the 9 trillion tokens used in training, which includes English, multilingual, and coding language data, Nemotron-4 matches GPT-4\\'s high-quality synthetic data generation capabilities.\\\\nConclusion\\\\nThe landscape of large language models is rapidly evolving, with new breakthroughs and innovations emerging at an unprecedented pace.\\\\nFrom compact models like Phi-2 and Alpaca 7B to cutting-edge architectures like Jamba and DBRX, the field of LLMs is pushing the boundaries of what\\'s possible in natural language processing (NLP).\\\\nWe will keep this list regularly updated with new models. If you liked learning about these LLMs, check out our lists of generative AI startups and AI startups.\\\\nFind Thousands of Trending Topics With Our Platform\\\\nTry Exploding Topics Pro\\\\n\\\\nExploding Topics\\\\n\\\\nJoin Pro\\\\nNewsletter\\\\nTrending Topics\\\\nAdd a Topic\\\\nCustomer Login\\\\n\\\\nCompany\\\\n\\\\nAbout Us\\\\nContact\\\\nMethodology\\\\nCookie Settings\\\\n\\\\nFree Tools\\\\n\\\\nKeyword Research\\\\nBacklink Checker\\\\nSERP Checker\\\\nKeyword Rank Checker\\\\nFree SEO Tools\\\\n\\\\nConnect\\\\n\\\\nYouTube\\\\nInstagram\\\\nX (Twitter)\\\\n\\\\nResources\\\\n\\\\nBlog\\\\nMarketing Academy\\\\nFree Webinars\\\\n\\\\n\\\\n\\\\n© 2025 \\xa0Exploding Topics is a Trademark of Semrush Inc\\\\n\\\\nPrivacy Policy\\\\nTerms of Service\\\\n\"}, {\"url\": \"https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models\", \"title\": \"25 of the best large language models in 2025 - TechTarget\", \"content\": \"Large language models are the dynamite behind the\\xa0generative AI\\xa0boom. Some of the most well-known language models today are based on the transformer model, including the\\xa0generative pre-trained transformer series\\xa0of LLMs and bidirectional encoder representations from transformers (BERT). Gemma\\xa0is a family of open-source language models from Google that were trained on the same resources as Gemini. GPT-3\\xa0is OpenAI\\'s large language model with more than 175 billion parameters, released in 2020. Large Language Model Meta AI (Llama) is Meta\\'s LLM which was first released in 2023. The\\xa0Pathways Language Model\\xa0is a 540 billion parameter transformer-based model from Google powering its AI chatbot\\xa0Bard. StableLM is a series of open language models developed by Stability AI, the company behind image generator Stable Diffusion.\", \"score\": 0.83549726, \"raw_content\": \"25 of the best large language models in 2025\\\\nWhatIs\\\\nSearch the TechTarget Network \\\\nBrowse Definitions :\\\\n\\\\nA\\\\nB\\\\nC\\\\nD\\\\nE\\\\nF\\\\nG\\\\nH\\\\nI\\\\nJ\\\\nK\\\\nL\\\\nM\\\\nN\\\\nO\\\\nP\\\\nQ\\\\nR\\\\nS\\\\nT\\\\nU\\\\nV\\\\nW\\\\nX\\\\nY\\\\nZ\\\\n#\\\\n\\\\nLogin Register\\\\n\\\\nTechTarget Network\\\\nTech Accelerator\\\\nNews\\\\n2024 IT Salary Survey Results\\\\n\\\\nRSS\\\\n\\\\n\\\\nWhatIs\\\\n\\\\n\\\\nBrowse Definitions Data analytics and AI\\\\nTopics View All\\\\n\\\\nBusiness software\\\\nCloud computing\\\\nComputer science\\\\nData centers\\\\nIT management\\\\nNetworking\\\\nSecurity\\\\nSoftware development\\\\n\\\\nPlease select a category\\\\n\\\\nTopics\\\\n\\\\n\\\\n\\\\nBrowse Features Resources\\\\n\\\\nBusiness strategies\\\\nCareer resources\\\\nEmerging tech\\\\nTech explainers\\\\n\\\\n\\\\n\\\\nFollow:\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nHome\\\\n\\\\nData analytics and AI\\\\n\\\\nTech Accelerator What is Gen AI? Generative AI explained\\\\nPrev Next Will AI replace jobs? 17 job types that might be affected Pros and cons of AI-generated content\\\\nDownload this guide1\\\\nFeature\\\\n25 of the best large language models in 2025\\\\nLarge language models have been affecting search for years and have been brought to the forefront by ChatGPT and other chatbots.\\\\n\\\\nShare this item with your network:\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nBy\\\\n\\\\nSean Michael Kerner\\\\nBen Lutkevich, Site Editor\\\\n\\\\nPublished: 31 Jan 2025\\\\nLarge language models are the dynamite behind the\\xa0generative AI\\xa0boom. However, they\\'ve been around for a while.\\\\nLLMs\\xa0are black box AI systems that use deep learning on extremely large datasets to understand and generate new text. Modern LLMs began taking shape in 2014 when the attention mechanism -- a machine learning technique designed to mimic human cognitive attention -- was introduced in a\\xa0research paper\\xa0titled \\\\\"Neural Machine Translation by Jointly Learning to Align and Translate.\\\\\" In 2017, that attention mechanism was honed with the introduction of the transformer model in another\\xa0paper, \\\\\"Attention Is All You Need.\\\\\"\\\\nSome of the most well-known language models today are based on the transformer model, including the\\xa0generative pre-trained transformer series\\xa0of LLMs and bidirectional encoder representations from transformers (BERT).\\\\nChatGPT, which runs on a set of language models from OpenAI, attracted more than 100 million users just two months after its release in 2022. Since then, many competing models have been released. Some belong to big companies such as Google, Amazon and Microsoft; others are open source.\\\\nConstant developments in the field can be difficult to keep track of. Here are some of the most influential models, both past and present. Included in it are models that paved the way for today\\'s leaders as well as those that could have a significant effect in the future.\\\\nThis article is part of\\\\nWhat is Gen AI? Generative AI explained\\\\n\\\\nWhich also includes:\\\\n8 top generative AI tool categories for 2025\\\\nWill AI replace jobs? 17 job types that might be affected\\\\n25 of the best large language models in 2025\\\\n\\\\nTop current LLMs\\\\nBelow are some of the most relevant large language models today. They do natural language processing and influence the architecture of future models.\\\\nBERT\\\\nBERT\\xa0is a family of LLMs that Google introduced in 2018. BERT is a\\xa0transformer-based\\xa0model that can convert sequences of data to other sequences of data. BERT\\'s architecture is a stack of transformer encoders and features 342 million parameters. BERT was pre-trained on a large corpus of data then fine-tuned to perform specific tasks along with natural language inference and sentence text similarity. It was used to improve query understanding in the 2019 iteration of Google search.\\\\nClaude\\\\nThe\\xa0Claude LLM\\xa0focuses on constitutional AI, which shapes AI outputs guided by a set of principles that help the AI assistant it powers helpful, harmless and accurate. Claude was created by the company Anthropic.\\\\nThere are three primary branches of Claude -- Opus, Haiku and Sonnet. The latest iteration of the Claude LLM is the Claude 3.5 Sonnet. It understands nuance, humor and complex instructions better than earlier versions of the LLM. It also has broad programming capabilities that make it well-suited for application development. In October 2024, Claude added a computer-use AI tool, that enables the LLM to use a computer like a human does. It\\'s available via Claude.ai, the Claude iOS app and through an API.\\\\nCohere\\\\nCohere is an enterprise AI platform that provides several LLMs including Command, Rerank and Embed. These\\xa0LLMs can be custom-trained\\xa0and fine-tuned to a specific company\\'s use case. The company that created the Cohere LLM was founded by one of the authors of Attention Is All You Need.\\\\nDeepSeek-R1\\\\nDeepSeek-R1 is an open-source reasoning model for tasks with complex reasoning, mathematical problem-solving and logical inference. The model uses reinforcement learning techniques to refine its reasoning ability and solve complex problems. DeepSeek-R1 can perform critical problem-solving through self-verification, chain-of-thought reasoning and reflection.\\\\nErnie\\\\nErnie is Baidu\\'s large language model which powers the Ernie 4.0 chatbot. The bot was released in August 2023 and has garnered more than 45 million users. Ernie is rumored to have 10 trillion parameters. The bot works best in Mandarin but is capable in other languages.\\\\nFalcon\\\\nFalcon is a family of transformer-based models developed by the Technology Innovation Institute. It is open source and has multi-lingual capabilities. Falcon 2 is available in an 11 billion parameter version that provide multimodal capabilities for both text and vision.\\\\nThe Falcon 1 series includes a pair of larger models with Falcon 40B and Falcon 180B. Falcon models are available on GitHub as well as on cloud provider including Amazon.\\\\nGemini\\\\nGemini\\xa0is Google\\'s family of LLMs that power the company\\'s chatbot of the same name. The model replaced Palm in powering the chatbot, which was rebranded from Bard to Gemini upon the model switch. Gemini models are multimodal, meaning they can handle images, audio and video as well as text. Gemini is also integrated in many Google applications and products. It comes in three sizes -- Ultra, Pro and Nano. Ultra is the largest and most capable model, Pro is the mid-tier model and Nano is the smallest model, designed for efficiency with on-device tasks.\\\\nAmong the most recent models is the Gemini 1.5 Pro update that debuted in May 2024 Gemini is available as a web chatbot, the Google Vertex AI service and via API. Early previews of Gemini 2.0 Flash became available in December 2024 with updated multimodal generation capabilities.\\\\nGemma\\\\nGemma\\xa0is a family of open-source language models from Google that were trained on the same resources as Gemini. Gemma 2 was released in June 2024 in two sizes -- a 9 billion parameter model and a 27 billion parameter model. Gemma models can be\\xa0run locally\\xa0on a personal computer, and are also available in Google Vertex AI.\\\\nGPT-3\\\\nGPT-3\\xa0is OpenAI\\'s large language model with more than 175 billion parameters, released in 2020. GPT-3 uses a decoder-only transformer architecture. In September 2022, Microsoft announced it had exclusive use of GPT-3\\'s underlying model. GPT-3 is 10 times larger than its predecessor. GPT-3\\'s training data includes Common Crawl, WebText2, Books1, Books2 and Wikipedia.\\\\nGPT-3 is the last of the GPT series of models in which OpenAI made the parameter counts publicly available. The GPT series was first introduced in 2018 with OpenAI\\'s paper \\\\\"Improving Language Understanding by Generative Pre-Training.\\\\\"\\\\nGPT-3.5\\\\nGPT-3.5 is an upgraded version of GPT-3 with fewer parameters. GPT-3.5 was fine-tuned using\\xa0reinforcement learning from human feedback. GPT-3.5 is the version of GPT that powers ChatGPT. There are several models, with GPT-3.5 turbo being the most capable, according to OpenAI. GPT-3.5\\'s training data extends to September 2021.\\\\nIt was also integrated into the Bing search engine but has since been replaced with GPT-4.\\\\nGPT-4\\\\nGPT-4\\xa0, was released in 2023 and like the others in the OpenAI GPT family, it\\'s a\\xa0transformer-based model. Unlike the others, its parameter count has not been released to the public, though there are rumors that the model has more than 170 trillion. OpenAI describes GPT-4 as a multimodal model, meaning it can\\xa0process and generate both language and images\\xa0as opposed to being limited to only language. GPT-4 also introduced a system message, which lets users specify tone of voice and task.\\\\nGPT-4 demonstrated human-level performance in multiple academic exams. At the model\\'s release, some speculated that GPT-4 came close to\\xa0artificial general intelligence, which means it is as smart or smarter than a human. That speculation turned out to be unfounded.\\\\nGPT-4o\\\\nGPT-4 Omni (GPT-4o) is OpenAI\\'s successor to GPT-4 and offers several improvements over the previous model. GPT-4o creates a more natural human interaction for ChatGPT and is a large multimodal model, accepting various inputs including audio, image and text. The conversations let users engage as they would in a normal human conversation, and the real-time interactivity can also pick up on emotions. GPT-4o can see photos or screens and ask questions about them during interaction.\\\\nGPT-4o can respond in 232 milliseconds, similar to human response time and faster than GPT-4 Turbo.\\\\nGranite\\\\nThe IBM Granite family of models are fully open source models under the Apache v.2 license. The first iteration of the open source model models debuted in May 2024, followed by Granite 3.0 in October and Granite 3.1 in December 2024.\\\\nThere are multiple variants in the Granite model family including General-purpose models (8B and 2B variants), guardrail model and Mixture-of-Experts models. While the model can be used for general purpose deployments, IBM itself is focusing deployment and optimization for enterprise use cases like customer service, IT automation and cybersecurity.\\\\nLamda\\\\nLamda (Language Model for Dialogue Applications) is a family of LLMs developed by Google Brain announced in 2021. Lamda used a decoder-only transformer language model and was pre-trained on a large corpus of text. In 2022, LaMDA gained widespread attention when then-Google engineer Blake Lemoine went public with claims that the\\xa0program was sentient. It was built on the Seq2Seq architecture.\\\\nLlama\\\\nLarge Language Model Meta AI (Llama) is Meta\\'s LLM which was first released in 2023. The Llama 3.1 models were released in July 2024, including both a 405 billion and 70 billion parameter model.\\\\nThe most recent version is Llama 3.2 which was released in September 2024, initially with smaller parameter counts of 11 billion and 90 billion.\\\\nLlama uses a transformer architecture and was trained on a variety of public data sources, including webpages from CommonCrawl, GitHub, Wikipedia and Project Gutenberg. Llama was effectively leaked and spawned many descendants, including Vicuna and Orca. Llama is available under an open license, allowing for free use of the models. Lllama models are available in many locations including llama.com and Hugging Face.\\\\nMistral\\\\nMistral is a family of a mixture of expert models from Mistral AI. Among the newest models is Mistral Large 2 which was first released in July 2024. The model operates with 123 billion parameters and a 128k context window, supporting dozens of languages including French, German, Spanish, Italian, and many others, along with more than 80 coding languages.\\\\nIn November 2024, Mistral released Pixtral Large, a 124-billion-parameter multimodal model that can handle text and visual data. Mistral models are available via Mistral\\'s API on its Le Platforme-managed web service.\\\\no1\\\\nThe OpenAI o1 model family was first introduced in Sept. 2024. The o1 model\\'s focus is to provide what OpenAI refers to as - reasoning models, that can reason through a problem or query before offering a response.\\\\nThe o1 models excel in STEM fields, with strong results in mathematical reasoning (scoring 83% on the International Mathematics Olympiad compared to GPT-4o\\'s 13%), code generation and scientific research tasks. While they offer enhanced reasoning and improved safety features, they operate more slowly than previous models due to their thorough reasoning processes and come with certain limitations, such as restricted access features and higher API costs. The models are available to ChatGPT Plus and Team users, with varying access levels for different user categories.\\\\no3\\\\nOpenAI introduced the successor model, o3, in December 2024. According to OpenAI, o3 is designed to handle tasks with more analytical thinking, problem-solving and complex reasoning and will improve o1\\'s capabilities and performance. The o3 model is in safety testing mode and is currently not available to the public.\\\\nOrca\\\\nOrca was developed by Microsoft and has 13 billion parameters, meaning it\\'s small enough to run on a laptop. It aims to improve on advancements made by other open source models by imitating the reasoning procedures achieved by LLMs. Orca achieves the same performance as GPT-4 with significantly fewer parameters and is on par with GPT-3.5 for many tasks. Orca is built on top of the 13 billion parameter version of Llama.\\\\nPalm\\\\nThe\\xa0Pathways Language Model\\xa0is a 540 billion parameter transformer-based model from Google powering its AI chatbot\\xa0Bard. It was trained across multiple\\xa0TPU\\xa04 Pods -- Google\\'s custom hardware for machine learning. Palm specializes in reasoning tasks such as coding, math, classification and question answering. Palm also excels at decomposing complex tasks into simpler subtasks.\\\\nPaLM gets its name from a Google research initiative to build Pathways, ultimately creating a single model that serves as a foundation for multiple use cases. There are\\xa0several fine-tuned versions\\xa0of Palm, including Med-Palm 2 for life sciences and medical information as well as Sec-Palm for cybersecurity deployments to speed up threat analysis.\\\\nPhi\\\\nPhi is a transformer-based language model from Microsoft. The Phi 3.5 models were first released in August 2024.\\\\nThe series includes Phi-3.5-mini-instruct (3.82 billion parameters), Phi-3.5-MoE-instruct (41.9 billion parameters), and Phi-3.5-vision-instruct (4.15 billion parameters), each designed for specific tasks ranging from basic reasoning to vision analysis. All three models support a 128k token context length.\\\\nReleased under a Microsoft-branded MIT License, they are available for developers to download, use, and modify without restrictions, including for commercial purposes.\\\\nQwen\\\\nQwen is large family of open models developed by Chinese internet giant Alibaba Cloud. The newest set of models are the Qwen2.5 suite, which support 29 different languages and currently scale up to 72 billion parameters. These models are suitable for a wide range of tasks, including code generation, structured data understanding, mathematical problem-solving as well as general language understanding and generation.\\\\nStableLM\\\\nStableLM is a series of open language models developed by Stability AI, the company behind image generator Stable Diffusion.\\\\nStableLM 2 debuted in January 2024 initially with a 1.6 billion parameter model. In April 2024 that was expanded to also include a 12 billion parameter model. StableLM 2 supports seven languages: English, Spanish, German, Italian, French, Portuguese, and Dutch. Stability AI positions these models as offering different options for various use cases, with the 1.6B model suitable for specific, narrow tasks and faster processing while the 12B model provides more capability but requires more computational resources.\\\\nTülu 3\\\\nAllen Institute for AI\\'s Tülu 3 is an open-source 405 billion-parameter LLM. The Tülu 3 405B model has post-training methods that combine supervised fine-tuning and reinforcement learning at a larger scale. Tülu 3 uses a \\\\\"reinforcement learning from verifiable rewards\\\\\" framework for fine-tuning tasks with verifiable outcomes -- such as solving mathematical problems and following instructions.\\\\nVicuna 33B\\\\nVicuna is another influential open source LLM derived from Llama. It was developed by LMSYS and was fine-tuned using data from sharegpt.com. It is smaller and less capable that GPT-4 according to several benchmarks, but does well for a model of its size. Vicuna has only 33 billion parameters, whereas GPT-4 has trillions.\\\\nLLM precursors\\\\nAlthough LLMs are a recent phenomenon, their precursors go back decades. Learn how recent precursor Seq2Seq and distant precursor ELIZA set the stage for modern LLMs.\\\\nSeq2Seq\\\\nSeq2Seq is a deep learning approach used for machine translation, image captioning and natural language processing. It was developed by Google and underlies some of their modern LLMs, including LaMDA. Seq2Seq also underlies AlexaTM 20B, Amazon\\'s large language model. It uses a mix of encoders and decoders.\\\\nEliza\\\\nEliza was an\\xa0early natural language processing program\\xa0created in 1966. It is one of the earliest examples of a language model. Eliza simulated conversation using pattern matching and substitution. Eliza, running a certain script, could parody the interaction between a patient and therapist by applying weights to certain keywords and responding to the user accordingly. The creator of Eliza, Joshua Weizenbaum, wrote a book on the limits of computation and artificial intelligence.\\\\nNext Steps\\\\nGenerative AI challenges that businesses should consider\\\\nGenerative AI ethics: Biggest concerns\\\\nGenerative AI landscape: Potential future trends\\\\nGenerative models: VAEs, GANs, diffusion, transformers, NeRFs\\\\nAI content generators to explore\\\\nRelated Resources\\\\n\\\\nFive data quality trends to prepare for in the year ahead –Video\\\\nThe Digital Transformation And Innovation Landscape –Wipro\\\\nCloudera and NVIDIA Accelerate AI in the Financial Services Industry –Cloudera\\\\nImprove customer satisfaction or cut costs? Who says you have to choose? –Video\\\\n\\\\nDig Deeper on Data analytics and AI\\\\n\\\\n ##### What is GPT-3? Everything you need to know  By: Nick Barney\\\\n ##### What is a small language model (SLM)?  By: Sean Kerner\\\\n ##### GPT-4  By: Ben Lutkevich\\\\n ##### What are large language models (LLMs)?  By: Sean Kerner\\\\n\\\\nSponsored News\\\\n\\\\nSustainability, AI and Dell PowerEdge Servers –Dell Technologies and Intel\\\\nThree Innovative AI Use Cases for Natural Language Processing –Dell Technologies\\\\nAutonomous coding: The future of the revenue cycle –Solventum\\\\n\\\\nRelated Content\\\\n\\\\nExploring GPT-3 architecture – Search Enterprise AI\\\\nWhat is GPT-3? Everything you need to know – Search Enterprise AI\\\\nMicrosoft exclusively licenses OpenAI\\'s GPT-3 ... – Search Enterprise AI\\\\n\\\\nLatest TechTarget resources\\\\n\\\\nNetworking\\\\nSecurity\\\\nCIO\\\\nHR Software\\\\nCustomer Experience\\\\n\\\\nSearch Networking\\\\n\\\\n\\\\nWhat is a thin client (lean client)?A thin client (lean client) is a virtual desktop computing model that runs on the resources stored on a central server instead of...\\\\n\\\\n\\\\nWhat is network monitoring?Network monitoring, also frequently called network management, is the practice of consistently overseeing a computer network for ...\\\\n\\\\n\\\\nWhat is network automation?Network automation is a process that uses intelligent software to automate the management, configuration, deployment, testing and...\\\\n\\\\n\\\\nSearch Security\\\\n\\\\n\\\\nWhat is Internet Key Exchange (IKE)?Internet Key Exchange (IKE) is a standard protocol used to set up a secure and authenticated communication channel between two ...\\\\n\\\\n\\\\nWhat is a certificate revocation list (CRL) and how is it used?A certificate revocation list (CRL) is a list of digital certificates that have been revoked by the issuing certificate authority...\\\\n\\\\n\\\\nWhat is cryptology?Cryptology is the mathematics, such as number theory and the application of formulas and algorithms, that underpin cryptography ...\\\\n\\\\n\\\\nSearch CIO\\\\n\\\\n\\\\nWhat is an IT project manager?An IT project manager is a professional charged with overseeing the process of planning, executing and delegating ...\\\\n\\\\n\\\\nWhat is a cyberthreat hunter (cybersecurity threat analyst)?A cyberthreat hunter, also called a cybersecurity threat analyst, proactively identifies security incidents that might go ...\\\\n\\\\n\\\\nWhat is blockchain? Definition, examples and how it worksBlockchain is a distributed ledger technology (DLT) that\\'s shared across a network of computers to keep a digital record of ...\\\\n\\\\n\\\\nSearch HRSoftware\\\\n\\\\n\\\\nWhat is employee self-service (ESS)?Employee self-service (ESS) is a widely used human resources technology that enables employees to perform many job-related ...\\\\n\\\\n\\\\nWhat is DEI? Diversity, equity and inclusion explainedDiversity, equity and inclusion is a term used to describe policies and programs that promote the representation and ...\\\\n\\\\n\\\\nWhat is payroll software?Payroll software automates the process of paying salaried, hourly and contingent employees.\\\\n\\\\n\\\\nSearch Customer Experience\\\\n\\\\n\\\\nWhat is account-based selling? Everything you need to knowAccount-based selling (ABS) is a strategic sales approach in business-to-business sales and marketing that centers around ...\\\\n\\\\n\\\\nWhat is interactive voice response (IVR)?Interactive voice response (IVR) is an automated telephony system that interacts with callers, gathers information and routes ...\\\\n\\\\n\\\\nWhat is an AI assistant?An AI assistant, or digital assistant, is software that uses artificial intelligence to understand natural language voice ...\\\\n\\\\n\\\\nBrowse by Topic\\\\n\\\\n\\\\nBrowse Resources\\\\n\\\\n\\\\nAbout Us\\\\n\\\\nMeet The Editors\\\\nEditorial Ethics Policy\\\\nContact Us\\\\nAdvertisers\\\\nBusiness Partners\\\\nEvents\\\\nMedia Kit\\\\nCorporate Site\\\\nReprints\\\\n\\\\nAll Rights Reserved, Copyright 1999 - 2025, TechTarget  \\\\nPrivacy Policy\\\\nCookie Preferences\\\\nCookie Preferences\\\\nDo Not Sell or Share My Personal Information\\\\nClose\\\\n\\\\nX\\\\nFree Download What is generative AI? Everything you need to know\\\\nThe potential of AI technology has been percolating in the background for years. But when ChatGPT, the AI chatbot, began grabbing headlines in early 2023, it put generative AI in the spotlight. This guide is your go-to manual for generative AI, covering its benefits, limits, use cases, prospects and much more.\\\\n\"}, {\"url\": \"https://hatchworks.com/blog/gen-ai/large-language-models-guide/\", \"title\": \"Large Language Models: What You Need to Know in 2025\", \"content\": \"Large language models (LLMs) are the unsung heroes of recent Generative AI advancements, quietly working behind the scenes to understand and generate language as we know it. OpenAI released GPT-4, an even more powerful and versatile model than its predecessors, with improvements in understanding, reasoning, and generating text across a broader range of contexts and languages. 2022: The emergence of GPT-4 and other advanced models such as Midjourney, continuing to push the boundaries of what’s possible with LLMs in terms of generating and understanding natural language across various domains and tasks, including image generation. These models are trained on massive data sets and can perform a broad range of tasks like generating text, translating languages, and more. Tags: AI, artificial intelligence, gen ai, Generative AI, large language models, LLMs\", \"score\": 0.77994305, \"raw_content\": \"Large Language Models: What You Need to Know in 2025 | HatchWorks AI\\\\nSkip to content\\\\n\\\\n\\\\nWhat We Do\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nServices\\\\n\\\\n\\\\nAI Strategy & Roadmap\\\\n\\\\nData Engineering & Analytics\\\\nAI-Powered Software Development\\\\n\\\\nAI Engineering Teams\\\\n        *   *   Accelerators\\\\n\\\\n\\\\nGenerative Driven Development™\\\\n\\\\nAI Roadmap & ROI Workshop\\\\nAI Solution Accelerator\\\\nRAG\\\\n\\\\nGenIQ\\\\n        *   *   Industries\\\\n\\\\n\\\\nCommunications and IoT\\\\n\\\\nTechnology\\\\nHealthcare\\\\nFinance\\\\n\\\\nRetail\\\\n        *   *   Partnerships\\\\n\\\\n\\\\nDatabricks\\\\n\\\\nIndustries\\\\nCommunications and IoT Solutions\\\\nTechnology\\\\nHealthcare\\\\nFinance\\\\nRetail\\\\n\\\\n\\\\nAbout Us\\\\nAbout Us\\\\nCareers & Culture\\\\nHatchFutures\\\\nFAQ\\\\n\\\\n\\\\n\\\\nResources\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nInsights\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nBlog\\\\n\\\\nTalking AI Podcast\\\\n\\\\nTalking AI Newsletter\\\\n        *   *   Tools & Reports\\\\n\\\\n\\\\nState of AI Report 2025\\\\n\\\\nTech Talent Report 2024\\\\nNearshore Budget Calculator\\\\n\\\\nBuild your Own GPT\\\\n        *   *   Learn & Connect\\\\n\\\\n\\\\nEvents\\\\n        *   *   Media\\\\n\\\\n\\\\nNewsroom\\\\n\\\\nOur Work\\\\nCareers\\\\nContact\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nCareers\\\\nContact us\\\\nLarge Language Models: What You Need to Know in 2025\\\\n\\\\nMelissa Malec\\\\n\\\\nDecember 2, 2024\\\\n\\\\n\\\\nUpdated: January 16, 2025\\\\n\\\\n\\\\nLarge language models (LLMs) are the unsung heroes of recent Generative AI advancements, quietly working behind the scenes to understand and generate language as we know it.\\\\nBut how do they work? What are they capable of? And what should we look out for when using them?\\\\n\\\\nRead on and find out in this guide for LLMs in 2024. Jump ahead:\\\\n\\\\nUnderstanding Large Language Models\\\\nWhat is a Large Language Model?\\\\nHow Do Large Language Models Work?\\\\nKey Milestones in Large Language Model Development\\\\nCapabilities of Large Language Models\\\\nChallenges and Limitations of LLMs\\\\nThe Future of Language Models: What Comes Next?\\\\n\\\\nUnderstanding Large Language Models\\\\nLet’s get the basics out of the way. Here we’ll define the large language model (LLM), explain how they work, and provide a timeline of key milestones in LLM development.\\\\nWhat is a Large Language Model?\\\\nA large language model, often abbreviated to LLM, is a type of artificial intelligence model designed to understand natural language as well as generate it at a large scale.\\\\nWhen we say human language, we don’t just mean English, Spanish, or Cantonese. Those are certainly part of what LLMs are trained on but human language, in this context, also extends to:\\\\n\\\\nArt\\\\nDance\\\\nMorse code\\\\nGenetic code\\\\nHieroglyphics\\\\nCryptography\\\\nSign language\\\\nBody language\\\\nMusical notation\\\\nChemical signaling\\\\nEmojis and symbols\\\\nAnimal communication\\\\nHaptic communications\\\\nTraffic signs and signals\\\\nMathematical equations\\\\nProgramming languages\\\\n\\\\nLLMs are trained on billions of parameters and have the ability to learn from a wide range of data sources.\\\\nThis extensive training enables them to predict and produce text based on the input they receive so that they can engage in conversations, answer queries, or even write code.\\\\nSome of the leading very large models include giants like GPT, LLaMa, LaMDA, PaLM 2, BERT, and ERNIE.\\\\nThey’re at the heart of various applications, aiding in everything from customer service chatbots to content creation and software development.\\\\nSome companies even build their own LLMs but that requires significant time, investment, and tech knowledge. It’s much easier to integrate a pre-trained LLM into your own systems.\\\\nHow Do Large Language Models Work?\\\\nLarge Language Models use a blend of neural networks and machine learning (ML). It’s this blend that allows the technology to first process and then generate original text and imagery.\\\\nThink of neural networks as the LLM’s brain. It’s these networks that learn from vast amounts of data, improving over time as they’re exposed to more.\\\\nAs the model is trained on more data, it learns patterns, structures, and the nuances of language. It’s like teaching it the rules of grammar, the rhythm of poetry, and the jargon of technical manuals all at once.\\\\nMachine learning models then help the model to predict the next word in a sentence based on the words that come before it. This is done countless times, refining the model’s ability to generate coherent and contextually relevant text.\\\\nLLMs now also operate on a Transformer Architecture. This architecture allows the model to look at and weigh the importance of different words in a sentence. It’s the same as when we read a sentence and look for context clues to understand its meaning.\\\\n⚠️ While LLMs can generate original content, the quality, relevance, and innovativeness of their output can vary and require human oversight and refinement.\\\\nThe originality is also influenced by how the prompts are structured, the model’s training data, and the specific capabilities of the LLM in question.\\\\nKey Milestones in Large Language Model Development\\\\nLarge language models haven’t always been as useful as they are today. They’ve developed and been iterated upon significantly over time.\\\\nLet’s look at some of those key moments in LLM history. That way you can appreciate how far they’ve come and the rapid evolution in the last few years compared to decades of slow progress.\\\\n1966\\\\nELIZA\\\\n\\\\nThe first chatbot created by Joseph Weizenbaum, simulating a psychotherapist in conversation.\\\\n2013\\\\nword2vec\\\\n\\\\nA groundbreaking tool developed by a team led by Tomas Mikolov at Google, introducing efficient methods for learning word embeddings from raw text.\\\\n2018\\\\nGPT and BERT\\\\n\\\\nGPT (Generative Pretrained Transformer): OpenAI introduced GPT, showcasing a powerful model for understanding and generating human-like text.\\\\nBERT (Bidirectional Encoder Representations from Transformers): Developed by Google, BERT significantly advanced the state of the art in natural language understanding tasks.\\\\n\\\\n2020\\\\nGPT 3\\\\n\\\\nOpenAI released GPT-3, a model with 175 billion parameters, achieving unprecedented levels of language understanding and generation capabilities.\\\\nLate 2021\\\\nIntroduction of ChatGPT\\\\n\\\\nOpenAI introduced ChatGPT, a conversational agent based on the GPT-3.5 model, designed to provide more engaging and natural dialogue experiences. ChatGPT showcased the potential of GPT models in interactive applications.\\\\n2022\\\\nGPT-4\\\\nOpenAI released GPT-4, an even more powerful and versatile model than its predecessors, with improvements in understanding, reasoning, and generating text across a broader range of contexts and languages.\\\\n2022\\\\nMidjourney and Other Innovations\\\\n\\\\nThe launch of Midjourney, along with other models and platforms, reflected the growing diversity and application of AI in creative processes, design, and beyond, indicating a broader trend towards multimodal and specialized AI systems.\\\\nPre-2010: Early Foundations\\\\n\\\\n1950s-1970s: Early AI research lays the groundwork for natural language processing. Most famously, a tech called ‘Eliza’ was the world’s first chatbot.\\\\n1980s-1990s: Development of statistical methods for NLP, moving away from rule-based systems.\\\\n\\\\n2010: Initial Models\\\\n\\\\n2013: Introduction of word2vec, a tool for computing vector representations of words, which significantly improved the quality of NLP tasks by capturing semantic meanings of words.\\\\n\\\\n2014-2017: RNNs and Attention Mechanisms\\\\n\\\\n2014: Sequence to sequence (seq2seq) models and Recurrent Neural Networks (RNNs) become popular for tasks like machine translation.\\\\n2015: Introduction of Attention Mechanism, improving the performance of neural machine translation systems.\\\\n2017: The Transformer model is introduced in the paper “Attention is All You Need”, setting a new standard for NLP tasks with its efficient handling of sequences.\\\\n\\\\n2018: Emergence of GPT and BERT\\\\n\\\\nJune 2018: OpenAI introduces GPT (Generative Pretrained Transformer), a model that leverages unsupervised learning to generate coherent and diverse text.\\\\nOctober 2018: Google AI introduces BERT (Bidirectional Encoder Representations from Transformers), which uses bidirectional training of Transformer models to improve understanding of context in language.\\\\n\\\\n2019-2020: Larger and More Powerful Models\\\\n\\\\n2019: Introduction of GPT-2, an improved version of GPT with 1.5 billion parameters, showcasing the model’s ability to generate coherent and contextually relevant text over extended passages.\\\\n2020: OpenAI releases GPT-3, a much larger model with 175 billion parameters, demonstrating remarkable abilities in generating human-like text, translation, and answering questions.\\\\n\\\\n2021-2023: Specialization, Multimodality, and Democratization of LLMs\\\\n\\\\n2021-2022: Development of specialized models like Google’s LaMDA for conversational applications and Facebook’s OPT for open pre-trained transformers.\\\\n2021: Introduction of multimodal models like DALL·E by OpenAI, capable of generating images from textual descriptions, and CLIP, which can understand images in the context of natural language.\\\\n2022: The emergence of GPT-4 and other advanced models such as Midjourney, continuing to push the boundaries of what’s possible with LLMs in terms of generating and understanding natural language across various domains and tasks, including image generation. It’s also more accessible to larger numbers of people.\\\\n\\\\nCapabilities of Large Language Models\\\\nThe capabilities of Large Language Models are as vast as the datasets they’re trained on. Use cases range from generating code to suggesting strategy for a product launch and analyzing data points.\\\\nThis is because LLMs serve as foundation models that can be applied across multiple uses.\\\\nHere’s a list of LLM capabilities:\\\\n\\\\nText generation\\\\nLanguage translation\\\\nSummarization\\\\nQuestion answering\\\\nSentiment analysis\\\\nConversational agents\\\\nCode generation and explanation\\\\nNamed entity recognition\\\\nText classification\\\\nContent recommendation\\\\nLanguage modeling\\\\nSpell checking and grammar correction\\\\nParaphrasing and rewriting\\\\nKeyword and phrase extraction\\\\nDialogue systems\\\\n\\\\nAnd here’s a breakdown of some of the more common ones we see:\\\\nAutomated Code Generation\\\\nLLMs can generate code snippets, functions, or even entire modules based on natural language descriptions, reducing the time and effort required to implement common functionalities.\\\\nHere’s an example to illustrate how LLMs can be used for automated code generation:\\\\nPrompt:\\\\n“Write a Python function that takes a list of numbers as input and returns a list containing only the even numbers.”\\\\n\\\\nText Generation\\\\nLLMs can generate coherent, contextually relevant text based on prompts. This includes creating articles, stories, and even generating product descriptions.\\\\nHere’s an example to illustrate how LLMs can be used for text generation:\\\\nPrompt:\\\\n“Generate a product description for a cutting-edge smartwatch designed for fitness enthusiasts. The description should highlight its advanced health and fitness tracking, personalized coaching, long battery life, durability, connectivity features, and customizable design. Target the description to appeal to both seasoned athletes and beginners interested in tracking their fitness progress.”\\\\n\\\\nLanguage Translation\\\\nThey can translate text between different languages, often with a high degree of accuracy, depending on the languages involved and the model’s training data.\\\\nHere’s an example to illustrate how LLMs can be used for language translation:\\\\nPrompt:\\\\n“Translate the following English text into Spanish: ‘The quick brown fox jumps over the lazy dog.\\'”\\\\n\\\\nBug Detection and Correction\\\\nLLMs can help identify bugs in code by analyzing code patterns and suggesting fixes for common errors, potentially integrating with IDEs (Integrated Development Environments) to provide real-time assistance.\\\\nHere’s an example to illustrate how LLMs can be used for bug detection:\\\\nPrompt:\\\\n“The Python function below intends to return the nth Fibonacci number. Please identify and correct any bugs in the function.\\\\nPython Function:\\\\ndef fibonacci(n):\\\\nif n <\\\\\\\\= 1:\\\\nreturn n\\\\nelse:\\\\nreturn fibonacci(n – 1) + fibonacci(n – 2)”\\\\n\\\\nParaphrasing and Rewriting\\\\nThey can rephrase or rewrite text while maintaining the original meaning, useful for content creation and academic purposes.\\\\nHere’s an example to illustrate how LLMs can be used for paraphrasing:\\\\nPrompt:\\\\n“Rewrite the following sentence in a simpler and more concise way without losing its original meaning: ‘The comprehensive study on climate change incorporates a wide array of data, including historical weather patterns, satellite imagery, and computer model predictions, to provide a holistic view of the impacts of global warming.\\'”\\\\n\\\\nDialogue Systems\\\\nLLMs power sophisticated dialogue systems for customer service, interactive storytelling, and educational purposes, providing responses that can adapt to the user’s input.\\\\nThink of a chatbot on a software product you use where you can ask it questions and it generates insightful, helpful responses.\\\\nChallenges and Limitations of LLMs\\\\nLarge language models have come a long way since the early days of Eliza.\\\\nIn the last two years alone, we’ve seen LLMs power Generative AI and create high-quality text, music, video, and images.\\\\nBut with any technology, there will always be growing pains.\\\\nTechnical Limitations of Language Models\\\\nLarge Language Models sometimes face technical limitations impacting their accuracy and ability to understand context.\\\\nDomain Mismatch\\\\nModels trained on broad datasets may struggle with specific or niche subjects due to a lack of detailed data in those areas. This can lead to inaccuracies or overly generic responses when dealing with specialized knowledge.\\\\nWord Prediction\\\\nLLMs often falter with less common words or phrases, impacting their ability to fully understand or accurately generate text involving these terms. This limitation can affect the quality of translation, writing, and technical documentation tasks.\\\\nReal-time Translation Efficiency\\\\nWhile LLMs have made strides in translation accuracy, the computational demands of processing and generating translations in real-time can strain resources, especially for languages with complex grammatical structures or those less represented in training data.\\\\nHallucinations and Bias\\\\nOn occasion, LLM technology is too original. So original in fact that it’s making up information.\\\\nThis is a lesson Air Canada learned the hard way when its chatbot told a customer about a refund policy when no such policy exists, which they then had to honor.\\\\nFinally, LLMs can inadvertently propagate and amplify biases present in their training data, leading to outputs that may be discriminatory or offensive.\\\\nScalability and Environmental Impact\\\\nThe scalability of LLMs is tied to the impact it has on the environment. And that impact is turning out to be a big one.\\\\nTraining a system like GPT-3 took 1,287 Megawatt hours (MWh) of energy. To put that into perspective, 1 MWh could power about 330 homes for one hour in the United States.\\\\nThe image below shows the energy consumption of training four different LLMs.\\\\n\\\\nEnergy consumption doesn’t end at training—operating LLMs also uses a grotesque level of energy.\\\\nIn one report, Alex de Vries, founder of Digiconomist, has calculated that by 2027 the AI sector will consume between 85 to 134 Terawatt hours each year. That’s almost the same as the annual energy demand of the Netherlands.\\\\nWe can’t help but wonder how sustainable that is and what the long-term environmental impact will be on our energy sources. Especially when you consider LLMs are only going to become larger and more complex as we advance their capabilities.\\\\nAnd to maintain large language models, we’ll need to update them with new data and parameters as they arise. That will only expend more energy and resources.\\\\nThe Future of Language Models: What Comes Next?\\\\nNow that we’ve seen drastic and rapid improvement in the capabilities of LLMs through Generative AI, we expect users of AI to be fine-tuning prompts and discovering new use cases and applications.\\\\nIn the workplace especially, the focus will be on productivity hacks. It’s something we experiment with already through our Generative Driven Development™ offering, where our team has increased the productivity of software development by 30-50%.\\\\nHilary Ashton, Chief Product Officer at Teradata, shared her predictions for the future of LLMs and AI in AI Magazine:\\\\n\\\\nFirst, I foresee a massive productivity leap forward through GenAI, especially in technology and software. It’s getting more cost-effective to get into GenAI, and there are lots more solutions available that can help improve GenAI solutions. It will be the year when conversations gravitate to GenAI, ethics, and what it means to be human. In some cases, we’ll start to see the workforce shift and be reshaped, with the technology helping to usher in a four-day work week for some full-time employees.”\\\\nHilary Ashton\\\\n\\\\nAnd she’s right, especially when it comes to ethical considerations and where we humans add value AI can’t replicate.\\\\nWe’ll also see further democratization of AI with it infiltrating other areas of our life, much the same the computer has done since its invention.\\\\nWhat we know for certain is the development of LLMs and Generative AI is only getting started. And we want to be leading conversations on its use, ethics, scalability, and more as it evolves.\\\\nYou can be part of that conversation too:\\\\nListen or watch our Talking AI podcast where we interview AI experts and talk or sign up for our newsletter where we share insights and developments on LLMs, AI/ML, and Data governance, curated by our very own CTO, Omar Shanti.\\\\nFrequently Asked Questions About Large Language Models LLMs\\\\n1. What is a Large Language Model (LLM)?\\\\nA Large Language Model (LLM) is an artificial intelligence model that uses machine learning techniques, particularly deep learning and neural networks, to understand and generate human language. These models are trained on massive data sets and can perform a broad range of tasks like generating text, translating languages, and more.\\\\n2. How do Large Language Models work?\\\\nLarge Language Models work by leveraging transformer models, which utilize self-attention mechanisms to process input text. They are pre-trained on vast amounts of data and can perform in-context learning, allowing them to generate coherent and contextually relevant responses based on user inputs.\\\\n3. What is the significance of transformer models in LLMs?\\\\nTransformer models are crucial because they enable LLMs to handle long-range dependencies in text through self-attention. This mechanism allows the model to weigh the importance of different words in a sentence, improving the language model’s performance in understanding and generating language.\\\\n4. Why are Large Language Models important in AI technologies?\\\\nLarge Language Models are important because they serve as foundation models for various AI technologies like virtual assistants, conversational AI, and search engines. They enhance the ability of machines to understand and generate human language, making interactions with technology more natural.\\\\n5. What is fine-tuning in the context of LLMs?\\\\nFine-tuning involves taking a pre-trained language model and further training it on a specific task or dataset. This process adjusts the model to perform better on specific tasks like sentiment analysis, handling programming languages, or other specialized applications.\\\\n6. How does model size affect the performance of Large Language Models?\\\\nThe model size, often measured by the parameter count, affects an LLM’s ability to capture complex language patterns. Very large models with hundreds of billions of parameters generally perform better but require more computational resources during the training process.\\\\n7. Can LLMs generate code in programming languages?\\\\nYes, Large Language Models can generate code in various programming languages. They assist developers by providing code snippets, debugging help, and translating code, thanks to their training on diverse datasets that include programming code.\\\\n8. What is “in-context learning” in Large Language Models?\\\\nIn-context learning refers to an LLM’s ability to learn and perform specific tasks based solely on the input text provided during inference, without additional fine-tuning. This allows the model to adapt to new tasks or instructions on the fly, enhancing its versatility across a broad range of applications.\\\\n9. How do LLMs handle multiple tasks like text generation and sentiment analysis?\\\\nLLMs are versatile due to their training on diverse data. They can perform multiple tasks like text generation, sentiment analysis, and more by leveraging their learned knowledge. Through fine-tuning, they can be adapted to perform specific tasks more effectively.\\\\n10. What are “zero-shot” and “few-shot” learning in Large Language Models?\\\\nZero-shot learning allows an LLM to perform a specific task it wasn’t explicitly trained on by leveraging its general language understanding. Few-shot learning involves providing the model with a few examples of the task within the prompt to guide its response. Both methods showcase the model’s ability to generalize and adapt to new tasks with minimal or no additional training data.\\\\nInstantly access the power of AI and our team of AI-enabled practitioners\\\\nWe are ready to support you on your project!\\\\nContact us\\\\n\\\\nCategory: Gen AI\\\\nTags: AI, artificial intelligence, gen ai, Generative AI, large language models, LLMs\\\\n\\\\nGet the best of our content\\\\nstraight to your inbox!\\\\nDon’t worry, we don’t spam!\\\\nRelated Posts\\\\n\\\\nProprietary Patient Management System Unlocks 99% Faster Implementation and Client Onboarding\\\\n\\\\nAmazon Q Developer: The AWS Tool Revolutionizing Cloud Interaction\\\\n\\\\nPractical Data Governance Pillars: Safeguarding Your Digital Assets\\\\n\\\\nTesting Your RAG-Powered AI Chatbot\\\\nCategories\\\\n\\\\nAgile\\\\nCulture\\\\nModernization\\\\nNearshore Development\\\\nProduct + Design\\\\nSoftware Development\\\\nTalent\\\\n\\\\n\\\\nSubscribe to our newsletter and stay up to date on the latest in AI\\\\nServices\\\\n\\\\nAI Strategy Roadmap\\\\nData Engineering & Analytics\\\\nAI-Powered Software Development\\\\nAI Engineering Teams\\\\n\\\\nPartnerships\\\\n\\\\nDatabricks\\\\n\\\\nAccelerators\\\\n\\\\nGen AI Innovation Workshop\\\\nGen AI Solution Accelerator\\\\nRAG\\\\nGenIQ\\\\n\\\\nIndustries\\\\n\\\\nCommunications & IoT\\\\nTechnology\\\\nHealthcare\\\\nFinance\\\\nRetail\\\\n\\\\nResources\\\\n\\\\nBlog\\\\nTalking AI Podcast\\\\nTalking AI Newsletter\\\\nEvents\\\\nNearshore Budget Calculator\\\\n\\\\nGet in touch\\\\n\\\\nBook a call\\\\n1-800-621-7063\\\\n\\\\nFacebook Youtube \\\\n\\\\nAtlanta, GA [HQ]\\\\nChicago, IL\\\\nDallas, TX \\u200b\\\\nSan Jose, Costa Rica [HQ]\\\\nBogota, Colombia\\\\nMedellin, Colombia\\\\nBarranquilla, Colombia\\\\nLima, Peru\\\\n\\\\n\\\\n\\\\n©2023 HatchWorks Inc. All rights reserved.\\\\nPrivacy Policy\\u200b\\\\nTerms and Conditions\\\\nRecruitment Fraud Disclaimer\\\\n\\\\nClose this module\\\\n\\\\nFREE E-BOOKState of AI 2025\\\\nA round-up of industry stats, research, and insights to understand where AI stands, how it got here, and where it’s going.\\\\nNameName\\\\nEmailEmail\\\\nCompany NameCompany Name\\\\nDownload E-book\\\\nNo thanks, I’m not interested!\"}, {\"url\": \"https://www.shakudo.io/blog/top-9-large-language-models\", \"title\": \"Top 9 Large Language Models as of Feburary 2025 - Shakudo\", \"content\": \"The latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. The latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. The latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data.\", \"score\": 0.7730283, \"raw_content\": \"Top 9 Large Language Models as of Feburary 2025 | Shakudo\\\\nLatest in Insights : When to Choose Deep Learning Over Machine Learning (And Vice Versa)\\\\n\\\\n\\\\nWhy SHakudo\\\\n\\\\n Data & AI OS Build your ideal data stack on one unified platform Learn more >\\\\nshakudo AI Applications\\\\n Text to SQL Workflow Automation Vector Database Reverse ETLSee all >\\\\nComponents\\\\nSolutions\\\\n\\\\nShakudo for Industries\\\\nAerospace\\\\nAutomotive & Transportation\\\\nClimate & Energy\\\\nFinancial Services\\\\nHealthcare & Life Sciences\\\\nHigher Education\\\\nManufacturing\\\\nReal Estate\\\\nRetail\\\\nSports\\\\nTechnology & Software\\\\nShakudo Use Cases\\\\nAutomate Custom Sustainability Report Population\\\\nChat with Enterprise Knowledge Base Using AI Assistants\\\\nGenerate Real-World Evidence for Healthcare Decisions\\\\nOptimize Ticket Pricing with Dynamic Demand Modeling\\\\nDetect Hidden Red Flags in Company Data\\\\nMonitor Market Sentiment Across Multiple Sources\\\\nSee all >\\\\nResources\\\\n\\\\n Case Studies Learn how leading companies leverage data & AI on Shakudo blog Read what\\'s new at Shakudo and the data and AI world white papers Access in-depth reports and guides on data & AI solutions Docs Explore comprehensive guides on the Shakudo platform\\\\n  Case Study How CentralReach uses Shakudo to Cut Time-To-Deployment to Launch New AI- Powered Solutions\\\\n  Case Study How AI is Changing the Game for the Cleveland Cavaliers\\\\nCompany\\\\n\\\\n ABout Us Learn about our mission and values Careers Join us in building the next-gen data stack Partners Learn about the relationships that make it happen Contact us Have a question? We\\'re here to help\\\\nAI WorkshopGet a Demo\\\\n\\\\n← Back to Blog\\\\nInsights\\\\nTop 9 Large Language Models as of Feburary 2025\\\\nAuthor(s):\\\\n\\\\nNo items found.\\\\nUpdated on:\\\\nFebruary 7, 2025\\\\n\\\\nTable of contents\\\\nExample H2\\\\nExample H3\\\\nMentioned Components\\\\nNo items found.\\\\n<>\\\\nGet the latest updates in Data & AI straight to your inboxWe’ll email you once per week—and never share your information.\\\\n🎉 Success! You\\'re now signed up for the Shakudo newsletter.\\\\nOops! Something went wrong while submitting the form.\\\\nIntroduction\\\\nIf we had to choose one word to describe the rapid evolution of AI today, it would probably be something along the lines of explosive. As predicted by the Market Research Future report, the large language model (LLM) market in North America alone is expected to reach $105.5 billion by 2030. The exponential growth of AI tools combined with access to massive troves of text data has opened gates for better and more advanced content generation than we had ever hoped. Yet, such rapid expansion also makes it harder than ever to navigate and select the right tools among the diverse LLM models available.\\\\nThe goal of this post is to keep you, the AI enthusiast and professional, up-to-date with current trends and essential innovations in the field. Below, we highlighted the top 9 LLMs that we think are currently making waves in the industry, each with distinct capabilities and specialized strengths, excelling in areas such as natural language processing, code synthesis, few-shot learning, or scalability. While we believe there is no one-size-fits-all LLM for every use case, we hope that this list can help you identify the most current and well-suited LLM model that meets your business’s unique requirements.\\\\n1. GPT\\\\n\\\\nOur list kicks off with OpenAI\\'s Generative Pre-trained Transformer (GPT) models, which have consistently exceeded their previous capabilities with each new release. Compared to its prior models, the latest ChatGPT-4o and ChatGPT-4o mini models offer significantly faster processing speeds and enhanced capabilities across text, voice, and vision.\\\\nThe latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. Both of these models are equipped with multimodal capabilities to handle images as well as audio data.\\\\nDespite having advanced conversational and reasoning capabilities, note that GPT is a proprietary model, meaning that the training data and parameters are kept confidential by OpenAI, and access to full functionality is restricted–a commercial license or subscription is often required to unlock the complete range of features. In this case, we recommend this model for businesses looking to adopt an LLM that excels in conversational dialogue, multi-step reasoning, efficient computation, and real-time interactions without the constraints of a budget.\\\\nFor companies who are curious to try out the proprietary models on the market before fully committing to one due to budget constraints or uncertainties about its long-term integration, Shakudo offers a compelling alternative. Our platform currently features a diverse selection of advanced LLMs with simplified deployment and scalability. With a simple subscription, you can access and assess the value of proprietary models, like GPT, before making a substantial investment.\\\\n2.DeepSeek\\\\n\\\\nDeepseek-R1 Benchmark. Source: deepseek.com\\\\nWith its latest R1 model, the Chinese AI company DeepSeek has once again set new benchmarks for innovation in the AI community. As of January 24th, the DeepSeek-R1 model is ranked fourth on Chatbot Arena, and top as the best open-source LM.\\\\nThe DeepSeek-R1 is a 671B parameter Mixture-of-Experts (MoE) model with 37B activated parameters per token, trained through large-scale reinforcement learning with a strong focus on reasoning capabilities. The model excels at understanding and handling long-form content and demonstrates superior performance in complex tasks such as mathematics and code generation. The model is approximately 30 times more cost-efficient than OpenAI-o1 and 5 times faster, offering groundbreaking performance at a fraction of the cost. Moreover, it has shown exceptional precision in tasks requiring complex pattern recognition, such as genomic data analysis, medical imaging, and large-scale scientific simulations.\\\\nDeepSeek-R1’s capabilities are transformative when it comes to integration with proprietary enterprise data such as PII and financial records. Leveraging retrieval-augmented generation (RAG), enterprises can connect the model to their internal data sources to enable highly personalized, context-aware interactions—all while maintaining stringent security and compliance standards. With Shakudo, you can streamline the deployment and integration of advanced AI models like DeepSeek by automating the setup, deployment, and management processes. This eliminates the need for businesses to invest in and maintain extensive computing infrastructure. By operating within your existing infrastructure, the platform ensures seamless integration, enhanced security, and optimal performance without requiring significant in-house resources or specialized expertise.\\\\n3. Qwen\\\\n\\\\n\\u200d\\\\nAlibaba released Qwen2.5-Max earlier last month, the new model designed to deliver enhanced performance for large-scale natural language processing tasks. In instruct model evaluations, this new model outperforms DeepSeek V3 in benchmarks such as Arena-Hard, LiveBench, LiveCodeBench, and GPQA-Diamond, while also delivering strong performance in other assessments like MMLU-Pro.\\\\nQwen2.5-Max is pretrained on over 20 trillion tokens. While specific details about its parameter count and token window size are not publicly disclosed, Qwen2.5-Max is designed for low-latency, high-efficiency tasks, making it suitable for applications requiring quick, accurate responses with low latency. Its smaller size allows for deployment on devices with limited computational resources.\\\\nFor businesses and users looking for higher-performance models for natural language processing and AI-driven tasks, the Qwen 2.5 model is available on platforms like Hugging Face and ModelScope. This model boasts from 0.5 billion to 72 billion parameters, featuring context windows of up to 128,000 tokens, and is excellent for code generation, debugging, and automated forecasting.\\\\n4. LG AI\\\\n\\\\nsource\\\\nEXAONE 3.0 is a bilingual LLM developed by LG AI Research. The company released its latest model in December 2024 with excellent performance across various benchmarks and real-world applications.\\\\nWith 7.8 billion parameters, EXAONE 3.0 is capable of understanding and generating human-like text in multiple languages across complex domains, including coding, mathematics, patents, and chemistry. This model has also been optimized to reduce inference processing time by 56%, memory usage by 35%, and operating costs by 72%, ensuring that it remains cost-effective while maintaining high performance.\\\\nLG AI Research has open-sourced the 7.8B parameter instruction-tuned version of EXAONE 3.0 for non-commercial research purposes. This is a model we recommend to software companies or tech startups for generating Python code, assisting developers in troubleshooting, and creating APIs or other backend components.\\\\n5. LlaMA\\\\n\\\\nMeta is still leading the front with their state-of-the-art LlaMa models. The company released its latest LlaMA 3.3 model in December 2024, featuring multimodal capabilities that can process both text and image for in-depth analysis and response generation, such as interpreting charts, maps, or translating texts identified in an image.\\\\nLlaMA 3.3 improves on previous models with a longer context window of up to 128,000 tokens and an optimized transformer architecture. With a parameter of 70 billion, this model outperforms open-source and proprietary alternatives in areas such as multilingual dialogue, reasoning, and coding.\\\\nUnlike ChatGPT models, LlaMA 3 is open-source, giving users the flexibility to access and deploy freely on their cloud depending on the specific requirements of their infrastructure, security preferences, or customization needs. We recommend this model to businesses looking for advanced content generation and language understanding, such as those in customer service, education, marketing, and consumer markets. The openness of these models also allows for your greater control over the model’s performance, tuning, and integration into existing workflows.\\\\n6. Claude\\\\n\\\\nNext on our list is Claude, more specifically, the latest Claude 3.5 Sonnet model developed by Anthropic. We believe that Claude is arguably one of the most significant competitors to GPT since all of its current models–Claude 3 Haiku, Claude 3.5 Sonnet, and Claude 3 Opus–are designed with incredible contextual understanding capabilities that position themselves as the top conversational AI closely aligned with nuanced human interactions.\\\\nWhile the specific parameters of Claude 3.5 Sonnet remain undisclosed, the model boasts an impressive context window of 200,000 tokens, equivalent to approximately 150,000 words or 300 pages of text.\\\\nThe current Claude subscription service is credit-based, and the cost can go as high as $2,304/month for enterprise plans tailored to high-volume users. We recommend Claude to mature or mid-stage businesses looking not only to adopt an AI that facilitates human-like interactions but also to enhance their coding capabilities since the Claude 3.5 Sonnet model is currently reaching a 49.0% performance score on the SWE-bench Verified benchmark, placing it as third among all publicly available models, including reasoning models and systems specifically designed for agentic coding.\\\\n7. Mistral\\\\n\\\\nMistral\\'s latest model – Mistral Small 3, a latency-optimized model was released under the Apache 2.0 license at the end of January. This 24-billion-parameter model is designed for low-latency, high-efficiency tasks. It processes approximately 150 tokens per second, making it over three times faster than Llama 3.3 70B on the same hardware.\\\\nThis new model is ideal for applications requiring quick, accurate responses with low latency, such as virtual assistants, real-time data processing, and on-device command and control. Its smaller size allows for deployment on devices with limited computational resources.\\\\nMistral Small 3 is currently open-source under the Apache 2.0 license. This means you can freely access and use the model for your own applications, provided you comply with the license terms. Since it is designed to be easily deployable, including on hardware with limited resources like a single GPU or even a MacBook with 32GB RAM, we\\'d recommend this to early-stage businesses looking to implement low-latency AI solutions without the need for extensive hardware infrastructure.\\\\n8. Gemini\\\\n\\\\nGemini is a family of closed-source LLM models developed by Google. The latest model—Gemini 2.0 Flash—operates at twice the speed of Gemini 1.5 Pro, offering substantial improvements in speed, reasoning, and multimodal processing capabilities.\\\\nWith that being said, Gemini remains a proprietary model; if your company deals with sensitive or confidential data regularly, you might be concerned about sending it to external servers due to security reasons. To address this concern, we recommend that you double-check vendor compliance regulations to ensure data privacy and security standards are met, such as adherence to GDPR, HIPAA, or other relevant data protection laws.\\\\nIf you’re looking for an open-source alternative that exhibits capabilities almost as good as Gemini, Google’s latest Gemma model, Gemma 2, offers three models available in 2 billion, 9 billion, and 27 billion parameters with a context window of 8,200. For businesses looking for a rather economic option, this is the optimal choice that interprets and understands messages with remarkable accuracy.\\\\n9. Command\\\\n\\\\nCommand R is a family of scalable models developed by Cohere with the goal of balancing high performance with strong accuracy, just like Claude. Both the Command R and Command R+ models offer APIs specifically optimized for Retrieval Augmented Generation (RAG). This means that these models can combine large-scale language generation with real-time information retrieval techniques for much more contextually aware outputs.\\\\nCurrently, the Command R+ model boasts 104 billion parameters and offers an industry-leading 128,000 token context window for enhanced long-form processing and multi-turn conversation capabilities.\\\\nOne of the perks of working with an open-source model is also to avoid vendor lock-in. Heavy reliance on a particular type of proprietary model may make it difficult for you to switch to alternative models when your business starts growing or the landscape changes. Cohere approaches this in a hybrid way, meaning that you can access and modify the model for personal usage but need a license for commercial use. In this case, we recommend this model for businesses that want flexibility in experimentation without a long-term commitment to a single vendor.\\\\n\\\\nWhitepaper\\\\nIntroduction\\\\nIf we had to choose one word to describe the rapid evolution of AI today, it would probably be something along the lines of explosive. As predicted by the Market Research Future report, the large language model (LLM) market in North America alone is expected to reach $105.5 billion by 2030. The exponential growth of AI tools combined with access to massive troves of text data has opened gates for better and more advanced content generation than we had ever hoped. Yet, such rapid expansion also makes it harder than ever to navigate and select the right tools among the diverse LLM models available.\\\\nThe goal of this post is to keep you, the AI enthusiast and professional, up-to-date with current trends and essential innovations in the field. Below, we highlighted the top 9 LLMs that we think are currently making waves in the industry, each with distinct capabilities and specialized strengths, excelling in areas such as natural language processing, code synthesis, few-shot learning, or scalability. While we believe there is no one-size-fits-all LLM for every use case, we hope that this list can help you identify the most current and well-suited LLM model that meets your business’s unique requirements.\\\\n1. GPT\\\\n\\\\nOur list kicks off with OpenAI\\'s Generative Pre-trained Transformer (GPT) models, which have consistently exceeded their previous capabilities with each new release. Compared to its prior models, the latest ChatGPT-4o and ChatGPT-4o mini models offer significantly faster processing speeds and enhanced capabilities across text, voice, and vision.\\\\nThe latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. Both of these models are equipped with multimodal capabilities to handle images as well as audio data.\\\\nDespite having advanced conversational and reasoning capabilities, note that GPT is a proprietary model, meaning that the training data and parameters are kept confidential by OpenAI, and access to full functionality is restricted–a commercial license or subscription is often required to unlock the complete range of features. In this case, we recommend this model for businesses looking to adopt an LLM that excels in conversational dialogue, multi-step reasoning, efficient computation, and real-time interactions without the constraints of a budget.\\\\nFor companies who are curious to try out the proprietary models on the market before fully committing to one due to budget constraints or uncertainties about its long-term integration, Shakudo offers a compelling alternative. Our platform currently features a diverse selection of advanced LLMs with simplified deployment and scalability. With a simple subscription, you can access and assess the value of proprietary models, like GPT, before making a substantial investment.\\\\n2.DeepSeek\\\\n\\\\nDeepseek-R1 Benchmark. Source: deepseek.com\\\\nWith its latest R1 model, the Chinese AI company DeepSeek has once again set new benchmarks for innovation in the AI community. As of January 24th, the DeepSeek-R1 model is ranked fourth on Chatbot Arena, and top as the best open-source LM.\\\\nThe DeepSeek-R1 is a 671B parameter Mixture-of-Experts (MoE) model with 37B activated parameters per token, trained through large-scale reinforcement learning with a strong focus on reasoning capabilities. The model excels at understanding and handling long-form content and demonstrates superior performance in complex tasks such as mathematics and code generation. The model is approximately 30 times more cost-efficient than OpenAI-o1 and 5 times faster, offering groundbreaking performance at a fraction of the cost. Moreover, it has shown exceptional precision in tasks requiring complex pattern recognition, such as genomic data analysis, medical imaging, and large-scale scientific simulations.\\\\nDeepSeek-R1’s capabilities are transformative when it comes to integration with proprietary enterprise data such as PII and financial records. Leveraging retrieval-augmented generation (RAG), enterprises can connect the model to their internal data sources to enable highly personalized, context-aware interactions—all while maintaining stringent security and compliance standards. With Shakudo, you can streamline the deployment and integration of advanced AI models like DeepSeek by automating the setup, deployment, and management processes. This eliminates the need for businesses to invest in and maintain extensive computing infrastructure. By operating within your existing infrastructure, the platform ensures seamless integration, enhanced security, and optimal performance without requiring significant in-house resources or specialized expertise.\\\\n3. Qwen\\\\n\\\\n\\u200d\\\\nAlibaba released Qwen2.5-Max earlier last month, the new model designed to deliver enhanced performance for large-scale natural language processing tasks. In instruct model evaluations, this new model outperforms DeepSeek V3 in benchmarks such as Arena-Hard, LiveBench, LiveCodeBench, and GPQA-Diamond, while also delivering strong performance in other assessments like MMLU-Pro.\\\\nQwen2.5-Max is pretrained on over 20 trillion tokens. While specific details about its parameter count and token window size are not publicly disclosed, Qwen2.5-Max is designed for low-latency, high-efficiency tasks, making it suitable for applications requiring quick, accurate responses with low latency. Its smaller size allows for deployment on devices with limited computational resources.\\\\nFor businesses and users looking for higher-performance models for natural language processing and AI-driven tasks, the Qwen 2.5 model is available on platforms like Hugging Face and ModelScope. This model boasts from 0.5 billion to 72 billion parameters, featuring context windows of up to 128,000 tokens, and is excellent for code generation, debugging, and automated forecasting.\\\\n4. LG AI\\\\n\\\\nsource\\\\nEXAONE 3.0 is a bilingual LLM developed by LG AI Research. The company released its latest model in December 2024 with excellent performance across various benchmarks and real-world applications.\\\\nWith 7.8 billion parameters, EXAONE 3.0 is capable of understanding and generating human-like text in multiple languages across complex domains, including coding, mathematics, patents, and chemistry. This model has also been optimized to reduce inference processing time by 56%, memory usage by 35%, and operating costs by 72%, ensuring that it remains cost-effective while maintaining high performance.\\\\nLG AI Research has open-sourced the 7.8B parameter instruction-tuned version of EXAONE 3.0 for non-commercial research purposes. This is a model we recommend to software companies or tech startups for generating Python code, assisting developers in troubleshooting, and creating APIs or other backend components.\\\\n5. LlaMA\\\\n\\\\nMeta is still leading the front with their state-of-the-art LlaMa models. The company released its latest LlaMA 3.3 model in December 2024, featuring multimodal capabilities that can process both text and image for in-depth analysis and response generation, such as interpreting charts, maps, or translating texts identified in an image.\\\\nLlaMA 3.3 improves on previous models with a longer context window of up to 128,000 tokens and an optimized transformer architecture. With a parameter of 70 billion, this model outperforms open-source and proprietary alternatives in areas such as multilingual dialogue, reasoning, and coding.\\\\nUnlike ChatGPT models, LlaMA 3 is open-source, giving users the flexibility to access and deploy freely on their cloud depending on the specific requirements of their infrastructure, security preferences, or customization needs. We recommend this model to businesses looking for advanced content generation and language understanding, such as those in customer service, education, marketing, and consumer markets. The openness of these models also allows for your greater control over the model’s performance, tuning, and integration into existing workflows.\\\\n6. Claude\\\\n\\\\nNext on our list is Claude, more specifically, the latest Claude 3.5 Sonnet model developed by Anthropic. We believe that Claude is arguably one of the most significant competitors to GPT since all of its current models–Claude 3 Haiku, Claude 3.5 Sonnet, and Claude 3 Opus–are designed with incredible contextual understanding capabilities that position themselves as the top conversational AI closely aligned with nuanced human interactions.\\\\nWhile the specific parameters of Claude 3.5 Sonnet remain undisclosed, the model boasts an impressive context window of 200,000 tokens, equivalent to approximately 150,000 words or 300 pages of text.\\\\nThe current Claude subscription service is credit-based, and the cost can go as high as $2,304/month for enterprise plans tailored to high-volume users. We recommend Claude to mature or mid-stage businesses looking not only to adopt an AI that facilitates human-like interactions but also to enhance their coding capabilities since the Claude 3.5 Sonnet model is currently reaching a 49.0% performance score on the SWE-bench Verified benchmark, placing it as third among all publicly available models, including reasoning models and systems specifically designed for agentic coding.\\\\n7. Mistral\\\\n\\\\nMistral\\'s latest model – Mistral Small 3, a latency-optimized model was released under the Apache 2.0 license at the end of January. This 24-billion-parameter model is designed for low-latency, high-efficiency tasks. It processes approximately 150 tokens per second, making it over three times faster than Llama 3.3 70B on the same hardware.\\\\nThis new model is ideal for applications requiring quick, accurate responses with low latency, such as virtual assistants, real-time data processing, and on-device command and control. Its smaller size allows for deployment on devices with limited computational resources.\\\\nMistral Small 3 is currently open-source under the Apache 2.0 license. This means you can freely access and use the model for your own applications, provided you comply with the license terms. Since it is designed to be easily deployable, including on hardware with limited resources like a single GPU or even a MacBook with 32GB RAM, we\\'d recommend this to early-stage businesses looking to implement low-latency AI solutions without the need for extensive hardware infrastructure.\\\\n8. Gemini\\\\n\\\\nGemini is a family of closed-source LLM models developed by Google. The latest model—Gemini 2.0 Flash—operates at twice the speed of Gemini 1.5 Pro, offering substantial improvements in speed, reasoning, and multimodal processing capabilities.\\\\nWith that being said, Gemini remains a proprietary model; if your company deals with sensitive or confidential data regularly, you might be concerned about sending it to external servers due to security reasons. To address this concern, we recommend that you double-check vendor compliance regulations to ensure data privacy and security standards are met, such as adherence to GDPR, HIPAA, or other relevant data protection laws.\\\\nIf you’re looking for an open-source alternative that exhibits capabilities almost as good as Gemini, Google’s latest Gemma model, Gemma 2, offers three models available in 2 billion, 9 billion, and 27 billion parameters with a context window of 8,200. For businesses looking for a rather economic option, this is the optimal choice that interprets and understands messages with remarkable accuracy.\\\\n9. Command\\\\n\\\\nCommand R is a family of scalable models developed by Cohere with the goal of balancing high performance with strong accuracy, just like Claude. Both the Command R and Command R+ models offer APIs specifically optimized for Retrieval Augmented Generation (RAG). This means that these models can combine large-scale language generation with real-time information retrieval techniques for much more contextually aware outputs.\\\\nCurrently, the Command R+ model boasts 104 billion parameters and offers an industry-leading 128,000 token context window for enhanced long-form processing and multi-turn conversation capabilities.\\\\nOne of the perks of working with an open-source model is also to avoid vendor lock-in. Heavy reliance on a particular type of proprietary model may make it difficult for you to switch to alternative models when your business starts growing or the landscape changes. Cohere approaches this in a hybrid way, meaning that you can access and modify the model for personal usage but need a license for commercial use. In this case, we recommend this model for businesses that want flexibility in experimentation without a long-term commitment to a single vendor.\\\\nGet the whitepaper\\\\nTop 9 Large Language Models as of Feburary 2025\\\\nBy clicking \\\\\"Download,\\\\\" you agree to Shakudo processing your personal data in accordance with its Privacy Notice.\\\\nThank you for filling out the form. The whitepaper you have requested is available for download below.  \\\\nDownload White Paper\\\\nOops! Something went wrong while submitting the form.\\\\nGet the whitepaper\\\\nTop 9 Large Language Models as of Feburary 2025\\\\nThank you for your interest. Click the button below to download whitepaper you have requested.  \\\\nDownload White Paper\\\\n\\\\nTop 9 Large Language Models as of Feburary 2025\\\\nExplore the top 9 LLMs making waves in the AI world and what each of them excel at\\\\n\\\\n| Case Study\\\\nTop 9 Large Language Models as of Feburary 2025\\\\n\\\\nKey results\\\\nAbout\\\\nindustry\\\\nTech Stack\\\\nNo items found.\\\\n<>\\\\nIntroduction\\\\nIf we had to choose one word to describe the rapid evolution of AI today, it would probably be something along the lines of explosive. As predicted by the Market Research Future report, the large language model (LLM) market in North America alone is expected to reach $105.5 billion by 2030. The exponential growth of AI tools combined with access to massive troves of text data has opened gates for better and more advanced content generation than we had ever hoped. Yet, such rapid expansion also makes it harder than ever to navigate and select the right tools among the diverse LLM models available.\\\\nThe goal of this post is to keep you, the AI enthusiast and professional, up-to-date with current trends and essential innovations in the field. Below, we highlighted the top 9 LLMs that we think are currently making waves in the industry, each with distinct capabilities and specialized strengths, excelling in areas such as natural language processing, code synthesis, few-shot learning, or scalability. While we believe there is no one-size-fits-all LLM for every use case, we hope that this list can help you identify the most current and well-suited LLM model that meets your business’s unique requirements.\\\\n1. GPT\\\\n\\\\nOur list kicks off with OpenAI\\'s Generative Pre-trained Transformer (GPT) models, which have consistently exceeded their previous capabilities with each new release. Compared to its prior models, the latest ChatGPT-4o and ChatGPT-4o mini models offer significantly faster processing speeds and enhanced capabilities across text, voice, and vision.\\\\nThe latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. Both of these models are equipped with multimodal capabilities to handle images as well as audio data.\\\\nDespite having advanced conversational and reasoning capabilities, note that GPT is a proprietary model, meaning that the training data and parameters are kept confidential by OpenAI, and access to full functionality is restricted–a commercial license or subscription is often required to unlock the complete range of features. In this case, we recommend this model for businesses looking to adopt an LLM that excels in conversational dialogue, multi-step reasoning, efficient computation, and real-time interactions without the constraints of a budget.\\\\nFor companies who are curious to try out the proprietary models on the market before fully committing to one due to budget constraints or uncertainties about its long-term integration, Shakudo offers a compelling alternative. Our platform currently features a diverse selection of advanced LLMs with simplified deployment and scalability. With a simple subscription, you can access and assess the value of proprietary models, like GPT, before making a substantial investment.\\\\n2.DeepSeek\\\\n\\\\nDeepseek-R1 Benchmark. Source: deepseek.com\\\\nWith its latest R1 model, the Chinese AI company DeepSeek has once again set new benchmarks for innovation in the AI community. As of January 24th, the DeepSeek-R1 model is ranked fourth on Chatbot Arena, and top as the best open-source LM.\\\\nThe DeepSeek-R1 is a 671B parameter Mixture-of-Experts (MoE) model with 37B activated parameters per token, trained through large-scale reinforcement learning with a strong focus on reasoning capabilities. The model excels at understanding and handling long-form content and demonstrates superior performance in complex tasks such as mathematics and code generation. The model is approximately 30 times more cost-efficient than OpenAI-o1 and 5 times faster, offering groundbreaking performance at a fraction of the cost. Moreover, it has shown exceptional precision in tasks requiring complex pattern recognition, such as genomic data analysis, medical imaging, and large-scale scientific simulations.\\\\nDeepSeek-R1’s capabilities are transformative when it comes to integration with proprietary enterprise data such as PII and financial records. Leveraging retrieval-augmented generation (RAG), enterprises can connect the model to their internal data sources to enable highly personalized, context-aware interactions—all while maintaining stringent security and compliance standards. With Shakudo, you can streamline the deployment and integration of advanced AI models like DeepSeek by automating the setup, deployment, and management processes. This eliminates the need for businesses to invest in and maintain extensive computing infrastructure. By operating within your existing infrastructure, the platform ensures seamless integration, enhanced security, and optimal performance without requiring significant in-house resources or specialized expertise.\\\\n3. Qwen\\\\n\\\\n\\u200d\\\\nAlibaba released Qwen2.5-Max earlier last month, the new model designed to deliver enhanced performance for large-scale natural language processing tasks. In instruct model evaluations, this new model outperforms DeepSeek V3 in benchmarks such as Arena-Hard, LiveBench, LiveCodeBench, and GPQA-Diamond, while also delivering strong performance in other assessments like MMLU-Pro.\\\\nQwen2.5-Max is pretrained on over 20 trillion tokens. While specific details about its parameter count and token window size are not publicly disclosed, Qwen2.5-Max is designed for low-latency, high-efficiency tasks, making it suitable for applications requiring quick, accurate responses with low latency. Its smaller size allows for deployment on devices with limited computational resources.\\\\nFor businesses and users looking for higher-performance models for natural language processing and AI-driven tasks, the Qwen 2.5 model is available on platforms like Hugging Face and ModelScope. This model boasts from 0.5 billion to 72 billion parameters, featuring context windows of up to 128,000 tokens, and is excellent for code generation, debugging, and automated forecasting.\\\\n4. LG AI\\\\n\\\\nsource\\\\nEXAONE 3.0 is a bilingual LLM developed by LG AI Research. The company released its latest model in December 2024 with excellent performance across various benchmarks and real-world applications.\\\\nWith 7.8 billion parameters, EXAONE 3.0 is capable of understanding and generating human-like text in multiple languages across complex domains, including coding, mathematics, patents, and chemistry. This model has also been optimized to reduce inference processing time by 56%, memory usage by 35%, and operating costs by 72%, ensuring that it remains cost-effective while maintaining high performance.\\\\nLG AI Research has open-sourced the 7.8B parameter instruction-tuned version of EXAONE 3.0 for non-commercial research purposes. This is a model we recommend to software companies or tech startups for generating Python code, assisting developers in troubleshooting, and creating APIs or other backend components.\\\\n5. LlaMA\\\\n\\\\nMeta is still leading the front with their state-of-the-art LlaMa models. The company released its latest LlaMA 3.3 model in December 2024, featuring multimodal capabilities that can process both text and image for in-depth analysis and response generation, such as interpreting charts, maps, or translating texts identified in an image.\\\\nLlaMA 3.3 improves on previous models with a longer context window of up to 128,000 tokens and an optimized transformer architecture. With a parameter of 70 billion, this model outperforms open-source and proprietary alternatives in areas such as multilingual dialogue, reasoning, and coding.\\\\nUnlike ChatGPT models, LlaMA 3 is open-source, giving users the flexibility to access and deploy freely on their cloud depending on the specific requirements of their infrastructure, security preferences, or customization needs. We recommend this model to businesses looking for advanced content generation and language understanding, such as those in customer service, education, marketing, and consumer markets. The openness of these models also allows for your greater control over the model’s performance, tuning, and integration into existing workflows.\\\\n6. Claude\\\\n\\\\nNext on our list is Claude, more specifically, the latest Claude 3.5 Sonnet model developed by Anthropic. We believe that Claude is arguably one of the most significant competitors to GPT since all of its current models–Claude 3 Haiku, Claude 3.5 Sonnet, and Claude 3 Opus–are designed with incredible contextual understanding capabilities that position themselves as the top conversational AI closely aligned with nuanced human interactions.\\\\nWhile the specific parameters of Claude 3.5 Sonnet remain undisclosed, the model boasts an impressive context window of 200,000 tokens, equivalent to approximately 150,000 words or 300 pages of text.\\\\nThe current Claude subscription service is credit-based, and the cost can go as high as $2,304/month for enterprise plans tailored to high-volume users. We recommend Claude to mature or mid-stage businesses looking not only to adopt an AI that facilitates human-like interactions but also to enhance their coding capabilities since the Claude 3.5 Sonnet model is currently reaching a 49.0% performance score on the SWE-bench Verified benchmark, placing it as third among all publicly available models, including reasoning models and systems specifically designed for agentic coding.\\\\n7. Mistral\\\\n\\\\nMistral\\'s latest model – Mistral Small 3, a latency-optimized model was released under the Apache 2.0 license at the end of January. This 24-billion-parameter model is designed for low-latency, high-efficiency tasks. It processes approximately 150 tokens per second, making it over three times faster than Llama 3.3 70B on the same hardware.\\\\nThis new model is ideal for applications requiring quick, accurate responses with low latency, such as virtual assistants, real-time data processing, and on-device command and control. Its smaller size allows for deployment on devices with limited computational resources.\\\\nMistral Small 3 is currently open-source under the Apache 2.0 license. This means you can freely access and use the model for your own applications, provided you comply with the license terms. Since it is designed to be easily deployable, including on hardware with limited resources like a single GPU or even a MacBook with 32GB RAM, we\\'d recommend this to early-stage businesses looking to implement low-latency AI solutions without the need for extensive hardware infrastructure.\\\\n8. Gemini\\\\n\\\\nGemini is a family of closed-source LLM models developed by Google. The latest model—Gemini 2.0 Flash—operates at twice the speed of Gemini 1.5 Pro, offering substantial improvements in speed, reasoning, and multimodal processing capabilities.\\\\nWith that being said, Gemini remains a proprietary model; if your company deals with sensitive or confidential data regularly, you might be concerned about sending it to external servers due to security reasons. To address this concern, we recommend that you double-check vendor compliance regulations to ensure data privacy and security standards are met, such as adherence to GDPR, HIPAA, or other relevant data protection laws.\\\\nIf you’re looking for an open-source alternative that exhibits capabilities almost as good as Gemini, Google’s latest Gemma model, Gemma 2, offers three models available in 2 billion, 9 billion, and 27 billion parameters with a context window of 8,200. For businesses looking for a rather economic option, this is the optimal choice that interprets and understands messages with remarkable accuracy.\\\\n9. Command\\\\n\\\\nCommand R is a family of scalable models developed by Cohere with the goal of balancing high performance with strong accuracy, just like Claude. Both the Command R and Command R+ models offer APIs specifically optimized for Retrieval Augmented Generation (RAG). This means that these models can combine large-scale language generation with real-time information retrieval techniques for much more contextually aware outputs.\\\\nCurrently, the Command R+ model boasts 104 billion parameters and offers an industry-leading 128,000 token context window for enhanced long-form processing and multi-turn conversation capabilities.\\\\nOne of the perks of working with an open-source model is also to avoid vendor lock-in. Heavy reliance on a particular type of proprietary model may make it difficult for you to switch to alternative models when your business starts growing or the landscape changes. Cohere approaches this in a hybrid way, meaning that you can access and modify the model for personal usage but need a license for commercial use. In this case, we recommend this model for businesses that want flexibility in experimentation without a long-term commitment to a single vendor.\\\\nExplore more from Shakudo\\\\n How VPCs Enable AI Deployments with a Modern Data Stack Insights January 28, 2025\\\\n The Power of Simple Questions: How to Choose the Right Natural Language to SQL Query Tool Insights May 15, 2024\\\\n Bring Data and AI tooling right to MongoDB Atlas with Shakudo News August 26, 2024\\\\nTake the next step\\\\n\\\\\"Shakudo gave us the flexibility to use the data stack components that fit our needs and evolve the stack to keep up with the industry.\\\\\"\\\\n\\\\nNeal Gilmore\\\\nSenior Vice President, Enterprise Data & Analytics\\\\nDiscover Shakudo\\\\n\\\\nShakudo brings the best data and AI products into your VPC and operates them for you automatically achieving a more reliable, performant, and cost effective data stack than ever before.\\\\n\\\\n Book Demo Email X (Twitter) Linkedin Youtube\\\\nNewsletter\\\\nSign up for the latest Shakudo news:\\\\n🎉 Success! You\\'re now signed up for the Shakudo newsletter.\\\\nOops! Something went wrong while submitting the form.\\\\nApplications\\\\nData and AI OSStack ComponentsLanguage to SQLVector Database + LLMReverse ETLWorkflow Automation\\\\nIndustries\\\\nAutomotive & Transportation\\\\nAerospace\\\\nManufacturing\\\\nHigher Education\\\\nHealthcare & Life Sciences\\\\nClimate & Energy\\\\nTechnology & Software\\\\nSports\\\\nReal Estate\\\\nRetail\\\\nFinancial Services\\\\nResources\\\\nUse Cases\\\\nInsights\\\\nWhite Paper\\\\nCase Study\\\\nPress\\\\nProduct\\\\nTutorial\\\\nNews\\\\nWebinarGlossaryDocumentation\\\\nCompany\\\\nAboutPartnersDGX PartnerCareersMedia Kit\\\\nGet Started\\\\nSignupContact UsNewsletter\\\\n© 2025 Shakudo\\\\nToronto, CA\\\\nContact usPrivacy PolicyTerms/ConditionsSitemap\\\\nTrusted by industry leaders\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSee Shakudo in Action  \\\\nWatch the 3 Minute Demo\\\\n\\\\nThis field is required\\\\n\\\\nFor information about how Shakudo handles your personal data, please see our Privacy Policy.\\\\nThank you for your submission. A Shakudo expert will be in touch with you shortly.  \\\\nIn the meantime, feel free to check out our data insights, case studies, and latest industry news that help data teams win.  \\\\n Live chat Live chat will provide the quickest answer to any of your questions.\\\\nOops! Something went wrong while submitting the form.\\\\n⨉\"}, {\"url\": \"https://en.wikipedia.org/wiki/List_of_large_language_models\", \"title\": \"List of large language models - Wikipedia\", \"content\": \"GLaM (Generalist Language Model)    December 2021   Google  1200[35]    1.6 trillion tokens[35] 5600[35]    Proprietary Sparse mixture of experts model, making it more expensive to train but cheaper to run inference compared to GPT-3. PaLM (Pathways Language Model)  April 2022  Google  540[43] 768 billion tokens[42]  29,250[38]  Proprietary Trained for ~60 days on ~6000 TPU v4 chips.[38] As of October\\xa02024, it is the largest dense Transformer published. Mixtral 8x7B    December 2023   Mistral AI  46.7    Unknown Unknown Apache 2.0  Outperforms GPT-3.5 and Llama 2 70B on many benchmarks.[82] Mixture of experts model, with 12.9 billion parameters activated per token.[83] ^ a b c d Table 20 and page 66 of PaLM: Scaling Language Modeling with Pathways Archived 2023-06-10 at the Wayback Machine\", \"score\": 0.7230167, \"raw_content\": \"Jump to content\\\\nMain menu\\\\nSearch\\\\nDonate\\\\nCreate account\\\\nLog in\\\\nPersonal tools\\\\nToggle the table of contents\\\\nList of large language models\\\\nAdd languages\\\\nArticle\\\\nTalk\\\\nRead\\\\nEdit\\\\nView history\\\\nTools\\\\nFrom Wikipedia, the free encyclopedia\\\\nA large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\\\nThis page lists notable large language models.\\\\nFor the training cost column, 1 petaFLOP-day = 1 petaFLOP/sec × 1 day = 8.64E19 FLOP. Also, only the largest model\\'s cost is written.\\\\nName    Release date[a] Developer   Number of parameters (billion) [b]  Corpus size Training cost (petaFLOP-day)    License[c]  Notes\\\\nGPT-1   June 2018   OpenAI  0.117       1[1]    MIT[2]  First GPT model, decoder-only transformer. Trained for 30 days on 8 P600 GPUs.\\\\nBERT    October 2018    Google  0.340[3]    3.3 billion words[3]    9[4]    Apache 2.0[5]   An early and influential language model.[6]Encoder-only and thus not built to be prompted or generative.[7] Training took 4 days on 64 TPUv2 chips.[8]\\\\nT5  October 2019    Google  11[9]   34 billion tokens[9]        Apache 2.0[10]  Base model for many Google projects, such as Imagen.[11]\\\\nXLNet   June 2019   Google  0.340[12]   33 billion words    330 Apache 2.0[13]  An alternative to BERT; designed as encoder-only. Trained on 512 TPU v3 chips for 5.5 days.[14]\\\\nGPT-2   February 2019   OpenAI  1.5[15] 40GB[16] (~10 billion tokens)[17]   28[18]  MIT[19] Trained on 32 TPUv3 chips for 1 week.[18]\\\\nGPT-3   May 2020    OpenAI  175[20] 300 billion tokens[17]  3640[21]    proprietary A fine-tuned variant of GPT-3, termed GPT-3.5, was made available to the public through a web interface called ChatGPT in 2022.[22]\\\\nGPT-Neo March 2021  EleutherAI  2.7[23] 825 GiB[24]     MIT[25] The first of a series of free GPT-3 alternatives released by EleutherAI. GPT-Neo outperformed an equivalent-size GPT-3 model on some benchmarks, but was significantly worse than the largest GPT-3.[25]\\\\nGPT-J   June 2021   EleutherAI  6[26]   825 GiB[24] 200[27] Apache 2.0  GPT-3-style language model\\\\nMegatron-Turing NLG October 2021[28]    Microsoft and Nvidia    530[29] 338.6 billion tokens[29]    38000[30]   Restricted web access   Trained for 3 months on over 2000 A100 GPUs on the NVIDIA Selene Supercomputer, for over 3 million GPU-hours.[30]\\\\nErnie 3.0 Titan December 2021   Baidu   260[31] 4 Tb        Proprietary Chinese-language LLM. Ernie Bot is based on this model.\\\\nClaude[32]  December 2021   Anthropic   52[33]  400 billion tokens[33]      beta    Fine-tuned for desirable behavior in conversations.[34]\\\\nGLaM (Generalist Language Model)    December 2021   Google  1200[35]    1.6 trillion tokens[35] 5600[35]    Proprietary Sparse mixture of experts model, making it more expensive to train but cheaper to run inference compared to GPT-3.\\\\nGopher  December 2021   DeepMind    280[36] 300 billion tokens[37]  5833[38]    Proprietary Later developed into the Chinchilla model.\\\\nLaMDA (Language Models for Dialog Applications) January 2022    Google  137[39] 1.56T words,[39] 168 billion tokens[37] 4110[40]    Proprietary Specialized for response generation in conversations.\\\\nGPT-NeoX    February 2022   EleutherAI  20[41]  825 GiB[24] 740[27] Apache 2.0  based on the Megatron architecture\\\\nChinchilla  March 2022  DeepMind    70[42]  1.4 trillion tokens[42][37] 6805[38]    Proprietary Reduced-parameter model trained on more data. Used in the Sparrow bot. Often cited for its neural scaling law.\\\\nPaLM (Pathways Language Model)  April 2022  Google  540[43] 768 billion tokens[42]  29,250[38]  Proprietary Trained for ~60 days on ~6000 TPU v4 chips.[38] As of October\\xa02024, it is the largest dense Transformer published.\\\\nOPT (Open Pretrained Transformer)   May 2022    Meta    175[44] 180 billion tokens[45]  310[27] Non-commercial research[d]  GPT-3 architecture with some adaptations from Megatron. Uniquely, the training logbook written by the team was published.[46]\\\\nYaLM 100B   June 2022   Yandex  100[47] 1.7TB[47]       Apache 2.0  English-Russian model based on Microsoft\\'s Megatron-LM.\\\\nMinerva June 2022   Google  540[48] 38.5B tokens from webpages filtered for mathematical content and from papers submitted to the arXiv preprint server[48]     Proprietary For solving \\\\\"mathematical and scientific questions using step-by-step reasoning\\\\\".[49] Initialized from PaLM models, then finetuned on mathematical and scientific data.\\\\nBLOOM   July 2022   Large collaboration led by Hugging Face 175[50] 350 billion tokens (1.6TB)[51]      Responsible AI  Essentially GPT-3 but trained on a multi-lingual corpus (30% English excluding programming languages)\\\\nGalactica   November 2022   Meta    120 106 billion tokens[52]  unknown CC-BY-NC-4.0    Trained on scientific text and modalities.\\\\nAlexaTM (Teacher Models)    November 2022   Amazon  20[53]  1.3 trillion[54]        proprietary[55] bidirectional sequence-to-sequence architecture\\\\nNeuro-sama  December 2022   Independent Unknown Unknown     privately-owned A language model designed for live-streaming on Twitch.\\\\nLLaMA (Large Language Model Meta AI)    February 2023   Meta AI 65[56]  1.4 trillion[56]    6300[57]    Non-commercial research[e]  Corpus has 20 languages. \\\\\"Overtrained\\\\\" (compared to Chinchilla scaling law) for better performance with fewer parameters.[56]\\\\nGPT-4   March 2023  OpenAI  Unknown[f] (According to rumors: 1760)[59]  Unknown Unknown proprietary Available for ChatGPT Plus users and used in several products.\\\\nChameleon   June 2024   Meta AI 34[60]  4.4 trillion      \\\\nCerebras-GPT    March 2023  Cerebras    13[61]      270[27] Apache 2.0  Trained with Chinchilla formula.\\\\nFalcon  March 2023  Technology Innovation Institute 40[62]  1 trillion tokens, from RefinedWeb (filtered web text corpus)[63] plus some \\\\\"curated corpora\\\\\".[64]  2800[57]    Apache 2.0[65]\\\\nBloombergGPT    March 2023  Bloomberg L.P.  50  363 billion token dataset based on Bloomberg\\'s data sources, plus 345 billion tokens from general purpose datasets[66]      Proprietary Trained on financial data from proprietary sources, for financial tasks.\\\\nPanGu-Σ March 2023  Huawei  1085    329 billion tokens[67]      Proprietary \\\\nOpenAssistant[68]   March 2023  LAION   17  1.5 trillion tokens     Apache 2.0  Trained on crowdsourced open data\\\\nJurassic-2[69]  March 2023  AI21 Labs   Unknown Unknown     Proprietary Multilingual[70]\\\\nPaLM 2 (Pathways Language Model 2)  May 2023    Google  340[71] 3.6 trillion tokens[71] 85,000[57]  Proprietary Was used in Bard chatbot.[72]\\\\nLlama 2 July 2023   Meta AI 70[73]  2 trillion tokens[73]   21,000  Llama 2 license 1.7 million A100-hours.[74]\\\\nClaude 2    July 2023   Anthropic   Unknown Unknown Unknown Proprietary Used in Claude chatbot.[75]\\\\nGranite 13b July 2023   IBM Unknown Unknown Unknown Proprietary Used in IBM Watsonx.[76]\\\\nMistral 7B  September 2023  Mistral AI  7.3[77] Unknown     Apache 2.0\\\\nClaude 2.1  November 2023   Anthropic   Unknown Unknown Unknown Proprietary Used in Claude chatbot. Has a context window of 200,000 tokens, or ~500 pages.[78]\\\\nGrok-1[79]  November 2023   xAI 314 Unknown Unknown Apache 2.0  Used in Grok chatbot. Grok-1 has a context length of 8,192 tokens and has access to X (Twitter).[80]\\\\nGemini 1.0  December 2023   Google DeepMind Unknown Unknown Unknown Proprietary Multimodal model, comes in three sizes. Used in the chatbot of the same name.[81]\\\\nMixtral 8x7B    December 2023   Mistral AI  46.7    Unknown Unknown Apache 2.0  Outperforms GPT-3.5 and Llama 2 70B on many benchmarks.[82] Mixture of experts model, with 12.9 billion parameters activated per token.[83]\\\\nMixtral 8x22B   April 2024  Mistral AI  141 Unknown Unknown Apache 2.0  [84]\\\\nPhi-2   December 2023   Microsoft   2.7 1.4T tokens 419[85] MIT Trained on real and synthetic \\\\\"textbook-quality\\\\\" data, for 14 days on 96 A100 GPUs.[85]\\\\nGemini 1.5  February 2024   Google DeepMind Unknown Unknown Unknown Proprietary Multimodal model, based on a Mixture-of-Experts (MoE) architecture. Context window above 1 million tokens.[86]\\\\nGemini Ultra    February 2024   Google DeepMind Unknown Unknown Unknown   \\\\nGemma   February 2024   Google DeepMind 7   6T tokens   Unknown Gemma Terms of Use[87]\\\\nClaude 3    March 2024  Anthropic   Unknown Unknown Unknown Proprietary Includes three models, Haiku, Sonnet, and Opus.[88]\\\\nNova    October 2024    Rubik\\'s AI  Unknown Unknown Unknown Proprietary Includes three models, Nova-Instant, Nova-Air, and Nova-Pro.\\\\nDBRX    March 2024  Databricks and Mosaic ML    136 12T Tokens      Databricks Open Model License   Training cost 10 million USD.\\\\nFugaku-LLM  May 2024    Fujitsu, Tokyo Institute of Technology, etc.    13  380B Tokens         The largest model ever trained on CPU-only, on the Fugaku.[89]\\\\nPhi-3   April 2024  Microsoft   14[90]  4.8T Tokens     MIT Microsoft markets them as \\\\\"small language model\\\\\".[91]\\\\nGranite Code Models May 2024    IBM Unknown Unknown Unknown Apache 2.0\\\\nQwen2   June 2024   Alibaba Cloud   72[92]  3T Tokens           Multiple sizes, the smallest being 0.5B.\\\\nNemotron-4  June 2024   Nvidia  340 9T Tokens   200,000 NVIDIA Open Model License   Trained for 1 epoch. Trained on 6144 H100 GPUs between December 2023 and May 2024.[93][94]\\\\nLlama 3.1   July 2024   Meta AI 405 15.6T tokens    440,000 Llama 3 license 405B version took 31 million hours on H100-80GB, at 3.8E25 FLOPs.[95][96]\\\\nDeepSeek V3 December 2024   DeepSeek    671 14.8T tokens    440,00  DeepSeek License    2.788M hours on H800 GPUs.[97]\\\\nAmazon Nova December 2024   Amazon  Unknown Unknown Unknown Proprietary Includes three models, Nova Micro, Nova Lite, and Nova Pro[98]\\\\nSee also[edit]\\\\nList of chatbots\\\\nNotes[edit]\\\\n^ This is the date that documentation describing the model\\'s architecture was first released.\\\\n^ In many cases, researchers release or report on multiple versions of a model having different sizes. In these cases, the size of the largest model is listed here.\\\\n^ This is the license of the pre-trained model weights. In almost all cases the training code itself is open-source or can be easily replicated.\\\\n^ The smaller models including 66B are publicly available, while the 175B model is available on request.\\\\n^ Facebook\\'s license and distribution scheme restricted access to approved researchers, but the model weights were leaked and became widely available.\\\\n^ As stated in Technical report: \\\\\"Given both the competitive landscape and the safety implications of large-scale models like GPT-4, this report contains no further details about the architecture (including model size), hardware, training compute, dataset construction, training method ...\\\\\"[58]\\\\nReferences[edit]\\\\n^ \\\\\"Improving language understanding with unsupervised learning\\\\\". openai.com. June 11, 2018. Archived from the original on 2023-03-18. Retrieved 2023-03-18.\\\\n^ \\\\\"finetune-transformer-lm\\\\\". GitHub. Archived from the original on 19 May 2023. Retrieved 2 January 2024.\\\\n^ a b Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (11 October 2018). \\\\\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\\\\". arXiv:1810.04805v2 [cs.CL].\\\\n^ Prickett, Nicole Hemsoth (2021-08-24). \\\\\"Cerebras Shifts Architecture To Meet Massive AI/ML Models\\\\\". The Next Platform. Archived from the original on 2023-06-20. Retrieved 2023-06-20.\\\\n^ \\\\\"BERT\\\\\". March 13, 2023. Archived from the original on January 13, 2021. Retrieved March 13, 2023 – via GitHub.\\\\n^ Manning, Christopher D. (2022). \\\\\"Human Language Understanding & Reasoning\\\\\". Daedalus. 151 (2): 127–138. doi:10.1162/daed_a_01905. S2CID\\xa0248377870. Archived from the original on 2023-11-17. Retrieved 2023-03-09.\\\\n^ Patel, Ajay; Li, Bryan; Rasooli, Mohammad Sadegh; Constant, Noah; Raffel, Colin; Callison-Burch, Chris (2022). \\\\\"Bidirectional Language Models Are Also Few-shot Learners\\\\\". arXiv:2209.14500 [cs.LG].\\\\n^ Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (11 October 2018). \\\\\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\\\\". arXiv:1810.04805v2 [cs.CL].\\\\n^ a b Raffel, Colin; Shazeer, Noam; Roberts, Adam; Lee, Katherine; Narang, Sharan; Matena, Michael; Zhou, Yanqi; Li, Wei; Liu, Peter J. (2020). \\\\\"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\\\\". Journal of Machine Learning Research. 21 (140): 1–67. arXiv:1910.10683. ISSN\\xa01533-7928.\\\\n^ google-research/text-to-text-transfer-transformer, Google Research, 2024-04-02, archived from the original on 2024-03-29, retrieved 2024-04-04\\\\n^ \\\\\"Imagen: Text-to-Image Diffusion Models\\\\\". imagen.research.google. Archived from the original on 2024-03-27. Retrieved 2024-04-04.\\\\n^ \\\\\"Pretrained models — transformers 2.0.0 documentation\\\\\". huggingface.co. Archived from the original on 2024-08-05. Retrieved 2024-08-05.\\\\n^ \\\\\"xlnet\\\\\". GitHub. Archived from the original on 2 January 2024. Retrieved 2 January 2024.\\\\n^ Yang, Zhilin; Dai, Zihang; Yang, Yiming; Carbonell, Jaime; Salakhutdinov, Ruslan; Le, Quoc V. (2 January 2020). \\\\\"XLNet: Generalized Autoregressive Pretraining for Language Understanding\\\\\". arXiv:1906.08237 [cs.CL].\\\\n^ \\\\\"GPT-2: 1.5B Release\\\\\". OpenAI. 2019-11-05. Archived from the original on 2019-11-14. Retrieved 2019-11-14.\\\\n^ \\\\\"Better language models and their implications\\\\\". openai.com. Archived from the original on 2023-03-16. Retrieved 2023-03-13.\\\\n^ a b \\\\\"OpenAI\\'s GPT-3 Language Model: A Technical Overview\\\\\". lambdalabs.com. 3 June 2020. Archived from the original on 27 March 2023. Retrieved 13 March 2023.\\\\n^ a b \\\\\"openai-community/gpt2-xl · Hugging Face\\\\\". huggingface.co. Archived from the original on 2024-07-24. Retrieved 2024-07-24.\\\\n^ \\\\\"gpt-2\\\\\". GitHub. Archived from the original on 11 March 2023. Retrieved 13 March 2023.\\\\n^ Wiggers, Kyle (28 April 2022). \\\\\"The emerging types of language models and why they matter\\\\\". TechCrunch. Archived from the original on 16 March 2023. Retrieved 9 March 2023.\\\\n^ Table D.1 in Brown, Tom B.; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Gretchen; Henighan, Tom; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel M.; Wu, Jeffrey; Winter, Clemens; Hesse, Christopher; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Scott; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (May 28, 2020). \\\\\"Language Models are Few-Shot Learners\\\\\". arXiv:2005.14165v4 [cs.CL].\\\\n^ \\\\\"ChatGPT: Optimizing Language Models for Dialogue\\\\\". OpenAI. 2022-11-30. Archived from the original on 2022-11-30. Retrieved 2023-01-13.\\\\n^ \\\\\"GPT Neo\\\\\". March 15, 2023. Archived from the original on March 12, 2023. Retrieved March 12, 2023 – via GitHub.\\\\n^ a b c Gao, Leo; Biderman, Stella; Black, Sid; Golding, Laurence; Hoppe, Travis; Foster, Charles; Phang, Jason; He, Horace; Thite, Anish; Nabeshima, Noa; Presser, Shawn; Leahy, Connor (31 December 2020). \\\\\"The Pile: An 800GB Dataset of Diverse Text for Language Modeling\\\\\". arXiv:2101.00027 [cs.CL].\\\\n^ a b Iyer, Abhishek (15 May 2021). \\\\\"GPT-3\\'s free alternative GPT-Neo is something to be excited about\\\\\". VentureBeat. Archived from the original on 9 March 2023. Retrieved 13 March 2023.\\\\n^ \\\\\"GPT-J-6B: An Introduction to the Largest Open Source GPT Model | Forefront\\\\\". www.forefront.ai. Archived from the original on 2023-03-09. Retrieved 2023-02-28.\\\\n^ a b c d Dey, Nolan; Gosal, Gurpreet; Zhiming; Chen; Khachane, Hemant; Marshall, William; Pathria, Ribhu; Tom, Marvin; Hestness, Joel (2023-04-01). \\\\\"Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster\\\\\". arXiv:2304.03208 [cs.LG].\\\\n^ Alvi, Ali; Kharya, Paresh (11 October 2021). \\\\\"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World\\'s Largest and Most Powerful Generative Language Model\\\\\". Microsoft Research. Archived from the original on 13 March 2023. Retrieved 13 March 2023.\\\\n^ a b Smith, Shaden; Patwary, Mostofa; Norick, Brandon; LeGresley, Patrick; Rajbhandari, Samyam; Casper, Jared; Liu, Zhun; Prabhumoye, Shrimai; Zerveas, George; Korthikanti, Vijay; Zhang, Elton; Child, Rewon; Aminabadi, Reza Yazdani; Bernauer, Julie; Song, Xia (2022-02-04). \\\\\"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model\\\\\". arXiv:2201.11990 [cs.CL].\\\\n^ a b Rajbhandari, Samyam; Li, Conglong; Yao, Zhewei; Zhang, Minjia; Aminabadi, Reza Yazdani; Awan, Ammar Ahmad; Rasley, Jeff; He, Yuxiong (2022-07-21), DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale, arXiv:2201.05596\\\\n^ Wang, Shuohuan; Sun, Yu; Xiang, Yang; Wu, Zhihua; Ding, Siyu; Gong, Weibao; Feng, Shikun; Shang, Junyuan; Zhao, Yanbin; Pang, Chao; Liu, Jiaxiang; Chen, Xuyi; Lu, Yuxiang; Liu, Weixin; Wang, Xi; Bai, Yangfan; Chen, Qiuliang; Zhao, Li; Li, Shiyong; Sun, Peng; Yu, Dianhai; Ma, Yanjun; Tian, Hao; Wu, Hua; Wu, Tian; Zeng, Wei; Li, Ge; Gao, Wen; Wang, Haifeng (December 23, 2021). \\\\\"ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation\\\\\". arXiv:2112.12731 [cs.CL].\\\\n^ \\\\\"Product\\\\\". Anthropic. Archived from the original on 16 March 2023. Retrieved 14 March 2023.\\\\n^ a b Askell, Amanda; Bai, Yuntao; Chen, Anna; et\\xa0al. (9 December 2021). \\\\\"A General Language Assistant as a Laboratory for Alignment\\\\\". arXiv:2112.00861 [cs.CL].\\\\n^ Bai, Yuntao; Kadavath, Saurav; Kundu, Sandipan; et\\xa0al. (15 December 2022). \\\\\"Constitutional AI: Harmlessness from AI Feedback\\\\\". arXiv:2212.08073 [cs.CL].\\\\n^ a b c Dai, Andrew M; Du, Nan (December 9, 2021). \\\\\"More Efficient In-Context Learning with GLaM\\\\\". ai.googleblog.com. Archived from the original on 2023-03-12. Retrieved 2023-03-09.\\\\n^ \\\\\"Language modelling at scale: Gopher, ethical considerations, and retrieval\\\\\". www.deepmind.com. 8 December 2021. Archived from the original on 20 March 2023. Retrieved 20 March 2023.\\\\n^ a b c Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; et\\xa0al. (29 March 2022). \\\\\"Training Compute-Optimal Large Language Models\\\\\". arXiv:2203.15556 [cs.CL].\\\\n^ a b c d Table 20 and page 66 of PaLM: Scaling Language Modeling with Pathways Archived 2023-06-10 at the Wayback Machine\\\\n^ a b Cheng, Heng-Tze; Thoppilan, Romal (January 21, 2022). \\\\\"LaMDA: Towards Safe, Grounded, and High-Quality Dialog Models for Everything\\\\\". ai.googleblog.com. Archived from the original on 2022-03-25. Retrieved 2023-03-09.\\\\n^ Thoppilan, Romal; De Freitas, Daniel; Hall, Jamie; Shazeer, Noam; Kulshreshtha, Apoorv; Cheng, Heng-Tze; Jin, Alicia; Bos, Taylor; Baker, Leslie; Du, Yu; Li, YaGuang; Lee, Hongrae; Zheng, Huaixiu Steven; Ghafouri, Amin; Menegali, Marcelo (2022-01-01). \\\\\"LaMDA: Language Models for Dialog Applications\\\\\". arXiv:2201.08239 [cs.CL].\\\\n^ Black, Sidney; Biderman, Stella; Hallahan, Eric; et\\xa0al. (2022-05-01). GPT-NeoX-20B: An Open-Source Autoregressive Language Model. Proceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models. Vol.\\xa0Proceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models. pp.\\xa095–136. Archived from the original on 2022-12-10. Retrieved 2022-12-19.\\\\n^ a b c Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; Sifre, Laurent (12 April 2022). \\\\\"An empirical analysis of compute-optimal large language model training\\\\\". Deepmind Blog. Archived from the original on 13 April 2022. Retrieved 9 March 2023.\\\\n^ Narang, Sharan; Chowdhery, Aakanksha (April 4, 2022). \\\\\"Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance\\\\\". ai.googleblog.com. Archived from the original on 2022-04-04. Retrieved 2023-03-09.\\\\n^ Susan Zhang; Mona Diab; Luke Zettlemoyer. \\\\\"Democratizing access to large-scale language models with OPT-175B\\\\\". ai.facebook.com. Archived from the original on 2023-03-12. Retrieved 2023-03-12.\\\\n^ Zhang, Susan; Roller, Stephen; Goyal, Naman; Artetxe, Mikel; Chen, Moya; Chen, Shuohui; Dewan, Christopher; Diab, Mona; Li, Xian; Lin, Xi Victoria; Mihaylov, Todor; Ott, Myle; Shleifer, Sam; Shuster, Kurt; Simig, Daniel; Koura, Punit Singh; Sridhar, Anjali; Wang, Tianlu; Zettlemoyer, Luke (21 June 2022). \\\\\"OPT: Open Pre-trained Transformer Language Models\\\\\". arXiv:2205.01068 [cs.CL].\\\\n^ \\\\\"metaseq/projects/OPT/chronicles at main · facebookresearch/metaseq\\\\\". GitHub. Retrieved 2024-10-18.\\\\n^ a b Khrushchev, Mikhail; Vasilev, Ruslan; Petrov, Alexey; Zinov, Nikolay (2022-06-22), YaLM 100B, archived from the original on 2023-06-16, retrieved 2023-03-18\\\\n^ a b Lewkowycz, Aitor; Andreassen, Anders; Dohan, David; Dyer, Ethan; Michalewski, Henryk; Ramasesh, Vinay; Slone, Ambrose; Anil, Cem; Schlag, Imanol; Gutman-Solo, Theo; Wu, Yuhuai; Neyshabur, Behnam; Gur-Ari, Guy; Misra, Vedant (30 June 2022). \\\\\"Solving Quantitative Reasoning Problems with Language Models\\\\\". arXiv:2206.14858 [cs.CL].\\\\n^ \\\\\"Minerva: Solving Quantitative Reasoning Problems with Language Models\\\\\". ai.googleblog.com. 30 June 2022. Retrieved 20 March 2023.\\\\n^ Ananthaswamy, Anil (8 March 2023). \\\\\"In AI, is bigger always better?\\\\\". Nature. 615 (7951): 202–205. Bibcode:2023Natur.615..202A. doi:10.1038/d41586-023-00641-w. PMID\\xa036890378. S2CID\\xa0257380916. Archived from the original on 16 March 2023. Retrieved 9 March 2023.\\\\n^ \\\\\"bigscience/bloom · Hugging Face\\\\\". huggingface.co. Archived from the original on 2023-04-12. Retrieved 2023-03-13.\\\\n^ Taylor, Ross; Kardas, Marcin; Cucurull, Guillem; Scialom, Thomas; Hartshorn, Anthony; Saravia, Elvis; Poulton, Andrew; Kerkez, Viktor; Stojnic, Robert (16 November 2022). \\\\\"Galactica: A Large Language Model for Science\\\\\". arXiv:2211.09085 [cs.CL].\\\\n^ \\\\\"20B-parameter Alexa model sets new marks in few-shot learning\\\\\". Amazon Science. 2 August 2022. Archived from the original on 15 March 2023. Retrieved 12 March 2023.\\\\n^ Soltan, Saleh; Ananthakrishnan, Shankar; FitzGerald, Jack; et\\xa0al. (3 August 2022). \\\\\"AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model\\\\\". arXiv:2208.01448 [cs.CL].\\\\n^ \\\\\"AlexaTM 20B is now available in Amazon SageMaker JumpStart | AWS Machine Learning Blog\\\\\". aws.amazon.com. 17 November 2022. Archived from the original on 13 March 2023. Retrieved 13 March 2023.\\\\n^ a b c \\\\\"Introducing LLaMA: A foundational, 65-billion-parameter large language model\\\\\". Meta AI. 24 February 2023. Archived from the original on 3 March 2023. Retrieved 9 March 2023.\\\\n^ a b c \\\\\"The Falcon has landed in the Hugging Face ecosystem\\\\\". huggingface.co. Archived from the original on 2023-06-20. Retrieved 2023-06-20.\\\\n^ \\\\\"GPT-4 Technical Report\\\\\" (PDF). OpenAI. 2023. Archived (PDF) from the original on March 14, 2023. Retrieved March 14, 2023.\\\\n^ Schreiner, Maximilian (2023-07-11). \\\\\"GPT-4 architecture, datasets, costs and more leaked\\\\\". THE DECODER. Archived from the original on 2023-07-12. Retrieved 2024-07-26.\\\\n^ Dickson, Ben (22 May 2024). \\\\\"Meta introduces Chameleon, a state-of-the-art multimodal model\\\\\". VentureBeat.\\\\n^ Dey, Nolan (March 28, 2023). \\\\\"Cerebras-GPT: A Family of Open, Compute-efficient, Large Language Models\\\\\". Cerebras. Archived from the original on March 28, 2023. Retrieved March 28, 2023.\\\\n^ \\\\\"Abu Dhabi-based TII launches its own version of ChatGPT\\\\\". tii.ae. Archived from the original on 2023-04-03. Retrieved 2023-04-03.\\\\n^ Penedo, Guilherme; Malartic, Quentin; Hesslow, Daniel; Cojocaru, Ruxandra; Cappelli, Alessandro; Alobeidli, Hamza; Pannier, Baptiste; Almazrouei, Ebtesam; Launay, Julien (2023-06-01). \\\\\"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only\\\\\". arXiv:2306.01116 [cs.CL].\\\\n^ \\\\\"tiiuae/falcon-40b · Hugging Face\\\\\". huggingface.co. 2023-06-09. Retrieved 2023-06-20.\\\\n^ UAE\\'s Falcon 40B, World\\'s Top-Ranked AI Model from Technology Innovation Institute, is Now Royalty-Free Archived 2024-02-08 at the Wayback Machine, 31 May 2023\\\\n^ Wu, Shijie; Irsoy, Ozan; Lu, Steven; Dabravolski, Vadim; Dredze, Mark; Gehrmann, Sebastian; Kambadur, Prabhanjan; Rosenberg, David; Mann, Gideon (March 30, 2023). \\\\\"BloombergGPT: A Large Language Model for Finance\\\\\". arXiv:2303.17564 [cs.LG].\\\\n^ Ren, Xiaozhe; Zhou, Pingyi; Meng, Xinfan; Huang, Xinjing; Wang, Yadao; Wang, Weichao; Li, Pengfei; Zhang, Xiaoda; Podolskiy, Alexander; Arshinov, Grigory; Bout, Andrey; Piontkovskaya, Irina; Wei, Jiansheng; Jiang, Xin; Su, Teng; Liu, Qun; Yao, Jun (March 19, 2023). \\\\\"PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing\\\\\". arXiv:2303.10845 [cs.CL].\\\\n^ Köpf, Andreas; Kilcher, Yannic; von Rütte, Dimitri; Anagnostidis, Sotiris; Tam, Zhi-Rui; Stevens, Keith; Barhoum, Abdullah; Duc, Nguyen Minh; Stanley, Oliver; Nagyfi, Richárd; ES, Shahul; Suri, Sameer; Glushkov, David; Dantuluri, Arnav; Maguire, Andrew (2023-04-14). \\\\\"OpenAssistant Conversations – Democratizing Large Language Model Alignment\\\\\". arXiv:2304.07327 [cs.CL].\\\\n^ Wrobel, Sharon. \\\\\"Tel Aviv startup rolls out new advanced AI language model to rival OpenAI\\\\\". www.timesofisrael.com. Archived from the original on 2023-07-24. Retrieved 2023-07-24.\\\\n^ Wiggers, Kyle (2023-04-13). \\\\\"With Bedrock, Amazon enters the generative AI race\\\\\". TechCrunch. Archived from the original on 2023-07-24. Retrieved 2023-07-24.\\\\n^ a b Elias, Jennifer (16 May 2023). \\\\\"Google\\'s newest A.I. model uses nearly five times more text data for training than its predecessor\\\\\". CNBC. Archived from the original on 16 May 2023. Retrieved 18 May 2023.\\\\n^ \\\\\"Introducing PaLM 2\\\\\". Google. May 10, 2023. Archived from the original on May 18, 2023. Retrieved May 18, 2023.\\\\n^ a b \\\\\"Introducing Llama 2: The Next Generation of Our Open Source Large Language Model\\\\\". Meta AI. 2023. Archived from the original on 2024-01-05. Retrieved 2023-07-19.\\\\n^ \\\\\"llama/MODEL_CARD.md at main · meta-llama/llama\\\\\". GitHub. Archived from the original on 2024-05-28. Retrieved 2024-05-28.\\\\n^ \\\\\"Claude 2\\\\\". anthropic.com. Archived from the original on 15 December 2023. Retrieved 12 December 2023.\\\\n^ Nirmal, Dinesh (2023-09-07). \\\\\"Building AI for business: IBM\\'s Granite foundation models\\\\\". IBM Blog. Archived from the original on 2024-07-22. Retrieved 2024-08-11.\\\\n^ \\\\\"Announcing Mistral 7B\\\\\". Mistral. 2023. Archived from the original on 2024-01-06. Retrieved 2023-10-06.\\\\n^ \\\\\"Introducing Claude 2.1\\\\\". anthropic.com. Archived from the original on 15 December 2023. Retrieved 12 December 2023.\\\\n^ xai-org/grok-1, xai-org, 2024-03-19, archived from the original on 2024-05-28, retrieved 2024-03-19\\\\n^ \\\\\"Grok-1 model card\\\\\". x.ai. Retrieved 12 December 2023.\\\\n^ \\\\\"Gemini – Google DeepMind\\\\\". deepmind.google. Archived from the original on 8 December 2023. Retrieved 12 December 2023.\\\\n^ Franzen, Carl (11 December 2023). \\\\\"Mistral shocks AI community as latest open source model eclipses GPT-3.5 performance\\\\\". VentureBeat. Archived from the original on 11 December 2023. Retrieved 12 December 2023.\\\\n^ \\\\\"Mixtral of experts\\\\\". mistral.ai. 11 December 2023. Archived from the original on 13 February 2024. Retrieved 12 December 2023.\\\\n^ AI, Mistral (2024-04-17). \\\\\"Cheaper, Better, Faster, Stronger\\\\\". mistral.ai. Archived from the original on 2024-05-05. Retrieved 2024-05-05.\\\\n^ a b Hughes, Alyssa (12 December 2023). \\\\\"Phi-2: The surprising power of small language models\\\\\". Microsoft Research. Archived from the original on 12 December 2023. Retrieved 13 December 2023.\\\\n^ \\\\\"Our next-generation model: Gemini 1.5\\\\\". Google. 15 February 2024. Archived from the original on 16 February 2024. Retrieved 16 February 2024. This means 1.5 Pro can process vast amounts of information in one go — including 1 hour of video, 11 hours of audio, codebases with over 30,000 lines of code or over 700,000 words. In our research, we\\'ve also successfully tested up to 10 million tokens.\\\\n^ \\\\\"Gemma\\\\\" – via GitHub.\\\\n^ \\\\\"Introducing the next generation of Claude\\\\\". www.anthropic.com. Archived from the original on 2024-03-04. Retrieved 2024-03-04.\\\\n^ \\\\\"Fugaku-LLM/Fugaku-LLM-13B · Hugging Face\\\\\". huggingface.co. Archived from the original on 2024-05-17. Retrieved 2024-05-17.\\\\n^ \\\\\"Phi-3\\\\\". azure.microsoft.com. 23 April 2024. Archived from the original on 2024-04-27. Retrieved 2024-04-28.\\\\n^ \\\\\"Phi-3 Model Documentation\\\\\". huggingface.co. Archived from the original on 2024-05-13. Retrieved 2024-04-28.\\\\n^ \\\\\"Qwen2\\\\\". GitHub. Archived from the original on 2024-06-17. Retrieved 2024-06-17.\\\\n^ \\\\\"nvidia/Nemotron-4-340B-Base · Hugging Face\\\\\". huggingface.co. 2024-06-14. Archived from the original on 2024-06-15. Retrieved 2024-06-15.\\\\n^ \\\\\"Nemotron-4 340B | Research\\\\\". research.nvidia.com. Archived from the original on 2024-06-15. Retrieved 2024-06-15.\\\\n^ \\\\\"The Llama 3 Herd of Models\\\\\" (July 23, 2024) Llama Team, AI @ Meta\\\\n^ \\\\\"llama-models/models/llama3_1/MODEL_CARD.md at main · meta-llama/llama-models\\\\\". GitHub. Archived from the original on 2024-07-23. Retrieved 2024-07-23.\\\\n^ deepseek-ai/DeepSeek-V3, DeepSeek, 2024-12-26, retrieved 2024-12-26\\\\n^ Amazon Nova Micro, Lite, and Pro - AWS AI Service Cards3, Amazon, 2024-12-27, retrieved 2024-12-27\\\\nvte\\\\nNatural language processing\\\\nGeneral terms \\\\nAI-completeBag-of-wordsn-gram BigramTrigramComputational linguisticsNatural language understandingStop wordsText processing\\\\nText analysis \\\\nArgument miningCollocation extractionConcept miningCoreference resolutionDeep linguistic processingDistant readingInformation extractionNamed-entity recognitionOntology learningParsing Semantic parsingSyntactic parsingPart-of-speech taggingSemantic analysisSemantic role labelingSemantic decompositionSemantic similaritySentiment analysis\\\\nTerminology extractionText miningTextual entailmentTruecasingWord-sense disambiguationWord-sense induction\\\\nText segmentation \\\\nCompound-term processingLemmatisationLexical analysisText chunkingStemmingSentence segmentationWord segmentation\\\\nAutomatic summarization \\\\nMulti-document summarizationSentence extractionText simplification\\\\nMachine translation \\\\nComputer-assistedExample-basedRule-basedStatisticalTransfer-basedNeural\\\\nDistributional semantics models \\\\nBERTDocument-term matrixExplicit semantic analysisfastTextGloVeLanguage model (large)Latent semantic analysisSeq2seqWord embeddingWord2vec\\\\nLanguage resources,\\\\ndatasets and corpora  \\\\nTypes and\\\\nstandards \\\\nCorpus linguisticsLexical resourceLinguistic Linked Open DataMachine-readable dictionaryParallel textPropBankSemantic networkSimple Knowledge Organization SystemSpeech corpusText corpusThesaurus (information retrieval)TreebankUniversal Dependencies\\\\nData  \\\\nBabelNetBank of EnglishDBpediaFrameNetGoogle Ngram ViewerUBYWordNetWikidata\\\\nAutomatic identification\\\\nand data capture  \\\\nSpeech recognitionSpeech segmentationSpeech synthesisNatural language generationOptical character recognition\\\\nTopic model \\\\nDocument classificationLatent Dirichlet allocationPachinko allocation\\\\nComputer-assisted\\\\nreviewing \\\\nAutomated essay scoringConcordancerGrammar checkerPredictive textPronunciation assessmentSpell checker\\\\nNatural language\\\\nuser interface\\\\nChatbotInteractive fiction (c.f. Syntax guessing)Question answeringVirtual assistantVoice user interface\\\\nRelated \\\\nFormal semanticsHallucinationNatural Language ToolkitspaCy\\\\nPortal:\\\\n Language\\\\nCategory: Software comparisons\\\\nThis page was last edited on 28 December 2024, at 21:13\\xa0(UTC).\\\\nText is available under the Creative Commons Attribution-ShareAlike 4.0 License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\\\\nPrivacy policy\\\\nAbout Wikipedia\\\\nDisclaimers\\\\nContact Wikipedia\\\\nCode of Conduct\\\\nDevelopers\\\\nStatistics\\\\nCookie statement\\\\nMobile view\"}, {\"url\": \"https://ai.meta.com/blog/meta-llama-3/\", \"title\": \"Introducing Meta Llama 3: The most capable openly available LLM ...\", \"content\": \"Today, we’re introducing Meta Llama 3, the next generation of our state-of-the-art open source large language model. Today, we’re excited to share the first two models of the next generation of Llama, Meta Llama 3, available for broad use. We wanted to address developer feedback to increase the overall helpfulness of Llama 3 and are doing so while continuing to play a leading role on responsible use and deployment of LLMs. We are embracing the open source ethos of releasing early and often to enable the community to get access to these models while they are still in development. Please note that this data is based on an early checkpoint of Llama 3 that is still training and these capabilities are not supported as part of the models released today.\", \"score\": 0.7216064, \"raw_content\": \"Introducing Meta Llama 3: The most capable openly available LLM to date\\\\n\\\\n\\\\nOur approach\\\\nResearch\\\\nProduct experiences\\\\nLlama\\\\nBlog\\\\nTry Meta AI\\\\n\\\\n\\\\nLarge Language Model\\\\nIntroducing Meta Llama 3: The most capable openly available LLM to date\\\\nApril 18, 2024\\\\nTakeaways:\\\\nRECOMMENDED READS\\\\n\\\\n5 Steps to Getting Started with Llama 2\\\\nThe Llama Ecosystem: Past, Present, and Future\\\\nIntroducing Code Llama, a state-of-the-art large language model for coding\\\\n\\\\nMeta and Microsoft Introduce the Next Generation of Llama\\\\n\\\\n\\\\nToday, we’re introducing Meta Llama 3, the next generation of our state-of-the-art open source large language model.\\\\n\\\\nLlama 3 models will soon be available on AWS, Databricks, Google Cloud, Hugging Face, Kaggle, IBM WatsonX, Microsoft Azure, NVIDIA NIM, and Snowflake, and with support from hardware platforms offered by AMD, AWS, Dell, Intel, NVIDIA, and Qualcomm.\\\\nWe’re dedicated to developing Llama 3 in a responsible way, and we’re offering various resources to help others use it responsibly as well. This includes introducing new trust and safety tools with Llama Guard 2, Code Shield, and CyberSec Eval 2.\\\\nIn the coming months, we expect to introduce new capabilities, longer context windows, additional model sizes, and enhanced performance, and we’ll share the Llama 3 research paper.\\\\nMeta AI, built with Llama 3 technology, is now one of the world’s leading AI assistants that can boost your intelligence and lighten your load—helping you learn, get things done, create content, and connect to make the most out of every moment. You can try Meta AI here.\\\\n\\\\nToday, we’re excited to share the first two models of the next generation of Llama, Meta Llama 3, available for broad use. This release features pretrained and instruction-fine-tuned language models with 8B and 70B parameters that can support a broad range of use cases. This next generation of Llama demonstrates state-of-the-art performance on a wide range of industry benchmarks and offers new capabilities, including improved reasoning. We believe these are the best open source models of their class, period. In support of our longstanding open approach, we’re putting Llama 3 in the hands of the community. We want to kickstart the next wave of innovation in AI across the stack—from applications to developer tools to evals to inference optimizations and more. We can’t wait to see what you build and look forward to your feedback.\\\\nOur goals for Llama 3\\\\nWith Llama 3, we set out to build the best open models that are on par with the best proprietary models available today. We wanted to address developer feedback to increase the overall helpfulness of Llama 3 and are doing so while continuing to play a leading role on responsible use and deployment of LLMs. We are embracing the open source ethos of releasing early and often to enable the community to get access to these models while they are still in development. The text-based models we are releasing today are the first in the Llama 3 collection of models. Our goal in the near future is to make Llama 3 multilingual and multimodal, have longer context, and continue to improve overall performance across core LLM capabilities such as reasoning and coding.\\\\nState-of-the-art performance\\\\nOur new 8B and 70B parameter Llama 3 models are a major leap over Llama 2 and establish a new state-of-the-art for LLM models at those scales. Thanks to improvements in pretraining and post-training, our pretrained and instruction-fine-tuned models are the best models existing today at the 8B and 70B parameter scale. Improvements in our post-training procedures substantially reduced false refusal rates, improved alignment, and increased diversity in model responses. We also saw greatly improved capabilities like reasoning, code generation, and instruction following making Llama 3 more steerable.\\\\n*Please see evaluation details for setting and parameters with which these evaluations are calculated.\\\\nIn the development of Llama 3, we looked at model performance on standard benchmarks and also sought to optimize for performance for real-world scenarios. To this end, we developed a new high-quality human evaluation set. This evaluation set contains 1,800 prompts that cover 12 key use cases: asking for advice, brainstorming, classification, closed question answering, coding, creative writing, extraction, inhabiting a character/persona, open question answering, reasoning, rewriting, and summarization. To prevent accidental overfitting of our models on this evaluation set, even our own modeling teams do not have access to it. The chart below shows aggregated results of our human evaluations across of these categories and prompts against Claude Sonnet, Mistral Medium, and GPT-3.5.\\\\n\\\\nPreference rankings by human annotators based on this evaluation set highlight the strong performance of our 70B instruction-following model compared to competing models of comparable size in real-world scenarios.\\\\nOur pretrained model also establishes a new state-of-the-art for LLM models at those scales.\\\\n*Please see evaluation details for setting and parameters with which these evaluations are calculated.\\\\nTo develop a great language model, we believe it’s important to innovate, scale, and optimize for simplicity. We adopted this design philosophy throughout the Llama 3 project with a focus on four key ingredients: the model architecture, the pretraining data, scaling up pretraining, and instruction fine-tuning.\\\\nModel architecture\\\\nIn line with our design philosophy, we opted for a relatively standard decoder-only transformer architecture in Llama 3. Compared to Llama 2, we made several key improvements. Llama 3 uses a tokenizer with a vocabulary of 128K tokens that encodes language much more efficiently, which leads to substantially improved model performance. To improve the inference efficiency of Llama 3 models, we’ve adopted grouped query attention (GQA) across both the 8B and 70B sizes. We trained the models on sequences of 8,192 tokens, using a mask to ensure self-attention does not cross document boundaries.\\\\nTraining data\\\\nTo train the best language model, the curation of a large, high-quality training dataset is paramount. In line with our design principles, we invested heavily in pretraining data. Llama 3 is pretrained on over 15T tokens that were all collected from publicly available sources. Our training dataset is seven times larger than that used for Llama 2, and it includes four times more code. To prepare for upcoming multilingual use cases, over 5% of the Llama 3 pretraining dataset consists of high-quality non-English data that covers over 30 languages. However, we do not expect the same level of performance in these languages as in English.\\\\nTo ensure Llama 3 is trained on data of the highest quality, we developed a series of data-filtering pipelines. These pipelines include using heuristic filters, NSFW filters, semantic deduplication approaches, and text classifiers to predict data quality. We found that previous generations of Llama are surprisingly good at identifying high-quality data, hence we used Llama 2 to generate the training data for the text-quality classifiers that are powering Llama 3.\\\\nWe also performed extensive experiments to evaluate the best ways of mixing data from different sources in our final pretraining dataset. These experiments enabled us to select a data mix that ensures that Llama 3 performs well across use cases including trivia questions, STEM, coding, historical knowledge, etc.\\\\nScaling up pretraining\\\\nTo effectively leverage our pretraining data in Llama 3 models, we put substantial effort into scaling up pretraining. Specifically, we have developed a series of detailed scaling laws for downstream benchmark evaluations. These scaling laws enable us to select an optimal data mix and to make informed decisions on how to best use our training compute. Importantly, scaling laws allow us to predict the performance of our largest models on key tasks (for example, code generation as evaluated on the HumanEval benchmark—see above) before we actually train the models. This helps us ensure strong performance of our final models across a variety of use cases and capabilities.\\\\nWe made several new observations on scaling behavior during the development of Llama 3. For example, while the Chinchilla-optimal amount of training compute for an 8B parameter model corresponds to ~200B tokens, we found that model performance continues to improve even after the model is trained on two orders of magnitude more data. Both our 8B and 70B parameter models continued to improve log-linearly after we trained them on up to 15T tokens. Larger models can match the performance of these smaller models with less training compute, but smaller models are generally preferred because they are much more efficient during inference.\\\\nTo train our largest Llama 3 models, we combined three types of parallelization: data parallelization, model parallelization, and pipeline parallelization. Our most efficient implementation achieves a compute utilization of over 400 TFLOPS per GPU when trained on 16K GPUs simultaneously. We performed training runs on two custom-built 24K GPU clusters. To maximize GPU uptime, we developed an advanced new training stack that automates error detection, handling, and maintenance. We also greatly improved our hardware reliability and detection mechanisms for silent data corruption, and we developed new scalable storage systems that reduce overheads of checkpointing and rollback. Those improvements resulted in an overall effective training time of more than 95%. Combined, these improvements increased the efficiency of Llama 3 training by ~three times compared to Llama 2.\\\\nInstruction fine-tuning\\\\nTo fully unlock the potential of our pretrained models in chat use cases, we innovated on our approach to instruction-tuning as well. Our approach to post-training is a combination of supervised fine-tuning (SFT), rejection sampling, proximal policy optimization (PPO), and direct preference optimization (DPO). The quality of the prompts that are used in SFT and the preference rankings that are used in PPO and DPO has an outsized influence on the performance of aligned models. Some of our biggest improvements in model quality came from carefully curating this data and performing multiple rounds of quality assurance on annotations provided by human annotators.\\\\nLearning from preference rankings via PPO and DPO also greatly improved the performance of Llama 3 on reasoning and coding tasks. We found that if you ask a model a reasoning question that it struggles to answer, the model will sometimes produce the right reasoning trace: The model knows how to produce the right answer, but it does not know how to select it. Training on preference rankings enables the model to learn how to select it.\\\\nBuilding with Llama 3\\\\nOur vision is to enable developers to customize Llama 3 to support relevant use cases and to make it easier to adopt best practices and improve the open ecosystem. With this release, we’re providing new trust and safety tools including updated components with both Llama Guard 2 and Cybersec Eval 2, and the introduction of Code Shield—an inference time guardrail for filtering insecure code produced by LLMs.\\\\nWe’ve also co-developed Llama 3 with torchtune, the new PyTorch-native library for easily authoring, fine-tuning, and experimenting with LLMs. torchtune provides memory efficient and hackable training recipes written entirely in PyTorch. The library is integrated with popular platforms such as Hugging Face, Weights & Biases, and EleutherAI and even supports Executorch for enabling efficient inference to be run on a wide variety of mobile and edge devices. For everything from prompt engineering to using Llama 3 with LangChain we have a comprehensive getting started guide and takes you from downloading Llama 3 all the way to deployment at scale within your generative AI application.\\\\nA system-level approach to responsibility\\\\nWe have designed Llama 3 models to be maximally helpful while ensuring an industry leading approach to responsibly deploying them. To achieve this, we have adopted a new, system-level approach to the responsible development and deployment of Llama. We envision Llama models as part of a broader system that puts the developer in the driver’s seat. Llama models will serve as a foundational piece of a system that developers design with their unique end goals in mind.\\\\n\\\\nInstruction fine-tuning also plays a major role in ensuring the safety of our models. Our instruction-fine-tuned models have been red-teamed (tested) for safety through internal and external efforts. \\u200b\\u200bOur red teaming approach leverages human experts and automation methods to generate adversarial prompts that try to elicit problematic responses. For instance, we apply comprehensive testing to assess risks of misuse related to Chemical, Biological, Cyber Security, and other risk areas. All of these efforts are iterative and used to inform safety fine-tuning of the models being released. You can read more about our efforts in the model card.\\\\nLlama Guard models are meant to be a foundation for prompt and response safety and can easily be fine-tuned to create a new taxonomy depending on application needs. As a starting point, the new Llama Guard 2 uses the recently announced MLCommons taxonomy, in an effort to support the emergence of industry standards in this important area. Additionally, CyberSecEval 2 expands on its predecessor by adding measures of an LLM’s propensity to allow for abuse of its code interpreter, offensive cybersecurity capabilities, and susceptibility to prompt injection attacks (learn more in our technical paper). Finally, we’re introducing Code Shield which adds support for inference-time filtering of insecure code produced by LLMs. This offers mitigation of risks around insecure code suggestions, code interpreter abuse prevention, and secure command execution.\\\\nWith the speed at which the generative AI space is moving, we believe an open approach is an important way to bring the ecosystem together and mitigate these potential harms. As part of that, we’re updating our Responsible Use Guide (RUG) that provides a comprehensive guide to responsible development with LLMs. As we outlined in the RUG, we recommend that all inputs and outputs be checked and filtered in accordance with content guidelines appropriate to the application. Additionally, many cloud service providers offer content moderation APIs and other tools for responsible deployment, and we encourage developers to also consider using these options.\\\\nDeploying Llama 3 at scale\\\\nLlama 3 will soon be available on all major platforms including cloud providers, model API providers, and much more. Llama 3 will be everywhere.\\\\nOur benchmarks show the tokenizer offers improved token efficiency, yielding up to 15% fewer tokens compared to Llama 2. Also, Group Query Attention (GQA) now has been added to Llama 3 8B as well. As a result, we observed that despite the model having 1B more parameters compared to Llama 2 7B, the improved tokenizer efficiency and GQA contribute to maintaining the inference efficiency on par with Llama 2 7B.\\\\nFor examples of how to leverage all of these capabilities, check out Llama Recipes which contains all of our open source code that can be leveraged for everything from fine-tuning to deployment to model evaluation.\\\\nWhat’s next for Llama 3?\\\\nThe Llama 3 8B and 70B models mark the beginning of what we plan to release for Llama 3. And there’s a lot more to come.\\\\nOur largest models are over 400B parameters and, while these models are still training, our team is excited about how they’re trending. Over the coming months, we’ll release multiple models with new capabilities including multimodality, the ability to converse in multiple languages, a much longer context window, and stronger overall capabilities. We will also publish a detailed research paper once we are done training Llama 3.\\\\nTo give you a sneak preview for where these models are today as they continue training, we thought we could share some snapshots of how our largest LLM model is trending. Please note that this data is based on an early checkpoint of Llama 3 that is still training and these capabilities are not supported as part of the models released today.\\\\n*Please see evaluation details for setting and parameters with which these evaluations are calculated.\\\\nWe’re committed to the continued growth and development of an open AI ecosystem for releasing our models responsibly. We have long believed that openness leads to better, safer products, faster innovation, and a healthier overall market. This is good for Meta, and it is good for society. We’re taking a community-first approach with Llama 3, and starting today, these models are available on the leading cloud, hosting, and hardware platforms with many more to come.\\\\nTry Meta Llama 3 today\\\\nWe’ve integrated our latest models into Meta AI, which we believe is the world’s leading AI assistant. It’s now built with Llama 3 technology and it’s available in more countries across our apps.\\\\nYou can use Meta AI on Facebook, Instagram, WhatsApp, Messenger, and the web to get things done, learn, create, and connect with the things that matter to you. You can read more about the Meta AI experience here.\\\\nVisit the Llama 3 website to download the models and reference the Getting Started Guide for the latest list of all available platforms.\\\\nYou’ll also soon be able to test multimodal Meta AI on our Ray-Ban Meta smart glasses.\\\\nAs always, we look forward to seeing all the amazing products and experiences you will build with Meta Llama 3.\\\\n\\\\nShare:\\\\n\\\\nOur latest updates delivered to your inbox\\\\nSubscribe to our newsletter to keep up with Meta AI news, events, research breakthroughs, and more.\\\\nJoin us in the pursuit of what’s possible with AI.\\\\nSee all open positions\\\\nRelated Posts\\\\n\\\\nComputer Vision\\\\nIntroducing Segment Anything: Working toward the first foundation model for image segmentation\\\\nApril 5, 2023\\\\nRead post\\\\nFEATURED\\\\n\\\\nResearch\\\\nMultiRay: Optimizing efficiency for large-scale AI models\\\\nNovember 18, 2022\\\\nRead post\\\\nFEATURED\\\\n\\\\nML Applications\\\\nMuAViC: The first audio-video speech translation benchmark\\\\nMarch 8, 2023\\\\nRead post\\\\nOur approach\\\\nAbout AI at Meta\\\\nPeople\\\\nCareers\\\\nResearch\\\\nInfrastructure\\\\nResources\\\\nDemos\\\\nProduct experiences\\\\nMeta AI\\\\nAI Studio\\\\nLatest news\\\\nBlog\\\\nNewsletter\\\\nFoundational models\\\\nLlama\\\\n\\\\n \\\\n \\\\n \\\\n \\\\nOur approach\\\\nOur approachAbout AI at MetaPeopleCareers\\\\nResearch\\\\nResearchInfrastructureResourcesDemos\\\\nProduct experiences\\\\nMeta AIAI Studio\\\\nLatest news\\\\nLatest newsBlogNewsletter\\\\nFoundational models\\\\nLlama\\\\n \\\\n \\\\n \\\\n \\\\nPrivacy Policy\\\\nTerms\\\\nCookies\\\\nMeta © 2025\\\\n \\\\n \\\\n \\\\n \"}, {\"url\": \"https://www.hostinger.com/tutorials/large-language-models\", \"title\": \"8 Best Large Language Models for 2025 - Hostinger\", \"content\": \"To simplify this process, our team of experts has crafted this list of large language models, making it easy for you to pick the perfect AI model for your website needs. Optimizing SEO – When it comes to optimizing website content with language models, GPT 3.5 stands out the most. Creating AI-powered ad copy – Gemini can produce AI-powered ad copy and promotional materials that are tailored to the website’s content and target audience, which helps increase brand awareness, drive traffic, and generate leads. Small websites – such as blog sites, can do good with an LLM like GPT-3.5, which can affordably create content; it can also be used for a specific task, such as answering questions and translating languages.\", \"score\": 0.64826655, \"raw_content\": null}, {\"url\": \"https://en.wikipedia.org/wiki/Large_language_model\", \"title\": \"Large language model - Wikipedia\", \"content\": \"A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. A smoothed n-gram model in 2001 trained on 0.3 billion words achieved state-of-the-art perplexity at the time.[4] In the 2000s, as Internet use became prevalent, some researchers constructed Internet-scale language datasets (\\\\\"web as corpus\\\\\"[5]), upon which they trained statistical language models.[6][7] In 2009, in most language processing tasks, statistical language models dominated over symbolic language models, as they can usefully ingest large datasets.[8]\", \"score\": 0.61668247, \"raw_content\": \"Jump to content\\\\nMain menu\\\\nSearch\\\\nDonate\\\\nCreate account\\\\nLog in\\\\nPersonal tools\\\\nToggle the table of contents\\\\nLarge language model\\\\n53 languages\\\\nArticle\\\\nTalk\\\\nRead\\\\nEdit\\\\nView history\\\\nTools\\\\nFrom Wikipedia, the free encyclopedia\\\\nNot to be confused with Logic learning machine.\\\\nPart of a series on\\\\nMachine learning\\\\nand data mining\\\\nParadigms\\\\nProblems\\\\nSupervised learning\\\\n(classification\\xa0• regression)\\\\nClustering\\\\nDimensionality reduction\\\\nStructured prediction\\\\nAnomaly detection\\\\nArtificial neural network\\\\nAutoencoderDeep learningFeedforward neural networkRecurrent neural network LSTMGRUESNreservoir computingBoltzmann machine RestrictedGANDiffusion modelSOMConvolutional neural network U-NetLeNetAlexNetDeepDreamNeural radiance fieldTransformer VisionMambaSpiking neural networkMemtransistorElectrochemical RAM (ECRAM)\\\\nReinforcement learning\\\\nLearning with humans\\\\nModel diagnostics\\\\nMathematical foundations\\\\nJournals and conferences\\\\nRelated articles\\\\nvte\\\\nA large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering.[1] These models acquire predictive power regarding syntax, semantics, and ontologies[2] inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.[3]\\\\nHistory[edit]\\\\nThe training compute of notable large models in FLOPs vs publication date over the period 2010-2024. For overall notable models (top left), frontier models (top right), top language models (bottom left) and top models within leading companies (bottom right). The majority of these models are language models.\\\\nThe training compute of notable large AI models in FLOPs vs publication date over the period 2017-2024. The majority of large models are language models or multimodal models with language capacity.\\\\nBefore 2017, there were a few language models that were large as compared to capacities then available. In the 1990s, the IBM alignment models pioneered statistical language modelling. A smoothed n-gram model in 2001 trained on 0.3 billion words achieved state-of-the-art perplexity at the time.[4] In the 2000s, as Internet use became prevalent, some researchers constructed Internet-scale language datasets (\\\\\"web as corpus\\\\\"[5]), upon which they trained statistical language models.[6][7] In 2009, in most language processing tasks, statistical language models dominated over symbolic language models, as they can usefully ingest large datasets.[8]\\\\nAfter neural networks became dominant in image processing around 2012,[9] they were applied to language modelling as well. Google converted its translation service to Neural Machine Translation in 2016. As it was before transformers, it was done by seq2seq deep LSTM networks.\\\\nAn illustration of main components of the transformer model from the original paper, where layers were normalized after (instead of before) multiheaded attention\\\\nAt the 2017 NeurIPS conference, Google researchers introduced the transformer architecture in their landmark paper \\\\\"Attention Is All You Need\\\\\". This paper\\'s goal was to improve upon 2014 seq2seq technology,[10] and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014.[11] The following year in 2018, BERT was introduced and quickly became \\\\\"ubiquitous\\\\\".[12] Though the original transformer has both encoder and decoder blocks, BERT is an encoder-only model. Academic and research usage of BERT began to decline in 2023, following rapid improvements in the abilities of decoder-only models (such as GPT) to solve tasks via prompting.[13]\\\\nAlthough decoder-only GPT-1 was introduced in 2018, it was GPT-2 in 2019 that caught widespread attention because OpenAI at first deemed it too powerful to release publicly, out of fear of malicious use.[14] GPT-3 in 2020 went a step further and as of 2024 is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing browser-based ChatGPT that captured the imaginations of the general population and caused some media hype and online buzz.[15] The 2023 GPT-4 was praised for its increased accuracy and as a \\\\\"holy grail\\\\\" for its multimodal capabilities.[16] OpenAI did not reveal the high-level architecture and the number of parameters of GPT-4. The release of ChatGPT led to an uptick in LLM usage across several research subfields of computer science, including robotics, software engineering, and societal impact work.[17] In 2024 OpenAI released the reasoning model OpenAI o1, which generates long chains of thought before returning a final answer.\\\\nCompeting language models have for the most part been attempting to equal the GPT series, at least in terms of number of parameters.[18]\\\\nSince 2022, source-available models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on the field of use. Mistral AI\\'s models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. In January 2025, DeepSeek released DeepSeek R1, a 671-billion-parameter open-weight model that performs comparably to OpenAI o1 but at a much lower cost.[19]\\\\nSince 2023, many LLMs have been trained to be multimodal, having the ability to also process or generate other types of data, such as images or audio. These LLMs are also called large multimodal models (LMMs).[20]\\\\nAs of 2024, the largest and most capable models are all based on the transformer architecture. Some recent implementations are based on other architectures, such as recurrent neural network variants and Mamba (a state space model).[21][22][23]\\\\nDataset preprocessing[edit]\\\\nSee also: List of datasets for machine-learning research §\\xa0Internet\\\\nTokenization[edit]\\\\nAs machine learning algorithms process numbers rather than text, the text must be converted to numbers. In the first step, a vocabulary is decided upon, then integer indices are arbitrarily but uniquely assigned to each vocabulary entry, and finally, an embedding is associated to the integer index. Algorithms include byte-pair encoding (BPE) and WordPiece. There are also special tokens serving as control characters, such as [MASK] for masked-out token (as used in BERT), and [UNK] (\\\\\"unknown\\\\\") for characters not appearing in the vocabulary. Also, some special symbols are used to denote special text formatting. For example, \\\\\"Ġ\\\\\" denotes a preceding whitespace in RoBERTa and GPT. \\\\\"##\\\\\" denotes continuation of a preceding word in BERT.[24]\\\\nFor example, the BPE tokenizer used by GPT-3 (Legacy) would split tokenizer: texts -> series of numerical \\\\\"tokens\\\\\" as\\\\ntoken   izer    :   \\xa0texts  \\xa0-> series  \\xa0of \\xa0numerical  \\xa0\\\\\"  t   ok  ens \\\\\"\\\\nTokenization also compresses the datasets. Because LLMs generally require input to be an array that is not jagged, the shorter texts must be \\\\\"padded\\\\\" until they match the length of the longest one. How many tokens are, on average, needed per word depends on the language of the dataset.[25][26]\\\\nBPE[edit]\\\\nMain article: Byte pair encoding\\\\nAs an example, consider a tokenizer based on byte-pair encoding. In the first step, all unique characters (including blanks and punctuation marks) are treated as an initial set of n-grams (i.e. initial set of uni-grams). Successively the most frequent pair of adjacent characters is merged into a bi-gram and all instances of the pair are replaced by it. All occurrences of adjacent pairs of (previously merged) n-grams that most frequently occur together are then again merged into even lengthier n-gram, until a vocabulary of prescribed size is obtained (in case of GPT-3, the size is 50257).[27] After a tokenizer is trained, any text can be tokenized by it, as long as it does not contain characters not appearing in the initial-set of uni-grams.[28]\\\\nProblems[edit]\\\\nA token vocabulary based on the frequencies extracted from mainly English corpora uses as few tokens as possible for an average English word. However, an average word in another language encoded by such an English-optimized tokenizer is split into a suboptimal amount of tokens. GPT-2 tokenizer can use up to 15 times more tokens per word for some languages, for example for the Shan language from Myanmar. Even more widespread languages such as Portuguese and German have \\\\\"a premium of 50%\\\\\" compared to English.[29]\\\\nGreedy tokenization also causes subtle problems with text completion.[30]\\\\nDataset cleaning[edit]\\\\nMain article: Data cleansing\\\\nIn the context of training LLMs, datasets are typically cleaned by removing low-quality, duplicated, or toxic data.[31] Cleaned datasets can increase training efficiency and lead to improved downstream performance.[32][33] A trained LLM can be used to clean datasets for training a further LLM.[34]\\\\nWith the increasing proportion of LLM-generated content on the web, data cleaning in the future may include filtering out such content. LLM-generated content can pose a problem if the content is similar to human text (making filtering difficult) but of lower quality (degrading performance of models trained on it).[35]\\\\nSynthetic data[edit]\\\\nMain article: Synthetic data\\\\nTraining of largest language models might need more linguistic data than naturally available, or that the naturally occurring data is of insufficient quality. In these cases, synthetic data might be used. Microsoft\\'s Phi series of LLMs is trained on textbook-like data generated by another LLM.[36]\\\\nTraining and architecture[edit]\\\\nSee also: Fine-tuning (machine learning)\\\\nReinforcement learning from human feedback (RLHF)[edit]\\\\nMain article: Reinforcement learning from human feedback\\\\nReinforcement learning from human feedback (RLHF) through algorithms, such as proximal policy optimization, is used to further fine-tune a model based on a dataset of human preferences.[37]\\\\nInstruction tuning[edit]\\\\nUsing \\\\\"self-instruct\\\\\" approaches, LLMs have been able to bootstrap correct responses, replacing any naive responses, starting from human-generated corrections of a few cases. For example, in the instruction \\\\\"Write an essay about the main themes represented in Hamlet,\\\\\" an initial naive completion might be \\\\\"If you submit the essay after March 17, your grade will be reduced by 10% for each day of delay,\\\\\" based on the frequency of this textual sequence in the corpus.[38]\\\\nMixture of experts[edit]\\\\nMain article: Mixture of experts\\\\nThe largest LLM may be too expensive to train and use directly. For such models, mixture of experts (MoE) can be applied, a line of research pursued by Google researchers since 2017 to train models reaching up to 1 trillion parameters.[39][40][41]\\\\nPrompt engineering, attention mechanism, and context window[edit]\\\\nSee also: Prompt engineering and Attention (machine learning)\\\\nMost results previously achievable only by (costly) fine-tuning, can be achieved through prompt engineering, although limited to the scope of a single conversation (more precisely, limited to the scope of a context window).[42]\\\\nWhen each head calculates, according to its own criteria, how much other tokens are relevant for the \\\\\"it_\\\\\" token, note that the second attention head, represented by the second column, is focusing most on the first two rows, i.e. the tokens \\\\\"The\\\\\" and \\\\\"animal\\\\\", while the third column is focusing most on the bottom two rows, i.e. on \\\\\"tired\\\\\", which has been tokenized into two tokens.[43]\\\\nIn order to find out which tokens are relevant to each other within the scope of the context window, the attention mechanism calculates \\\\\"soft\\\\\" weights for each token, more precisely for its embedding, by using multiple attention heads, each with its own \\\\\"relevance\\\\\" for calculating its own soft weights. For example, the small (i.e. 117M parameter sized) GPT-2 model has had twelve attention heads and a context window of only 1k tokens.[44] In its medium version it has 345M parameters and contains 24 layers, each with 12 attention heads. For the training with gradient descent a batch size of 512 was utilized.[28]\\\\nThe largest models, such as Google\\'s Gemini 1.5, presented in February 2024, can have a context window sized up to 1 million (context window of 10 million was also \\\\\"successfully tested\\\\\").[45] Other models with large context windows includes Anthropic\\'s Claude 2.1, with a context window of up to 200k tokens.[46] Note that this maximum refers to the number of input tokens and that the maximum number of output tokens differs from the input and is often smaller. For example, the GPT-4 Turbo model has a maximum output of 4096 tokens.[47]\\\\nLength of a conversation that the model can take into account when generating its next answer is limited by the size of a context window, as well. If the length of a conversation, for example with ChatGPT, is longer than its context window, only the parts inside the context window are taken into account when generating the next answer, or the model needs to apply some algorithm to summarize the too distant parts of conversation.\\\\nThe shortcomings of making a context window larger include higher computational cost and possibly diluting the focus on local context, while making it smaller can cause a model to miss an important long-range dependency. Balancing them is a matter of experimentation and domain-specific considerations.\\\\nA model may be pre-trained either to predict how the segment continues, or what is missing in the segment, given a segment from its training dataset.[48] It can be either\\\\nautoregressive (i.e. predicting how the segment continues, as GPTs do): for example given a segment \\\\\"I like to eat\\\\\", the model predicts \\\\\"ice cream\\\\\", or \\\\\"sushi\\\\\".\\\\n\\\\\"masked\\\\\" (i.e. filling in the parts missing from the segment, the way \\\\\"BERT\\\\\"[49] does it): for example, given a segment \\\\\"I like to [] [] cream\\\\\", the model predicts that \\\\\"eat\\\\\" and \\\\\"ice\\\\\" are missing.\\\\nModels may be trained on auxiliary tasks which test their understanding of the data distribution, such as Next Sentence Prediction (NSP), in which pairs of sentences are presented and the model must predict whether they appear consecutively in the training corpus.[49] During training, regularization loss is also used to stabilize training. However regularization loss is usually not used during testing and evaluation.\\\\nInfrastructure[edit]\\\\nSubstantial infrastructure is necessary for training the largest models.[50][51][52]\\\\nTraining cost[edit]\\\\nThe qualifier \\\\\"large\\\\\" in \\\\\"large language model\\\\\" is inherently vague, as there is no definitive threshold for the number of parameters required to qualify as \\\\\"large\\\\\". As time goes on, what was previously considered \\\\\"large\\\\\" may evolve. GPT-1 of 2018 is usually considered the first LLM, even though it has only 0.117 billion parameters. The tendency towards larger models is visible in the list of large language models.\\\\nAdvances in software and hardware have reduced the cost substantially since 2020, such that in 2023 training of a 12-billion-parameter LLM computational cost is 72,300 A100-GPU-hours, while in 2020 the cost of training a 1.5-billion-parameter LLM (which was two orders of magnitude smaller than the state of the art in 2020) was between $80,000 and $1,600,000.[53][54][55] Since 2020, large sums were invested in increasingly large models. For example, training of the GPT-2 (i.e. a 1.5-billion-parameters model) in 2019 cost $50,000, while training of the PaLM (i.e. a 540-billion-parameters model) in 2022 cost $8 million, and Megatron-Turing NLG 530B (in 2021) cost around $11 million.[56]\\\\nFor Transformer-based LLM, training cost is much higher than inference cost. It costs 6 FLOPs per parameter to train on one token, whereas it costs 1 to 2 FLOPs per parameter to infer on one token.[57]\\\\nTool use[edit]\\\\nThere are certain tasks that, in principle, cannot be solved by any LLM, at least not without the use of external tools or additional software. An example of such a task is responding to the user\\'s input \\'354 * 139 = \\', provided that the LLM has not already encountered a continuation of this calculation in its training corpus.[dubious – discuss] In such cases, the LLM needs to resort to running program code that calculates the result, which can then be included in its response.[dubious – discuss]: Another example is \\\\\"What is the time now? It is \\\\\", where a separate program interpreter would need to execute a code to get system time on the computer, so that the LLM can include it in its reply.[58][59] This basic strategy can be sophisticated with multiple attempts of generated programs, and other sampling strategies.[60]\\\\nGenerally, in order to get an LLM to use tools, one must fine-tune it for tool-use. If the number of tools is finite, then fine-tuning may be done just once. If the number of tools can grow arbitrarily, as with online API services, then the LLM can be fine-tuned to be able to read API documentation and call API correctly.[61][62]\\\\nA simpler form of tool use is retrieval-augmented generation: the augmentation of an LLM with document retrieval. Given a query, a document retriever is called to retrieve the most relevant documents. This is usually done by encoding the query and the documents into vectors, then finding the documents with vectors (usually stored in a vector database) most similar to the vector of the query. The LLM then generates an output based on both the query and context included from the retrieved documents.[63]\\\\nAgency[edit]\\\\nAn LLM is typically not an autonomous agent by itself, as it lacks the ability to interact with dynamic environments, recall past behaviors, and plan future actions, but can be transformed into one by integrating modules like profiling, memory, planning, and action.[64]\\\\nThe ReAct pattern, a portmanteau of \\\\\"Reason\\xa0+\\xa0Act\\\\\", constructs an agent out of an LLM, using the LLM as a planner. The LLM is prompted to \\\\\"think out loud\\\\\". Specifically, the language model is prompted with a textual description of the environment, a goal, a list of possible actions, and a record of the actions and observations so far. It generates one or more thoughts before generating an action, which is then executed in the environment.[65] The linguistic description of the environment given to the LLM planner can even be the LaTeX code of a paper describing the environment.[66]\\\\nIn the DEPS (\\\\\"Describe, Explain, Plan and Select\\\\\") method, an LLM is first connected to the visual world via image descriptions, then it is prompted to produce plans for complex tasks and behaviors based on its pretrained knowledge and environmental feedback it receives.[67]\\\\nThe Reflexion method[68] constructs an agent that learns over multiple episodes. At the end of each episode, the LLM is given the record of the episode, and prompted to think up \\\\\"lessons learned\\\\\", which would help it perform better at a subsequent episode. These \\\\\"lessons learned\\\\\" are given to the agent in the subsequent episodes.[citation needed]\\\\nMonte Carlo tree search can use an LLM as rollout heuristic. When a programmatic world model is not available, an LLM can also be prompted with a description of the environment to act as world model.[69]\\\\nFor open-ended exploration, an LLM can be used to score observations for their \\\\\"interestingness\\\\\", which can be used as a reward signal to guide a normal (non-LLM) reinforcement learning agent.[70] Alternatively, it can propose increasingly difficult tasks for curriculum learning.[71] Instead of outputting individual actions, an LLM planner can also construct \\\\\"skills\\\\\", or functions for complex action sequences. The skills can be stored and later invoked, allowing increasing levels of abstraction in planning.[71]\\\\nLLM-powered agents can keep a long-term memory of its previous contexts, and the memory can be retrieved in the same way as Retrieval Augmented Generation. Multiple such agents can interact socially.[72]\\\\nCompression[edit]\\\\nTypically, LLMs are trained with single- or half-precision floating point numbers (float32 and float16). One float16 has 16 bits, or 2 bytes, and so one billion parameters require 2 gigabytes. The largest models typically have 100 billion parameters, requiring 200 gigabytes to load, which places them outside the range of most consumer electronics.[73]\\\\nPost-training quantization[74] aims to decrease the space requirement by lowering precision of the parameters of a trained model, while preserving most of its performance.[75][76] The simplest form of quantization simply truncates all numbers to a given number of bits. It can be improved by using a different quantization codebook per layer. Further improvement can be done by applying different precisions to different parameters, with higher precision for particularly important parameters (\\\\\"outlier weights\\\\\").[77] See the visual guide to quantization by Maarten Grootendorst[78] for a visual depiction.\\\\nWhile quantized models are typically frozen, and only pre-quantized models are fine-tuned, quantized models can still be fine-tuned.[79]\\\\nMultimodality[edit]\\\\nSee also: Multimodal learning\\\\nMultimodality means \\\\\"having several modalities\\\\\", and a \\\\\"modality\\\\\" refers to a type of input or output, such as video, image, audio, text, proprioception, etc.[80] There have been many AI models trained specifically to ingest one modality and output another modality, such as AlexNet for image to label,[81] visual question answering for image-text to text,[82] and speech recognition for speech to text.\\\\nA common method to create multimodal models out of an LLM is to \\\\\"tokenize\\\\\" the output of a trained encoder. Concretely, one can construct an LLM that can understand images as follows: take a trained LLM, and take a trained image encoder \\\\n𝐸\\\\n. Make a small multilayered perceptron \\\\n𝑓\\\\n, so that for any image \\\\n𝑦\\\\n, the post-processed vector \\\\n𝑓\\\\n(\\\\n𝐸\\\\n(\\\\n𝑦\\\\n)\\\\n)\\\\n has the same dimensions as an encoded token. That is an \\\\\"image token\\\\\". Then, one can interleave text tokens and image tokens. The compound model is then fine-tuned on an image-text dataset. This basic construction can be applied with more sophistication to improve the model. The image encoder may be frozen to improve stability.[83]\\\\nFlamingo demonstrated the effectiveness of the tokenization method, finetuning a pair of pretrained language model and image encoder to perform better on visual question answering than models trained from scratch.[84] Google PaLM model was fine-tuned into a multimodal model PaLM-E using the tokenization method, and applied to robotic control.[85] LLaMA models have also been turned multimodal using the tokenization method, to allow image inputs,[86] and video inputs.[87]\\\\nGPT-4 can use both text and image as inputs[88] (although the vision component was not released to the public until GPT-4V[89]); Google DeepMind\\'s Gemini is also multimodal.[90] Mistral introduced its own multimodel Pixtral 12B model in September 2024.[91]\\\\nReasoning[edit]\\\\nIn late 2024, a new direction emerged in LLM development with models specifically designed for complex reasoning tasks. These \\\\\"reasoning models\\\\\" were trained to spend more time generating step-by-step solutions before providing final answers, similar to human problem-solving processes.[92] OpenAI introduced this trend with their o1 model in September 2024, followed by o3 in December 2024. These models showed significant improvements in mathematics, science, and coding tasks compared to traditional LLMs. For example, on International Mathematics Olympiad qualifying exam problems, GPT-4o achieved 13% accuracy while o1 reached 83%.[92][93] In January 2025, the Chinese company DeepSeek released DeepSeek-R1, a 671-billion-parameter open-weight reasoning model that achieved comparable performance to OpenAI\\'s o1 while being significantly more cost-effective to operate. Unlike proprietary models from OpenAI, DeepSeek-R1\\'s open-weight nature allowed researchers to study and build upon the algorithm, though its training data remained private.[94] These reasoning models typically require more computational resources per query compared to traditional LLMs, as they perform more extensive processing to work through problems step-by-step. However, they have shown superior capabilities in domains requiring structured logical thinking, such as mathematics, scientific research, and computer programming.[93]\\\\nProperties[edit]\\\\nScaling laws[edit]\\\\nMain article: Neural scaling law\\\\nThe performance of an LLM after pretraining largely depends on the:\\\\ncost of pretraining \\\\n𝐶\\\\n (the total amount of compute used),\\\\nsize of the artificial neural network itself, such as number of parameters \\\\n𝑁\\\\n (i.e. amount of neurons in its layers, amount of weights between them and biases),\\\\nsize of its pretraining dataset (i.e. number of tokens in corpus, \\\\n𝐷\\\\n).\\\\n\\\\\"Scaling laws\\\\\" are empirical statistical laws that predict LLM performance based on such factors. One particular scaling law (\\\\\"Chinchilla scaling\\\\\") for LLM autoregressively trained for one epoch, with a log-log learning rate schedule, states that:[95]\\\\n{\\\\n𝐶\\\\n=\\\\n𝐶\\\\n0\\\\n𝑁\\\\n𝐷\\\\n𝐿\\\\n𝐴\\\\n𝑁\\\\n𝛼\\\\n+\\\\n𝐵\\\\n𝐷\\\\n𝛽\\\\n+\\\\n𝐿\\\\n0\\\\nwhere the variables are\\\\n𝐶\\\\n is the cost of training the model, in FLOPs.\\\\n𝑁\\\\n is the number of parameters in the model.\\\\n𝐷\\\\n is the number of tokens in the training set.\\\\n𝐿\\\\n is the average negative log-likelihood loss per token (nats/token), achieved by the trained LLM on the test dataset.\\\\nand the statistical hyper-parameters are\\\\n𝐶\\\\n0\\\\n=\\\\n6\\\\n, meaning that it costs 6 FLOPs per parameter to train on one token. Note that training cost is much higher than inference cost, where it costs 1 to 2 FLOPs per parameter to infer on one token.[57]\\\\n𝛼\\\\n=\\\\n0.34\\\\n,\\\\n𝛽\\\\n=\\\\n0.28\\\\n,\\\\n𝐴\\\\n=\\\\n406.4\\\\n,\\\\n𝐵\\\\n=\\\\n410.7\\\\n,\\\\n𝐿\\\\n0\\\\n=\\\\n1.69\\\\nEmergent abilities[edit]\\\\nAt point(s) referred to as breaks,[96] the lines change their slopes, appearing on a linear-log plot as a series of linear segments connected by arcs.\\\\nPerformance of bigger models on various tasks, when plotted on a log-log scale, appears as a linear extrapolation of performance achieved by smaller models. However, this linearity may be punctuated by \\\\\"break(s)\\\\\"[96] in the scaling law, where the slope of the line changes abruptly, and where larger models acquire \\\\\"emergent abilities\\\\\".[42][97] They arise from the complex interaction of the model\\'s components and are not explicitly programmed or designed.[98]\\\\nFurthermore, recent research has demonstrated that AI systems, including large language models, can employ heuristic reasoning akin to human cognition. They balance between exhaustive logical processing and the use of cognitive shortcuts (heuristics), adapting their reasoning strategies to optimize between accuracy and effort. This behavior aligns with principles of resource-rational human cognition, as discussed in classical theories of bounded rationality and dual-process theory.[99]\\\\nThe most intriguing among emergent abilities is in-context learning from example demonstrations.[100] In-context learning is involved in tasks, such as:\\\\nreported arithmetics\\\\ndecoding the International Phonetic Alphabet\\\\nunscrambling a word\\'s letters\\\\ndisambiguating word-in-context datasets[42][101][102]\\\\nconverting spatial words\\\\ncardinal directions (for example, replying \\\\\"northeast\\\\\" in response to a 3x3 grid of 8 zeros and a 1 in the top-right), color terms represented in text.[103]\\\\nchain-of-thought prompting: Model outputs are improved by chain-of-thought prompting only when model size exceeds 62B. Smaller models perform better when prompted to answer immediately, without chain of thought.[104]\\\\nidentifying offensive content in paragraphs of Hinglish (a combination of Hindi and English), and generating a similar English equivalent of Kiswahili proverbs.[105]\\\\nSchaeffer et. al. argue that the emergent abilities are not unpredictably acquired, but predictably acquired according to a smooth scaling law. The authors considered a toy statistical model of an LLM solving multiple-choice questions, and showed that this statistical model, modified to account for other types of tasks, applies to these tasks as well.[106]\\\\nLet \\\\n𝑥\\\\n be the number of parameter count, and \\\\n𝑦\\\\n be the performance of the model.\\\\nWhen \\\\n𝑦\\\\n=\\\\naverage\\xa0\\\\nPr\\\\n(\\\\ncorrect token\\\\n)\\\\n, then \\\\n(\\\\nlog\\\\n\\u2061\\\\n𝑥\\\\n,\\\\n𝑦\\\\n)\\\\n is an exponential curve (before it hits the plateau at one), which looks like emergence.\\\\nWhen \\\\n𝑦\\\\n=\\\\naverage\\xa0\\\\nlog\\\\n\\u2061\\\\n(\\\\nPr\\\\n(\\\\ncorrect token\\\\n)\\\\n)\\\\n, then the \\\\n(\\\\nlog\\\\n\\u2061\\\\n𝑥\\\\n,\\\\n𝑦\\\\n)\\\\n plot is a straight line (before it hits the plateau at zero), which does not look like emergence.\\\\nWhen \\\\n𝑦\\\\n=\\\\naverage\\xa0\\\\nPr\\\\n(\\\\nthe most likely token is correct\\\\n)\\\\n, then \\\\n(\\\\nlog\\\\n\\u2061\\\\n𝑥\\\\n,\\\\n𝑦\\\\n)\\\\n is a step-function, which looks like emergence.\\\\nInterpretation[edit]\\\\nLarge language models by themselves are black boxes, and it is not clear how they can perform linguistic tasks. There are several methods for understanding how LLM work.\\\\nMechanistic interpretability aims to reverse-engineer LLM by discovering symbolic algorithms that approximate the inference performed by LLM. One example is Othello-GPT, where a small Transformer is trained to predict legal Othello moves. It is found that there is a linear representation of Othello board, and modifying the representation changes the predicted legal Othello moves in the correct way.[107][108] In another example, a small Transformer is trained on Karel programs. Similar to the Othello-GPT example, there is a linear representation of Karel program semantics, and modifying the representation changes output in the correct way. The model also generates correct programs that are on average shorter than those in the training set.[109]\\\\nIn another example, the authors trained small transformers on modular arithmetic addition. The resulting models were reverse-engineered, and it turned out they used discrete Fourier transform.[110]\\\\nA related concept is AI explainability, which focuses on understanding how an AI model arrives at a given result.\\\\nUnderstanding and intelligence[edit]\\\\nSee also: Philosophy of artificial intelligence and Artificial consciousness\\\\nNLP researchers were evenly split when asked, in a 2022 survey, whether (untuned) LLMs \\\\\"could (ever) understand natural language in some nontrivial sense\\\\\".[111] Proponents of \\\\\"LLM understanding\\\\\" believe that some LLM abilities, such as mathematical reasoning, imply an ability to \\\\\"understand\\\\\" certain concepts. A Microsoft team argued in 2023 that GPT-4 \\\\\"can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more\\\\\" and that GPT-4 \\\\\"could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence system\\\\\": \\\\\"Can one reasonably say that a system that passes exams for software engineering candidates is not really intelligent?\\\\\"[112][113] Ilya Sutskever argues that predicting the next word sometimes involves reasoning and deep insights, for example if the LLM has to predict the name of the criminal in an unknown detective novel after processing the entire story leading up to the revelation.[114] Some researchers characterize LLMs as \\\\\"alien intelligence\\\\\".[115][116] For example, Conjecture CEO Connor Leahy considers untuned LLMs to be like inscrutable alien \\\\\"Shoggoths\\\\\", and believes that RLHF tuning creates a \\\\\"smiling facade\\\\\" obscuring the inner workings of the LLM: \\\\\"If you don\\'t push it too far, the smiley face stays on. But then you give it [an unexpected] prompt, and suddenly you see this massive underbelly of insanity, of weird thought processes and clearly non-human understanding.\\\\\"[117][118]\\\\nIn contrast, some skeptics of LLM understanding believe that existing LLMs are \\\\\"simply remixing and recombining existing writing\\\\\",[116] a phenomenon known as stochastic parrot, or they point to the deficits existing LLMs continue to have in prediction skills, reasoning skills, agency, and explainability.[111] For example, GPT-4 has natural deficits in planning and in real-time learning.[113] Generative LLMs have been observed to confidently assert claims of fact which do not seem to be justified by their training data, a phenomenon which has been termed \\\\\"hallucination\\\\\".[119] Specifically, hallucinations in the context of LLMs correspond to the generation of text or responses that seem syntactically sound, fluent, and natural but are factually incorrect, nonsensical, or unfaithful to the provided source input.[120] Neuroscientist Terrence Sejnowski has argued that \\\\\"The diverging opinions of experts on the intelligence of LLMs suggests that our old ideas based on natural intelligence are inadequate\\\\\".[111]\\\\nThe matter of LLM\\'s exhibiting intelligence or understanding has two main aspects – the first is how to model thought and language in a computer system, and the second is how to enable the computer system to generate human like language.[111] These aspects of language as a model of cognition have been developed in the field of cognitive linguistics. American linguist George Lakoff presented Neural Theory of Language (NTL)[121] as a computational basis for using language as a model of learning tasks and understanding. The NTL Model outlines how specific neural structures of the human brain shape the nature of thought and language and in turn what are the computational properties of such neural systems that can be applied to model thought and language in a computer system. After a framework for modeling language in a computer systems was established, the focus shifted to establishing frameworks for computer systems to generate language with acceptable grammar. In his 2014 book titled The Language Myth: Why Language Is Not An Instinct, British cognitive linguist and digital communication technologist Vyvyan Evans mapped out the role of probabilistic context-free grammar (PCFG) in enabling NLP to model cognitive patterns and generate human like language.[122][123]\\\\nEvaluation[edit]\\\\nPerplexity[edit]\\\\nThe canonical measure of the performance of an LLM is its perplexity on a given text corpus. Perplexity measures how well a model predicts the contents of a dataset; the higher the likelihood the model assigns to the dataset, the lower the perplexity. In mathematical terms, perplexity is the exponential of the average negative log likelihood per token.\\\\nlog\\\\n\\u2061\\\\n(\\\\nPerplexity\\\\n)\\\\n=\\\\n−\\\\n1\\\\n𝑁\\\\n∑\\\\n𝑖\\\\n=\\\\n1\\\\n𝑁\\\\nlog\\\\n\\u2061\\\\n(\\\\nPr\\\\n(\\\\ntoken\\\\n𝑖\\\\n∣\\\\ncontext for token\\\\n𝑖\\\\n)\\\\n)\\\\nHere, \\\\n𝑁\\\\n is the number of tokens in the text corpus, and \\\\\"context for token \\\\n𝑖\\\\n\\\\\" depends on the specific type of LLM. If the LLM is autoregressive, then \\\\\"context for token \\\\n𝑖\\\\n\\\\\" is the segment of text appearing before token \\\\n𝑖\\\\n. If the LLM is masked, then \\\\\"context for token \\\\n𝑖\\\\n\\\\\" is the segment of text surrounding token \\\\n𝑖\\\\n.\\\\nBecause language models may overfit to training data, models are usually evaluated by their perplexity on a test set.[49] This evaluation is potentially problematic for larger models which, as they are trained on increasingly large corpora of text, are increasingly likely to inadvertently include portions of any given test set.[1]\\\\nBPW, BPC, and BPT[edit]\\\\nIn information theory, the concept of entropy is intricately linked to perplexity, a relationship notably established by Claude Shannon.[124] This relationship is mathematically expressed as \\\\nEntropy\\\\n=\\\\nlog\\\\n2\\\\n\\u2061\\\\n(\\\\nPerplexity\\\\n)\\\\n.\\\\nEntropy, in this context, is commonly quantified in terms of bits per word (BPW) or bits per character (BPC), which hinges on whether the language model utilizes word-based or character-based tokenization.\\\\nNotably, in the case of larger language models that predominantly employ sub-word tokenization, bits per token (BPT) emerges as a seemingly more appropriate measure. However, due to the variance in tokenization methods across different Large Language Models (LLMs), BPT does not serve as a reliable metric for comparative analysis among diverse models. To convert BPT into BPW, one can multiply it by the average number of tokens per word.\\\\nIn the evaluation and comparison of language models, cross-entropy is generally the preferred metric over entropy. The underlying principle is that a lower BPW is indicative of a model\\'s enhanced capability for compression. This, in turn, reflects the model\\'s proficiency in making accurate predictions.\\\\nTask-specific datasets and benchmarks[edit]\\\\nA large number of testing datasets and benchmarks have also been developed to evaluate the capabilities of language models on more specific downstream tasks. Tests may be designed to evaluate a variety of capabilities, including general knowledge, commonsense reasoning, and mathematical problem-solving.\\\\nOne broad category of evaluation dataset is question answering datasets, consisting of pairs of questions and correct answers, for example, (\\\\\"Have the San Jose Sharks won the Stanley Cup?\\\\\", \\\\\"No\\\\\").[125] A question answering task is considered \\\\\"open book\\\\\" if the model\\'s prompt includes text from which the expected answer can be derived (for example, the previous question could be adjoined with some text which includes the sentence \\\\\"The Sharks have advanced to the Stanley Cup finals once, losing to the Pittsburgh Penguins in 2016.\\\\\"[125]). Otherwise, the task is considered \\\\\"closed book\\\\\", and the model must draw on knowledge retained during training.[126] Some examples of commonly used question answering datasets include TruthfulQA, Web Questions, TriviaQA, and SQuAD.[126]\\\\nEvaluation datasets may also take the form of text completion, having the model select the most likely word or sentence to complete a prompt, for example: \\\\\"Alice was friends with Bob. Alice went to visit her friend, ____\\\\\".[1]\\\\nSome composite benchmarks have also been developed which combine a diversity of different evaluation datasets and tasks. Examples include GLUE, SuperGLUE, MMLU, BIG-bench, HELM, and HLE (Humanity\\'s Last Exam).[124][126] OpenAI has released tools for running composite benchmarks, but noted that the eval results are sensitive to the prompting method.[127][128] Some public datasets contain questions that are mislabeled, ambiguous, unanswerable, or otherwise of low-quality, which can be cleaned to give more reliable benchmark scores.[129]\\\\nIt was previously standard to report results on a heldout portion of an evaluation dataset after doing supervised fine-tuning on the remainder. It is now more common to evaluate a pre-trained model directly through prompting techniques, though researchers vary in the details of how they formulate prompts for particular tasks, particularly with respect to how many examples of solved tasks are adjoined to the prompt (i.e. the value of n in n-shot prompting).\\\\nAdversarially constructed evaluations[edit]\\\\nBecause of the rapid pace of improvement of large language models, evaluation benchmarks have suffered from short lifespans, with state of the art models quickly \\\\\"saturating\\\\\" existing benchmarks, exceeding the performance of human annotators, leading to efforts to replace or augment the benchmark with more challenging tasks.[130] In addition, there are cases of \\\\\"shortcut learning\\\\\" wherein AIs sometimes \\\\\"cheat\\\\\" on multiple-choice tests by using statistical correlations in superficial test question wording in order to guess the correct responses, without necessarily understanding the actual question being asked.[111]\\\\nSome datasets have been constructed adversarially, focusing on particular problems on which extant language models seem to have unusually poor performance compared to humans. One example is the TruthfulQA dataset, a question answering dataset consisting of 817 questions which language models are susceptible to answering incorrectly by mimicking falsehoods to which they were repeatedly exposed during training. For example, an LLM may answer \\\\\"No\\\\\" to the question \\\\\"Can you teach an old dog new tricks?\\\\\" because of its exposure to the English idiom you can\\'t teach an old dog new tricks, even though this is not literally true.[131]\\\\nAnother example of an adversarial evaluation dataset is Swag and its successor, HellaSwag, collections of problems in which one of multiple options must be selected to complete a text passage. The incorrect completions were generated by sampling from a language model and filtering with a set of classifiers. The resulting problems are trivial for humans but at the time the datasets were created state of the art language models had poor accuracy on them. For example:\\\\nWe see a fitness center sign. We then see a man talking to the camera and sitting and laying on a exercise ball. The man...\\\\na) demonstrates how to increase efficient exercise work by running up and down balls.\\\\nb) moves all his arms and legs and builds up a lot of muscle.\\\\nc) then plays the ball and we see a graphics and hedge trimming demonstration.\\\\nd) performs sit ups while on the ball and talking.[132]\\\\nBERT selects b) as the most likely completion, though the correct answer is d).[132]\\\\nLimitations of LLM benchmarks[edit]\\\\nBenchmarks can become outdated rapidly. Once a model attains near-perfect scores on a given benchmark, that benchmark ceases to serve as a meaningful indicator of progress. This phenomenon, known as \\\\\"benchmark saturation,\\\\\" necessitates the development of more challenging and nuanced tasks to continue advancing LLM capabilities. For instance, traditional benchmarks like HellaSwag and MMLU have seen models achieving high accuracy already.\\\\nWider impact[edit]\\\\nIn 2023, Nature Biomedical Engineering wrote that \\\\\"it is no longer possible to accurately distinguish\\\\\" human-written text from text created by large language models, and that \\\\\"It is all but certain that general-purpose large language models will rapidly proliferate... It is a rather safe bet that they will change many industries over time.\\\\\"[133] Goldman Sachs suggested in 2023 that generative language AI could increase global GDP by 7% in the next ten years, and could expose to automation 300 million jobs globally.[134][135]\\\\nMemorization and copyright[edit]\\\\nFurther information: Artificial intelligence and copyright\\\\nMemorization is an emergent behavior in LLMs in which long strings of text are occasionally output verbatim from training data, contrary to typical behavior of traditional artificial neural nets. Evaluations of controlled LLM output measure the amount memorized from training data (focused on GPT-2-series models) as variously over 1% for exact duplicates[136] or up to about 7%.[137]\\\\nA 2023 study showed that when ChatGPT 3.5 turbo was prompted to repeat the same word indefinitely, after a few hundreds of repetitions, it would start outputting excerpts from its training data.[138]\\\\nSecurity[edit]\\\\nSome commenters expressed concern over accidental or deliberate creation of misinformation, or other forms of misuse.[139] For example, the availability of large language models could reduce the skill-level required to commit bioterrorism; biosecurity researcher Kevin Esvelt has suggested that LLM creators should exclude from their training data papers on creating or enhancing pathogens.[140]\\\\nThe potential presence of \\\\\"sleeper agents\\\\\" within LLM models is another emerging security concern. These are hidden functionalities built into the model that remain dormant until triggered by a specific event or condition. Upon activation, the LLM deviates from its expected behavior to make insecure actions.[141]\\\\nLLM applications accessible to the public, like ChatGPT or Claude, typically incorporate safety measures designed to filter out harmful content. However, implementing these controls effectively has proven challenging. For instance, a 2023 study[142] proposed a method for circumventing LLM safety systems. Similarly, Yongge Wang[143] illustrated in 2024 how a potential criminal could potentially bypass ChatGPT 4o\\'s safety controls to obtain information on establishing a drug trafficking operation.\\\\nAlgorithmic bias[edit]\\\\nMain article: Algorithmic bias\\\\nWhile LLMs have shown remarkable capabilities in generating human-like text, they are susceptible to inheriting and amplifying biases present in their training data. This can manifest in skewed representations or unfair treatment of different demographics, such as those based on race, gender, language, and cultural groups.[144] Since English data is overrepresented in current large language models\\' training data, it may also downplay non-English views.[145]\\\\nStereotyping[edit]\\\\nAI models can reinforce a wide range of stereotypes, including those based on gender, ethnicity, age, nationality, religion, or occupation. This can lead to outputs that unfairly generalize or caricature groups of people, sometimes in harmful or derogatory ways.[146]\\\\nNotably, gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained. Large language models often assign roles and characteristics based on traditional gender norms.[144] For example, it might associate nurses or secretaries predominantly with women and engineers or CEOs with men.[147]\\\\nPolitical bias[edit]\\\\nPolitical bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.[148]\\\\nEnergy demands[edit]\\\\nThe energy demands of LLMs have grown along with their size and capabilities. Data centers that enable LLM training require substantial amounts of electricity. Much of that electricity is generated by non-renewable resources that create greenhouse gases and contribute to climate change.[149] Nuclear power and geothermal energy are two options tech companies are exploring to meet the sizable energy demands of LLM training.[150] The significant expense of investing in geothermal solutions has led to major shale producers like Chevron and Exxon Mobil advocating for tech companies to use electricity produced via natural gas to fuel their large energy demands.[151]\\\\nSee also[edit]\\\\nFoundation models\\\\nList of large language models\\\\nList of chatbots\\\\nReferences[edit]\\\\n^ a b c Brown, Tom B.; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Gretchen; Henighan, Tom; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel M.; Wu, Jeffrey; Winter, Clemens; Hesse, Christopher; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Scott; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (Dec 2020). Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.F.; Lin, H. (eds.). \\\\\"Language Models are Few-Shot Learners\\\\\" (PDF). Advances in Neural Information Processing Systems. 33. Curran Associates, Inc.: 1877–1901. Archived (PDF) from the original on 2023-11-17. Retrieved 2023-03-14.\\\\n^ Fathallah, Nadeen; Das, Arunav; De Giorgis, Stefano; Poltronieri, Andrea; Haase, Peter; Kovriguina, Liubov (2024-05-26). NeOn-GPT: A Large Language Model-Powered Pipeline for Ontology Learning (PDF). Extended Semantic Web Conference 2024. Hersonissos, Greece.\\\\n^ Manning, Christopher D. (2022). \\\\\"Human Language Understanding & Reasoning\\\\\". Daedalus. 151 (2): 127–138. doi:10.1162/daed_a_01905. S2CID\\xa0248377870. Archived from the original on 2023-11-17. Retrieved 2023-03-09.\\\\n^ Goodman, Joshua (2001-08-09), A Bit of Progress in Language Modeling, arXiv:cs/0108005, Bibcode:2001cs........8005G\\\\n^ Kilgarriff, Adam; Grefenstette, Gregory (September 2003). \\\\\"Introduction to the Special Issue on the Web as Corpus\\\\\". Computational Linguistics. 29 (3): 333–347. doi:10.1162/089120103322711569. ISSN\\xa00891-2017.\\\\n^ Banko, Michele; Brill, Eric (2001). \\\\\"Scaling to very very large corpora for natural language disambiguation\\\\\". Proceedings of the 39th Annual Meeting on Association for Computational Linguistics - ACL \\'01. Morristown, NJ, USA: Association for Computational Linguistics: 26–33. doi:10.3115/1073012.1073017.\\\\n^ Resnik, Philip; Smith, Noah A. (September 2003). \\\\\"The Web as a Parallel Corpus\\\\\". Computational Linguistics. 29 (3): 349–380. doi:10.1162/089120103322711578. ISSN\\xa00891-2017. Archived from the original on 2024-06-07. Retrieved 2024-06-07.\\\\n^ Halevy, Alon; Norvig, Peter; Pereira, Fernando (March 2009). \\\\\"The Unreasonable Effectiveness of Data\\\\\". IEEE Intelligent Systems. 24 (2): 8–12. doi:10.1109/MIS.2009.36. ISSN\\xa01541-1672.\\\\n^ Chen, Leiyu; Li, Shaobo; Bai, Qiang; Yang, Jing; Jiang, Sanlong; Miao, Yanming (2021). \\\\\"Review of Image Classification Algorithms Based on Convolutional Neural Networks\\\\\". Remote Sensing. 13 (22): 4712. Bibcode:2021RemS...13.4712C. doi:10.3390/rs13224712.\\\\n^ Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Łukasz; Polosukhin, Illia (2017). \\\\\"Attention is All you Need\\\\\" (PDF). Advances in Neural Information Processing Systems. 30. Curran Associates, Inc. Archived (PDF) from the original on 2024-02-21. Retrieved 2024-01-21.\\\\n^ Bahdanau, Dzmitry; Cho, Kyunghyun; Bengio, Yoshua (2014). \\\\\"Neural Machine Translation by Jointly Learning to Align and Translate\\\\\". arXiv:1409.0473 [cs.CL].\\\\n^ Rogers, Anna; Kovaleva, Olga; Rumshisky, Anna (2020). \\\\\"A Primer in BERTology: What We Know About How BERT Works\\\\\". Transactions of the Association for Computational Linguistics. 8: 842–866. arXiv:2002.12327. doi:10.1162/tacl_a_00349. S2CID\\xa0211532403. Archived from the original on 2022-04-03. Retrieved 2024-01-21.\\\\n^ Movva, Rajiv; Balachandar, Sidhika; Peng, Kenny; Agostini, Gabriel; Garg, Nikhil; Pierson, Emma (2024). \\\\\"Topics, Authors, and Institutions in Large Language Model Research: Trends from 17K arXiv Papers\\\\\". Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). pp.\\xa01223–1243. arXiv:2307.10700. doi:10.18653/v1/2024.naacl-long.67. Retrieved 2024-12-08.\\\\n^ Hern, Alex (14 February 2019). \\\\\"New AI fake text generator may be too dangerous to release, say creators\\\\\". The Guardian. Archived from the original on 14 February 2019. Retrieved 20 January 2024.\\\\n^ \\\\\"ChatGPT a year on: 3 ways the AI chatbot has completely changed the world in 12 months\\\\\". Euronews. November 30, 2023. Archived from the original on January 14, 2024. Retrieved January 20, 2024.\\\\n^ Heaven, Will (March 14, 2023). \\\\\"GPT-4 is bigger and better than ChatGPT—but OpenAI won\\'t say why\\\\\". MIT Technology Review. Archived from the original on March 17, 2023. Retrieved January 20, 2024.\\\\n^ Movva, Rajiv; Balachandar, Sidhika; Peng, Kenny; Agostini, Gabriel; Garg, Nikhil; Pierson, Emma (2024). \\\\\"Topics, Authors, and Institutions in Large Language Model Research: Trends from 17K arXiv Papers\\\\\". Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). pp.\\xa01223–1243. arXiv:2307.10700. doi:10.18653/v1/2024.naacl-long.67. Retrieved 2024-12-08.\\\\n^ \\\\\"Parameters in notable artificial intelligence systems\\\\\". ourworldindata.org. November 30, 2023. Retrieved January 20, 2024.\\\\n^ Sharma, Shubham (2025-01-20). \\\\\"Open-source DeepSeek-R1 uses pure reinforcement learning to match OpenAI o1 — at 95% less cost\\\\\". VentureBeat. Retrieved 2025-01-26.\\\\n^ Zia, Dr Tehseen (2024-01-08). \\\\\"Unveiling of Large Multimodal Models: Shaping the Landscape of Language Models in 2024\\\\\". Unite.AI. Retrieved 2024-12-28.\\\\n^ Peng, Bo; et\\xa0al. (2023). \\\\\"RWKV: Reinventing RNNS for the Transformer Era\\\\\". arXiv:2305.13048 [cs.CL].\\\\n^ Merritt, Rick (2022-03-25). \\\\\"What Is a Transformer Model?\\\\\". NVIDIA Blog. Archived from the original on 2023-11-17. Retrieved 2023-07-25.\\\\n^ Gu, Albert; Dao, Tri (2023-12-01), Mamba: Linear-Time Sequence Modeling with Selective State Spaces, arXiv:2312.00752\\\\n^ Kaushal, Ayush; Mahowald, Kyle (2022-06-06), What do tokens know about their characters and how do they know it?, arXiv:2206.02608\\\\n^ Yennie Jun (2023-05-03). \\\\\"All languages are NOT created (tokenized) equal\\\\\". Language models cost much more in some languages than others. Archived from the original on 2023-08-17. Retrieved 2023-08-17. In other words, to express the same sentiment, some languages require up to 10 times more tokens.\\\\n^ Petrov, Aleksandar; Malfa, Emanuele La; Torr, Philip; Bibi, Adel (June 23, 2023). \\\\\"Language Model Tokenizers Introduce Unfairness Between Languages\\\\\". NeurIPS. arXiv:2305.15425. Archived from the original on December 15, 2023. Retrieved September 16, 2023 – via openreview.net.\\\\n^ \\\\\"OpenAI API\\\\\". platform.openai.com. Archived from the original on April 23, 2023. Retrieved 2023-04-30.\\\\n^ a b Paaß, Gerhard; Giesselbach, Sven (2022). \\\\\"Pre-trained Language Models\\\\\". Foundation Models for Natural Language Processing. Artificial Intelligence: Foundations, Theory, and Algorithms. pp.\\xa019–78. doi:10.1007/978-3-031-23190-2_2. ISBN\\xa09783031231902. Archived from the original on 3 August 2023. Retrieved 3 August 2023.\\\\n^ Petrov, Aleksandar; Emanuele La Malfa; Torr, Philip H. S.; Bibi, Adel (2023). \\\\\"Language Model Tokenizers Introduce Unfairness Between Languages\\\\\". arXiv:2305.15425 [cs.CL].\\\\n^ Lundberg, Scott (2023-12-12). \\\\\"The Art of Prompt Design: Prompt Boundaries and Token Healing\\\\\". Medium. Retrieved 2024-08-05.\\\\n^ Dodge, Jesse; Sap, Maarten; Marasović, Ana; Agnew, William; Ilharco, Gabriel; Groeneveld, Dirk; Mitchell, Margaret; Gardner, Matt (2021). \\\\\"Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus\\\\\". arXiv:2104.08758 [cs.CL].\\\\n^ Lee, Katherine; Ippolito, Daphne; Nystrom, Andrew; Zhang, Chiyuan; Eck, Douglas; Callison-Burch, Chris; Carlini, Nicholas (May 2022). \\\\\"Deduplicating Training Data Makes Language Models Better\\\\\" (PDF). Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. 1: Long Papers: 8424–8445. doi:10.18653/v1/2022.acl-long.577.\\\\n^ Li, Yuanzhi; Bubeck, Sébastien; Eldan, Ronen; Del Giorno, Allie; Gunasekar, Suriya; Lee, Yin Tat (2023-09-11), Textbooks Are All You Need II: phi-1.5 technical report, arXiv:2309.05463\\\\n^ Lin, Zhenghao; Gou, Zhibin; Gong, Yeyun; Liu, Xiao; Shen, Yelong; Xu, Ruochen; Lin, Chen; Yang, Yujiu; Jiao, Jian (2024-04-11). \\\\\"Rho-1: Not All Tokens Are What You Need\\\\\". arXiv:2404.07965 [cs.CL].\\\\n^ Brown, Tom B.; et\\xa0al. (2020). \\\\\"Language Models are Few-Shot Learners\\\\\". arXiv:2005.14165 [cs.CL].\\\\n^ Abdin, Marah; Jacobs, Sam Ade; Awan, Ammar Ahmad; Aneja, Jyoti; Awadallah, Ahmed; Awadalla, Hany; Bach, Nguyen; Bahree, Amit; Bakhtiari, Arash (2024-04-23). \\\\\"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone\\\\\". arXiv:2404.14219 [cs.CL].\\\\n^ Ouyang, Long; Wu, Jeff; Jiang, Xu; Almeida, Diogo; Wainwright, Carroll L.; Mishkin, Pamela; Zhang, Chong; Agarwal, Sandhini; Slama, Katarina; Ray, Alex; Schulman, John; Hilton, Jacob; Kelton, Fraser; Miller, Luke; Simens, Maddie; Askell, Amanda; Welinder, Peter; Christiano, Paul; Leike, Jan; Lowe, Ryan (2022). \\\\\"Training language models to follow instructions with human feedback\\\\\". arXiv:2203.02155 [cs.CL].\\\\n^ Wang, Yizhong; Kordi, Yeganeh; Mishra, Swaroop; Liu, Alisa; Smith, Noah A.; Khashabi, Daniel; Hajishirzi, Hannaneh (2022). \\\\\"Self-Instruct: Aligning Language Model with Self Generated Instructions\\\\\". arXiv:2212.10560 [cs.CL].\\\\n^ Shazeer, Noam; Mirhoseini, Azalia; Maziarz, Krzysztof; Davis, Andy; Le, Quoc; Hinton, Geoffrey; Dean, Jeff (2017-01-01). \\\\\"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\\\\\". arXiv:1701.06538 [cs.LG].\\\\n^ Lepikhin, Dmitry; Lee, HyoukJoong; Xu, Yuanzhong; Chen, Dehao; Firat, Orhan; Huang, Yanping; Krikun, Maxim; Shazeer, Noam; Chen, Zhifeng (2021-01-12). \\\\\"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\\\\\". arXiv:2006.16668 [cs.CL].\\\\n^ Dai, Andrew M; Du, Nan (December 9, 2021). \\\\\"More Efficient In-Context Learning with GLaM\\\\\". ai.googleblog.com. Archived from the original on 2023-03-12. Retrieved 2023-03-09.\\\\n^ a b c Wei, Jason; Tay, Yi; Bommasani, Rishi; Raffel, Colin; Zoph, Barret; Borgeaud, Sebastian; Yogatama, Dani; Bosma, Maarten; Zhou, Denny; Metzler, Donald; Chi, Ed H.; Hashimoto, Tatsunori; Vinyals, Oriol; Liang, Percy; Dean, Jeff; Fedus, William (31 August 2022). \\\\\"Emergent Abilities of Large Language Models\\\\\". Transactions on Machine Learning Research. ISSN\\xa02835-8856. Archived from the original on 22 March 2023. Retrieved 19 March 2023.\\\\n^ Allamar, Jay. \\\\\"Illustrated transformer\\\\\". Archived from the original on 2023-07-25. Retrieved 2023-07-29.\\\\n^ Allamar, Jay. \\\\\"The Illustrated GPT-2 (Visualizing Transformer Language Models)\\\\\". Retrieved 2023-08-01.\\\\n^ \\\\\"Our next-generation model: Gemini 1.5\\\\\". Google. 15 February 2024. Archived from the original on 18 February 2024. Retrieved 18 February 2024.\\\\n^ \\\\\"Long context prompting for Claude 2.1\\\\\". December 6, 2023. Archived from the original on August 27, 2024. Retrieved January 20, 2024.\\\\n^ \\\\\"Rate limits\\\\\". openai.com. Archived from the original on February 2, 2024. Retrieved January 20, 2024.\\\\n^ Zaib, Munazza; Sheng, Quan Z.; Emma Zhang, Wei (4 February 2020). \\\\\"A Short Survey of Pre-trained Language Models for Conversational AI-A New Age in NLP\\\\\". Proceedings of the Australasian Computer Science Week Multiconference. pp.\\xa01–4. arXiv:2104.10810. doi:10.1145/3373017.3373028. ISBN\\xa09781450376976. S2CID\\xa0211040895.\\\\n^ a b c Jurafsky, Dan; Martin, James H. (7 January 2023). Speech and Language Processing (PDF) (3rd edition draft\\xa0ed.). Archived (PDF) from the original on 23 March 2023. Retrieved 24 May 2022.\\\\n^ \\\\\"From bare metal to a 70B model: infrastructure set-up and scripts\\\\\". imbue.com. Archived from the original on 2024-07-26. Retrieved 2024-07-24.\\\\n^ \\\\\"metaseq/projects/OPT/chronicles at main · facebookresearch/metaseq\\\\\". GitHub. Archived from the original on 2024-01-24. Retrieved 2024-07-24.\\\\n^ Albrecht, Josh (2024-07-23). \\\\\"State of the Art: Training >70B LLMs on 10,000 H100 clusters\\\\\". www.latent.space. Retrieved 2024-07-24.\\\\n^ Wiggers, Kyle (28 April 2022). \\\\\"The emerging types of language models and why they matter\\\\\". TechCrunch. Archived from the original on 16 March 2023. Retrieved 9 March 2023.\\\\n^ Sharir, Or; Peleg, Barak; Shoham, Yoav (2020). \\\\\"The Cost of Training NLP Models: A Concise Overview\\\\\". arXiv:2004.08900 [cs.CL].\\\\n^ Biderman, Stella; Schoelkopf, Hailey; Anthony, Quentin; Bradley, Herbie; Khan, Mohammad Aflah; Purohit, Shivanshu; Prashanth, USVSN Sai (April 2023). \\\\\"Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling\\\\\". arXiv:2304.01373 [cs.CL].\\\\n^ Maslej, Nestor; Fattorini, Loredana; Brynjolfsson, Erik; Etchemendy, John; Ligett, Katrina; Lyons, Terah; Manyika, James; Ngo, Helen; Niebles, Juan Carlos (2023-10-05), Artificial Intelligence Index Report 2023, arXiv:2310.03715\\\\n^ a b Section 2.1 and Table 1, Kaplan, Jared; McCandlish, Sam; Henighan, Tom; Brown, Tom B.; Chess, Benjamin; Child, Rewon; Gray, Scott; Radford, Alec; Wu, Jeffrey; Amodei, Dario (2020). \\\\\"Scaling Laws for Neural Language Models\\\\\". arXiv:2001.08361 [cs.LG].\\\\n^ Gao, Luyu; Madaan, Aman; Zhou, Shuyan; Alon, Uri; Liu, Pengfei; Yang, Yiming; Callan, Jamie; Neubig, Graham (2022-11-01). \\\\\"PAL: Program-aided Language Models\\\\\". arXiv:2211.10435 [cs.CL].\\\\n^ \\\\\"PAL: Program-aided Language Models\\\\\". reasonwithpal.com. Archived from the original on 2023-06-12. Retrieved 2023-06-12.\\\\n^ Paranjape, Bhargavi; Lundberg, Scott; Singh, Sameer; Hajishirzi, Hannaneh; Zettlemoyer, Luke; Tulio Ribeiro, Marco (2023-03-01). \\\\\"ART: Automatic multi-step reasoning and tool-use for large language models\\\\\". arXiv:2303.09014 [cs.CL].\\\\n^ Liang, Yaobo; Wu, Chenfei; Song, Ting; Wu, Wenshan; Xia, Yan; Liu, Yu; Ou, Yang; Lu, Shuai; Ji, Lei; Mao, Shaoguang; Wang, Yun; Shou, Linjun; Gong, Ming; Duan, Nan (2023-03-01). \\\\\"TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs\\\\\". arXiv:2303.16434 [cs.AI].\\\\n^ Patil, Shishir G.; Zhang, Tianjun; Wang, Xin; Gonzalez, Joseph E. (2023-05-01). \\\\\"Gorilla: Large Language Model Connected with Massive APIs\\\\\". arXiv:2305.15334 [cs.CL].\\\\n^ Lewis, Patrick; Perez, Ethan; Piktus, Aleksandra; Petroni, Fabio; Karpukhin, Vladimir; Goyal, Naman; Küttler, Heinrich; Lewis, Mike; Yih, Wen-tau; Rocktäschel, Tim; Riedel, Sebastian; Kiela, Douwe (2020). \\\\\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\\\\". Advances in Neural Information Processing Systems. 33. Curran Associates, Inc.: 9459–9474. arXiv:2005.11401. Archived from the original on 2023-06-12. Retrieved 2023-06-12.\\\\n^ \\\\\"The Growth Behind LLM-based Autonomous Agents\\\\\". KDnuggets. October 23, 2023.\\\\n^ Yao, Shunyu; Zhao, Jeffrey; Yu, Dian; Du, Nan; Shafran, Izhak; Narasimhan, Karthik; Cao, Yuan (2022-10-01). \\\\\"ReAct: Synergizing Reasoning and Acting in Language Models\\\\\". arXiv:2210.03629 [cs.CL].\\\\n^ Wu, Yue; Prabhumoye, Shrimai; Min, So Yeon (24 May 2023). \\\\\"SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning\\\\\". arXiv:2305.15486 [cs.AI].\\\\n^ Wang, Zihao; Cai, Shaofei; Liu, Anji; Ma, Xiaojian; Liang, Yitao (2023-02-03). \\\\\"Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents\\\\\". arXiv:2302.01560 [cs.AI].\\\\n^ Shinn, Noah; Cassano, Federico; Labash, Beck; Gopinath, Ashwin; Narasimhan, Karthik; Yao, Shunyu (2023-03-01). \\\\\"Reflexion: Language Agents with Verbal Reinforcement Learning\\\\\". arXiv:2303.11366 [cs.AI].\\\\n^ Hao, Shibo; Gu, Yi; Ma, Haodi; Jiahua Hong, Joshua; Wang, Zhen; Zhe Wang, Daisy; Hu, Zhiting (2023-05-01). \\\\\"Reasoning with Language Model is Planning with World Model\\\\\". arXiv:2305.14992 [cs.CL].\\\\n^ Zhang, Jenny; Lehman, Joel; Stanley, Kenneth; Clune, Jeff (2 June 2023). \\\\\"OMNI: Open-endedness via Models of human Notions of Interestingness\\\\\". arXiv:2306.01711 [cs.AI].\\\\n^ a b \\\\\"Voyager | An Open-Ended Embodied Agent with Large Language Models\\\\\". voyager.minedojo.org. Archived from the original on 2023-06-08. Retrieved 2023-06-09.\\\\n^ Park, Joon Sung; O\\'Brien, Joseph C.; Cai, Carrie J.; Ringel Morris, Meredith; Liang, Percy; Bernstein, Michael S. (2023-04-01). \\\\\"Generative Agents: Interactive Simulacra of Human Behavior\\\\\". arXiv:2304.03442 [cs.HC].\\\\n^ Mann, Tobias. \\\\\"How to run an LLM locally on your PC in less than 10 minutes\\\\\". www.theregister.com. Retrieved 2024-05-17.\\\\n^ Nagel, Markus; Amjad, Rana Ali; Baalen, Mart Van; Louizos, Christos; Blankevoort, Tijmen (2020-11-21). \\\\\"Up or Down? Adaptive Rounding for Post-Training Quantization\\\\\". Proceedings of the 37th International Conference on Machine Learning. PMLR: 7197–7206. Archived from the original on 2023-06-14. Retrieved 2023-06-14.\\\\n^ Polino, Antonio; Pascanu, Razvan; Alistarh, Dan (2018-02-01). \\\\\"Model compression via distillation and quantization\\\\\". arXiv:1802.05668 [cs.NE].\\\\n^ Frantar, Elias; Ashkboos, Saleh; Hoefler, Torsten; Alistarh, Dan (2022-10-01). \\\\\"GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers\\\\\". arXiv:2210.17323 [cs.LG].\\\\n^ Dettmers, Tim; Svirschevski, Ruslan; Egiazarian, Vage; Kuznedelev, Denis; Frantar, Elias; Ashkboos, Saleh; Borzunov, Alexander; Hoefler, Torsten; Alistarh, Dan (2023-06-01). \\\\\"SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression\\\\\". arXiv:2306.03078 [cs.CL].\\\\n^ Grootendorst, Maarten. \\\\\"A Visual Guide to Quantization\\\\\". newsletter.maartengrootendorst.com. Archived from the original on 31 Jul 2024. Retrieved 2024-07-31.\\\\n^ Dettmers, Tim; Pagnoni, Artidoro; Holtzman, Ari; Zettlemoyer, Luke (2023-05-01). \\\\\"QLoRA: Efficient Finetuning of Quantized LLMs\\\\\". arXiv:2305.14314 [cs.LG].\\\\n^ Kiros, Ryan; Salakhutdinov, Ruslan; Zemel, Rich (2014-06-18). \\\\\"Multimodal Neural Language Models\\\\\". Proceedings of the 31st International Conference on Machine Learning. PMLR: 595–603. Archived from the original on 2023-07-02. Retrieved 2023-07-02.\\\\n^ Krizhevsky, Alex; Sutskever, Ilya; Hinton, Geoffrey E (2012). \\\\\"ImageNet Classification with Deep Convolutional Neural Networks\\\\\". Advances in Neural Information Processing Systems. 25. Curran Associates, Inc. Archived from the original on 2023-07-02. Retrieved 2023-07-02.\\\\n^ Antol, Stanislaw; Agrawal, Aishwarya; Lu, Jiasen; Mitchell, Margaret; Batra, Dhruv; Zitnick, C. Lawrence; Parikh, Devi (2015). \\\\\"VQA: Visual Question Answering\\\\\". ICCV: 2425–2433. Archived from the original on 2023-07-02. Retrieved 2023-07-02.\\\\n^ Li, Junnan; Li, Dongxu; Savarese, Silvio; Hoi, Steven (2023-01-01). \\\\\"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\\\\\". arXiv:2301.12597 [cs.CV].\\\\n^ Alayrac, Jean-Baptiste; Donahue, Jeff; Luc, Pauline; Miech, Antoine; Barr, Iain; Hasson, Yana; Lenc, Karel; Mensch, Arthur; Millican, Katherine; Reynolds, Malcolm; Ring, Roman; Rutherford, Eliza; Cabi, Serkan; Han, Tengda; Gong, Zhitao (2022-12-06). \\\\\"Flamingo: a Visual Language Model for Few-Shot Learning\\\\\". Advances in Neural Information Processing Systems. 35: 23716–23736. arXiv:2204.14198. Archived from the original on 2023-07-02. Retrieved 2023-07-02.\\\\n^ Driess, Danny; Xia, Fei; Sajjadi, Mehdi S. M.; Lynch, Corey; Chowdhery, Aakanksha; Ichter, Brian; Wahid, Ayzaan; Tompson, Jonathan; Vuong, Quan; Yu, Tianhe; Huang, Wenlong; Chebotar, Yevgen; Sermanet, Pierre; Duckworth, Daniel; Levine, Sergey (2023-03-01). \\\\\"PaLM-E: An Embodied Multimodal Language Model\\\\\". arXiv:2303.03378 [cs.LG].\\\\n^ Liu, Haotian; Li, Chunyuan; Wu, Qingyang; Lee, Yong Jae (2023-04-01). \\\\\"Visual Instruction Tuning\\\\\". arXiv:2304.08485 [cs.CV].\\\\n^ Zhang, Hang; Li, Xin; Bing, Lidong (2023-06-01). \\\\\"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding\\\\\". arXiv:2306.02858 [cs.CL].\\\\n^ OpenAI (2023-03-27). \\\\\"GPT-4 Technical Report\\\\\". arXiv:2303.08774 [cs.CL].\\\\n^ OpenAI (September 25, 2023). \\\\\"GPT-4V(ision) System Card\\\\\" (PDF).\\\\n^ Pichai, Sundar (10 May 2023), Google Keynote (Google I/O \\'23), timestamp 15:31, retrieved 2023-07-02\\\\n^ Wiggers, Kyle (11 September 2024). \\\\\"Mistral releases Pixtral 12B, its first multimodal model\\\\\". TechCrunch. Retrieved 14 September 2024.\\\\n^ a b \\\\\"Introducing OpenAI o1-preview\\\\\". OpenAI. 2024-09-12. Retrieved 2025-02-03.\\\\n^ a b Metz, Cade (2024-12-20). \\\\\"OpenAI Unveils New A.I. That Can \\'Reason\\' Through Math and Science Problems\\\\\". The New York Times. Retrieved 2025-02-03.\\\\n^ Gibney, Elizabeth (2025-01-30). \\\\\"China\\'s cheap, open AI model DeepSeek thrills scientists\\\\\". Nature. Retrieved 2025-02-03.\\\\n^ Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; Buchatskaya, Elena; Cai, Trevor; Rutherford, Eliza; Casas, Diego de Las; Hendricks, Lisa Anne; Welbl, Johannes; Clark, Aidan; Hennigan, Tom; Noland, Eric; Millican, Katie; Driessche, George van den; Damoc, Bogdan (2022-03-29). \\\\\"Training Compute-Optimal Large Language Models\\\\\". arXiv:2203.15556 [cs.CL].\\\\n^ a b Caballero, Ethan; Gupta, Kshitij; Rish, Irina; Krueger, David (2022). \\\\\"Broken Neural Scaling Laws\\\\\". arXiv:2210.14891 [cs.LG].\\\\n^ \\\\\"137 emergent abilities of large language models\\\\\". Jason Wei. Retrieved 2023-06-24.\\\\n^ Bowman, Samuel R. (2023). \\\\\"Eight Things to Know about Large Language Models\\\\\". arXiv:2304.00612 [cs.CL].\\\\n^ Mukherjee, Anirban; Chang, Hannah (2024). \\\\\"Heuristic Reasoning in AI: Instrumental Use and Mimetic Absorption\\\\\". arXiv:2403.09404 [cs.AI].\\\\n^ Hahn, Michael; Goyal, Navin (2023-03-14). \\\\\"A Theory of Emergent In-Context Learning as Implicit Structure Induction\\\\\". arXiv:2303.07971 [cs.LG].\\\\n^ Pilehvar, Mohammad Taher; Camacho-Collados, Jose (June 2019). \\\\\"Proceedings of the 2019 Conference of the North\\\\\". Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Minneapolis, Minnesota: Association for Computational Linguistics: 1267–1273. doi:10.18653/v1/N19-1128. S2CID\\xa0102353817. Archived from the original on 2023-06-27. Retrieved 2023-06-27.\\\\n^ \\\\\"WiC: The Word-in-Context Dataset\\\\\". pilehvar.github.io. Archived from the original on 2023-06-27. Retrieved 2023-06-27.\\\\n^ Patel, Roma; Pavlick, Ellie (2021-10-06). \\\\\"Mapping Language Models to Grounded Conceptual Spaces\\\\\". ICLR. Archived from the original on 2023-06-24. Retrieved 2023-06-27.\\\\n^ A Closer Look at Large Language Models Emergent Abilities Archived 2023-06-24 at the Wayback Machine (Yao Fu, Nov 20, 2022)\\\\n^ Ornes, Stephen (March 16, 2023). \\\\\"The Unpredictable Abilities Emerging From Large AI Models\\\\\". Quanta Magazine. Archived from the original on March 16, 2023. Retrieved March 16, 2023.\\\\n^ Schaeffer, Rylan; Miranda, Brando; Koyejo, Sanmi (2023-04-01). \\\\\"Are Emergent Abilities of Large Language Models a Mirage?\\\\\". arXiv:2304.15004 [cs.AI].\\\\n^ Li, Kenneth; Hopkins, Aspen K.; Bau, David; Viégas, Fernanda; Pfister, Hanspeter; Wattenberg, Martin (2022-10-01). \\\\\"Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task\\\\\". arXiv:2210.13382 [cs.LG].\\\\n^ \\\\\"Large Language Model: world models or surface statistics?\\\\\". The Gradient. 2023-01-21. Retrieved 2023-06-12.\\\\n^ Jin, Charles; Rinard, Martin (2023-05-01). \\\\\"Evidence of Meaning in Language Models Trained on Programs\\\\\". arXiv:2305.11169 [cs.LG].\\\\n^ Nanda, Neel; Chan, Lawrence; Lieberum, Tom; Smith, Jess; Steinhardt, Jacob (2023-01-01). \\\\\"Progress measures for grokking via mechanistic interpretability\\\\\". arXiv:2301.05217 [cs.LG].\\\\n^ a b c d e Mitchell, Melanie; Krakauer, David C. (28 March 2023). \\\\\"The debate over understanding in AI\\'s large language models\\\\\". Proceedings of the National Academy of Sciences. 120 (13): e2215907120. arXiv:2210.13966. Bibcode:2023PNAS..12015907M. doi:10.1073/pnas.2215907120. PMC\\xa010068812. PMID\\xa036943882.\\\\n^ Metz, Cade (16 May 2023). \\\\\"Microsoft Says New A.I. Shows Signs of Human Reasoning\\\\\". The New York Times.\\\\n^ a b Bubeck, Sébastien; Chandrasekaran, Varun; Eldan, Ronen; Gehrke, Johannes; Horvitz, Eric; Kamar, Ece; Lee, Peter; Lee, Yin Tat; Li, Yuanzhi; Lundberg, Scott; Nori, Harsha; Palangi, Hamid; Ribeiro, Marco Tulio; Zhang, Yi (2023). \\\\\"Sparks of Artificial General Intelligence: Early experiments with GPT-4\\\\\". arXiv:2303.12712 [cs.CL].\\\\n^ \\\\\"Anthropic CEO Dario Amodei pens a smart look at our AI future\\\\\". Fast Company. October 17, 2024.\\\\n^ \\\\\"ChatGPT is more like an \\'alien intelligence\\' than a human brain, says futurist\\\\\". ZDNET. 2023. Archived from the original on 12 June 2023. Retrieved 12 June 2023.\\\\n^ a b Newport, Cal (13 April 2023). \\\\\"What Kind of Mind Does ChatGPT Have?\\\\\". The New Yorker. Archived from the original on 12 June 2023. Retrieved 12 June 2023.\\\\n^ Roose, Kevin (30 May 2023). \\\\\"Why an Octopus-like Creature Has Come to Symbolize the State of A.I.\\\\\" The New York Times. Archived from the original on 30 May 2023. Retrieved 12 June 2023.\\\\n^ \\\\\"The A to Z of Artificial Intelligence\\\\\". Time Magazine. 13 April 2023. Archived from the original on 16 June 2023. Retrieved 12 June 2023.\\\\n^ Ji, Ziwei; Lee, Nayeon; Frieske, Rita; Yu, Tiezheng; Su, Dan; Xu, Yan; Ishii, Etsuko; Bang, Yejin; Dai, Wenliang; Madotto, Andrea; Fung, Pascale (November 2022). \\\\\"Survey of Hallucination in Natural Language Generation\\\\\" (pdf). ACM Computing Surveys. 55 (12). Association for Computing Machinery: 1–38. arXiv:2202.03629. doi:10.1145/3571730. S2CID\\xa0246652372. Archived from the original on 26 March 2023. Retrieved 15 January 2023.\\\\n^ Varshney, Neeraj; Yao, Wenlin; Zhang, Hongming; Chen, Jianshu; Yu, Dong (2023). \\\\\"A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation\\\\\". arXiv:2307.03987 [cs.CL].\\\\n^ Lakoff, George (1999). Philosophy in the Flesh: The Embodied Mind and Its Challenge to Western Philosophy; Appendix: The Neural Theory of Language Paradigm. New York Basic Books. pp.\\xa0569–583. ISBN\\xa0978-0-465-05674-3.\\\\n^ Evans, Vyvyan. (2014). The Language Myth. Cambridge University Press. ISBN\\xa0978-1-107-04396-1.\\\\n^ Friston, Karl J. (2022). Active Inference: The Free Energy Principle in Mind, Brain, and Behavior; Chapter 4 The Generative Models of Active Inference. The MIT Press. ISBN\\xa0978-0-262-36997-8.\\\\n^ a b Huyen, Chip (October 18, 2019). \\\\\"Evaluation Metrics for Language Modeling\\\\\". The Gradient. Retrieved January 14, 2024.\\\\n^ a b Clark, Christopher; Lee, Kenton; Chang, Ming-Wei; Kwiatkowski, Tom; Collins, Michael; Toutanova, Kristina (2019). \\\\\"BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions\\\\\". arXiv:1905.10044 [cs.CL].\\\\n^ a b c Wayne Xin Zhao; Zhou, Kun; Li, Junyi; Tang, Tianyi; Wang, Xiaolei; Hou, Yupeng; Min, Yingqian; Zhang, Beichen; Zhang, Junjie; Dong, Zican; Du, Yifan; Yang, Chen; Chen, Yushuo; Chen, Zhipeng; Jiang, Jinhao; Ren, Ruiyang; Li, Yifan; Tang, Xinyu; Liu, Zikang; Liu, Peiyu; Nie, Jian-Yun; Wen, Ji-Rong (2023). \\\\\"A Survey of Large Language Models\\\\\". arXiv:2303.18223 [cs.CL].\\\\n^ openai/simple-evals, OpenAI, 2024-05-28, retrieved 2024-05-28\\\\n^ openai/evals, OpenAI, 2024-05-28, archived from the original on 2024-05-08, retrieved 2024-05-28\\\\n^ \\\\\"Sanitized open-source datasets for natural language and code understanding: how we evaluated our 70B model\\\\\". imbue.com. Archived from the original on 2024-07-26. Retrieved 2024-07-24.\\\\n^ Srivastava, Aarohi; et\\xa0al. (2022). \\\\\"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models\\\\\". arXiv:2206.04615 [cs.CL].\\\\n^ Lin, Stephanie; Hilton, Jacob; Evans, Owain (2021). \\\\\"TruthfulQA: Measuring How Models Mimic Human Falsehoods\\\\\". arXiv:2109.07958 [cs.CL].\\\\n^ a b Zellers, Rowan; Holtzman, Ari; Bisk, Yonatan; Farhadi, Ali; Choi, Yejin (2019). \\\\\"HellaSwag: Can a Machine Really Finish Your Sentence?\\\\\". arXiv:1905.07830 [cs.CL].\\\\n^ \\\\\"Prepare for truly useful large language models\\\\\". Nature Biomedical Engineering. 7 (2): 85–86. 7 March 2023. doi:10.1038/s41551-023-01012-6. PMID\\xa036882584. S2CID\\xa0257403466.\\\\n^ \\\\\"Your job is (probably) safe from artificial intelligence\\\\\". The Economist. 7 May 2023. Archived from the original on 17 June 2023. Retrieved 18 June 2023.\\\\n^ \\\\\"Generative AI Could Raise Global GDP by 7%\\\\\". Goldman Sachs. Archived from the original on 18 June 2023. Retrieved 18 June 2023.\\\\n^ Peng, Zhencan; Wang, Zhizhi; Deng, Dong (13 June 2023). \\\\\"Near-Duplicate Sequence Search at Scale for Large Language Model Memorization Evaluation\\\\\" (PDF). Proceedings of the ACM on Management of Data. 1 (2): 1–18. doi:10.1145/3589324. S2CID\\xa0259213212. Archived (PDF) from the original on 2024-08-27. Retrieved 2024-01-20. Citing Lee et al 2022.\\\\n^ Peng, Wang & Deng 2023, p.\\xa08.\\\\n^ Stephen Council (1 Dec 2023). \\\\\"How Googlers cracked an SF rival\\'s tech model with a single word\\\\\". SFGATE. Archived from the original on 16 December 2023.\\\\n^ Alba, Davey (1 May 2023). \\\\\"AI chatbots have been used to create dozens of news content farms\\\\\". The Japan Times. Retrieved 18 June 2023.\\\\n^ \\\\\"Could chatbots help devise the next pandemic virus?\\\\\". Science. 14 June 2023. doi:10.1126/science.adj2463. Archived from the original on 18 June 2023. Retrieved 18 June 2023.\\\\n^ Hubinger, Evan (10 January 2024). \\\\\"Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training\\\\\". arXiv:2401.05566 [cs.CR].\\\\n^ Kang, Daniel (2023). \\\\\"Exploiting programmatic behavior of LLMs: Dual-use through standard security attacks\\\\\". arXiv:2302.05733 [cs.CR].\\\\n^ Wang, Yongge (20 June 2024). \\\\\"Encryption Based Covert Channel for Large Language Models\\\\\" (PDF). IACR ePrint 2024/586. Archived (PDF) from the original on 24 June 2024. Retrieved 24 June 2024.\\\\n^ a b Stokel-Walker, Chris (November 22, 2023). \\\\\"ChatGPT Replicates Gender Bias in Recommendation Letters\\\\\". Scientific American. Archived from the original on 2023-12-29. Retrieved 2023-12-29.\\\\n^ Luo, Queenie; Puett, Michael J.; Smith, Michael D. (2023-03-28). \\\\\"A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, Wikipedia, and YouTube\\\\\". arXiv:2303.16281v2 [cs.CY].\\\\n^ Cheng, Myra; Durmus, Esin; Jurafsky, Dan (2023-05-29), Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models, arXiv:2305.18189\\\\n^ Kotek, Hadas; Dockum, Rikker; Sun, David (2023-11-05). \\\\\"Gender bias and stereotypes in Large Language Models\\\\\". Proceedings of the ACM Collective Intelligence Conference. CI \\'23. New York, NY, USA: Association for Computing Machinery. pp.\\xa012–24. doi:10.1145/3582269.3615599. ISBN\\xa0979-8-4007-0113-9.\\\\n^ Heikkilä, Melissa (August 7, 2023). \\\\\"AI language models are rife with different political biases\\\\\". MIT Technology Review. Retrieved 2023-12-29.\\\\n^ Mehta, Sourabh (2024-07-03). \\\\\"How Much Energy Do LLMs Consume? Unveiling the Power Behind AI\\\\\". Association of Data Scientists. Retrieved 2025-01-27.\\\\n^ \\\\\"Artificial Intelligence wants to go nuclear. Will it work?\\\\\". NPR. Retrieved 2025-01-27.\\\\n^ Roy, Dareen (December 19, 2024). \\\\\"AI\\'s energy hunger fuels geothermal startups but natgas rivalry clouds future\\\\\". Reuters.\\\\nFurther reading[edit]\\\\nJurafsky, Dan, Martin, James. H. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition, 3rd Edition draft, 2023.\\\\nZhao, Wayne Xin; et\\xa0al. (2023). \\\\\"A Survey of Large Language Models\\\\\". arXiv:2303.18223 [cs.CL].\\\\nKaddour, Jean; et\\xa0al. (2023). \\\\\"Challenges and Applications of Large Language Models\\\\\". arXiv:2307.10169 [cs.CL].\\\\nYin, Shukang; Fu, Chaoyou; Zhao, Sirui; Li, Ke; Sun, Xing; Xu, Tong; Chen, Enhong (2024). \\\\\"A Survey on Multimodal Large Language Models\\\\\". National Science Review. 11 (12): nwae403. arXiv:2306.13549. doi:10.1093/nsr/nwae403. PMC\\xa011645129. PMID\\xa039679213.\\\\n\\\\\"AI Index Report 2024 – Artificial Intelligence Index\\\\\". aiindex.stanford.edu. Retrieved 2024-05-05.\\\\nFrank, Michael C. (27 June 2023). \\\\\"Baby steps in evaluating the capacities of large language models\\\\\". Nature Reviews Psychology. 2 (8): 451–452. doi:10.1038/s44159-023-00211-x. ISSN\\xa02731-0574. S2CID\\xa0259713140. Retrieved 2 July 2023.\\\\nvte\\\\nNatural language processing\\\\nGeneral terms \\\\nAI-completeBag-of-wordsn-gram BigramTrigramComputational linguisticsNatural language understandingStop wordsText processing\\\\nText analysis \\\\nArgument miningCollocation extractionConcept miningCoreference resolutionDeep linguistic processingDistant readingInformation extractionNamed-entity recognitionOntology learningParsing Semantic parsingSyntactic parsingPart-of-speech taggingSemantic analysisSemantic role labelingSemantic decompositionSemantic similaritySentiment analysis\\\\nTerminology extractionText miningTextual entailmentTruecasingWord-sense disambiguationWord-sense induction\\\\nText segmentation \\\\nCompound-term processingLemmatisationLexical analysisText chunkingStemmingSentence segmentationWord segmentation\\\\nAutomatic summarization \\\\nMulti-document summarizationSentence extractionText simplification\\\\nMachine translation \\\\nComputer-assistedExample-basedRule-basedStatisticalTransfer-basedNeural\\\\nDistributional semantics models \\\\nBERTDocument-term matrixExplicit semantic analysisfastTextGloVeLanguage model (large)Latent semantic analysisSeq2seqWord embeddingWord2vec\\\\nLanguage resources,\\\\ndatasets and corpora  \\\\nTypes and\\\\nstandards \\\\nCorpus linguisticsLexical resourceLinguistic Linked Open DataMachine-readable dictionaryParallel textPropBankSemantic networkSimple Knowledge Organization SystemSpeech corpusText corpusThesaurus (information retrieval)TreebankUniversal Dependencies\\\\nData  \\\\nBabelNetBank of EnglishDBpediaFrameNetGoogle Ngram ViewerUBYWordNetWikidata\\\\nAutomatic identification\\\\nand data capture  \\\\nSpeech recognitionSpeech segmentationSpeech synthesisNatural language generationOptical character recognition\\\\nTopic model \\\\nDocument classificationLatent Dirichlet allocationPachinko allocation\\\\nComputer-assisted\\\\nreviewing \\\\nAutomated essay scoringConcordancerGrammar checkerPredictive textPronunciation assessmentSpell checker\\\\nNatural language\\\\nuser interface\\\\nChatbotInteractive fiction (c.f. Syntax guessing)Question answeringVirtual assistantVoice user interface\\\\nRelated \\\\nFormal semanticsHallucinationNatural Language ToolkitspaCy\\\\nvte\\\\nArtificial intelligence (AI)\\\\nHistory (timeline)\\\\nConcepts  \\\\nParameter HyperparameterLoss functionsRegression Bias–variance tradeoffDouble descentOverfittingClusteringGradient descent SGDQuasi-Newton methodConjugate gradient methodBackpropagationAttentionConvolutionNormalization BatchnormActivation SoftmaxSigmoidRectifierGatingWeight initializationRegularizationDatasets AugmentationPrompt engineeringReinforcement learning Q-learningSARSAImitationPolicy gradientDiffusionLatent diffusion modelAutoregressionAdversaryRAGUncanny valleyRLHFSelf-supervised learningRecursive self-improvementWord embeddingHallucination\\\\nApplications  \\\\nMachine learning In-context learningArtificial neural network Deep learningLanguage model Large language modelNMTArtificial general intelligence\\\\nImplementations \\\\nAudio–visual  \\\\nAlexNetWaveNetHuman image synthesisHWROCRSpeech synthesis 15.aiElevenLabsSpeech recognition WhisperFacial recognitionAlphaFoldText-to-image models AuroraDALL-EFireflyFluxIdeogramImagenMidjourneyStable DiffusionText-to-video models Dream MachineGen-3 AlphaHailuo AIKlingSoraVeoMusic generation Suno AIUdio\\\\nText  \\\\nWord2vecSeq2seqGloVeBERTT5LlamaChinchilla AIPaLMGPT 123JChatGPT44oo1o3ClaudeGemini chatbotGrokLaMDABLOOMProject DebaterIBM WatsonIBM WatsonxGranitePanGu-ΣDeepSeekQwen\\\\nDecisional\\\\nAlphaGoAlphaZeroOpenAI FiveSelf-driving carMuZeroAction selection AutoGPTRobot control\\\\nPeople\\\\nAlan TuringWarren Sturgis McCullochWalter PittsJohn von NeumannClaude ShannonMarvin MinskyJohn McCarthyNathaniel RochesterAllen NewellCliff ShawHerbert A. SimonOliver SelfridgeFrank RosenblattBernard WidrowJoseph WeizenbaumSeymour PapertSeppo LinnainmaaPaul WerbosJürgen SchmidhuberYann LeCunGeoffrey HintonJohn HopfieldYoshua BengioLotfi A. ZadehStephen GrossbergAlex GravesAndrew NgFei-Fei LiAlex KrizhevskyIlya SutskeverDemis HassabisDavid SilverIan GoodfellowAndrej Karpathy\\\\nArchitectures \\\\nNeural Turing machineDifferentiable neural computerTransformer Vision transformer (ViT)Recurrent neural network (RNN)Long short-term memory (LSTM)Gated recurrent unit (GRU)Echo state networkMultilayer perceptron (MLP)Convolutional neural network (CNN)Residual neural network (RNN)Highway networkMambaAutoencoderVariational autoencoder (VAE)Generative adversarial network (GAN)Graph neural network (GNN)\\\\nPortals Technology Category Artificial neural networksMachine learning List CompaniesProjects\\\\nCategories: Large language modelsDeep learningNatural language processing\\\\nThis page was last edited on 7 February 2025, at 09:16\\xa0(UTC).\\\\nText is available under the Creative Commons Attribution-ShareAlike 4.0 License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\\\\nPrivacy policy\\\\nAbout Wikipedia\\\\nDisclaimers\\\\nContact Wikipedia\\\\nCode of Conduct\\\\nDevelopers\\\\nStatistics\\\\nCookie statement\\\\nMobile view\"}], \"response_time\": 1.4}', name='search_web', id='4a645730-dde6-47e3-b6ae-0b7c2d475acb', tool_call_id='04859d0a-76ba-49db-9670-5c6fc0cf187a'),\n",
              "  AIMessage(content=\"Based on my web search, here are some of the latest LLMs that have been released:\\n\\n*   **DeepSeek R1:** Released in January 2025 by DeepSeek, this open-source model excels in math and coding.\\n*   **GPT-4o:** OpenAI's latest model, released in May 2024, offers multimodal capabilities (text, image, video, voice) and improved speed and cost-efficiency compared to GPT-4.\\n*   **Claude 3.5 Sonnet:** Anthropic's newest model, released in June 2024, is a strong competitor to GPT-4o, with improvements in coding, text reasoning, and vision tasks.\\n*   **Llama 3.1:** Meta AI's updated Llama 3 model, released in June 2024, features 405 billion parameters and an expanded context length.\\n*   **Nemotron-4:** Nvidia's language model, released in June 2024, is designed for synthetic data generation and AI model training.\\n*   **Gemma 2:** Google DeepMind's open-source language model, released in June 2024, is available in various sizes and can be run locally.\\n\\nKeep in mind that the field of LLMs is constantly evolving, so new models are being released frequently.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-068a635e-2c62-4a1e-a160-882d2aa995e0-0', usage_metadata={'input_tokens': 62524, 'output_tokens': 289, 'total_tokens': 62813})]}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response['messages'][-1].content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "oEdjrx5NQhi7",
        "outputId": "cdb20e4a-6fc1-47af-ccb5-ed37a3c00a02"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on my web search, here are some of the latest LLMs that have been released:\n\n*   **DeepSeek R1:** Released in January 2025 by DeepSeek, this open-source model excels in math and coding.\n*   **GPT-4o:** OpenAI's latest model, released in May 2024, offers multimodal capabilities (text, image, video, voice) and improved speed and cost-efficiency compared to GPT-4.\n*   **Claude 3.5 Sonnet:** Anthropic's newest model, released in June 2024, is a strong competitor to GPT-4o, with improvements in coding, text reasoning, and vision tasks.\n*   **Llama 3.1:** Meta AI's updated Llama 3 model, released in June 2024, features 405 billion parameters and an expanded context length.\n*   **Nemotron-4:** Nvidia's language model, released in June 2024, is designed for synthetic data generation and AI model training.\n*   **Gemma 2:** Google DeepMind's open-source language model, released in June 2024, is available in various sizes and can be run locally.\n\nKeep in mind that the field of LLMs is constantly evolving, so new models are being released frequently."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"What are its key features\"\"\"\n",
        "response = agent.invoke({\"messages\": (\"user\", prompt)})\n",
        "display(Markdown(response['messages'][-1].content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "id": "d31YzMp0QePA",
        "outputId": "a3db0906-8c4e-4a35-96cc-109b592ff7e3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Could you please specify what you are asking about? I can search the web for information about a topic, or I can get the current weather for a location.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def call_agent(agent, prompt):\n",
        "    events = agent.stream(\n",
        "        {\"messages\": [{\"role\": \"user\", \"content\": prompt}]},\n",
        "        {\"configurable\": {\"thread_id\": \"any\"}},\n",
        "        stream_mode=\"values\",\n",
        "    )\n",
        "\n",
        "    for event in events:\n",
        "        event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "    print()\n",
        "    print('Final Response:\\n')\n",
        "    display(Markdown(event[\"messages\"][-1].content))"
      ],
      "metadata": {
        "id": "cuDtE-G76JNa"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"What is the weather in bangalore?\n",
        "show detailed statistics\"\"\"\n",
        "call_agent(agent, prompt)"
      ],
      "metadata": {
        "id": "2UgxjeIrwN1h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "outputId": "cdaf5c69-baa0-4f5b-b5ad-58e20300161c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is the weather in bangalore?\n",
            "show detailed statistics\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  get_weather (a1f8b04b-a751-4d4c-b4d6-63d526b1cddf)\n",
            " Call ID: a1f8b04b-a751-4d4c-b4d6-63d526b1cddf\n",
            "  Args:\n",
            "    query: bangalore\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: get_weather\n",
            "\n",
            "{\"location\": {\"name\": \"Bangalore\", \"region\": \"Karnataka\", \"country\": \"India\", \"lat\": 12.9833, \"lon\": 77.5833, \"tz_id\": \"Asia/Kolkata\", \"localtime_epoch\": 1740131141, \"localtime\": \"2025-02-21 15:15\"}, \"current\": {\"last_updated_epoch\": 1740130200, \"last_updated\": \"2025-02-21 15:00\", \"temp_c\": 32.3, \"temp_f\": 90.1, \"is_day\": 1, \"condition\": {\"text\": \"Sunny\", \"icon\": \"//cdn.weatherapi.com/weather/64x64/day/113.png\", \"code\": 1000}, \"wind_mph\": 10.7, \"wind_kph\": 17.3, \"wind_degree\": 88, \"wind_dir\": \"E\", \"pressure_mb\": 1017.0, \"pressure_in\": 30.03, \"precip_mm\": 0.0, \"precip_in\": 0.0, \"humidity\": 28, \"cloud\": 0, \"feelslike_c\": 30.1, \"feelslike_f\": 86.2, \"windchill_c\": 31.7, \"windchill_f\": 89.0, \"heatindex_c\": 29.5, \"heatindex_f\": 85.0, \"dewpoint_c\": 5.2, \"dewpoint_f\": 41.4, \"vis_km\": 10.0, \"vis_miles\": 6.0, \"uv\": 3.8, \"gust_mph\": 12.3, \"gust_kph\": 19.9}}\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The weather in Bangalore, India is sunny with a temperature of 32.3 degrees Celsius (90.1 degrees Fahrenheit). The humidity is 28% and the wind is blowing from the East at 17.3 km/h (10.7 mph). The feels like temperature is 30.1 degrees Celsius (86.2 degrees Fahrenheit). The UV index is 3.8.\n",
            "\n",
            "Final Response:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The weather in Bangalore, India is sunny with a temperature of 32.3 degrees Celsius (90.1 degrees Fahrenheit). The humidity is 28% and the wind is blowing from the East at 17.3 km/h (10.7 mph). The feels like temperature is 30.1 degrees Celsius (86.2 degrees Fahrenheit). The UV index is 3.8."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"What is the weather in hyderabad?\n",
        "show detailed statistics\"\"\"\n",
        "call_agent(agent, prompt)"
      ],
      "metadata": {
        "id": "hLqZiU6exnN5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "outputId": "eb64bf52-623e-4c60-d7ab-53cdf7387b10"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is the weather in hyderabad?\n",
            "show detailed statistics\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  get_weather (eb0ccc3a-7708-4b50-b315-8ad89ced55ca)\n",
            " Call ID: eb0ccc3a-7708-4b50-b315-8ad89ced55ca\n",
            "  Args:\n",
            "    query: hyderabad\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: get_weather\n",
            "\n",
            "{\"location\": {\"name\": \"Hyderabad\", \"region\": \"Telangana\", \"country\": \"India\", \"lat\": 17.3753, \"lon\": 78.4744, \"tz_id\": \"Asia/Kolkata\", \"localtime_epoch\": 1740131353, \"localtime\": \"2025-02-21 15:19\"}, \"current\": {\"last_updated_epoch\": 1740131100, \"last_updated\": \"2025-02-21 15:15\", \"temp_c\": 33.1, \"temp_f\": 91.6, \"is_day\": 1, \"condition\": {\"text\": \"Partly cloudy\", \"icon\": \"//cdn.weatherapi.com/weather/64x64/day/116.png\", \"code\": 1003}, \"wind_mph\": 7.6, \"wind_kph\": 12.2, \"wind_degree\": 348, \"wind_dir\": \"NNW\", \"pressure_mb\": 1016.0, \"pressure_in\": 30.0, \"precip_mm\": 0.0, \"precip_in\": 0.0, \"humidity\": 30, \"cloud\": 50, \"feelslike_c\": 31.0, \"feelslike_f\": 87.7, \"windchill_c\": 33.8, \"windchill_f\": 92.9, \"heatindex_c\": 31.8, \"heatindex_f\": 89.2, \"dewpoint_c\": 5.9, \"dewpoint_f\": 42.6, \"vis_km\": 6.0, \"vis_miles\": 3.0, \"uv\": 3.3, \"gust_mph\": 8.7, \"gust_kph\": 14.1}}\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "OK. The weather in Hyderabad, India is partly cloudy. The temperature is 33.1 degrees Celsius, which feels like 31 degrees Celsius. The humidity is 30%. The wind is coming from the NNW at 12.2 kph. The UV index is 3.3.\n",
            "\n",
            "Final Response:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "OK. The weather in Hyderabad, India is partly cloudy. The temperature is 33.1 degrees Celsius, which feels like 31 degrees Celsius. The humidity is 30%. The wind is coming from the NNW at 12.2 kph. The UV index is 3.3."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Which city is hotter among the above?\"\"\"\n",
        "call_agent(agent, prompt)"
      ],
      "metadata": {
        "id": "2JRHxzdPxo8A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "outputId": "f05f0ef0-31a1-46ba-f9cb-e64b3f128686"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Which city is hotter among the above?\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Could you please provide the cities you're asking about?\n",
            "\n",
            "Final Response:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Could you please provide the cities you're asking about?\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have successfully built an AI Agent which can search the web, get weather for us but it is not yet conversational. We will add in the that functionality in Part III next."
      ],
      "metadata": {
        "id": "dtXAlOdBxt-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a multi-user conversational ReAct Agent with LangGraph\n",
        "\n",
        "Now, we will build a multi-user conversational ReAct agent with LangGraph which will use the web search or weather tool based on our input prompts to get relevant data which the LLM might not know by default and give relevant responses\n",
        "\n",
        "Our agentic chatbot from Part II can use tools to answer user questions, but it doesn't remember the context of previous interactions. This limits its ability to have coherent, multi-turn conversations.\n",
        "\n",
        "LangGraph solves this problem through **persistent checkpointing**. If you provide a `checkpointer` when compiling the graph and a `thread_id` when calling your graph, LangGraph automatically saves the state after each step. When you invoke the graph again using the same `thread_id`, the graph loads its saved state, allowing the chatbot to pick up where it left off.\n",
        "\n",
        "**checkpointing** is _much_ more powerful than simple chat memory - it lets you save and resume complex state at any time for error recovery, human-in-the-loop workflows, time travel interactions, and more\n",
        "\n",
        "While the legacy syntax uses `session_id`, in LangGraph, each user session is identified by `thread_id`\n",
        "\n",
        "We will use `SqliteSaver` which helps to store separate conversation histories per user or session.\n",
        "\n",
        "This will help us build a conversational Agentic Chatbot which will be accessed by many users at the same time.\n",
        "\n"
      ],
      "metadata": {
        "id": "rcfps7EJy570"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# removes the memory database file - usually not needed\n",
        "# you can run this only when you want to remove ALL conversation histories\n",
        "# ok if you get rm: cannot remove 'memory.db': No such file or directory  because initially no memory exists\n",
        "!rm memory.db*"
      ],
      "metadata": {
        "id": "Cph9LvyvIRrA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77536236-7585-45ae-f0a2-f92d7d4608a8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'memory.db*': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "\n",
        "def chat_with_agent(agent_graph, prompt, session_id, verbose=False):\n",
        "    with SqliteSaver.from_conn_string(\"memory.db\") as memory:\n",
        "        agent = agent_graph.compile(checkpointer=memory)\n",
        "        events = agent.stream(\n",
        "            {\"messages\": [{\"role\": \"user\", \"content\": prompt}]},\n",
        "            {\"configurable\": {\"thread_id\": session_id}},\n",
        "            stream_mode=\"values\",\n",
        "        )\n",
        "\n",
        "        print('Running Agent, please wait...')\n",
        "        for event in events:\n",
        "            if verbose:\n",
        "                event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "    display(Markdown(event[\"messages\"][-1].content))"
      ],
      "metadata": {
        "id": "qzwnf7vfzTiI"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_builder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7_S8X7r78Ei",
        "outputId": "c737130d-479e-414b-a03c-957adf8683b9"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x79beac40db50>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now simulate User 1 using the agent"
      ],
      "metadata": {
        "id": "vYTNL_iJ6ibC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = 'jack001'\n",
        "prompt = \"Tell me about recent LLMs released\"\n",
        "chat_with_agent(graph_builder, prompt, user_id, verbose=True)"
      ],
      "metadata": {
        "id": "irMU68Ds0NTm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "outputId": "d2507694-6465-4e3b-f093-a72de48d7b10"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Agent, please wait...\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Tell me about recent LLMs released\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  search_web (41a27856-66a3-4442-8a20-63fac168f2ed)\n",
            " Call ID: 41a27856-66a3-4442-8a20-63fac168f2ed\n",
            "  Args:\n",
            "    query: recent large language models released\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: search_web\n",
            "\n",
            "{\"query\": \"recent large language models released\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://explodingtopics.com/blog/list-of-llms\", \"title\": \"Best 22 Large Language Models (LLMs) (February 2025)\", \"content\": \"Inflection-2.5 is the latest large language model (LLM) developed by Inflection AI to power its conversational AI assistant, Pi. Significant upgrades have been made, as the model currently achieves over 94% of GPT-4’s average performance while only having 40% of the training FLOPs. In March 2024, the Microsoft-backed startup reached 1+ million daily active users on Pi. 13. Gemma is a series of lightweight open-source language models developed and released by Google DeepMind. Pythia is a series of 16 large language models developed and released by EleutherAI, a non-profit AI research lab. Alpaca is a 7 billion-parameter language model developed by a Stanford research team and fine-tuned from Meta's LLaMA 7B model.\", \"score\": 0.8815438, \"raw_content\": \"Best 22 Large Language Models (LLMs) (February 2025)\\n\\n\\n\\nAbout\\nNewsletter\\nBlog\\n\\n\\nBest 22 Large Language Models (LLMs) (February 2025)\\n\\nby Anthony Cardillo\\nFebruary 7, 2025\\nLarge language models are pre-trained on large datasets and use natural language processing to perform linguistic tasks such as text generation, code completion, paraphrasing, and more.\\nThe initial release of ChatGPT sparked the rapid adoption of generative AI, which has led to large language model innovations and industry growth.\\nIn fact, 92% of Fortune 500 firms have started using generative AI in their workflows.\\nAs adoption continues to grow, so does the LLM industry. The global large language model market is projected to grow from $6.5 billion in 2024 to $140.8 billion by 2033.\\nWith that, here is a list of the top 21 LLMs available in September 2024.\\nLLM NameDeveloperRelease DateAccessParametersDeepSeek R1DeepSeekJanuary 20, 2025Open-Source671 billionGPT-4oOpenAIMay 13, 2024APIUnknownClaude 3.5AnthropicJune 20, 2024APIUnknownGrok-1xAINovember 4, 2023Open-Source314 billionMistral 7BMistral AISeptember 27, 2023Open-Source7.3 billionPaLM 2GoogleMay 10, 2023Open-Source340 billionFalcon 180BTechnology Innovation InstituteSeptember 6, 2023Open-Source180 billionStable LM 2Stability AIJanuary 19, 2024Open-Source1.6 billion, 12 billionGemini 1.5Google DeepMindFebruary 2nd, 2024APIUnknownLlama 3.1Meta AIJune 23, 2024Open-Source405 billionMixtral 8x22BMistral AIApril 10, 2024Open-Source141 billionInflection-2.5Inflection AIMarch 10, 2024ProprietaryUnknownJambaAI21 LabsMarch 29, 2024Open-Source52 billionCommand RCohereMarch 11, 2024Both35 billionGemmaGoogle DeepMindFebruary 21, 2024Open-Source2 billion, 7 billionPhi-3MicrosoftApril 23, 2024Both3.8 billionXGen-7BSalesforceJuly 3, 2023Open-Source7 billionDBRXDatabricks' Mosaic MLMarch 27, 2024Open-Source132 billionPythiaEleutherAIFebruary 13, 2023Open-Source70 million to 12 billionSoraOpenAIFebruary 15, 2024 (announced)APIUnknownAlpaca 7BStanford CRFMMarch 13, 2023Open-Source7 billionNemotron-4NvidiaJune 14, 2024Open-Source340 billion\\n1. DeepSeek R1\\n\\nDeveloper: DeepSeek\\nRelease date: January 2025\\nNumber of Parameters: 671B total, 37B active\\nWhat is it? DeepSeek R1 is a reasoning model that excels in math and coding. It beats or matches OpenAI o1 in several benchmarks, including MATH-500 and AIME 2024.\\nOn its release, DeepSeek immediately hit headlines due to the low cost of training compared to most major LLMs.\\nDeepSeek R1 is free to use and open-source. It's accessible via the API, the DeepSeek website, and mobile apps.\\n2. GPT-4o\\n\\nDeveloper: OpenAI\\nRelease date: May 13, 2024\\nNumber of Parameters: Unknown\\nWhat is it? GPT-4o is the latest and most advanced OpenAI language model, succeeding GPT-4, GPT-3.5, and GPT-3. OpenAI claims that GPT-4o is 50% cheaper than GPT-4 despite being 2x faster at generating tokens. This multimodal model includes text, image, video, and voice capabilities packaged into one.\\nGPT-4o's biggest upgrade is the Voice-to-Voice function, which will improve input response times to an average of 320 milliseconds (compared to a few seconds with GPT-4). This feature is expected to be released in the coming weeks.\\n3. Claude 3.5\\n\\nDeveloper: Anthropic\\nRelease date: March 14, 2024\\nNumber of Parameters: Unknown\\nWhat is it? As a new upgrade from the highly rated Claude 3, Claude 3.5 Sonnet is the first release of the new Claude 3.5 model family. Similar to Claude 3, it'll also include the Haiku and Opus models. As debatably the biggest competitor to GPT-4 and ChatGPT, Claude made even bigger improvements to this model by maintaining the 200,000 token context window at a lower cost. This is much larger than GPT-4's 32,000 token capabilities.\\nAccording to Anthropic's report, Claude 3.5 Sonnet outperformed GPT-4o in major benchmarks like coding and text reasoning. Plus, this is Claude's most advanced vision model, with the ability to transcribe text from images or generate insights from charts.\\nAmazon has invested over $4 billion in Anthropic, bringing the startup's valuation to $15 billion. The Claude mobile app was also released in May 2024.\\n4. Grok-1\\n\\nDeveloper: xAI\\nRelease date: November 4, 2023\\nNumber of Parameters: 314 billion\\nWhat is it? Created by Elon Musk's artificial intelligence startup xAI, Grok-1 is currently the largest open-source LLM released to date at 314 billion parameters. Grok directly integrates with X (Twitter), and users must pay for an X Premium+ subscription to gain access.\\nBecause of the model’s size, Grok has a mixture-of-experts (MoE) architecture that only uses 25% of its weights for any given input token to maximize calculation efficiency.\\nIn August 2024, both Grok-2 and Grok-2 mini were released to X users in beta. According to xAI's reports, Grok-2 outperforms GPT-4o in numerous categories, such as GPQA, MMLU-Pro, and DocVQA.\\n5. Mistral 7B\\n\\nDeveloper: Mistral AI\\nRelease date: September 27, 2023\\nNumber of Parameters: 7.3 billion\\nWhat is it? Mistral 7B is an open-source language model with 32 layers, 32 attention heads, and eight key-value heads. Despite running with fewer parameters, they outperformed the Llama 2 family of models in nearly all metrics, including MMLU, reading comprehension, math, coding, etc.\\nMistral 7B is released under an Apache 2.0 license. Customers are free to download it locally, deploy it on the cloud, or run it on HuggingFace. The Paris-based startup is close to securing a new $600 million funding round that would value the company at $6 billion.\\n6. PaLM 2\\n\\nDeveloper: Google\\nRelease date: May 10, 2023\\nNumber of Parameters: 340 billion\\nWhat is it? PaLM 2 is an advanced large language model developed by Google. As the successor to the original Pathways Language Model (PaLM), it’s trained on 3.6 trillion tokens (compared to 780 billion) and 340 billion parameters (compared to 540 billion). PaLM 2 was originally used to power Google's first generative AI chatbot, Bard (rebranded to Gemini in February 2024).\\n7. Falcon 180B\\n\\nDeveloper: Technology Innovation Institute (TII)\\nRelease date: September 6, 2023\\nNumber of Parameters: 180 billion\\nWhat is it? Developed and funded by the Technology Innovation Institute, Falcon 180B is an upgraded version of the earlier Falcon 40B LLM. It has 180 billion parameters, which is 4.5 times larger than the 40 billion parameters of Falcon 40B.\\nIn addition to Falcon 40B, it also outperforms other large language models like GPT-3.5 and LLaMA 2 on tasks such as reasoning, question answering, and coding. In February 2024, the UAE-based Technology Innovation Institute (TII) committed $300 million in funding to the Falcon Foundation.\\n8. Stable LM 2\\n\\nDeveloper: Stability AI\\nRelease date: January 19, 2024\\nNumber of Parameters: 1.6 billion and 12 billion\\nWhat is it? Stability AI, the creators of the Stable Diffusion text-to-image model, are the developers behind Stable LM 2. This series of large language models includes Stable LM 2 12B (12 billion parameters) and Stable LM 2 1.6B (1.6 billion parameters). Released in April 2024, the larger 12B model outperforms models like LLaMA 2 70B on key benchmarks despite being much smaller.\\n9. Gemini 1.5\\n\\nDeveloper: Google DeepMind\\nRelease date: February 2nd, 2024\\nNumber of Parameters: Unknown\\nWhat is it? Gemini 1.5 is Google's next-generation large language model, offering a significant upgrade over its predecessor, Gemini 1.0. While it’s only available for early testing, Gemini 1.5 Pro provides a one million-token context window (1 hour of video, 700,000 words, or 30,000 lines of code), the largest to date compared to alternative LLMs and chatbots. This upgrade is 35 times larger than Gemini 1.0 Pro and surpasses the previous largest record of 200,000 tokens held by Anthropic’s Claude 2.1.\\n10. Llama 3.1\\n\\nDeveloper: Meta AI\\nRelease date: June 23, 2024\\nNumber of Parameters: 405 billion\\nWhat is it? Llama 3, the predecessor to Llama 3.1, was available in both 70B and 8B versions that outperformed other open-source models like Mistral 7B and Google's Gemma 7B on MMLU, reasoning, coding, and math benchmarks. Now, users will notice major upgrades to the latest version, including 405 billion parameters and an expended context length of 128,000.\\nUsers will also notice more accuracy because of the impressive knowledge base, which has been trained on over 15 trillion tokens. Plus, Meta added eight additional languages for this model. The increased size of this model makes it the largest open-source model released to date.\\nCustomers can still access its predecessor, Llama 2, which is available in three versions: 7 billion, 13 billion, and 70 billion parameters.\\n11. Mixtral 8x22B\\n\\nDeveloper: Mistral AI\\nRelease date: April 10, 2024\\nNumber of Parameters: 141 billion\\nWhat is it? Mixtral 8x22B is Mistral AI's latest and most advanced large language model. This sparse Mixture-of-Experts (SMoE) model has 141 billion total parameters but only uses 39B active parameters to focus on improving the model’s performance-to-cost ratio.\\nThe startup also recently released Mistral Large, a ChatGPT alternative that ranks second behind GPT-4 among API-based LLMs.\\n12. Inflection-2.5\\n\\nDeveloper: Inflection AI\\nRelease date: March 10, 2024\\nNumber of Parameters: Unknown\\nWhat is it? Inflection-2.5 is the latest large language model (LLM) developed by Inflection AI to power its conversational AI assistant, Pi. Significant upgrades have been made, as the model currently achieves over 94% of GPT-4’s average performance while only having 40% of the training FLOPs. In March 2024, the Microsoft-backed startup reached 1+ million daily active users on Pi.\\n13. Jamba\\n\\nDeveloper: AI21 Labs\\nRelease date: March 29, 2024\\nNumber of Parameters: 52 billion\\nWhat is it? AI21 Labs created Jamba, the world's first production-grade Mamba-style large language model. It integrates SSM technology with elements of a traditional transformer model to create a hybrid architecture. The model is efficient and highly scalable, with a context window of 256K and deployment support of 140K context on a single GPU.\\n14. Command R\\n\\nDeveloper: Cohere\\nRelease date: March 11, 2024\\nNumber of Parameters: 35 billion\\nWhat is it? Command R is a series of scalable LLMs from Cohere that support ten languages and 128,000-token context length (around 100 pages of text). This model primarily excels at retrieval-augmented generation, code-related tasks like explanations or rewrites, and reasoning. In April 2024, Command R+ was released to support larger workloads and provide real-world enterprise support.\\n15. Gemma\\n\\nDeveloper: Google DeepMind\\nRelease date: February 21, 2024\\nNumber of Parameters: 2 billion and 7 billion\\nWhat is it? Gemma is a series of lightweight open-source language models developed and released by Google DeepMind. The Gemma models are built with similar tech to the Gemini models, but Gemma is limited to text inputs and outputs only. The models have a context window of 8,000 tokens and are available in 2 billion and 7 billion parameter sizes.\\n16. Phi-3\\n\\nDeveloper: Microsoft\\nRelease date: April 23, 2024\\nNumber of Parameters: 3.8 billion\\nWhat is it? Classified as a small language model (SLM), Phi-3 is Microsoft's latest release with 3.8 billion parameters. Despite the smaller size, it's been trained on 3.3 trillion tokens of data to compete with Mistral 8x7B and GPT-3.5 performance on MT-bench and MMLU benchmarks.\\nTo date, Phi-3-mini is the only model available. However, Microsoft plans to release the Phi-3-small and Phi-3-medium models later this year.\\n17. XGen-7B\\n\\nDeveloper: Salesforce\\nRelease date: July 3, 2023\\nNumber of Parameters: 7 billion\\nWhat is it? XGen-7B is a large language model from Salesforce with 7 billion parameters and an 8k context window. The model was trained on 1.37 trillion tokens from various sources, such as RedPajama, Wikipedia, and Salesforce's own Starcoder dataset.\\nSalesforce has released two open-source versions, a 4,000 and 8,000 token context window base, hosted under an Apache 2.0 license.\\n18. DBRX\\n\\nDeveloper: Databricks' Mosaic ML\\nRelease date: March 27, 2024\\nNumber of Parameters: 132 billion\\nWhat is it? DBRX is an open-source LLM built by Databricks and the Mosaic ML research team. The mixture-of-experts architecture has 36 billion (of 132 billion total) active parameters on an input. DBRX has 16 experts and chooses 4 of them during inference, providing 65 times more expert combinations compared to similar models like Mixtral and Grok-1\\n19. Pythia\\n\\nDeveloper: EleutherAI\\nRelease date: February 13, 2023\\nNumber of Parameters: 70 million to 12 billion\\nWhat is it? Pythia is a series of 16 large language models developed and released by EleutherAI, a non-profit AI research lab. There are eight different model sizes: 70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B. Because of Pythia's open-source license, these LLMs serve as a base model for fine-tuned, instruction-following LLMs like Dolly 2.0 by Databricks.\\n20. Sora\\n\\nDeveloper: OpenAI\\nRelease date: February 15, 2024 (announced)\\nNumber of Parameters: Unknown\\nWhat is it? OpenAI's latest development is Sora, a text-to-video model that combines LLMs and generative AI to turn text prompts into realistic videos up to 60 seconds long. The model uses a transformer architecture that operates on \\\"spacetime patches\\\" of video and image data rather than text tokens like other LLMs. No official release date for Sora has been announced, but OpenAI expects it to open to the public in late 2024.\\n21. Alpaca 7B\\n\\nDeveloper: Stanford CRFM\\nRelease date: March 27, 2024\\nNumber of Parameters: 7 billion\\nWhat is it? Alpaca is a 7 billion-parameter language model developed by a Stanford research team and fine-tuned from Meta's LLaMA 7B model. Users will notice that although being much smaller, Alpaca performs similarly to text-DaVinci-003 (ChatGPT 3.5). However, Alpaca 7B is available for research purposes, and no commercial licenses are available.\\n22. Nemotron-4 340B\\n\\nDeveloper: NVIDIA\\nRelease date: June 14, 2024\\nNumber of Parameters: 340 billion\\nWhat is it? Nemotron-4 340B is a family of large language models for synthetic data generation and AI model training. These models help businesses create new LLMs without larger and more expensive datasets. Instead, Nemotron-4 can create high-quality synthetic data to train other AI models, which reduces the need for extensive human-annotated data.\\nThe model family includes Nemotron-4-340B-Base (foundation model), Nemotron-4-340B-Instruct (fine-tuned chatbot), and Nemotron-4-340B-Reward (quality assessment and preference ranking). Due to the 9 trillion tokens used in training, which includes English, multilingual, and coding language data, Nemotron-4 matches GPT-4's high-quality synthetic data generation capabilities.\\nConclusion\\nThe landscape of large language models is rapidly evolving, with new breakthroughs and innovations emerging at an unprecedented pace.\\nFrom compact models like Phi-2 and Alpaca 7B to cutting-edge architectures like Jamba and DBRX, the field of LLMs is pushing the boundaries of what's possible in natural language processing (NLP).\\nWe will keep this list regularly updated with new models. If you liked learning about these LLMs, check out our lists of generative AI startups and AI startups.\\nFind Thousands of Trending Topics With Our Platform\\nTry Exploding Topics Pro\\n\\nExploding Topics\\n\\nJoin Pro\\nNewsletter\\nTrending Topics\\nAdd a Topic\\nCustomer Login\\n\\nCompany\\n\\nAbout Us\\nContact\\nMethodology\\nCookie Settings\\n\\nFree Tools\\n\\nKeyword Research\\nBacklink Checker\\nSERP Checker\\nKeyword Rank Checker\\nFree SEO Tools\\n\\nConnect\\n\\nYouTube\\nInstagram\\nX (Twitter)\\n\\nResources\\n\\nBlog\\nMarketing Academy\\nFree Webinars\\n\\n\\n\\n© 2025  Exploding Topics is a Trademark of Semrush Inc\\n\\nPrivacy Policy\\nTerms of Service\\n\"}, {\"url\": \"https://hatchworks.com/blog/gen-ai/large-language-models-guide/\", \"title\": \"Large Language Models: What You Need to Know in 2025\", \"content\": \"Large language models (LLMs) are the unsung heroes of recent Generative AI advancements, quietly working behind the scenes to understand and generate language as we know it. OpenAI released GPT-4, an even more powerful and versatile model than its predecessors, with improvements in understanding, reasoning, and generating text across a broader range of contexts and languages. 2022: The emergence of GPT-4 and other advanced models such as Midjourney, continuing to push the boundaries of what’s possible with LLMs in terms of generating and understanding natural language across various domains and tasks, including image generation. These models are trained on massive data sets and can perform a broad range of tasks like generating text, translating languages, and more. Tags: AI, artificial intelligence, gen ai, Generative AI, large language models, LLMs\", \"score\": 0.83678174, \"raw_content\": \"Large Language Models: What You Need to Know in 2025 | HatchWorks AI\\nSkip to content\\n\\n\\nWhat We Do\\n\\n\\n\\n\\n\\nServices\\n\\n\\nAI Strategy & Roadmap\\n\\nData Engineering & Analytics\\nAI-Powered Software Development\\n\\nAI Engineering Teams\\n        *   *   Accelerators\\n\\n\\nGenerative Driven Development™\\n\\nAI Roadmap & ROI Workshop\\nAI Solution Accelerator\\nRAG\\n\\nGenIQ\\n        *   *   Industries\\n\\n\\nCommunications and IoT\\n\\nTechnology\\nHealthcare\\nFinance\\n\\nRetail\\n        *   *   Partnerships\\n\\n\\nDatabricks\\n\\nIndustries\\nCommunications and IoT Solutions\\nTechnology\\nHealthcare\\nFinance\\nRetail\\n\\n\\nAbout Us\\nAbout Us\\nCareers & Culture\\nHatchFutures\\nFAQ\\n\\n\\n\\nResources\\n\\n\\n\\n\\n\\nInsights\\n\\n\\n\\n\\n\\n\\n\\nBlog\\n\\nTalking AI Podcast\\n\\nTalking AI Newsletter\\n        *   *   Tools & Reports\\n\\n\\nState of AI Report 2025\\n\\nTech Talent Report 2024\\nNearshore Budget Calculator\\n\\nBuild your Own GPT\\n        *   *   Learn & Connect\\n\\n\\nEvents\\n        *   *   Media\\n\\n\\nNewsroom\\n\\nOur Work\\nCareers\\nContact\\n\\n\\n\\n\\n\\n\\n\\n\\nCareers\\nContact us\\nLarge Language Models: What You Need to Know in 2025\\n\\nMelissa Malec\\n\\nDecember 2, 2024\\n\\n\\nUpdated: January 16, 2025\\n\\n\\nLarge language models (LLMs) are the unsung heroes of recent Generative AI advancements, quietly working behind the scenes to understand and generate language as we know it.\\nBut how do they work? What are they capable of? And what should we look out for when using them?\\n\\nRead on and find out in this guide for LLMs in 2024. Jump ahead:\\n\\nUnderstanding Large Language Models\\nWhat is a Large Language Model?\\nHow Do Large Language Models Work?\\nKey Milestones in Large Language Model Development\\nCapabilities of Large Language Models\\nChallenges and Limitations of LLMs\\nThe Future of Language Models: What Comes Next?\\n\\nUnderstanding Large Language Models\\nLet’s get the basics out of the way. Here we’ll define the large language model (LLM), explain how they work, and provide a timeline of key milestones in LLM development.\\nWhat is a Large Language Model?\\nA large language model, often abbreviated to LLM, is a type of artificial intelligence model designed to understand natural language as well as generate it at a large scale.\\nWhen we say human language, we don’t just mean English, Spanish, or Cantonese. Those are certainly part of what LLMs are trained on but human language, in this context, also extends to:\\n\\nArt\\nDance\\nMorse code\\nGenetic code\\nHieroglyphics\\nCryptography\\nSign language\\nBody language\\nMusical notation\\nChemical signaling\\nEmojis and symbols\\nAnimal communication\\nHaptic communications\\nTraffic signs and signals\\nMathematical equations\\nProgramming languages\\n\\nLLMs are trained on billions of parameters and have the ability to learn from a wide range of data sources.\\nThis extensive training enables them to predict and produce text based on the input they receive so that they can engage in conversations, answer queries, or even write code.\\nSome of the leading very large models include giants like GPT, LLaMa, LaMDA, PaLM 2, BERT, and ERNIE.\\nThey’re at the heart of various applications, aiding in everything from customer service chatbots to content creation and software development.\\nSome companies even build their own LLMs but that requires significant time, investment, and tech knowledge. It’s much easier to integrate a pre-trained LLM into your own systems.\\nHow Do Large Language Models Work?\\nLarge Language Models use a blend of neural networks and machine learning (ML). It’s this blend that allows the technology to first process and then generate original text and imagery.\\nThink of neural networks as the LLM’s brain. It’s these networks that learn from vast amounts of data, improving over time as they’re exposed to more.\\nAs the model is trained on more data, it learns patterns, structures, and the nuances of language. It’s like teaching it the rules of grammar, the rhythm of poetry, and the jargon of technical manuals all at once.\\nMachine learning models then help the model to predict the next word in a sentence based on the words that come before it. This is done countless times, refining the model’s ability to generate coherent and contextually relevant text.\\nLLMs now also operate on a Transformer Architecture. This architecture allows the model to look at and weigh the importance of different words in a sentence. It’s the same as when we read a sentence and look for context clues to understand its meaning.\\n⚠️ While LLMs can generate original content, the quality, relevance, and innovativeness of their output can vary and require human oversight and refinement.\\nThe originality is also influenced by how the prompts are structured, the model’s training data, and the specific capabilities of the LLM in question.\\nKey Milestones in Large Language Model Development\\nLarge language models haven’t always been as useful as they are today. They’ve developed and been iterated upon significantly over time.\\nLet’s look at some of those key moments in LLM history. That way you can appreciate how far they’ve come and the rapid evolution in the last few years compared to decades of slow progress.\\n1966\\nELIZA\\n\\nThe first chatbot created by Joseph Weizenbaum, simulating a psychotherapist in conversation.\\n2013\\nword2vec\\n\\nA groundbreaking tool developed by a team led by Tomas Mikolov at Google, introducing efficient methods for learning word embeddings from raw text.\\n2018\\nGPT and BERT\\n\\nGPT (Generative Pretrained Transformer): OpenAI introduced GPT, showcasing a powerful model for understanding and generating human-like text.\\nBERT (Bidirectional Encoder Representations from Transformers): Developed by Google, BERT significantly advanced the state of the art in natural language understanding tasks.\\n\\n2020\\nGPT 3\\n\\nOpenAI released GPT-3, a model with 175 billion parameters, achieving unprecedented levels of language understanding and generation capabilities.\\nLate 2021\\nIntroduction of ChatGPT\\n\\nOpenAI introduced ChatGPT, a conversational agent based on the GPT-3.5 model, designed to provide more engaging and natural dialogue experiences. ChatGPT showcased the potential of GPT models in interactive applications.\\n2022\\nGPT-4\\nOpenAI released GPT-4, an even more powerful and versatile model than its predecessors, with improvements in understanding, reasoning, and generating text across a broader range of contexts and languages.\\n2022\\nMidjourney and Other Innovations\\n\\nThe launch of Midjourney, along with other models and platforms, reflected the growing diversity and application of AI in creative processes, design, and beyond, indicating a broader trend towards multimodal and specialized AI systems.\\nPre-2010: Early Foundations\\n\\n1950s-1970s: Early AI research lays the groundwork for natural language processing. Most famously, a tech called ‘Eliza’ was the world’s first chatbot.\\n1980s-1990s: Development of statistical methods for NLP, moving away from rule-based systems.\\n\\n2010: Initial Models\\n\\n2013: Introduction of word2vec, a tool for computing vector representations of words, which significantly improved the quality of NLP tasks by capturing semantic meanings of words.\\n\\n2014-2017: RNNs and Attention Mechanisms\\n\\n2014: Sequence to sequence (seq2seq) models and Recurrent Neural Networks (RNNs) become popular for tasks like machine translation.\\n2015: Introduction of Attention Mechanism, improving the performance of neural machine translation systems.\\n2017: The Transformer model is introduced in the paper “Attention is All You Need”, setting a new standard for NLP tasks with its efficient handling of sequences.\\n\\n2018: Emergence of GPT and BERT\\n\\nJune 2018: OpenAI introduces GPT (Generative Pretrained Transformer), a model that leverages unsupervised learning to generate coherent and diverse text.\\nOctober 2018: Google AI introduces BERT (Bidirectional Encoder Representations from Transformers), which uses bidirectional training of Transformer models to improve understanding of context in language.\\n\\n2019-2020: Larger and More Powerful Models\\n\\n2019: Introduction of GPT-2, an improved version of GPT with 1.5 billion parameters, showcasing the model’s ability to generate coherent and contextually relevant text over extended passages.\\n2020: OpenAI releases GPT-3, a much larger model with 175 billion parameters, demonstrating remarkable abilities in generating human-like text, translation, and answering questions.\\n\\n2021-2023: Specialization, Multimodality, and Democratization of LLMs\\n\\n2021-2022: Development of specialized models like Google’s LaMDA for conversational applications and Facebook’s OPT for open pre-trained transformers.\\n2021: Introduction of multimodal models like DALL·E by OpenAI, capable of generating images from textual descriptions, and CLIP, which can understand images in the context of natural language.\\n2022: The emergence of GPT-4 and other advanced models such as Midjourney, continuing to push the boundaries of what’s possible with LLMs in terms of generating and understanding natural language across various domains and tasks, including image generation. It’s also more accessible to larger numbers of people.\\n\\nCapabilities of Large Language Models\\nThe capabilities of Large Language Models are as vast as the datasets they’re trained on. Use cases range from generating code to suggesting strategy for a product launch and analyzing data points.\\nThis is because LLMs serve as foundation models that can be applied across multiple uses.\\nHere’s a list of LLM capabilities:\\n\\nText generation\\nLanguage translation\\nSummarization\\nQuestion answering\\nSentiment analysis\\nConversational agents\\nCode generation and explanation\\nNamed entity recognition\\nText classification\\nContent recommendation\\nLanguage modeling\\nSpell checking and grammar correction\\nParaphrasing and rewriting\\nKeyword and phrase extraction\\nDialogue systems\\n\\nAnd here’s a breakdown of some of the more common ones we see:\\nAutomated Code Generation\\nLLMs can generate code snippets, functions, or even entire modules based on natural language descriptions, reducing the time and effort required to implement common functionalities.\\nHere’s an example to illustrate how LLMs can be used for automated code generation:\\nPrompt:\\n“Write a Python function that takes a list of numbers as input and returns a list containing only the even numbers.”\\n\\nText Generation\\nLLMs can generate coherent, contextually relevant text based on prompts. This includes creating articles, stories, and even generating product descriptions.\\nHere’s an example to illustrate how LLMs can be used for text generation:\\nPrompt:\\n“Generate a product description for a cutting-edge smartwatch designed for fitness enthusiasts. The description should highlight its advanced health and fitness tracking, personalized coaching, long battery life, durability, connectivity features, and customizable design. Target the description to appeal to both seasoned athletes and beginners interested in tracking their fitness progress.”\\n\\nLanguage Translation\\nThey can translate text between different languages, often with a high degree of accuracy, depending on the languages involved and the model’s training data.\\nHere’s an example to illustrate how LLMs can be used for language translation:\\nPrompt:\\n“Translate the following English text into Spanish: ‘The quick brown fox jumps over the lazy dog.'”\\n\\nBug Detection and Correction\\nLLMs can help identify bugs in code by analyzing code patterns and suggesting fixes for common errors, potentially integrating with IDEs (Integrated Development Environments) to provide real-time assistance.\\nHere’s an example to illustrate how LLMs can be used for bug detection:\\nPrompt:\\n“The Python function below intends to return the nth Fibonacci number. Please identify and correct any bugs in the function.\\nPython Function:\\ndef fibonacci(n):\\nif n <\\\\= 1:\\nreturn n\\nelse:\\nreturn fibonacci(n – 1) + fibonacci(n – 2)”\\n\\nParaphrasing and Rewriting\\nThey can rephrase or rewrite text while maintaining the original meaning, useful for content creation and academic purposes.\\nHere’s an example to illustrate how LLMs can be used for paraphrasing:\\nPrompt:\\n“Rewrite the following sentence in a simpler and more concise way without losing its original meaning: ‘The comprehensive study on climate change incorporates a wide array of data, including historical weather patterns, satellite imagery, and computer model predictions, to provide a holistic view of the impacts of global warming.'”\\n\\nDialogue Systems\\nLLMs power sophisticated dialogue systems for customer service, interactive storytelling, and educational purposes, providing responses that can adapt to the user’s input.\\nThink of a chatbot on a software product you use where you can ask it questions and it generates insightful, helpful responses.\\nChallenges and Limitations of LLMs\\nLarge language models have come a long way since the early days of Eliza.\\nIn the last two years alone, we’ve seen LLMs power Generative AI and create high-quality text, music, video, and images.\\nBut with any technology, there will always be growing pains.\\nTechnical Limitations of Language Models\\nLarge Language Models sometimes face technical limitations impacting their accuracy and ability to understand context.\\nDomain Mismatch\\nModels trained on broad datasets may struggle with specific or niche subjects due to a lack of detailed data in those areas. This can lead to inaccuracies or overly generic responses when dealing with specialized knowledge.\\nWord Prediction\\nLLMs often falter with less common words or phrases, impacting their ability to fully understand or accurately generate text involving these terms. This limitation can affect the quality of translation, writing, and technical documentation tasks.\\nReal-time Translation Efficiency\\nWhile LLMs have made strides in translation accuracy, the computational demands of processing and generating translations in real-time can strain resources, especially for languages with complex grammatical structures or those less represented in training data.\\nHallucinations and Bias\\nOn occasion, LLM technology is too original. So original in fact that it’s making up information.\\nThis is a lesson Air Canada learned the hard way when its chatbot told a customer about a refund policy when no such policy exists, which they then had to honor.\\nFinally, LLMs can inadvertently propagate and amplify biases present in their training data, leading to outputs that may be discriminatory or offensive.\\nScalability and Environmental Impact\\nThe scalability of LLMs is tied to the impact it has on the environment. And that impact is turning out to be a big one.\\nTraining a system like GPT-3 took 1,287 Megawatt hours (MWh) of energy. To put that into perspective, 1 MWh could power about 330 homes for one hour in the United States.\\nThe image below shows the energy consumption of training four different LLMs.\\n\\nEnergy consumption doesn’t end at training—operating LLMs also uses a grotesque level of energy.\\nIn one report, Alex de Vries, founder of Digiconomist, has calculated that by 2027 the AI sector will consume between 85 to 134 Terawatt hours each year. That’s almost the same as the annual energy demand of the Netherlands.\\nWe can’t help but wonder how sustainable that is and what the long-term environmental impact will be on our energy sources. Especially when you consider LLMs are only going to become larger and more complex as we advance their capabilities.\\nAnd to maintain large language models, we’ll need to update them with new data and parameters as they arise. That will only expend more energy and resources.\\nThe Future of Language Models: What Comes Next?\\nNow that we’ve seen drastic and rapid improvement in the capabilities of LLMs through Generative AI, we expect users of AI to be fine-tuning prompts and discovering new use cases and applications.\\nIn the workplace especially, the focus will be on productivity hacks. It’s something we experiment with already through our Generative Driven Development™ offering, where our team has increased the productivity of software development by 30-50%.\\nHilary Ashton, Chief Product Officer at Teradata, shared her predictions for the future of LLMs and AI in AI Magazine:\\n\\nFirst, I foresee a massive productivity leap forward through GenAI, especially in technology and software. It’s getting more cost-effective to get into GenAI, and there are lots more solutions available that can help improve GenAI solutions. It will be the year when conversations gravitate to GenAI, ethics, and what it means to be human. In some cases, we’ll start to see the workforce shift and be reshaped, with the technology helping to usher in a four-day work week for some full-time employees.”\\nHilary Ashton\\n\\nAnd she’s right, especially when it comes to ethical considerations and where we humans add value AI can’t replicate.\\nWe’ll also see further democratization of AI with it infiltrating other areas of our life, much the same the computer has done since its invention.\\nWhat we know for certain is the development of LLMs and Generative AI is only getting started. And we want to be leading conversations on its use, ethics, scalability, and more as it evolves.\\nYou can be part of that conversation too:\\nListen or watch our Talking AI podcast where we interview AI experts and talk or sign up for our newsletter where we share insights and developments on LLMs, AI/ML, and Data governance, curated by our very own CTO, Omar Shanti.\\nFrequently Asked Questions About Large Language Models LLMs\\n1. What is a Large Language Model (LLM)?\\nA Large Language Model (LLM) is an artificial intelligence model that uses machine learning techniques, particularly deep learning and neural networks, to understand and generate human language. These models are trained on massive data sets and can perform a broad range of tasks like generating text, translating languages, and more.\\n2. How do Large Language Models work?\\nLarge Language Models work by leveraging transformer models, which utilize self-attention mechanisms to process input text. They are pre-trained on vast amounts of data and can perform in-context learning, allowing them to generate coherent and contextually relevant responses based on user inputs.\\n3. What is the significance of transformer models in LLMs?\\nTransformer models are crucial because they enable LLMs to handle long-range dependencies in text through self-attention. This mechanism allows the model to weigh the importance of different words in a sentence, improving the language model’s performance in understanding and generating language.\\n4. Why are Large Language Models important in AI technologies?\\nLarge Language Models are important because they serve as foundation models for various AI technologies like virtual assistants, conversational AI, and search engines. They enhance the ability of machines to understand and generate human language, making interactions with technology more natural.\\n5. What is fine-tuning in the context of LLMs?\\nFine-tuning involves taking a pre-trained language model and further training it on a specific task or dataset. This process adjusts the model to perform better on specific tasks like sentiment analysis, handling programming languages, or other specialized applications.\\n6. How does model size affect the performance of Large Language Models?\\nThe model size, often measured by the parameter count, affects an LLM’s ability to capture complex language patterns. Very large models with hundreds of billions of parameters generally perform better but require more computational resources during the training process.\\n7. Can LLMs generate code in programming languages?\\nYes, Large Language Models can generate code in various programming languages. They assist developers by providing code snippets, debugging help, and translating code, thanks to their training on diverse datasets that include programming code.\\n8. What is “in-context learning” in Large Language Models?\\nIn-context learning refers to an LLM’s ability to learn and perform specific tasks based solely on the input text provided during inference, without additional fine-tuning. This allows the model to adapt to new tasks or instructions on the fly, enhancing its versatility across a broad range of applications.\\n9. How do LLMs handle multiple tasks like text generation and sentiment analysis?\\nLLMs are versatile due to their training on diverse data. They can perform multiple tasks like text generation, sentiment analysis, and more by leveraging their learned knowledge. Through fine-tuning, they can be adapted to perform specific tasks more effectively.\\n10. What are “zero-shot” and “few-shot” learning in Large Language Models?\\nZero-shot learning allows an LLM to perform a specific task it wasn’t explicitly trained on by leveraging its general language understanding. Few-shot learning involves providing the model with a few examples of the task within the prompt to guide its response. Both methods showcase the model’s ability to generalize and adapt to new tasks with minimal or no additional training data.\\nInstantly access the power of AI and our team of AI-enabled practitioners\\nWe are ready to support you on your project!\\nContact us\\n\\nCategory: Gen AI\\nTags: AI, artificial intelligence, gen ai, Generative AI, large language models, LLMs\\n\\nGet the best of our content\\nstraight to your inbox!\\nDon’t worry, we don’t spam!\\nRelated Posts\\n\\nProprietary Patient Management System Unlocks 99% Faster Implementation and Client Onboarding\\n\\nAmazon Q Developer: The AWS Tool Revolutionizing Cloud Interaction\\n\\nPractical Data Governance Pillars: Safeguarding Your Digital Assets\\n\\nTesting Your RAG-Powered AI Chatbot\\nCategories\\n\\nAgile\\nCulture\\nModernization\\nNearshore Development\\nProduct + Design\\nSoftware Development\\nTalent\\n\\n\\nSubscribe to our newsletter and stay up to date on the latest in AI\\nServices\\n\\nAI Strategy Roadmap\\nData Engineering & Analytics\\nAI-Powered Software Development\\nAI Engineering Teams\\n\\nPartnerships\\n\\nDatabricks\\n\\nAccelerators\\n\\nGen AI Innovation Workshop\\nGen AI Solution Accelerator\\nRAG\\nGenIQ\\n\\nIndustries\\n\\nCommunications & IoT\\nTechnology\\nHealthcare\\nFinance\\nRetail\\n\\nResources\\n\\nBlog\\nTalking AI Podcast\\nTalking AI Newsletter\\nEvents\\nNearshore Budget Calculator\\n\\nGet in touch\\n\\nBook a call\\n1-800-621-7063\\n\\nFacebook Youtube \\n\\nAtlanta, GA [HQ]\\nChicago, IL\\nDallas, TX ​\\nSan Jose, Costa Rica [HQ]\\nBogota, Colombia\\nMedellin, Colombia\\nBarranquilla, Colombia\\nLima, Peru\\n\\n\\n\\n©2023 HatchWorks Inc. All rights reserved.\\nPrivacy Policy​\\nTerms and Conditions\\nRecruitment Fraud Disclaimer\\n\\nClose this module\\n\\nFREE E-BOOKState of AI 2025\\nA round-up of industry stats, research, and insights to understand where AI stands, how it got here, and where it’s going.\\nNameName\\nEmailEmail\\nCompany NameCompany Name\\nDownload E-book\\nNo thanks, I’m not interested!\"}, {\"url\": \"https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models\", \"title\": \"25 of the best large language models in 2025 - TechTarget\", \"content\": \"Large language models are the dynamite behind the generative AI boom. Some of the most well-known language models today are based on the transformer model, including the generative pre-trained transformer series of LLMs and bidirectional encoder representations from transformers (BERT). Gemma is a family of open-source language models from Google that were trained on the same resources as Gemini. GPT-3 is OpenAI's large language model with more than 175 billion parameters, released in 2020. Large Language Model Meta AI (Llama) is Meta's LLM which was first released in 2023. The Pathways Language Model is a 540 billion parameter transformer-based model from Google powering its AI chatbot Bard. StableLM is a series of open language models developed by Stability AI, the company behind image generator Stable Diffusion.\", \"score\": 0.82254606, \"raw_content\": \"25 of the best large language models in 2025\\nWhatIs\\nSearch the TechTarget Network \\nBrowse Definitions :\\n\\nA\\nB\\nC\\nD\\nE\\nF\\nG\\nH\\nI\\nJ\\nK\\nL\\nM\\nN\\nO\\nP\\nQ\\nR\\nS\\nT\\nU\\nV\\nW\\nX\\nY\\nZ\\n#\\n\\nLogin Register\\n\\nTechTarget Network\\nTech Accelerator\\nNews\\n2024 IT Salary Survey Results\\n\\nRSS\\n\\n\\nWhatIs\\n\\n\\nBrowse Definitions Data analytics and AI\\nTopics View All\\n\\nBusiness software\\nCloud computing\\nComputer science\\nData centers\\nIT management\\nNetworking\\nSecurity\\nSoftware development\\n\\nPlease select a category\\n\\nTopics\\n\\n\\n\\nBrowse Features Resources\\n\\nBusiness strategies\\nCareer resources\\nEmerging tech\\nTech explainers\\n\\n\\n\\nFollow:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\n\\nData analytics and AI\\n\\nTech Accelerator What is Gen AI? Generative AI explained\\nPrev Next Will AI replace jobs? 17 job types that might be affected Pros and cons of AI-generated content\\nDownload this guide1\\nFeature\\n25 of the best large language models in 2025\\nLarge language models have been affecting search for years and have been brought to the forefront by ChatGPT and other chatbots.\\n\\nShare this item with your network:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy\\n\\nSean Michael Kerner\\nBen Lutkevich, Site Editor\\n\\nPublished: 31 Jan 2025\\nLarge language models are the dynamite behind the generative AI boom. However, they've been around for a while.\\nLLMs are black box AI systems that use deep learning on extremely large datasets to understand and generate new text. Modern LLMs began taking shape in 2014 when the attention mechanism -- a machine learning technique designed to mimic human cognitive attention -- was introduced in a research paper titled \\\"Neural Machine Translation by Jointly Learning to Align and Translate.\\\" In 2017, that attention mechanism was honed with the introduction of the transformer model in another paper, \\\"Attention Is All You Need.\\\"\\nSome of the most well-known language models today are based on the transformer model, including the generative pre-trained transformer series of LLMs and bidirectional encoder representations from transformers (BERT).\\nChatGPT, which runs on a set of language models from OpenAI, attracted more than 100 million users just two months after its release in 2022. Since then, many competing models have been released. Some belong to big companies such as Google, Amazon and Microsoft; others are open source.\\nConstant developments in the field can be difficult to keep track of. Here are some of the most influential models, both past and present. Included in it are models that paved the way for today's leaders as well as those that could have a significant effect in the future.\\nThis article is part of\\nWhat is Gen AI? Generative AI explained\\n\\nWhich also includes:\\n8 top generative AI tool categories for 2025\\nWill AI replace jobs? 17 job types that might be affected\\n25 of the best large language models in 2025\\n\\nTop current LLMs\\nBelow are some of the most relevant large language models today. They do natural language processing and influence the architecture of future models.\\nBERT\\nBERT is a family of LLMs that Google introduced in 2018. BERT is a transformer-based model that can convert sequences of data to other sequences of data. BERT's architecture is a stack of transformer encoders and features 342 million parameters. BERT was pre-trained on a large corpus of data then fine-tuned to perform specific tasks along with natural language inference and sentence text similarity. It was used to improve query understanding in the 2019 iteration of Google search.\\nClaude\\nThe Claude LLM focuses on constitutional AI, which shapes AI outputs guided by a set of principles that help the AI assistant it powers helpful, harmless and accurate. Claude was created by the company Anthropic.\\nThere are three primary branches of Claude -- Opus, Haiku and Sonnet. The latest iteration of the Claude LLM is the Claude 3.5 Sonnet. It understands nuance, humor and complex instructions better than earlier versions of the LLM. It also has broad programming capabilities that make it well-suited for application development. In October 2024, Claude added a computer-use AI tool, that enables the LLM to use a computer like a human does. It's available via Claude.ai, the Claude iOS app and through an API.\\nCohere\\nCohere is an enterprise AI platform that provides several LLMs including Command, Rerank and Embed. These LLMs can be custom-trained and fine-tuned to a specific company's use case. The company that created the Cohere LLM was founded by one of the authors of Attention Is All You Need.\\nDeepSeek-R1\\nDeepSeek-R1 is an open-source reasoning model for tasks with complex reasoning, mathematical problem-solving and logical inference. The model uses reinforcement learning techniques to refine its reasoning ability and solve complex problems. DeepSeek-R1 can perform critical problem-solving through self-verification, chain-of-thought reasoning and reflection.\\nErnie\\nErnie is Baidu's large language model which powers the Ernie 4.0 chatbot. The bot was released in August 2023 and has garnered more than 45 million users. Ernie is rumored to have 10 trillion parameters. The bot works best in Mandarin but is capable in other languages.\\nFalcon\\nFalcon is a family of transformer-based models developed by the Technology Innovation Institute. It is open source and has multi-lingual capabilities. Falcon 2 is available in an 11 billion parameter version that provide multimodal capabilities for both text and vision.\\nThe Falcon 1 series includes a pair of larger models with Falcon 40B and Falcon 180B. Falcon models are available on GitHub as well as on cloud provider including Amazon.\\nGemini\\nGemini is Google's family of LLMs that power the company's chatbot of the same name. The model replaced Palm in powering the chatbot, which was rebranded from Bard to Gemini upon the model switch. Gemini models are multimodal, meaning they can handle images, audio and video as well as text. Gemini is also integrated in many Google applications and products. It comes in three sizes -- Ultra, Pro and Nano. Ultra is the largest and most capable model, Pro is the mid-tier model and Nano is the smallest model, designed for efficiency with on-device tasks.\\nAmong the most recent models is the Gemini 1.5 Pro update that debuted in May 2024 Gemini is available as a web chatbot, the Google Vertex AI service and via API. Early previews of Gemini 2.0 Flash became available in December 2024 with updated multimodal generation capabilities.\\nGemma\\nGemma is a family of open-source language models from Google that were trained on the same resources as Gemini. Gemma 2 was released in June 2024 in two sizes -- a 9 billion parameter model and a 27 billion parameter model. Gemma models can be run locally on a personal computer, and are also available in Google Vertex AI.\\nGPT-3\\nGPT-3 is OpenAI's large language model with more than 175 billion parameters, released in 2020. GPT-3 uses a decoder-only transformer architecture. In September 2022, Microsoft announced it had exclusive use of GPT-3's underlying model. GPT-3 is 10 times larger than its predecessor. GPT-3's training data includes Common Crawl, WebText2, Books1, Books2 and Wikipedia.\\nGPT-3 is the last of the GPT series of models in which OpenAI made the parameter counts publicly available. The GPT series was first introduced in 2018 with OpenAI's paper \\\"Improving Language Understanding by Generative Pre-Training.\\\"\\nGPT-3.5\\nGPT-3.5 is an upgraded version of GPT-3 with fewer parameters. GPT-3.5 was fine-tuned using reinforcement learning from human feedback. GPT-3.5 is the version of GPT that powers ChatGPT. There are several models, with GPT-3.5 turbo being the most capable, according to OpenAI. GPT-3.5's training data extends to September 2021.\\nIt was also integrated into the Bing search engine but has since been replaced with GPT-4.\\nGPT-4\\nGPT-4 , was released in 2023 and like the others in the OpenAI GPT family, it's a transformer-based model. Unlike the others, its parameter count has not been released to the public, though there are rumors that the model has more than 170 trillion. OpenAI describes GPT-4 as a multimodal model, meaning it can process and generate both language and images as opposed to being limited to only language. GPT-4 also introduced a system message, which lets users specify tone of voice and task.\\nGPT-4 demonstrated human-level performance in multiple academic exams. At the model's release, some speculated that GPT-4 came close to artificial general intelligence, which means it is as smart or smarter than a human. That speculation turned out to be unfounded.\\nGPT-4o\\nGPT-4 Omni (GPT-4o) is OpenAI's successor to GPT-4 and offers several improvements over the previous model. GPT-4o creates a more natural human interaction for ChatGPT and is a large multimodal model, accepting various inputs including audio, image and text. The conversations let users engage as they would in a normal human conversation, and the real-time interactivity can also pick up on emotions. GPT-4o can see photos or screens and ask questions about them during interaction.\\nGPT-4o can respond in 232 milliseconds, similar to human response time and faster than GPT-4 Turbo.\\nGranite\\nThe IBM Granite family of models are fully open source models under the Apache v.2 license. The first iteration of the open source model models debuted in May 2024, followed by Granite 3.0 in October and Granite 3.1 in December 2024.\\nThere are multiple variants in the Granite model family including General-purpose models (8B and 2B variants), guardrail model and Mixture-of-Experts models. While the model can be used for general purpose deployments, IBM itself is focusing deployment and optimization for enterprise use cases like customer service, IT automation and cybersecurity.\\nLamda\\nLamda (Language Model for Dialogue Applications) is a family of LLMs developed by Google Brain announced in 2021. Lamda used a decoder-only transformer language model and was pre-trained on a large corpus of text. In 2022, LaMDA gained widespread attention when then-Google engineer Blake Lemoine went public with claims that the program was sentient. It was built on the Seq2Seq architecture.\\nLlama\\nLarge Language Model Meta AI (Llama) is Meta's LLM which was first released in 2023. The Llama 3.1 models were released in July 2024, including both a 405 billion and 70 billion parameter model.\\nThe most recent version is Llama 3.2 which was released in September 2024, initially with smaller parameter counts of 11 billion and 90 billion.\\nLlama uses a transformer architecture and was trained on a variety of public data sources, including webpages from CommonCrawl, GitHub, Wikipedia and Project Gutenberg. Llama was effectively leaked and spawned many descendants, including Vicuna and Orca. Llama is available under an open license, allowing for free use of the models. Lllama models are available in many locations including llama.com and Hugging Face.\\nMistral\\nMistral is a family of a mixture of expert models from Mistral AI. Among the newest models is Mistral Large 2 which was first released in July 2024. The model operates with 123 billion parameters and a 128k context window, supporting dozens of languages including French, German, Spanish, Italian, and many others, along with more than 80 coding languages.\\nIn November 2024, Mistral released Pixtral Large, a 124-billion-parameter multimodal model that can handle text and visual data. Mistral models are available via Mistral's API on its Le Platforme-managed web service.\\no1\\nThe OpenAI o1 model family was first introduced in Sept. 2024. The o1 model's focus is to provide what OpenAI refers to as - reasoning models, that can reason through a problem or query before offering a response.\\nThe o1 models excel in STEM fields, with strong results in mathematical reasoning (scoring 83% on the International Mathematics Olympiad compared to GPT-4o's 13%), code generation and scientific research tasks. While they offer enhanced reasoning and improved safety features, they operate more slowly than previous models due to their thorough reasoning processes and come with certain limitations, such as restricted access features and higher API costs. The models are available to ChatGPT Plus and Team users, with varying access levels for different user categories.\\no3\\nOpenAI introduced the successor model, o3, in December 2024. According to OpenAI, o3 is designed to handle tasks with more analytical thinking, problem-solving and complex reasoning and will improve o1's capabilities and performance. The o3 model is in safety testing mode and is currently not available to the public.\\nOrca\\nOrca was developed by Microsoft and has 13 billion parameters, meaning it's small enough to run on a laptop. It aims to improve on advancements made by other open source models by imitating the reasoning procedures achieved by LLMs. Orca achieves the same performance as GPT-4 with significantly fewer parameters and is on par with GPT-3.5 for many tasks. Orca is built on top of the 13 billion parameter version of Llama.\\nPalm\\nThe Pathways Language Model is a 540 billion parameter transformer-based model from Google powering its AI chatbot Bard. It was trained across multiple TPU 4 Pods -- Google's custom hardware for machine learning. Palm specializes in reasoning tasks such as coding, math, classification and question answering. Palm also excels at decomposing complex tasks into simpler subtasks.\\nPaLM gets its name from a Google research initiative to build Pathways, ultimately creating a single model that serves as a foundation for multiple use cases. There are several fine-tuned versions of Palm, including Med-Palm 2 for life sciences and medical information as well as Sec-Palm for cybersecurity deployments to speed up threat analysis.\\nPhi\\nPhi is a transformer-based language model from Microsoft. The Phi 3.5 models were first released in August 2024.\\nThe series includes Phi-3.5-mini-instruct (3.82 billion parameters), Phi-3.5-MoE-instruct (41.9 billion parameters), and Phi-3.5-vision-instruct (4.15 billion parameters), each designed for specific tasks ranging from basic reasoning to vision analysis. All three models support a 128k token context length.\\nReleased under a Microsoft-branded MIT License, they are available for developers to download, use, and modify without restrictions, including for commercial purposes.\\nQwen\\nQwen is large family of open models developed by Chinese internet giant Alibaba Cloud. The newest set of models are the Qwen2.5 suite, which support 29 different languages and currently scale up to 72 billion parameters. These models are suitable for a wide range of tasks, including code generation, structured data understanding, mathematical problem-solving as well as general language understanding and generation.\\nStableLM\\nStableLM is a series of open language models developed by Stability AI, the company behind image generator Stable Diffusion.\\nStableLM 2 debuted in January 2024 initially with a 1.6 billion parameter model. In April 2024 that was expanded to also include a 12 billion parameter model. StableLM 2 supports seven languages: English, Spanish, German, Italian, French, Portuguese, and Dutch. Stability AI positions these models as offering different options for various use cases, with the 1.6B model suitable for specific, narrow tasks and faster processing while the 12B model provides more capability but requires more computational resources.\\nTülu 3\\nAllen Institute for AI's Tülu 3 is an open-source 405 billion-parameter LLM. The Tülu 3 405B model has post-training methods that combine supervised fine-tuning and reinforcement learning at a larger scale. Tülu 3 uses a \\\"reinforcement learning from verifiable rewards\\\" framework for fine-tuning tasks with verifiable outcomes -- such as solving mathematical problems and following instructions.\\nVicuna 33B\\nVicuna is another influential open source LLM derived from Llama. It was developed by LMSYS and was fine-tuned using data from sharegpt.com. It is smaller and less capable that GPT-4 according to several benchmarks, but does well for a model of its size. Vicuna has only 33 billion parameters, whereas GPT-4 has trillions.\\nLLM precursors\\nAlthough LLMs are a recent phenomenon, their precursors go back decades. Learn how recent precursor Seq2Seq and distant precursor ELIZA set the stage for modern LLMs.\\nSeq2Seq\\nSeq2Seq is a deep learning approach used for machine translation, image captioning and natural language processing. It was developed by Google and underlies some of their modern LLMs, including LaMDA. Seq2Seq also underlies AlexaTM 20B, Amazon's large language model. It uses a mix of encoders and decoders.\\nEliza\\nEliza was an early natural language processing program created in 1966. It is one of the earliest examples of a language model. Eliza simulated conversation using pattern matching and substitution. Eliza, running a certain script, could parody the interaction between a patient and therapist by applying weights to certain keywords and responding to the user accordingly. The creator of Eliza, Joshua Weizenbaum, wrote a book on the limits of computation and artificial intelligence.\\nNext Steps\\nGenerative AI challenges that businesses should consider\\nGenerative AI ethics: Biggest concerns\\nGenerative AI landscape: Potential future trends\\nGenerative models: VAEs, GANs, diffusion, transformers, NeRFs\\nAI content generators to explore\\nRelated Resources\\n\\nFive data quality trends to prepare for in the year ahead –Video\\nThe Digital Transformation And Innovation Landscape –Wipro\\nCloudera and NVIDIA Accelerate AI in the Financial Services Industry –Cloudera\\nImprove customer satisfaction or cut costs? Who says you have to choose? –Video\\n\\nDig Deeper on Data analytics and AI\\n\\n ##### What is GPT-3? Everything you need to know  By: Nick Barney\\n ##### What is a small language model (SLM)?  By: Sean Kerner\\n ##### GPT-4  By: Ben Lutkevich\\n ##### What are large language models (LLMs)?  By: Sean Kerner\\n\\nSponsored News\\n\\nSustainability, AI and Dell PowerEdge Servers –Dell Technologies and Intel\\nThree Innovative AI Use Cases for Natural Language Processing –Dell Technologies\\nAutonomous coding: The future of the revenue cycle –Solventum\\n\\nRelated Content\\n\\nExploring GPT-3 architecture – Search Enterprise AI\\nWhat is GPT-3? Everything you need to know – Search Enterprise AI\\nMicrosoft exclusively licenses OpenAI's GPT-3 ... – Search Enterprise AI\\n\\nLatest TechTarget resources\\n\\nNetworking\\nSecurity\\nCIO\\nHR Software\\nCustomer Experience\\n\\nSearch Networking\\n\\n\\nWhat is a thin client (lean client)?A thin client (lean client) is a virtual desktop computing model that runs on the resources stored on a central server instead of...\\n\\n\\nWhat is network monitoring?Network monitoring, also frequently called network management, is the practice of consistently overseeing a computer network for ...\\n\\n\\nWhat is network automation?Network automation is a process that uses intelligent software to automate the management, configuration, deployment, testing and...\\n\\n\\nSearch Security\\n\\n\\nWhat is Internet Key Exchange (IKE)?Internet Key Exchange (IKE) is a standard protocol used to set up a secure and authenticated communication channel between two ...\\n\\n\\nWhat is a certificate revocation list (CRL) and how is it used?A certificate revocation list (CRL) is a list of digital certificates that have been revoked by the issuing certificate authority...\\n\\n\\nWhat is cryptology?Cryptology is the mathematics, such as number theory and the application of formulas and algorithms, that underpin cryptography ...\\n\\n\\nSearch CIO\\n\\n\\nWhat is an IT project manager?An IT project manager is a professional charged with overseeing the process of planning, executing and delegating ...\\n\\n\\nWhat is a cyberthreat hunter (cybersecurity threat analyst)?A cyberthreat hunter, also called a cybersecurity threat analyst, proactively identifies security incidents that might go ...\\n\\n\\nWhat is blockchain? Definition, examples and how it worksBlockchain is a distributed ledger technology (DLT) that's shared across a network of computers to keep a digital record of ...\\n\\n\\nSearch HRSoftware\\n\\n\\nWhat is employee self-service (ESS)?Employee self-service (ESS) is a widely used human resources technology that enables employees to perform many job-related ...\\n\\n\\nWhat is DEI? Diversity, equity and inclusion explainedDiversity, equity and inclusion is a term used to describe policies and programs that promote the representation and ...\\n\\n\\nWhat is payroll software?Payroll software automates the process of paying salaried, hourly and contingent employees.\\n\\n\\nSearch Customer Experience\\n\\n\\nWhat is account-based selling? Everything you need to knowAccount-based selling (ABS) is a strategic sales approach in business-to-business sales and marketing that centers around ...\\n\\n\\nWhat is interactive voice response (IVR)?Interactive voice response (IVR) is an automated telephony system that interacts with callers, gathers information and routes ...\\n\\n\\nWhat is an AI assistant?An AI assistant, or digital assistant, is software that uses artificial intelligence to understand natural language voice ...\\n\\n\\nBrowse by Topic\\n\\n\\nBrowse Resources\\n\\n\\nAbout Us\\n\\nMeet The Editors\\nEditorial Ethics Policy\\nContact Us\\nAdvertisers\\nBusiness Partners\\nEvents\\nMedia Kit\\nCorporate Site\\nReprints\\n\\nAll Rights Reserved, Copyright 1999 - 2025, TechTarget  \\nPrivacy Policy\\nCookie Preferences\\nCookie Preferences\\nDo Not Sell or Share My Personal Information\\nClose\\n\\nX\\nFree Download What is generative AI? Everything you need to know\\nThe potential of AI technology has been percolating in the background for years. But when ChatGPT, the AI chatbot, began grabbing headlines in early 2023, it put generative AI in the spotlight. This guide is your go-to manual for generative AI, covering its benefits, limits, use cases, prospects and much more.\\n\"}, {\"url\": \"https://botpress.com/blog/best-large-language-models\", \"title\": \"Best Large Language Models in 2025 (Open Source + Hosted LLMs)\", \"content\": \"Discord Join thousands of peers and share ideas Docs Comprehensive guides and references API Reference material for use with external systems LLM Ranking Real-time data to help you choose the right models Videos Tutorials, demos, and product walkthroughs CLI Command-line tools for faster building Whether it's adding natural language processing capabilities to an existing app or building new AI-driven features, APIs allow developers to use LLMs for tasks like sentiment analysis, language translation, or content generation without needing to build or train models themselves. Developed by the Technology Innovation Institute and released on September 6, 2023, Falcon 180B features a staggering 180 billion parameters, making it one of the largest and most powerful open-source LLMs. It was designed to excel in tasks like translation, text generation, and research​.\", \"score\": 0.7409441, \"raw_content\": \"The Best Large Language Models in 2025 (Open Source + Hosted)\\n\\nPlatform\\nFeatures\\nAgent Studio Build and customize your agent rapidlyAutonomous Engine Use LLMs to guide conversations and tasksKnowledge Bases Train your bot with custom knowledge sourcesTables Store and manage conversation data\\nChannels\\n WhatsApp Instagram Messenger Slack\\nAll channels\\nIntegrations\\n HubSpot Notion Jira Calendly\\nAll integrations\\nLLM Providers\\n OpenAI Anthropic Groq Hugging Face\\nAll LLMs\\nSolutions\\nFor\\nEnterprise Automate mission-critical production workflowsAgencies Provide sophisticated agent servicesDevelopers Explore a robust API for agent development\\n Customer Stories Discover from successful customers how Botpress is transforming business worldwide.\\nBy Industry\\nEcommerce\\nEducation\\nFinance\\nHospitality\\nAll industries\\nBy Department\\nSales\\nEngineering\\nProduct\\nITSM\\nAll departments\\nBy Use Case\\nShopping Assistant\\nLead Generation\\nEmployee Experience\\nTicket Management\\nAll use cases\\nResources\\nEssential\\n Academy Learn to build through curated courses Library Resources to enhance your AI workflows Blog Insights and updates on Botpress and AI agents\\nbuilding\\n Discord Join thousands of peers and share ideas Docs Comprehensive guides and references API Reference material for use with external systems LLM Ranking Real-time data to help you choose the right models Videos Tutorials, demos, and product walkthroughs CLI Command-line tools for faster building\\nPartners\\n Become a Partner Join our network of certified experts Hire an Expert Connect with partners and consultants\\nDocs\\nPricing\\nContactGet started for free\\nback to blog\\nAI Basics\\nFor Builders\\nThe Best Large Language Models in 2025 (Open Source + Hosted)\\nOctober 19, 2024\\n·\\nUpdated on\\nLarge language models are increasing in power and popularity. Here are some of the best available for users today.\\nBotpress\\n\\nWith so many large language models (LLMs), it can be hard to decide which to use.\\nThe latest models are constantly pushing the boundaries of what's possible in artificial intelligence. As these models continue to shape the way we interact with technology, the possibilities for generative AI applications are limitless.\\nWe are now presented with a powerful toolset at our fingertips. It's suddenly easy to create AI agents and AI chatbots, or to use an LLM as a personal AI assistant in day-to-day tasks.\\nThe world of LLMs is only just beginning.\\nWhat are large language models?\\nA large language model (LLM) is an advanced type of artificial intelligence designed to understand and generate human-like text.\\nLLMs use deep learning algorithms that have been trained on vast amounts of data to recognize patterns and context in language.\\nAfter training, they use natural language processing perform tasks like translation, content creation, summarization, and answering questions.\\n\\nBuild LLM chatbots\\nBuild LLM-powered AI agents\\nStart now\\nNo credit card required\\nHow to use a large language model\\nThere are infinite ways to apply the power of an LLM. But most fall into one of 3 main categories:\\n1. AI agents and chatbots\\nLLMs are commonly integrated into chatbots and AI agents. These days, most conversational AI is powered by an LLM.\\nYou can see the most popular LLMs for chatbots and AI agents on our real-time ranking of the LLMs used by 500,000+ bot builders.\\nThese models can handle complex queries, generate contextual responses, and even manage dynamic conversations that evolve based on user input.\\nCommon AI agents include customer support chatbots and HR bots. But as the technology expands, so do use cases. Now businesses can build bespoke chatbots for hotels, sales chatbots, or even chatbots for real estate.\\nBy understanding the intent and context behind questions, LLM-powered chatbots can be used for customer support, virtual assistants, or even in business process automation.\\n2. Daily use\\nLLMs have increasingly made their way into everyday tasks. People use them for content generation, text summarization, language translation, and even creative projects, like writing poems or generating art descriptions.\\nThere are plenty of tools that use LLM APIs to help with daily tasks. Software like writing assistants or code completion tools are typically powered by LLMs these days.\\n3. API use\\nIf you're a developer, you can be the one using an API to build other software and tools.\\nLLMs can be accessed via APIs, which provide flexibility for integrating language models into various software applications.\\nWhether it's adding natural language processing capabilities to an existing app or building new AI-driven features, APIs allow developers to use LLMs for tasks like sentiment analysis, language translation, or content generation without needing to build or train models themselves.\\n\\nDeploying AI Agents?\\nRead our Blueprint for AI Agent Implementation\\nRead Now\\nAccess is always free\\nThe 5 best LLMs\\nMost LLM use is hosted software, which means it's maintained and run by a third-party provider on their servers, rather than on the user’s local system.\\nUsers access it over the internet, benefiting from simplified maintenance, updates, and infrastructure management handled by the host.\\nHere are the 5 best hosted LLMs available today:\\n1. GPT-4o\\nOpenAI’s latest multimodal model, GPT-4o, was released in May 2024 and integrates text, image, video, and voice capabilities.\\nThis model is 50% cheaper and twice as fast as GPT-4, making it highly efficient for a wide range of tasks. It stands out with its Voice-to-Voice function, allowing for audio responses in real-time, with a latency of just 320 milliseconds.\\nGPT-4o also improves performance in non-English languages and offers a more interactive experience​.\\n2. Claude 3.5\\nLaunched by Anthropic in June 2024, Claude 3.5 is known for its ethical design and strong performance across various benchmarks.\\nAvailable through an API, it continues Anthropic’s focus on safer AI interactions. While the number of parameters remains undisclosed, its advanced capabilities make it a strong competitor for tasks involving conversational AI and content generation​.\\n3. Grok-1\\nDeveloped by Elon Musk’s xAI, Grok-1 debuted in November 2023 with 314 billion parameters, focusing on generating responses with personality and real-time data from X (formerly Twitter).\\nIn August 2024, xAI released Grok-2 and Grok-2 mini, which have reportedly outperformed GPT-4o in several performance metrics​.\\n4. Gemini 1.5\\nGoogle's Gemini 1.5 focuses on improving multilingual capabilities and translation accuracy, making it particularly valuable for global businesses.\\nReleased in mid-2024, it is also designed to enhance tasks like text generation, customer interaction, and more​.\\n5. Inflection-2.5\\nInflection AI’s Inflection-2.5 powers the conversational AI assistant Pi, released in March 2024.\\nThis model achieves over 94% of GPT-4’s performance while using only 40% of the training computational resources.\\nIts efficiency has led to over a million daily active users on Pi, making it one of the most popular conversational models today​\\nThe 5 best open-source LLMs\\nIf you're a builder, open source LLMs are your friend. Open-source software refers to code that is publicly available for anyone to view, modify, and distribute.\\nIt fosters collaboration and transparency, allowing developers to adapt the software to their specific needs while contributing to its improvement.\\nHere are the top 5 open-source LLMs available today:\\n1. LLaMA 3.1\\nMeta’s latest open-source LLM, LLaMA 3, launched in April 2024, with sizes ranging from 8 billion to 70 billion parameters.\\nIt offers improved reasoning and coding abilities and is open-source for developers. LLaMA 3 is designed to outperform models like Claude 3 and Gemini 1.5, making it a top choice for a range of real-world tasks.\\n2. Mistral 7B\\nReleased by Mistral AI on September 27, 2023, this model has 7.3 billion parameters but manages to outperform larger models in many benchmarks.\\nIts smaller size makes it highly efficient, ideal for self-hosting, and versatile across NLP tasks​.\\n3. Falcon 180B\\nDeveloped by the Technology Innovation Institute and released on September 6, 2023, Falcon 180B features a staggering 180 billion parameters, making it one of the largest and most powerful open-source LLMs.\\nIt was designed to excel in tasks like translation, text generation, and research​.\\n4. OLMo\\nCreated by the Allen Institute for AI, OLMo focuses on transparency and reproducibility, making it highly valuable for research purposes.\\nIt’s particularly favored by researchers who need full insight into the data and training process​.\\n5. Qwen-1.5\\nAlibaba’s Qwen-1.5 is their open-source LLM, which competes with models from Meta and Google in both capability and cost-effectiveness.\\nIt's aimed at high-performance tasks in language processing and is designed to scale across various applications, from e-commerce to customer service​.\\nDeploy an LLM-powered AI agent\\nLeverage LLMs in your day-to-day with custom AI agents.\\nWith the plethora of chatbot platforms out there, it’s easy to set up an AI agent to fulfill your specific needs.\\nBotpress is an endlessly extensible AI automation platform. With a pre-built library of integrations, drag-and-drop workflows, and comprehensive tutorials, it's accessible for builders at all stages of expertise.\\nPlug in any LLM to power your AI project, across any use case.\\nStart building today. It's free.\\n\\nBuild LLM chatbots\\nBuild LLM-powered AI agents\\nStart now\\nNo credit card required\\nTable of Contents\\nStep 1. the title of the step goes here as expected\\nStep 1. the title of the step goes here as expected\\n\\nLearn how to build AI agents\\nShare this on:\\n\\n\\n##### What is RCS?\\nMarc Mercier\\nNov 29\\n\\n##### Ultimate Guide to Artificial Intelligence (AI) and Augmented Reality (AR)\\nBotpress\\nSep 23\\n\\n##### What is Agentic AI?\\nSarah Chudleigh\\nDec 11\\nBuild Better with Botpress\\nGet started today - it’s free!\\nStart building now \\n\\nAll Systems Operational\\nSOC 2\\nCertified\\nGDPR\\nCompliant\\n\\n© 2025\\n\\n\\nPlatform\\nPricingAgent StudioAutonomous EngineKnowledge BasesTables\\nHub\\nIntegrationsChannelsLLMs\\nResources\\nTalk to SalesDocumentationHire an ExpertVideosCustomer StoriesAPI ReferenceBlogStatusv12 Resources\\nCommunity\\nCommunity SupportBecome a PartnerBecome an AmbassadorBecome an Affiliate\\nCompany\\nAboutCareersNews & PressLegalPrivacy\\n© Botpress 2025\"}, {\"url\": \"https://en.wikipedia.org/wiki/List_of_large_language_models\", \"title\": \"List of large language models - Wikipedia\", \"content\": \"GLaM (Generalist Language Model)    December 2021   Google  1200[35]    1.6 trillion tokens[35] 5600[35]    Proprietary Sparse mixture of experts model, making it more expensive to train but cheaper to run inference compared to GPT-3. PaLM (Pathways Language Model)  April 2022  Google  540[43] 768 billion tokens[42]  29,250[38]  Proprietary Trained for ~60 days on ~6000 TPU v4 chips.[38] As of October 2024, it is the largest dense Transformer published. Mixtral 8x7B    December 2023   Mistral AI  46.7    Unknown Unknown Apache 2.0  Outperforms GPT-3.5 and Llama 2 70B on many benchmarks.[82] Mixture of experts model, with 12.9 billion parameters activated per token.[83] ^ a b c d Table 20 and page 66 of PaLM: Scaling Language Modeling with Pathways Archived 2023-06-10 at the Wayback Machine\", \"score\": 0.7251239, \"raw_content\": \"Jump to content\\nMain menu\\nSearch\\nDonate\\nCreate account\\nLog in\\nPersonal tools\\nToggle the table of contents\\nList of large language models\\nAdd languages\\nArticle\\nTalk\\nRead\\nEdit\\nView history\\nTools\\nFrom Wikipedia, the free encyclopedia\\nA large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThis page lists notable large language models.\\nFor the training cost column, 1 petaFLOP-day = 1 petaFLOP/sec × 1 day = 8.64E19 FLOP. Also, only the largest model's cost is written.\\nName    Release date[a] Developer   Number of parameters (billion) [b]  Corpus size Training cost (petaFLOP-day)    License[c]  Notes\\nGPT-1   June 2018   OpenAI  0.117       1[1]    MIT[2]  First GPT model, decoder-only transformer. Trained for 30 days on 8 P600 GPUs.\\nBERT    October 2018    Google  0.340[3]    3.3 billion words[3]    9[4]    Apache 2.0[5]   An early and influential language model.[6]Encoder-only and thus not built to be prompted or generative.[7] Training took 4 days on 64 TPUv2 chips.[8]\\nT5  October 2019    Google  11[9]   34 billion tokens[9]        Apache 2.0[10]  Base model for many Google projects, such as Imagen.[11]\\nXLNet   June 2019   Google  0.340[12]   33 billion words    330 Apache 2.0[13]  An alternative to BERT; designed as encoder-only. Trained on 512 TPU v3 chips for 5.5 days.[14]\\nGPT-2   February 2019   OpenAI  1.5[15] 40GB[16] (~10 billion tokens)[17]   28[18]  MIT[19] Trained on 32 TPUv3 chips for 1 week.[18]\\nGPT-3   May 2020    OpenAI  175[20] 300 billion tokens[17]  3640[21]    proprietary A fine-tuned variant of GPT-3, termed GPT-3.5, was made available to the public through a web interface called ChatGPT in 2022.[22]\\nGPT-Neo March 2021  EleutherAI  2.7[23] 825 GiB[24]     MIT[25] The first of a series of free GPT-3 alternatives released by EleutherAI. GPT-Neo outperformed an equivalent-size GPT-3 model on some benchmarks, but was significantly worse than the largest GPT-3.[25]\\nGPT-J   June 2021   EleutherAI  6[26]   825 GiB[24] 200[27] Apache 2.0  GPT-3-style language model\\nMegatron-Turing NLG October 2021[28]    Microsoft and Nvidia    530[29] 338.6 billion tokens[29]    38000[30]   Restricted web access   Trained for 3 months on over 2000 A100 GPUs on the NVIDIA Selene Supercomputer, for over 3 million GPU-hours.[30]\\nErnie 3.0 Titan December 2021   Baidu   260[31] 4 Tb        Proprietary Chinese-language LLM. Ernie Bot is based on this model.\\nClaude[32]  December 2021   Anthropic   52[33]  400 billion tokens[33]      beta    Fine-tuned for desirable behavior in conversations.[34]\\nGLaM (Generalist Language Model)    December 2021   Google  1200[35]    1.6 trillion tokens[35] 5600[35]    Proprietary Sparse mixture of experts model, making it more expensive to train but cheaper to run inference compared to GPT-3.\\nGopher  December 2021   DeepMind    280[36] 300 billion tokens[37]  5833[38]    Proprietary Later developed into the Chinchilla model.\\nLaMDA (Language Models for Dialog Applications) January 2022    Google  137[39] 1.56T words,[39] 168 billion tokens[37] 4110[40]    Proprietary Specialized for response generation in conversations.\\nGPT-NeoX    February 2022   EleutherAI  20[41]  825 GiB[24] 740[27] Apache 2.0  based on the Megatron architecture\\nChinchilla  March 2022  DeepMind    70[42]  1.4 trillion tokens[42][37] 6805[38]    Proprietary Reduced-parameter model trained on more data. Used in the Sparrow bot. Often cited for its neural scaling law.\\nPaLM (Pathways Language Model)  April 2022  Google  540[43] 768 billion tokens[42]  29,250[38]  Proprietary Trained for ~60 days on ~6000 TPU v4 chips.[38] As of October 2024, it is the largest dense Transformer published.\\nOPT (Open Pretrained Transformer)   May 2022    Meta    175[44] 180 billion tokens[45]  310[27] Non-commercial research[d]  GPT-3 architecture with some adaptations from Megatron. Uniquely, the training logbook written by the team was published.[46]\\nYaLM 100B   June 2022   Yandex  100[47] 1.7TB[47]       Apache 2.0  English-Russian model based on Microsoft's Megatron-LM.\\nMinerva June 2022   Google  540[48] 38.5B tokens from webpages filtered for mathematical content and from papers submitted to the arXiv preprint server[48]     Proprietary For solving \\\"mathematical and scientific questions using step-by-step reasoning\\\".[49] Initialized from PaLM models, then finetuned on mathematical and scientific data.\\nBLOOM   July 2022   Large collaboration led by Hugging Face 175[50] 350 billion tokens (1.6TB)[51]      Responsible AI  Essentially GPT-3 but trained on a multi-lingual corpus (30% English excluding programming languages)\\nGalactica   November 2022   Meta    120 106 billion tokens[52]  unknown CC-BY-NC-4.0    Trained on scientific text and modalities.\\nAlexaTM (Teacher Models)    November 2022   Amazon  20[53]  1.3 trillion[54]        proprietary[55] bidirectional sequence-to-sequence architecture\\nNeuro-sama  December 2022   Independent Unknown Unknown     privately-owned A language model designed for live-streaming on Twitch.\\nLLaMA (Large Language Model Meta AI)    February 2023   Meta AI 65[56]  1.4 trillion[56]    6300[57]    Non-commercial research[e]  Corpus has 20 languages. \\\"Overtrained\\\" (compared to Chinchilla scaling law) for better performance with fewer parameters.[56]\\nGPT-4   March 2023  OpenAI  Unknown[f] (According to rumors: 1760)[59]  Unknown Unknown proprietary Available for ChatGPT Plus users and used in several products.\\nChameleon   June 2024   Meta AI 34[60]  4.4 trillion      \\nCerebras-GPT    March 2023  Cerebras    13[61]      270[27] Apache 2.0  Trained with Chinchilla formula.\\nFalcon  March 2023  Technology Innovation Institute 40[62]  1 trillion tokens, from RefinedWeb (filtered web text corpus)[63] plus some \\\"curated corpora\\\".[64]  2800[57]    Apache 2.0[65]\\nBloombergGPT    March 2023  Bloomberg L.P.  50  363 billion token dataset based on Bloomberg's data sources, plus 345 billion tokens from general purpose datasets[66]      Proprietary Trained on financial data from proprietary sources, for financial tasks.\\nPanGu-Σ March 2023  Huawei  1085    329 billion tokens[67]      Proprietary \\nOpenAssistant[68]   March 2023  LAION   17  1.5 trillion tokens     Apache 2.0  Trained on crowdsourced open data\\nJurassic-2[69]  March 2023  AI21 Labs   Unknown Unknown     Proprietary Multilingual[70]\\nPaLM 2 (Pathways Language Model 2)  May 2023    Google  340[71] 3.6 trillion tokens[71] 85,000[57]  Proprietary Was used in Bard chatbot.[72]\\nLlama 2 July 2023   Meta AI 70[73]  2 trillion tokens[73]   21,000  Llama 2 license 1.7 million A100-hours.[74]\\nClaude 2    July 2023   Anthropic   Unknown Unknown Unknown Proprietary Used in Claude chatbot.[75]\\nGranite 13b July 2023   IBM Unknown Unknown Unknown Proprietary Used in IBM Watsonx.[76]\\nMistral 7B  September 2023  Mistral AI  7.3[77] Unknown     Apache 2.0\\nClaude 2.1  November 2023   Anthropic   Unknown Unknown Unknown Proprietary Used in Claude chatbot. Has a context window of 200,000 tokens, or ~500 pages.[78]\\nGrok-1[79]  November 2023   xAI 314 Unknown Unknown Apache 2.0  Used in Grok chatbot. Grok-1 has a context length of 8,192 tokens and has access to X (Twitter).[80]\\nGemini 1.0  December 2023   Google DeepMind Unknown Unknown Unknown Proprietary Multimodal model, comes in three sizes. Used in the chatbot of the same name.[81]\\nMixtral 8x7B    December 2023   Mistral AI  46.7    Unknown Unknown Apache 2.0  Outperforms GPT-3.5 and Llama 2 70B on many benchmarks.[82] Mixture of experts model, with 12.9 billion parameters activated per token.[83]\\nMixtral 8x22B   April 2024  Mistral AI  141 Unknown Unknown Apache 2.0  [84]\\nPhi-2   December 2023   Microsoft   2.7 1.4T tokens 419[85] MIT Trained on real and synthetic \\\"textbook-quality\\\" data, for 14 days on 96 A100 GPUs.[85]\\nGemini 1.5  February 2024   Google DeepMind Unknown Unknown Unknown Proprietary Multimodal model, based on a Mixture-of-Experts (MoE) architecture. Context window above 1 million tokens.[86]\\nGemini Ultra    February 2024   Google DeepMind Unknown Unknown Unknown   \\nGemma   February 2024   Google DeepMind 7   6T tokens   Unknown Gemma Terms of Use[87]\\nClaude 3    March 2024  Anthropic   Unknown Unknown Unknown Proprietary Includes three models, Haiku, Sonnet, and Opus.[88]\\nNova    October 2024    Rubik's AI  Unknown Unknown Unknown Proprietary Includes three models, Nova-Instant, Nova-Air, and Nova-Pro.\\nDBRX    March 2024  Databricks and Mosaic ML    136 12T Tokens      Databricks Open Model License   Training cost 10 million USD.\\nFugaku-LLM  May 2024    Fujitsu, Tokyo Institute of Technology, etc.    13  380B Tokens         The largest model ever trained on CPU-only, on the Fugaku.[89]\\nPhi-3   April 2024  Microsoft   14[90]  4.8T Tokens     MIT Microsoft markets them as \\\"small language model\\\".[91]\\nGranite Code Models May 2024    IBM Unknown Unknown Unknown Apache 2.0\\nQwen2   June 2024   Alibaba Cloud   72[92]  3T Tokens           Multiple sizes, the smallest being 0.5B.\\nNemotron-4  June 2024   Nvidia  340 9T Tokens   200,000 NVIDIA Open Model License   Trained for 1 epoch. Trained on 6144 H100 GPUs between December 2023 and May 2024.[93][94]\\nLlama 3.1   July 2024   Meta AI 405 15.6T tokens    440,000 Llama 3 license 405B version took 31 million hours on H100-80GB, at 3.8E25 FLOPs.[95][96]\\nDeepSeek V3 December 2024   DeepSeek    671 14.8T tokens    440,00  DeepSeek License    2.788M hours on H800 GPUs.[97]\\nAmazon Nova December 2024   Amazon  Unknown Unknown Unknown Proprietary Includes three models, Nova Micro, Nova Lite, and Nova Pro[98]\\nSee also[edit]\\nList of chatbots\\nNotes[edit]\\n^ This is the date that documentation describing the model's architecture was first released.\\n^ In many cases, researchers release or report on multiple versions of a model having different sizes. In these cases, the size of the largest model is listed here.\\n^ This is the license of the pre-trained model weights. In almost all cases the training code itself is open-source or can be easily replicated.\\n^ The smaller models including 66B are publicly available, while the 175B model is available on request.\\n^ Facebook's license and distribution scheme restricted access to approved researchers, but the model weights were leaked and became widely available.\\n^ As stated in Technical report: \\\"Given both the competitive landscape and the safety implications of large-scale models like GPT-4, this report contains no further details about the architecture (including model size), hardware, training compute, dataset construction, training method ...\\\"[58]\\nReferences[edit]\\n^ \\\"Improving language understanding with unsupervised learning\\\". openai.com. June 11, 2018. Archived from the original on 2023-03-18. Retrieved 2023-03-18.\\n^ \\\"finetune-transformer-lm\\\". GitHub. Archived from the original on 19 May 2023. Retrieved 2 January 2024.\\n^ a b Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (11 October 2018). \\\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\\". arXiv:1810.04805v2 [cs.CL].\\n^ Prickett, Nicole Hemsoth (2021-08-24). \\\"Cerebras Shifts Architecture To Meet Massive AI/ML Models\\\". The Next Platform. Archived from the original on 2023-06-20. Retrieved 2023-06-20.\\n^ \\\"BERT\\\". March 13, 2023. Archived from the original on January 13, 2021. Retrieved March 13, 2023 – via GitHub.\\n^ Manning, Christopher D. (2022). \\\"Human Language Understanding & Reasoning\\\". Daedalus. 151 (2): 127–138. doi:10.1162/daed_a_01905. S2CID 248377870. Archived from the original on 2023-11-17. Retrieved 2023-03-09.\\n^ Patel, Ajay; Li, Bryan; Rasooli, Mohammad Sadegh; Constant, Noah; Raffel, Colin; Callison-Burch, Chris (2022). \\\"Bidirectional Language Models Are Also Few-shot Learners\\\". arXiv:2209.14500 [cs.LG].\\n^ Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (11 October 2018). \\\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\\". arXiv:1810.04805v2 [cs.CL].\\n^ a b Raffel, Colin; Shazeer, Noam; Roberts, Adam; Lee, Katherine; Narang, Sharan; Matena, Michael; Zhou, Yanqi; Li, Wei; Liu, Peter J. (2020). \\\"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\\". Journal of Machine Learning Research. 21 (140): 1–67. arXiv:1910.10683. ISSN 1533-7928.\\n^ google-research/text-to-text-transfer-transformer, Google Research, 2024-04-02, archived from the original on 2024-03-29, retrieved 2024-04-04\\n^ \\\"Imagen: Text-to-Image Diffusion Models\\\". imagen.research.google. Archived from the original on 2024-03-27. Retrieved 2024-04-04.\\n^ \\\"Pretrained models — transformers 2.0.0 documentation\\\". huggingface.co. Archived from the original on 2024-08-05. Retrieved 2024-08-05.\\n^ \\\"xlnet\\\". GitHub. Archived from the original on 2 January 2024. Retrieved 2 January 2024.\\n^ Yang, Zhilin; Dai, Zihang; Yang, Yiming; Carbonell, Jaime; Salakhutdinov, Ruslan; Le, Quoc V. (2 January 2020). \\\"XLNet: Generalized Autoregressive Pretraining for Language Understanding\\\". arXiv:1906.08237 [cs.CL].\\n^ \\\"GPT-2: 1.5B Release\\\". OpenAI. 2019-11-05. Archived from the original on 2019-11-14. Retrieved 2019-11-14.\\n^ \\\"Better language models and their implications\\\". openai.com. Archived from the original on 2023-03-16. Retrieved 2023-03-13.\\n^ a b \\\"OpenAI's GPT-3 Language Model: A Technical Overview\\\". lambdalabs.com. 3 June 2020. Archived from the original on 27 March 2023. Retrieved 13 March 2023.\\n^ a b \\\"openai-community/gpt2-xl · Hugging Face\\\". huggingface.co. Archived from the original on 2024-07-24. Retrieved 2024-07-24.\\n^ \\\"gpt-2\\\". GitHub. Archived from the original on 11 March 2023. Retrieved 13 March 2023.\\n^ Wiggers, Kyle (28 April 2022). \\\"The emerging types of language models and why they matter\\\". TechCrunch. Archived from the original on 16 March 2023. Retrieved 9 March 2023.\\n^ Table D.1 in Brown, Tom B.; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Gretchen; Henighan, Tom; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel M.; Wu, Jeffrey; Winter, Clemens; Hesse, Christopher; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Scott; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (May 28, 2020). \\\"Language Models are Few-Shot Learners\\\". arXiv:2005.14165v4 [cs.CL].\\n^ \\\"ChatGPT: Optimizing Language Models for Dialogue\\\". OpenAI. 2022-11-30. Archived from the original on 2022-11-30. Retrieved 2023-01-13.\\n^ \\\"GPT Neo\\\". March 15, 2023. Archived from the original on March 12, 2023. Retrieved March 12, 2023 – via GitHub.\\n^ a b c Gao, Leo; Biderman, Stella; Black, Sid; Golding, Laurence; Hoppe, Travis; Foster, Charles; Phang, Jason; He, Horace; Thite, Anish; Nabeshima, Noa; Presser, Shawn; Leahy, Connor (31 December 2020). \\\"The Pile: An 800GB Dataset of Diverse Text for Language Modeling\\\". arXiv:2101.00027 [cs.CL].\\n^ a b Iyer, Abhishek (15 May 2021). \\\"GPT-3's free alternative GPT-Neo is something to be excited about\\\". VentureBeat. Archived from the original on 9 March 2023. Retrieved 13 March 2023.\\n^ \\\"GPT-J-6B: An Introduction to the Largest Open Source GPT Model | Forefront\\\". www.forefront.ai. Archived from the original on 2023-03-09. Retrieved 2023-02-28.\\n^ a b c d Dey, Nolan; Gosal, Gurpreet; Zhiming; Chen; Khachane, Hemant; Marshall, William; Pathria, Ribhu; Tom, Marvin; Hestness, Joel (2023-04-01). \\\"Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster\\\". arXiv:2304.03208 [cs.LG].\\n^ Alvi, Ali; Kharya, Paresh (11 October 2021). \\\"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World's Largest and Most Powerful Generative Language Model\\\". Microsoft Research. Archived from the original on 13 March 2023. Retrieved 13 March 2023.\\n^ a b Smith, Shaden; Patwary, Mostofa; Norick, Brandon; LeGresley, Patrick; Rajbhandari, Samyam; Casper, Jared; Liu, Zhun; Prabhumoye, Shrimai; Zerveas, George; Korthikanti, Vijay; Zhang, Elton; Child, Rewon; Aminabadi, Reza Yazdani; Bernauer, Julie; Song, Xia (2022-02-04). \\\"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model\\\". arXiv:2201.11990 [cs.CL].\\n^ a b Rajbhandari, Samyam; Li, Conglong; Yao, Zhewei; Zhang, Minjia; Aminabadi, Reza Yazdani; Awan, Ammar Ahmad; Rasley, Jeff; He, Yuxiong (2022-07-21), DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale, arXiv:2201.05596\\n^ Wang, Shuohuan; Sun, Yu; Xiang, Yang; Wu, Zhihua; Ding, Siyu; Gong, Weibao; Feng, Shikun; Shang, Junyuan; Zhao, Yanbin; Pang, Chao; Liu, Jiaxiang; Chen, Xuyi; Lu, Yuxiang; Liu, Weixin; Wang, Xi; Bai, Yangfan; Chen, Qiuliang; Zhao, Li; Li, Shiyong; Sun, Peng; Yu, Dianhai; Ma, Yanjun; Tian, Hao; Wu, Hua; Wu, Tian; Zeng, Wei; Li, Ge; Gao, Wen; Wang, Haifeng (December 23, 2021). \\\"ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation\\\". arXiv:2112.12731 [cs.CL].\\n^ \\\"Product\\\". Anthropic. Archived from the original on 16 March 2023. Retrieved 14 March 2023.\\n^ a b Askell, Amanda; Bai, Yuntao; Chen, Anna; et al. (9 December 2021). \\\"A General Language Assistant as a Laboratory for Alignment\\\". arXiv:2112.00861 [cs.CL].\\n^ Bai, Yuntao; Kadavath, Saurav; Kundu, Sandipan; et al. (15 December 2022). \\\"Constitutional AI: Harmlessness from AI Feedback\\\". arXiv:2212.08073 [cs.CL].\\n^ a b c Dai, Andrew M; Du, Nan (December 9, 2021). \\\"More Efficient In-Context Learning with GLaM\\\". ai.googleblog.com. Archived from the original on 2023-03-12. Retrieved 2023-03-09.\\n^ \\\"Language modelling at scale: Gopher, ethical considerations, and retrieval\\\". www.deepmind.com. 8 December 2021. Archived from the original on 20 March 2023. Retrieved 20 March 2023.\\n^ a b c Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; et al. (29 March 2022). \\\"Training Compute-Optimal Large Language Models\\\". arXiv:2203.15556 [cs.CL].\\n^ a b c d Table 20 and page 66 of PaLM: Scaling Language Modeling with Pathways Archived 2023-06-10 at the Wayback Machine\\n^ a b Cheng, Heng-Tze; Thoppilan, Romal (January 21, 2022). \\\"LaMDA: Towards Safe, Grounded, and High-Quality Dialog Models for Everything\\\". ai.googleblog.com. Archived from the original on 2022-03-25. Retrieved 2023-03-09.\\n^ Thoppilan, Romal; De Freitas, Daniel; Hall, Jamie; Shazeer, Noam; Kulshreshtha, Apoorv; Cheng, Heng-Tze; Jin, Alicia; Bos, Taylor; Baker, Leslie; Du, Yu; Li, YaGuang; Lee, Hongrae; Zheng, Huaixiu Steven; Ghafouri, Amin; Menegali, Marcelo (2022-01-01). \\\"LaMDA: Language Models for Dialog Applications\\\". arXiv:2201.08239 [cs.CL].\\n^ Black, Sidney; Biderman, Stella; Hallahan, Eric; et al. (2022-05-01). GPT-NeoX-20B: An Open-Source Autoregressive Language Model. Proceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models. Vol. Proceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models. pp. 95–136. Archived from the original on 2022-12-10. Retrieved 2022-12-19.\\n^ a b c Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; Sifre, Laurent (12 April 2022). \\\"An empirical analysis of compute-optimal large language model training\\\". Deepmind Blog. Archived from the original on 13 April 2022. Retrieved 9 March 2023.\\n^ Narang, Sharan; Chowdhery, Aakanksha (April 4, 2022). \\\"Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance\\\". ai.googleblog.com. Archived from the original on 2022-04-04. Retrieved 2023-03-09.\\n^ Susan Zhang; Mona Diab; Luke Zettlemoyer. \\\"Democratizing access to large-scale language models with OPT-175B\\\". ai.facebook.com. Archived from the original on 2023-03-12. Retrieved 2023-03-12.\\n^ Zhang, Susan; Roller, Stephen; Goyal, Naman; Artetxe, Mikel; Chen, Moya; Chen, Shuohui; Dewan, Christopher; Diab, Mona; Li, Xian; Lin, Xi Victoria; Mihaylov, Todor; Ott, Myle; Shleifer, Sam; Shuster, Kurt; Simig, Daniel; Koura, Punit Singh; Sridhar, Anjali; Wang, Tianlu; Zettlemoyer, Luke (21 June 2022). \\\"OPT: Open Pre-trained Transformer Language Models\\\". arXiv:2205.01068 [cs.CL].\\n^ \\\"metaseq/projects/OPT/chronicles at main · facebookresearch/metaseq\\\". GitHub. Retrieved 2024-10-18.\\n^ a b Khrushchev, Mikhail; Vasilev, Ruslan; Petrov, Alexey; Zinov, Nikolay (2022-06-22), YaLM 100B, archived from the original on 2023-06-16, retrieved 2023-03-18\\n^ a b Lewkowycz, Aitor; Andreassen, Anders; Dohan, David; Dyer, Ethan; Michalewski, Henryk; Ramasesh, Vinay; Slone, Ambrose; Anil, Cem; Schlag, Imanol; Gutman-Solo, Theo; Wu, Yuhuai; Neyshabur, Behnam; Gur-Ari, Guy; Misra, Vedant (30 June 2022). \\\"Solving Quantitative Reasoning Problems with Language Models\\\". arXiv:2206.14858 [cs.CL].\\n^ \\\"Minerva: Solving Quantitative Reasoning Problems with Language Models\\\". ai.googleblog.com. 30 June 2022. Retrieved 20 March 2023.\\n^ Ananthaswamy, Anil (8 March 2023). \\\"In AI, is bigger always better?\\\". Nature. 615 (7951): 202–205. Bibcode:2023Natur.615..202A. doi:10.1038/d41586-023-00641-w. PMID 36890378. S2CID 257380916. Archived from the original on 16 March 2023. Retrieved 9 March 2023.\\n^ \\\"bigscience/bloom · Hugging Face\\\". huggingface.co. Archived from the original on 2023-04-12. Retrieved 2023-03-13.\\n^ Taylor, Ross; Kardas, Marcin; Cucurull, Guillem; Scialom, Thomas; Hartshorn, Anthony; Saravia, Elvis; Poulton, Andrew; Kerkez, Viktor; Stojnic, Robert (16 November 2022). \\\"Galactica: A Large Language Model for Science\\\". arXiv:2211.09085 [cs.CL].\\n^ \\\"20B-parameter Alexa model sets new marks in few-shot learning\\\". Amazon Science. 2 August 2022. Archived from the original on 15 March 2023. Retrieved 12 March 2023.\\n^ Soltan, Saleh; Ananthakrishnan, Shankar; FitzGerald, Jack; et al. (3 August 2022). \\\"AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model\\\". arXiv:2208.01448 [cs.CL].\\n^ \\\"AlexaTM 20B is now available in Amazon SageMaker JumpStart | AWS Machine Learning Blog\\\". aws.amazon.com. 17 November 2022. Archived from the original on 13 March 2023. Retrieved 13 March 2023.\\n^ a b c \\\"Introducing LLaMA: A foundational, 65-billion-parameter large language model\\\". Meta AI. 24 February 2023. Archived from the original on 3 March 2023. Retrieved 9 March 2023.\\n^ a b c \\\"The Falcon has landed in the Hugging Face ecosystem\\\". huggingface.co. Archived from the original on 2023-06-20. Retrieved 2023-06-20.\\n^ \\\"GPT-4 Technical Report\\\" (PDF). OpenAI. 2023. Archived (PDF) from the original on March 14, 2023. Retrieved March 14, 2023.\\n^ Schreiner, Maximilian (2023-07-11). \\\"GPT-4 architecture, datasets, costs and more leaked\\\". THE DECODER. Archived from the original on 2023-07-12. Retrieved 2024-07-26.\\n^ Dickson, Ben (22 May 2024). \\\"Meta introduces Chameleon, a state-of-the-art multimodal model\\\". VentureBeat.\\n^ Dey, Nolan (March 28, 2023). \\\"Cerebras-GPT: A Family of Open, Compute-efficient, Large Language Models\\\". Cerebras. Archived from the original on March 28, 2023. Retrieved March 28, 2023.\\n^ \\\"Abu Dhabi-based TII launches its own version of ChatGPT\\\". tii.ae. Archived from the original on 2023-04-03. Retrieved 2023-04-03.\\n^ Penedo, Guilherme; Malartic, Quentin; Hesslow, Daniel; Cojocaru, Ruxandra; Cappelli, Alessandro; Alobeidli, Hamza; Pannier, Baptiste; Almazrouei, Ebtesam; Launay, Julien (2023-06-01). \\\"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only\\\". arXiv:2306.01116 [cs.CL].\\n^ \\\"tiiuae/falcon-40b · Hugging Face\\\". huggingface.co. 2023-06-09. Retrieved 2023-06-20.\\n^ UAE's Falcon 40B, World's Top-Ranked AI Model from Technology Innovation Institute, is Now Royalty-Free Archived 2024-02-08 at the Wayback Machine, 31 May 2023\\n^ Wu, Shijie; Irsoy, Ozan; Lu, Steven; Dabravolski, Vadim; Dredze, Mark; Gehrmann, Sebastian; Kambadur, Prabhanjan; Rosenberg, David; Mann, Gideon (March 30, 2023). \\\"BloombergGPT: A Large Language Model for Finance\\\". arXiv:2303.17564 [cs.LG].\\n^ Ren, Xiaozhe; Zhou, Pingyi; Meng, Xinfan; Huang, Xinjing; Wang, Yadao; Wang, Weichao; Li, Pengfei; Zhang, Xiaoda; Podolskiy, Alexander; Arshinov, Grigory; Bout, Andrey; Piontkovskaya, Irina; Wei, Jiansheng; Jiang, Xin; Su, Teng; Liu, Qun; Yao, Jun (March 19, 2023). \\\"PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing\\\". arXiv:2303.10845 [cs.CL].\\n^ Köpf, Andreas; Kilcher, Yannic; von Rütte, Dimitri; Anagnostidis, Sotiris; Tam, Zhi-Rui; Stevens, Keith; Barhoum, Abdullah; Duc, Nguyen Minh; Stanley, Oliver; Nagyfi, Richárd; ES, Shahul; Suri, Sameer; Glushkov, David; Dantuluri, Arnav; Maguire, Andrew (2023-04-14). \\\"OpenAssistant Conversations – Democratizing Large Language Model Alignment\\\". arXiv:2304.07327 [cs.CL].\\n^ Wrobel, Sharon. \\\"Tel Aviv startup rolls out new advanced AI language model to rival OpenAI\\\". www.timesofisrael.com. Archived from the original on 2023-07-24. Retrieved 2023-07-24.\\n^ Wiggers, Kyle (2023-04-13). \\\"With Bedrock, Amazon enters the generative AI race\\\". TechCrunch. Archived from the original on 2023-07-24. Retrieved 2023-07-24.\\n^ a b Elias, Jennifer (16 May 2023). \\\"Google's newest A.I. model uses nearly five times more text data for training than its predecessor\\\". CNBC. Archived from the original on 16 May 2023. Retrieved 18 May 2023.\\n^ \\\"Introducing PaLM 2\\\". Google. May 10, 2023. Archived from the original on May 18, 2023. Retrieved May 18, 2023.\\n^ a b \\\"Introducing Llama 2: The Next Generation of Our Open Source Large Language Model\\\". Meta AI. 2023. Archived from the original on 2024-01-05. Retrieved 2023-07-19.\\n^ \\\"llama/MODEL_CARD.md at main · meta-llama/llama\\\". GitHub. Archived from the original on 2024-05-28. Retrieved 2024-05-28.\\n^ \\\"Claude 2\\\". anthropic.com. Archived from the original on 15 December 2023. Retrieved 12 December 2023.\\n^ Nirmal, Dinesh (2023-09-07). \\\"Building AI for business: IBM's Granite foundation models\\\". IBM Blog. Archived from the original on 2024-07-22. Retrieved 2024-08-11.\\n^ \\\"Announcing Mistral 7B\\\". Mistral. 2023. Archived from the original on 2024-01-06. Retrieved 2023-10-06.\\n^ \\\"Introducing Claude 2.1\\\". anthropic.com. Archived from the original on 15 December 2023. Retrieved 12 December 2023.\\n^ xai-org/grok-1, xai-org, 2024-03-19, archived from the original on 2024-05-28, retrieved 2024-03-19\\n^ \\\"Grok-1 model card\\\". x.ai. Retrieved 12 December 2023.\\n^ \\\"Gemini – Google DeepMind\\\". deepmind.google. Archived from the original on 8 December 2023. Retrieved 12 December 2023.\\n^ Franzen, Carl (11 December 2023). \\\"Mistral shocks AI community as latest open source model eclipses GPT-3.5 performance\\\". VentureBeat. Archived from the original on 11 December 2023. Retrieved 12 December 2023.\\n^ \\\"Mixtral of experts\\\". mistral.ai. 11 December 2023. Archived from the original on 13 February 2024. Retrieved 12 December 2023.\\n^ AI, Mistral (2024-04-17). \\\"Cheaper, Better, Faster, Stronger\\\". mistral.ai. Archived from the original on 2024-05-05. Retrieved 2024-05-05.\\n^ a b Hughes, Alyssa (12 December 2023). \\\"Phi-2: The surprising power of small language models\\\". Microsoft Research. Archived from the original on 12 December 2023. Retrieved 13 December 2023.\\n^ \\\"Our next-generation model: Gemini 1.5\\\". Google. 15 February 2024. Archived from the original on 16 February 2024. Retrieved 16 February 2024. This means 1.5 Pro can process vast amounts of information in one go — including 1 hour of video, 11 hours of audio, codebases with over 30,000 lines of code or over 700,000 words. In our research, we've also successfully tested up to 10 million tokens.\\n^ \\\"Gemma\\\" – via GitHub.\\n^ \\\"Introducing the next generation of Claude\\\". www.anthropic.com. Archived from the original on 2024-03-04. Retrieved 2024-03-04.\\n^ \\\"Fugaku-LLM/Fugaku-LLM-13B · Hugging Face\\\". huggingface.co. Archived from the original on 2024-05-17. Retrieved 2024-05-17.\\n^ \\\"Phi-3\\\". azure.microsoft.com. 23 April 2024. Archived from the original on 2024-04-27. Retrieved 2024-04-28.\\n^ \\\"Phi-3 Model Documentation\\\". huggingface.co. Archived from the original on 2024-05-13. Retrieved 2024-04-28.\\n^ \\\"Qwen2\\\". GitHub. Archived from the original on 2024-06-17. Retrieved 2024-06-17.\\n^ \\\"nvidia/Nemotron-4-340B-Base · Hugging Face\\\". huggingface.co. 2024-06-14. Archived from the original on 2024-06-15. Retrieved 2024-06-15.\\n^ \\\"Nemotron-4 340B | Research\\\". research.nvidia.com. Archived from the original on 2024-06-15. Retrieved 2024-06-15.\\n^ \\\"The Llama 3 Herd of Models\\\" (July 23, 2024) Llama Team, AI @ Meta\\n^ \\\"llama-models/models/llama3_1/MODEL_CARD.md at main · meta-llama/llama-models\\\". GitHub. Archived from the original on 2024-07-23. Retrieved 2024-07-23.\\n^ deepseek-ai/DeepSeek-V3, DeepSeek, 2024-12-26, retrieved 2024-12-26\\n^ Amazon Nova Micro, Lite, and Pro - AWS AI Service Cards3, Amazon, 2024-12-27, retrieved 2024-12-27\\nvte\\nNatural language processing\\nGeneral terms \\nAI-completeBag-of-wordsn-gram BigramTrigramComputational linguisticsNatural language understandingStop wordsText processing\\nText analysis \\nArgument miningCollocation extractionConcept miningCoreference resolutionDeep linguistic processingDistant readingInformation extractionNamed-entity recognitionOntology learningParsing Semantic parsingSyntactic parsingPart-of-speech taggingSemantic analysisSemantic role labelingSemantic decompositionSemantic similaritySentiment analysis\\nTerminology extractionText miningTextual entailmentTruecasingWord-sense disambiguationWord-sense induction\\nText segmentation \\nCompound-term processingLemmatisationLexical analysisText chunkingStemmingSentence segmentationWord segmentation\\nAutomatic summarization \\nMulti-document summarizationSentence extractionText simplification\\nMachine translation \\nComputer-assistedExample-basedRule-basedStatisticalTransfer-basedNeural\\nDistributional semantics models \\nBERTDocument-term matrixExplicit semantic analysisfastTextGloVeLanguage model (large)Latent semantic analysisSeq2seqWord embeddingWord2vec\\nLanguage resources,\\ndatasets and corpora  \\nTypes and\\nstandards \\nCorpus linguisticsLexical resourceLinguistic Linked Open DataMachine-readable dictionaryParallel textPropBankSemantic networkSimple Knowledge Organization SystemSpeech corpusText corpusThesaurus (information retrieval)TreebankUniversal Dependencies\\nData  \\nBabelNetBank of EnglishDBpediaFrameNetGoogle Ngram ViewerUBYWordNetWikidata\\nAutomatic identification\\nand data capture  \\nSpeech recognitionSpeech segmentationSpeech synthesisNatural language generationOptical character recognition\\nTopic model \\nDocument classificationLatent Dirichlet allocationPachinko allocation\\nComputer-assisted\\nreviewing \\nAutomated essay scoringConcordancerGrammar checkerPredictive textPronunciation assessmentSpell checker\\nNatural language\\nuser interface\\nChatbotInteractive fiction (c.f. Syntax guessing)Question answeringVirtual assistantVoice user interface\\nRelated \\nFormal semanticsHallucinationNatural Language ToolkitspaCy\\nPortal:\\n Language\\nCategory: Software comparisons\\nThis page was last edited on 28 December 2024, at 21:13 (UTC).\\nText is available under the Creative Commons Attribution-ShareAlike 4.0 License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\\nPrivacy policy\\nAbout Wikipedia\\nDisclaimers\\nContact Wikipedia\\nCode of Conduct\\nDevelopers\\nStatistics\\nCookie statement\\nMobile view\"}, {\"url\": \"https://aibusiness.com/nlp/language-models\", \"title\": \"Language models recent news | AI Business\", \"content\": \"Language models recent news | AI Business AI Policy Recent in Responsible AI Generative AI Recent in Generative AI Language models are a type of artificial intelligence (AI) that are trained on massive amounts of text data. Generative AI Generative AI Generative AI Generative AI How Generative AI and Ambient IoT Can Make Products Talk How Generative AI and Ambient IoT Can Make Products Talk Generative AI Generative AI Most Read: OpenAI Releases DeepSeek Challenger Model; SoftBank, OpenAI Create Joint Venture to Advance AI in Japan Most Read: OpenAI Releases DeepSeek Challenger Model; SoftBank, OpenAI Create Joint Venture to Advance AI in Japan Generative AI Generative AI on the Edge: Implications for the Technology Industry Generative AI on the Edge: Implications for the Technology Industry\", \"score\": 0.7036111, \"raw_content\": \"Language models recent news | AI Business\\n\\nTechTarget and Informa Tech’s Digital Business Combine.TechTarget and Informa\\nTechTarget and Informa Tech’s Digital Business Combine.\\nTogether, we power an unparalleled network of 220+ online properties covering 10,000+ granular topics, serving an audience of 50+ million professionals with original, objective content from trusted sources. We help you gain critical insights and make more informed decisions across your business priorities.\\nIoT World TodayEventsPartner with usOmdia\\n\\nSTAY UPDATED\\n\\nSTAY UPDATED\\nML\\nRelated Topics\\n\\nDeep learning\\n\\nNeural networks\\n\\n\\nPredictive analytics\\n\\n\\nRecent in ML\\nSee All\\nthumbnailAutomation\\n7 Ways Emerging Technologies Power Super Bowl LIX7 Ways Emerging Technologies Power Super Bowl LIX\\nbyLiz Hughes\\nFeb 7, 2025\\n3 Min Read\\nThe Super Bowl trophy between helmets from the Kansas City Chiefs and the Philadelphia Eagles.ML\\nSuper Bowl AI-Predicted Winner: Philadelphia EaglesSuper Bowl AI-Predicted Winner: Philadelphia Eagles\\nbyChuck Martin\\nFeb 7, 2025\\n2 Min Read\\nNLP\\nRelated Topics\\n\\nLanguage models\\n\\nSpeech recognition\\n\\n\\nChatbots\\n\\n\\nRecent in NLP\\nSee All\\nA team of five people chatting in an officeHealth Care\\nAI and the Art of LeadershipAI and the Art of Leadership\\nbyChantel Cohen\\nFeb 6, 2025\\n5 Min Read\\nBookshelves filled with folders full of documentsComputer Vision\\nFocused AI Is the Future of Enterprise Document ProcessingFocused AI Is the Future of Enterprise Document Processing\\nbyBhavani Vangala\\nFeb 5, 2025\\n8 Min Read\\nData\\nRelated Topics\\n\\nData science\\n\\nData analytics\\n\\n\\nData management\\n\\nSynthetic data\\n\\nRecent in Data\\nSee All\\nA sleeping woman wearing a smart watch superimposed with images representing health trackingHealth Care\\nBringing AI to Bed With YouBringing AI to Bed With You\\nbyMikael Kågebäck\\nFeb 6, 2025\\n3 Min Read\\nQuarterback Patrick Mahomes #15 of the Kansas City Chiefs celebrates after defeating the Buffalo Bills during the AFC Championship game on Jan. 26.Data\\nSuper Bowl Ball Spotting Waits for New TechnologySuper Bowl Ball Spotting Waits for New Technology\\nbyJohn Yellig\\nFeb 6, 2025\\n3 Min Read\\nAutomation\\nRelated Topics\\n\\n\\nRobotic process automation\\n\\n\\nIntelligent automation\\n\\n\\nRecent in Automation\\nSee All\\nthumbnailAutomation\\n7 Ways Emerging Technologies Power Super Bowl LIX7 Ways Emerging Technologies Power Super Bowl LIX\\nbyLiz Hughes\\nFeb 7, 2025\\n3 Min Read\\nA white Waabi, Volvo self-driving truck on the roadAutomation\\nAI Company, Volvo Team to Develop Self-Driving TrucksAI Company, Volvo Team to Develop Self-Driving Trucks\\nbyGraham Hope\\nFeb 6, 2025\\n2 Min Read\\nVerticals\\nRelated Topics\\n\\nIT\\nRobotics\\nCloud Computing\\nCybersecurity\\nEdge Computing\\nMetaverse\\n\\nData Centers\\n\\n\\nIoT\\n\\nQuantum Computing\\nIndustrials / Manufacturing\\nConsumer Tech\\nHealth Care\\nFinance\\nEnergy\\n\\nRecent in Verticals\\nSee All\\nthumbnailAutomation\\n7 Ways Emerging Technologies Power Super Bowl LIX7 Ways Emerging Technologies Power Super Bowl LIX\\nbyLiz Hughes\\nFeb 7, 2025\\n3 Min Read\\nOpenAI CEO Sam AltmanGenerative AI\\nMost Read: OpenAI Releases DeepSeek Challenger Model; SoftBank, OpenAI Create Joint Venture to Advance AI in JapanMost Read: OpenAI Releases DeepSeek Challenger Model; SoftBank, OpenAI Create Joint Venture to Advance AI in Japan\\nbyBerenice Baker\\nFeb 7, 2025\\n3 Min Read\\nResponsible AI\\nRelated Topics\\n\\nAI Policy\\n\\nData Governance\\n\\n\\nExplainable AI\\n\\nAI Ethics\\n\\nRecent in Responsible AI\\nSee All\\nBuilding 10 on the campus of Massachusetts Institute of Technology in Cambridge, Massachusetts.Responsible AI\\nMIT Launches Generative AI Impact ConsortiumMIT Launches Generative AI Impact Consortium\\nbyHeidi Vella\\nFeb 6, 2025\\n2 Min Read\\nA hand pointing at a workflow diagram  Generative AI\\nOrchestrating AI Agents: The Key to Unlocking Enterprise Efficiency and GrowthOrchestrating AI Agents: The Key to Unlocking Enterprise Efficiency and Growth\\nbyDorit Zilbershot\\nFeb 4, 2025\\n6 Min Read\\nGenerative AI\\nRelated Topics\\n\\nFoundation Models\\n\\nRecent in Generative AI\\nSee All\\nOpenAI CEO Sam AltmanGenerative AI\\nMost Read: OpenAI Releases DeepSeek Challenger Model; SoftBank, OpenAI Create Joint Venture to Advance AI in JapanMost Read: OpenAI Releases DeepSeek Challenger Model; SoftBank, OpenAI Create Joint Venture to Advance AI in Japan\\nbyBerenice Baker\\nFeb 7, 2025\\n3 Min Read\\nThree executives stand around a table  Generative AI\\nFrom Hype to Mastery: The Path to AI MaturityFrom Hype to Mastery: The Path to AI Maturity\\nbyRobert Harrington\\nFeb 7, 2025\\n5 Min Read\\nMore\\nRelated Topics\\n\\nPodcasts\\nWebinars\\nEbooks\\nVideos\\nEvents\\n\\nWhite Papers\\n\\n\\nOmdia\\n\\nGenerate leads with us\\nAI Business TV London 2022\\nTech TV Austin 2022\\nAI Business TV New York 2022\\n\\nAI Business TV London 2023\\n\\nHome\\nNLP\\nLanguage models\\n\\nLanguage models\\nLanguage models are a type of artificial intelligence (AI) that are trained on massive amounts of text data. This allows them to generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way. In recent years, language models have become increasingly powerful and sophisticated. Get the latest information on LLM's here at AI Business\\nA team of five people chatting in an officeHealth Care\\nAI and the Art of LeadershipAI and the Art of Leadership4 tips for using AI alongside emotional intelligence to leverage efficiency and foster connection\\n\\nbyChantel Cohen, Founder and CEO of CWC Coaching & Therapy\\nFeb 6, 2025\\n1 Min Read\\nBookshelves filled with folders full of documentsComputer Vision\\nFocused AI Is the Future of Enterprise Document ProcessingFocused AI Is the Future of Enterprise Document Processing\\nbyBhavani Vangala\\nFeb 5, 2025\\n1 Min Read\\nthumbnailNLP\\nSoftBank, OpenAI Create Joint Venture to Advance AI in JapanSoftBank, OpenAI Create Joint Venture to Advance AI in Japan\\nbyLiz Hughes\\nFeb 3, 2025\\n1 Min Read\\nA small screen with the DeepSeek logo in front of a larger screen with the ChatGPT logoGenerative AI\\nOpenAI Takes Aim at DeepSeek With New, Free ModelOpenAI Takes Aim at DeepSeek With New, Free Model\\nbyYiannis Antoniou\\nFeb 3, 2025\\n1 Min Read\\nOpenAI CEO Sam AltmanGenerative AI\\nOpenAI Releases DeepSeek Challenger ModelOpenAI Releases DeepSeek Challenger Model\\nbyBerenice Baker\\nFeb 3, 2025\\n1 Min Read\\nThe White HouseNLP\\nOpenAI Launches ChatGPT Gov for US Government AgenciesOpenAI Launches ChatGPT Gov for US Government Agencies\\nbyBerenice Baker\\nJan 29, 2025\\n1 Min Read\\n\\nNLP\\nDeepSeek Hit by Cyberattack; Limits Registrations\\nDeepSeek Hit by Cyberattack; Limits Registrations\\nJan 28, 2025\\n|\\n1 Min Read\\n\\nbyLiz Hughes, Editor, IoT World Today and AI Business\\n\\nNLP\\nIndustry Weighs in on DeepSeek Launch\\nIndustry Weighs in on DeepSeek Launch\\nJan 28, 2025\\n|\\n1 Min Read\\n\\nbyBerenice Baker, Editor\\n\\nNLP\\nChinese Startup DeepSeek Launches; Competes With OpenAI\\nChinese Startup DeepSeek Launches; Competes With OpenAI\\nJan 27, 2025\\n|\\n1 Min Read\\n\\nbyLiz Hughes, Editor, IoT World Today and AI Business\\n\\nFinance\\nNative AI on Horizon for Finance, Accounting Teams\\nNative AI on Horizon for Finance, Accounting Teams\\nJan 27, 2025\\n|\\n1 Min Read\\n\\nbyCharis Thomas, Chris Tredwell\\n\\nGenerative AI\\nDo You Need a Personal AI Chatbot?\\nDo You Need a Personal AI Chatbot?\\nJan 23, 2025\\n|\\n1 Min Read\\n\\nbyZain Jaffer, CEO and founder, Zain Ventures\\n\\nAutomation\\nAI Set to Transform Legal Services in 2025\\nAI Set to Transform Legal Services in 2025\\nJan 22, 2025\\n|\\n1 Min Read\\n\\nbyPaul Gaskell, Chief technology officer, Avantia Law\\n\\nGenerative AI\\nArtificial General Intelligence: EY on the Short-Term Future\\nArtificial General Intelligence: EY on the Short-Term Future\\nJan 22, 2025\\n|\\n1 Min Read\\n\\nbyBerenice Baker, Editor\\n\\nLanguage models\\nAI Model Scaling Isn’t Over: It’s Entering a New Era\\nAI Model Scaling Isn’t Over: It’s Entering a New Era\\nJan 21, 2025\\n|\\n1 Min Read\\n\\nbyAkash Sharma, CEO, Vellum\\n\\nLanguage models\\nWhy Agnostic AI Is the Key To Cost-Efficient, Scalable AI Solutions\\nWhy Agnostic AI Is the Key To Cost-Efficient, Scalable AI Solutions\\nJan 16, 2025\\n|\\n1 Min Read\\n\\nbyKasia Borowska, Managing director at Brainpool AI\\n\\nLanguage models\\nFourth Industrial Revolution: How AI Agents Are Transforming the Future of Work\\nFourth Industrial Revolution: How AI Agents Are Transforming the Future of Work\\nJan 8, 2025\\n|\\n1 Min Read\\n\\nbyKarli Kalpala, Head of UK & Ireland and strategic transformation, Digital Workforce\\n\\nNLP\\nSamsung Harman’s AI Will Make Cars More Empathetic\\nSamsung Harman’s AI Will Make Cars More Empathetic\\nJan 8, 2025\\n|\\n1 Min Read\\n\\nbyGraham Hope, Contributing Writer\\n\\nResponsible AI\\nAI vs. AI: Technologies That Help Guard Against AI-Generated Scams\\nAI vs. AI: Technologies That Help Guard Against AI-Generated Scams\\nJan 2, 2025\\n|\\n1 Min Read\\n\\nbyJames Stokes, Head of enterprise, U.K. and Nordics at Infobip\\n\\nResponsible AI\\nA Sustainable AI Future Needs Community Data Protection\\nA Sustainable AI Future Needs Community Data Protection\\nDec 30, 2024\\n|\\n1 Min Read\\n\\nbyEllen Brandenberger, senior director of product innovation, Stack Overflow\\n\\nVerticals\\nAI Could Ease Record Holiday Travel Disruptions\\nAI Could Ease Record Holiday Travel Disruptions\\nDec 27, 2024\\n|\\n1 Min Read\\n\\nbyAnna Jaffe, CEO of Mobi.AI\\n\\nChatbots\\nSoftware Will Learn to Work With You in 2025\\nSoftware Will Learn to Work With You in 2025\\nDec 24, 2024\\n|\\n1 Min Read\\n\\nbyBurley Kawasaki, Global VP of product marketing and strategy at Creatio\\n\\nLanguage models\\nSandboxAQ Secures $300M to Drive Large Quantitative Model Innovation\\nSandboxAQ Secures $300M to Drive Large Quantitative Model Innovation\\nDec 23, 2024\\n|\\n1 Min Read\\n\\nbyBerenice Baker, Editor\\n\\nData Centers\\nMost Read: Meta to Open $10B AI Data Center in Louisiana; How IBM is Using AI to Disrupt Consultancy\\nMost Read: Meta to Open $10B AI Data Center in Louisiana; How IBM is Using AI to Disrupt Consultancy\\nDec 20, 2024\\n|\\n1 Min Read\\n\\nbyBerenice Baker, Editor\\n\\nNLP\\nAI-Santa Demonstrates New Conversational Video\\nAI-Santa Demonstrates New Conversational Video\\nDec 20, 2024\\n|\\n1 Min Read\\n\\nbyHeidi Vella, Contributing Writer\\n\\nNLP\\nDigitas’ Louis Vainquer on Large Language Models at AI Summit New York\\nDigitas’ Louis Vainquer on Large Language Models at AI Summit New York\\nDec 19, 2024\\n|\\n1 Min Read\\n\\nNLP\\nAmazon Launches New Generation of LLM Foundation Models\\nAmazon Launches New Generation of LLM Foundation Models\\nDec 16, 2024\\n|\\n1 Min Read\\n\\nbyHeidi Vella, Contributing Writer\\n\\nLanguage models\\nThe Role of Large Quantitative Models in Drug Discovery\\nThe Role of Large Quantitative Models in Drug Discovery\\nDec 12, 2024\\n|\\n1 Min Read\\n\\nbyBerenice Baker, Editor\\n\\nResponsible AI\\nLeaders Divided on Level of Governance Needed for Responsible AI\\nLeaders Divided on Level of Governance Needed for Responsible AI\\nDec 9, 2024\\n|\\n1 Min Read\\n\\nbyHeidi Vella, Contributing Writer\\n\\nLanguage models\\nMost Read: Agentic AI Set to Rise, With New Cybersecurity Risks: Gartner; Finance Operations Lean Heavily on AI: KPMG\\nMost Read: Agentic AI Set to Rise, With New Cybersecurity Risks: Gartner; Finance Operations Lean Heavily on AI: KPMG\\nDec 5, 2024\\n|\\n1 Min Read\\n\\nbyBerenice Baker, Editor\\n\\nGenerative AI\\nAI-Powered Holograms Converse at AWS re:Invent 2024\\nAI-Powered Holograms Converse at AWS re:Invent 2024\\nDec 4, 2024\\n|\\n1 Min Read\\n\\nbyBerenice Baker, Editor\\n\\nGenerative AI\\nHow Generative AI and Ambient IoT Can Make Products Talk\\nHow Generative AI and Ambient IoT Can Make Products Talk\\nDec 3, 2024\\n|\\n1 Min Read\\n\\nbyIlan Lifshitz, vice president of research and development at Wiliot\\n\\nAutomation\\nAgentic AI Set to Rise, With New Cybersecurity Risks: Gartner\\nAgentic AI Set to Rise, With New Cybersecurity Risks: Gartner\\nDec 2, 2024\\n|\\n1 Min Read\\n\\nbyHeidi Vella, Contributing Writer\\n\\nData\\nHow AI Can Lead to Operational Transformation in Smaller US Companies\\nHow AI Can Lead to Operational Transformation in Smaller US Companies\\nNov 25, 2024\\n|\\n1 Min Read\\n\\nbyMariano Jurich, Project and product manager at Making Sense\\n\\nNLP\\nXPeng Launches AI-Defined P7+ Electric SUV to Rival Tesla\\nXPeng Launches AI-Defined P7+ Electric SUV to Rival Tesla\\nNov 21, 2024\\n|\\n1 Min Read\\n\\nbyGraham Hope, Contributing Writer\\n\\nGenerative AI\\nUnlocking the True Potential of Mobile AI\\nUnlocking the True Potential of Mobile AI\\nNov 18, 2024\\n|\\n1 Min Read\\n\\nbyMohan Varthakavi, Vice president of software development, AI and edge, Couchbase\\n\\nPrevious\\n1\\n2\\n3\\n4\\n5\\n…\\n32\\nNext\\n\\nLatest News\\nthumbnailAutomation\\n7 Ways Emerging Technologies Power Super Bowl LIX7 Ways Emerging Technologies Power Super Bowl LIX\\nbyLiz Hughes\\nFeb 7, 2025\\n3 Min Read\\nOpenAI CEO Sam AltmanGenerative AI\\nMost Read: OpenAI Releases DeepSeek Challenger Model; SoftBank, OpenAI Create Joint Venture to Advance AI in JapanMost Read: OpenAI Releases DeepSeek Challenger Model; SoftBank, OpenAI Create Joint Venture to Advance AI in Japan\\nbyBerenice Baker\\nFeb 7, 2025\\n3 Min Read\\nThe Super Bowl trophy between helmets from the Kansas City Chiefs and the Philadelphia Eagles.ML\\nSuper Bowl AI-Predicted Winner: Philadelphia EaglesSuper Bowl AI-Predicted Winner: Philadelphia Eagles\\nbyChuck Martin\\nFeb 7, 2025\\n2 Min Read\\nSign Up for the Newsletter\\nThe most up-to-date AI news and insights delivered right to your inbox!\\nStay Updated!\\nTrending articles\\n\\nAutomation\\n7 Ways Emerging Technologies Power Super Bowl LIX\\n7 Ways Emerging Technologies Power Super Bowl LIX\\nFeb 7, 2025\\nOpenAI CEO Sam Altman\\nGenerative AI\\nMost Read: OpenAI Releases DeepSeek Challenger Model; SoftBank, OpenAI Create Joint Venture to Advance AI in Japan\\nMost Read: OpenAI Releases DeepSeek Challenger Model; SoftBank, OpenAI Create Joint Venture to Advance AI in Japan\\nFeb 7, 2025\\nThree executives stand around a table  \\nGenerative AI\\nFrom Hype to Mastery: The Path to AI Maturity\\nFrom Hype to Mastery: The Path to AI Maturity\\nFeb 7, 2025\\nRobots and a supervisor in a smart factory\\nEdge Computing\\nGenerative AI on the Edge: Implications for the Technology Industry\\nGenerative AI on the Edge: Implications for the Technology Industry\\nFeb 7, 2025\\nLatest podcasts\\n\\nUnlocking Networking Potential With Generative AIApr 17, 2024\\nEmbedding AI in the Enterprise With IBM's WatsonxApr 4, 2024\\nReshaping Customer Experiences with AIMar 7, 2024\\nGenerative AI Journeys with CDW UK's Chief TechnologistFeb 28, 2024\\n\\nSee all\\nSponsored Content\\n\\nSponsored by Google Cloud\\nChoosing Your First Generative AI Use Cases -------------------------------------------\\nTo get started with generative AI, first focus on areas that can improve human experiences with information.\\nRead More\\nSign Up for the Newsletter\\nThe most up-to-date AI news and insights delivered right to your inbox!\\nStay Updated!\\n\\nDiscover more\\nThe AI Summit SeriesApplied Intelligence Live!AI Research and ConsultingOur Solutions\\nWorking with us\\nAdvertise\\nCommunicate\\nContact usAbout us\\nJoin Us\\nSTAY UPDATED\\nFollow Us\\n\\n\\n\\nCopyright © 2025. This website is owned and operated by Informa TechTarget, part of a global network that informs, influences and connects the world’s technology buyers and sellers. All copyright resides with them. Informa PLC’s registered office is 5 Howick Place, London SW1P 1WG. Registered in England and Wales. TechTarget, Inc.’s registered office is 275 Grove St. Newton, MA 02466.\\nHome|About us|Contact|Cookie Policy|Terms of Use\"}, {\"url\": \"https://www.algolia.com/blog/ai/examples-of-best-large-language-models\", \"title\": \"A definitive list of large language models (LLMs) | Algolia\", \"content\": \"A definitive list of large language models (LLMs) | Algolia Together, these large models are significantly impacting the field of natural language processing (NLP) and demonstrating remarkable data-science capabilities in understanding and generating human language. Through layers of attention mechanisms and normalization mechanisms, transformer models empower LLMs to cut through the complexities of language, providing the ability to generate text that’s not only grammatically correct but contextually relevant and meaningful. BLOOM (BigScience Large Open-science Open-access Multilingual Language Model), a multilingual LLM created mainly for text generation by Hugging Face. Llama (Large Language Model Meta AI): a multiversion LLM with performance similar to GPT-3. Large language models make search results more accurate, too.\", \"score\": 0.6711479, \"raw_content\": null}, {\"url\": \"https://www.shakudo.io/blog/top-9-large-language-models\", \"title\": \"Top 9 Large Language Models as of Feburary 2025 - Shakudo\", \"content\": \"The latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. The latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. The latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data.\", \"score\": 0.6485337, \"raw_content\": \"Top 9 Large Language Models as of Feburary 2025 | Shakudo\\nLatest in Insights : When to Choose Deep Learning Over Machine Learning (And Vice Versa)\\n\\n\\nWhy SHakudo\\n\\n Data & AI OS Build your ideal data stack on one unified platform Learn more >\\nshakudo AI Applications\\n Text to SQL Workflow Automation Vector Database Reverse ETLSee all >\\nComponents\\nSolutions\\n\\nShakudo for Industries\\nAerospace\\nAutomotive & Transportation\\nClimate & Energy\\nFinancial Services\\nHealthcare & Life Sciences\\nHigher Education\\nManufacturing\\nReal Estate\\nRetail\\nSports\\nTechnology & Software\\nShakudo Use Cases\\nAutomate Custom Sustainability Report Population\\nChat with Enterprise Knowledge Base Using AI Assistants\\nGenerate Real-World Evidence for Healthcare Decisions\\nOptimize Ticket Pricing with Dynamic Demand Modeling\\nDetect Hidden Red Flags in Company Data\\nMonitor Market Sentiment Across Multiple Sources\\nSee all >\\nResources\\n\\n Case Studies Learn how leading companies leverage data & AI on Shakudo blog Read what's new at Shakudo and the data and AI world white papers Access in-depth reports and guides on data & AI solutions Docs Explore comprehensive guides on the Shakudo platform\\n  Case Study How CentralReach uses Shakudo to Cut Time-To-Deployment to Launch New AI- Powered Solutions\\n  Case Study How AI is Changing the Game for the Cleveland Cavaliers\\nCompany\\n\\n ABout Us Learn about our mission and values Careers Join us in building the next-gen data stack Partners Learn about the relationships that make it happen Contact us Have a question? We're here to help\\nAI WorkshopGet a Demo\\n\\n← Back to Blog\\nInsights\\nTop 9 Large Language Models as of Feburary 2025\\nAuthor(s):\\n\\nNo items found.\\nUpdated on:\\nFebruary 7, 2025\\n\\nTable of contents\\nExample H2\\nExample H3\\nMentioned Components\\nNo items found.\\n<>\\nGet the latest updates in Data & AI straight to your inboxWe’ll email you once per week—and never share your information.\\n🎉 Success! You're now signed up for the Shakudo newsletter.\\nOops! Something went wrong while submitting the form.\\nIntroduction\\nIf we had to choose one word to describe the rapid evolution of AI today, it would probably be something along the lines of explosive. As predicted by the Market Research Future report, the large language model (LLM) market in North America alone is expected to reach $105.5 billion by 2030. The exponential growth of AI tools combined with access to massive troves of text data has opened gates for better and more advanced content generation than we had ever hoped. Yet, such rapid expansion also makes it harder than ever to navigate and select the right tools among the diverse LLM models available.\\nThe goal of this post is to keep you, the AI enthusiast and professional, up-to-date with current trends and essential innovations in the field. Below, we highlighted the top 9 LLMs that we think are currently making waves in the industry, each with distinct capabilities and specialized strengths, excelling in areas such as natural language processing, code synthesis, few-shot learning, or scalability. While we believe there is no one-size-fits-all LLM for every use case, we hope that this list can help you identify the most current and well-suited LLM model that meets your business’s unique requirements.\\n1. GPT\\n\\nOur list kicks off with OpenAI's Generative Pre-trained Transformer (GPT) models, which have consistently exceeded their previous capabilities with each new release. Compared to its prior models, the latest ChatGPT-4o and ChatGPT-4o mini models offer significantly faster processing speeds and enhanced capabilities across text, voice, and vision.\\nThe latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. Both of these models are equipped with multimodal capabilities to handle images as well as audio data.\\nDespite having advanced conversational and reasoning capabilities, note that GPT is a proprietary model, meaning that the training data and parameters are kept confidential by OpenAI, and access to full functionality is restricted–a commercial license or subscription is often required to unlock the complete range of features. In this case, we recommend this model for businesses looking to adopt an LLM that excels in conversational dialogue, multi-step reasoning, efficient computation, and real-time interactions without the constraints of a budget.\\nFor companies who are curious to try out the proprietary models on the market before fully committing to one due to budget constraints or uncertainties about its long-term integration, Shakudo offers a compelling alternative. Our platform currently features a diverse selection of advanced LLMs with simplified deployment and scalability. With a simple subscription, you can access and assess the value of proprietary models, like GPT, before making a substantial investment.\\n2.DeepSeek\\n\\nDeepseek-R1 Benchmark. Source: deepseek.com\\nWith its latest R1 model, the Chinese AI company DeepSeek has once again set new benchmarks for innovation in the AI community. As of January 24th, the DeepSeek-R1 model is ranked fourth on Chatbot Arena, and top as the best open-source LM.\\nThe DeepSeek-R1 is a 671B parameter Mixture-of-Experts (MoE) model with 37B activated parameters per token, trained through large-scale reinforcement learning with a strong focus on reasoning capabilities. The model excels at understanding and handling long-form content and demonstrates superior performance in complex tasks such as mathematics and code generation. The model is approximately 30 times more cost-efficient than OpenAI-o1 and 5 times faster, offering groundbreaking performance at a fraction of the cost. Moreover, it has shown exceptional precision in tasks requiring complex pattern recognition, such as genomic data analysis, medical imaging, and large-scale scientific simulations.\\nDeepSeek-R1’s capabilities are transformative when it comes to integration with proprietary enterprise data such as PII and financial records. Leveraging retrieval-augmented generation (RAG), enterprises can connect the model to their internal data sources to enable highly personalized, context-aware interactions—all while maintaining stringent security and compliance standards. With Shakudo, you can streamline the deployment and integration of advanced AI models like DeepSeek by automating the setup, deployment, and management processes. This eliminates the need for businesses to invest in and maintain extensive computing infrastructure. By operating within your existing infrastructure, the platform ensures seamless integration, enhanced security, and optimal performance without requiring significant in-house resources or specialized expertise.\\n3. Qwen\\n\\n‍\\nAlibaba released Qwen2.5-Max earlier last month, the new model designed to deliver enhanced performance for large-scale natural language processing tasks. In instruct model evaluations, this new model outperforms DeepSeek V3 in benchmarks such as Arena-Hard, LiveBench, LiveCodeBench, and GPQA-Diamond, while also delivering strong performance in other assessments like MMLU-Pro.\\nQwen2.5-Max is pretrained on over 20 trillion tokens. While specific details about its parameter count and token window size are not publicly disclosed, Qwen2.5-Max is designed for low-latency, high-efficiency tasks, making it suitable for applications requiring quick, accurate responses with low latency. Its smaller size allows for deployment on devices with limited computational resources.\\nFor businesses and users looking for higher-performance models for natural language processing and AI-driven tasks, the Qwen 2.5 model is available on platforms like Hugging Face and ModelScope. This model boasts from 0.5 billion to 72 billion parameters, featuring context windows of up to 128,000 tokens, and is excellent for code generation, debugging, and automated forecasting.\\n4. LG AI\\n\\nsource\\nEXAONE 3.0 is a bilingual LLM developed by LG AI Research. The company released its latest model in December 2024 with excellent performance across various benchmarks and real-world applications.\\nWith 7.8 billion parameters, EXAONE 3.0 is capable of understanding and generating human-like text in multiple languages across complex domains, including coding, mathematics, patents, and chemistry. This model has also been optimized to reduce inference processing time by 56%, memory usage by 35%, and operating costs by 72%, ensuring that it remains cost-effective while maintaining high performance.\\nLG AI Research has open-sourced the 7.8B parameter instruction-tuned version of EXAONE 3.0 for non-commercial research purposes. This is a model we recommend to software companies or tech startups for generating Python code, assisting developers in troubleshooting, and creating APIs or other backend components.\\n5. LlaMA\\n\\nMeta is still leading the front with their state-of-the-art LlaMa models. The company released its latest LlaMA 3.3 model in December 2024, featuring multimodal capabilities that can process both text and image for in-depth analysis and response generation, such as interpreting charts, maps, or translating texts identified in an image.\\nLlaMA 3.3 improves on previous models with a longer context window of up to 128,000 tokens and an optimized transformer architecture. With a parameter of 70 billion, this model outperforms open-source and proprietary alternatives in areas such as multilingual dialogue, reasoning, and coding.\\nUnlike ChatGPT models, LlaMA 3 is open-source, giving users the flexibility to access and deploy freely on their cloud depending on the specific requirements of their infrastructure, security preferences, or customization needs. We recommend this model to businesses looking for advanced content generation and language understanding, such as those in customer service, education, marketing, and consumer markets. The openness of these models also allows for your greater control over the model’s performance, tuning, and integration into existing workflows.\\n6. Claude\\n\\nNext on our list is Claude, more specifically, the latest Claude 3.5 Sonnet model developed by Anthropic. We believe that Claude is arguably one of the most significant competitors to GPT since all of its current models–Claude 3 Haiku, Claude 3.5 Sonnet, and Claude 3 Opus–are designed with incredible contextual understanding capabilities that position themselves as the top conversational AI closely aligned with nuanced human interactions.\\nWhile the specific parameters of Claude 3.5 Sonnet remain undisclosed, the model boasts an impressive context window of 200,000 tokens, equivalent to approximately 150,000 words or 300 pages of text.\\nThe current Claude subscription service is credit-based, and the cost can go as high as $2,304/month for enterprise plans tailored to high-volume users. We recommend Claude to mature or mid-stage businesses looking not only to adopt an AI that facilitates human-like interactions but also to enhance their coding capabilities since the Claude 3.5 Sonnet model is currently reaching a 49.0% performance score on the SWE-bench Verified benchmark, placing it as third among all publicly available models, including reasoning models and systems specifically designed for agentic coding.\\n7. Mistral\\n\\nMistral's latest model – Mistral Small 3, a latency-optimized model was released under the Apache 2.0 license at the end of January. This 24-billion-parameter model is designed for low-latency, high-efficiency tasks. It processes approximately 150 tokens per second, making it over three times faster than Llama 3.3 70B on the same hardware.\\nThis new model is ideal for applications requiring quick, accurate responses with low latency, such as virtual assistants, real-time data processing, and on-device command and control. Its smaller size allows for deployment on devices with limited computational resources.\\nMistral Small 3 is currently open-source under the Apache 2.0 license. This means you can freely access and use the model for your own applications, provided you comply with the license terms. Since it is designed to be easily deployable, including on hardware with limited resources like a single GPU or even a MacBook with 32GB RAM, we'd recommend this to early-stage businesses looking to implement low-latency AI solutions without the need for extensive hardware infrastructure.\\n8. Gemini\\n\\nGemini is a family of closed-source LLM models developed by Google. The latest model—Gemini 2.0 Flash—operates at twice the speed of Gemini 1.5 Pro, offering substantial improvements in speed, reasoning, and multimodal processing capabilities.\\nWith that being said, Gemini remains a proprietary model; if your company deals with sensitive or confidential data regularly, you might be concerned about sending it to external servers due to security reasons. To address this concern, we recommend that you double-check vendor compliance regulations to ensure data privacy and security standards are met, such as adherence to GDPR, HIPAA, or other relevant data protection laws.\\nIf you’re looking for an open-source alternative that exhibits capabilities almost as good as Gemini, Google’s latest Gemma model, Gemma 2, offers three models available in 2 billion, 9 billion, and 27 billion parameters with a context window of 8,200. For businesses looking for a rather economic option, this is the optimal choice that interprets and understands messages with remarkable accuracy.\\n9. Command\\n\\nCommand R is a family of scalable models developed by Cohere with the goal of balancing high performance with strong accuracy, just like Claude. Both the Command R and Command R+ models offer APIs specifically optimized for Retrieval Augmented Generation (RAG). This means that these models can combine large-scale language generation with real-time information retrieval techniques for much more contextually aware outputs.\\nCurrently, the Command R+ model boasts 104 billion parameters and offers an industry-leading 128,000 token context window for enhanced long-form processing and multi-turn conversation capabilities.\\nOne of the perks of working with an open-source model is also to avoid vendor lock-in. Heavy reliance on a particular type of proprietary model may make it difficult for you to switch to alternative models when your business starts growing or the landscape changes. Cohere approaches this in a hybrid way, meaning that you can access and modify the model for personal usage but need a license for commercial use. In this case, we recommend this model for businesses that want flexibility in experimentation without a long-term commitment to a single vendor.\\n\\nWhitepaper\\nIntroduction\\nIf we had to choose one word to describe the rapid evolution of AI today, it would probably be something along the lines of explosive. As predicted by the Market Research Future report, the large language model (LLM) market in North America alone is expected to reach $105.5 billion by 2030. The exponential growth of AI tools combined with access to massive troves of text data has opened gates for better and more advanced content generation than we had ever hoped. Yet, such rapid expansion also makes it harder than ever to navigate and select the right tools among the diverse LLM models available.\\nThe goal of this post is to keep you, the AI enthusiast and professional, up-to-date with current trends and essential innovations in the field. Below, we highlighted the top 9 LLMs that we think are currently making waves in the industry, each with distinct capabilities and specialized strengths, excelling in areas such as natural language processing, code synthesis, few-shot learning, or scalability. While we believe there is no one-size-fits-all LLM for every use case, we hope that this list can help you identify the most current and well-suited LLM model that meets your business’s unique requirements.\\n1. GPT\\n\\nOur list kicks off with OpenAI's Generative Pre-trained Transformer (GPT) models, which have consistently exceeded their previous capabilities with each new release. Compared to its prior models, the latest ChatGPT-4o and ChatGPT-4o mini models offer significantly faster processing speeds and enhanced capabilities across text, voice, and vision.\\nThe latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. Both of these models are equipped with multimodal capabilities to handle images as well as audio data.\\nDespite having advanced conversational and reasoning capabilities, note that GPT is a proprietary model, meaning that the training data and parameters are kept confidential by OpenAI, and access to full functionality is restricted–a commercial license or subscription is often required to unlock the complete range of features. In this case, we recommend this model for businesses looking to adopt an LLM that excels in conversational dialogue, multi-step reasoning, efficient computation, and real-time interactions without the constraints of a budget.\\nFor companies who are curious to try out the proprietary models on the market before fully committing to one due to budget constraints or uncertainties about its long-term integration, Shakudo offers a compelling alternative. Our platform currently features a diverse selection of advanced LLMs with simplified deployment and scalability. With a simple subscription, you can access and assess the value of proprietary models, like GPT, before making a substantial investment.\\n2.DeepSeek\\n\\nDeepseek-R1 Benchmark. Source: deepseek.com\\nWith its latest R1 model, the Chinese AI company DeepSeek has once again set new benchmarks for innovation in the AI community. As of January 24th, the DeepSeek-R1 model is ranked fourth on Chatbot Arena, and top as the best open-source LM.\\nThe DeepSeek-R1 is a 671B parameter Mixture-of-Experts (MoE) model with 37B activated parameters per token, trained through large-scale reinforcement learning with a strong focus on reasoning capabilities. The model excels at understanding and handling long-form content and demonstrates superior performance in complex tasks such as mathematics and code generation. The model is approximately 30 times more cost-efficient than OpenAI-o1 and 5 times faster, offering groundbreaking performance at a fraction of the cost. Moreover, it has shown exceptional precision in tasks requiring complex pattern recognition, such as genomic data analysis, medical imaging, and large-scale scientific simulations.\\nDeepSeek-R1’s capabilities are transformative when it comes to integration with proprietary enterprise data such as PII and financial records. Leveraging retrieval-augmented generation (RAG), enterprises can connect the model to their internal data sources to enable highly personalized, context-aware interactions—all while maintaining stringent security and compliance standards. With Shakudo, you can streamline the deployment and integration of advanced AI models like DeepSeek by automating the setup, deployment, and management processes. This eliminates the need for businesses to invest in and maintain extensive computing infrastructure. By operating within your existing infrastructure, the platform ensures seamless integration, enhanced security, and optimal performance without requiring significant in-house resources or specialized expertise.\\n3. Qwen\\n\\n‍\\nAlibaba released Qwen2.5-Max earlier last month, the new model designed to deliver enhanced performance for large-scale natural language processing tasks. In instruct model evaluations, this new model outperforms DeepSeek V3 in benchmarks such as Arena-Hard, LiveBench, LiveCodeBench, and GPQA-Diamond, while also delivering strong performance in other assessments like MMLU-Pro.\\nQwen2.5-Max is pretrained on over 20 trillion tokens. While specific details about its parameter count and token window size are not publicly disclosed, Qwen2.5-Max is designed for low-latency, high-efficiency tasks, making it suitable for applications requiring quick, accurate responses with low latency. Its smaller size allows for deployment on devices with limited computational resources.\\nFor businesses and users looking for higher-performance models for natural language processing and AI-driven tasks, the Qwen 2.5 model is available on platforms like Hugging Face and ModelScope. This model boasts from 0.5 billion to 72 billion parameters, featuring context windows of up to 128,000 tokens, and is excellent for code generation, debugging, and automated forecasting.\\n4. LG AI\\n\\nsource\\nEXAONE 3.0 is a bilingual LLM developed by LG AI Research. The company released its latest model in December 2024 with excellent performance across various benchmarks and real-world applications.\\nWith 7.8 billion parameters, EXAONE 3.0 is capable of understanding and generating human-like text in multiple languages across complex domains, including coding, mathematics, patents, and chemistry. This model has also been optimized to reduce inference processing time by 56%, memory usage by 35%, and operating costs by 72%, ensuring that it remains cost-effective while maintaining high performance.\\nLG AI Research has open-sourced the 7.8B parameter instruction-tuned version of EXAONE 3.0 for non-commercial research purposes. This is a model we recommend to software companies or tech startups for generating Python code, assisting developers in troubleshooting, and creating APIs or other backend components.\\n5. LlaMA\\n\\nMeta is still leading the front with their state-of-the-art LlaMa models. The company released its latest LlaMA 3.3 model in December 2024, featuring multimodal capabilities that can process both text and image for in-depth analysis and response generation, such as interpreting charts, maps, or translating texts identified in an image.\\nLlaMA 3.3 improves on previous models with a longer context window of up to 128,000 tokens and an optimized transformer architecture. With a parameter of 70 billion, this model outperforms open-source and proprietary alternatives in areas such as multilingual dialogue, reasoning, and coding.\\nUnlike ChatGPT models, LlaMA 3 is open-source, giving users the flexibility to access and deploy freely on their cloud depending on the specific requirements of their infrastructure, security preferences, or customization needs. We recommend this model to businesses looking for advanced content generation and language understanding, such as those in customer service, education, marketing, and consumer markets. The openness of these models also allows for your greater control over the model’s performance, tuning, and integration into existing workflows.\\n6. Claude\\n\\nNext on our list is Claude, more specifically, the latest Claude 3.5 Sonnet model developed by Anthropic. We believe that Claude is arguably one of the most significant competitors to GPT since all of its current models–Claude 3 Haiku, Claude 3.5 Sonnet, and Claude 3 Opus–are designed with incredible contextual understanding capabilities that position themselves as the top conversational AI closely aligned with nuanced human interactions.\\nWhile the specific parameters of Claude 3.5 Sonnet remain undisclosed, the model boasts an impressive context window of 200,000 tokens, equivalent to approximately 150,000 words or 300 pages of text.\\nThe current Claude subscription service is credit-based, and the cost can go as high as $2,304/month for enterprise plans tailored to high-volume users. We recommend Claude to mature or mid-stage businesses looking not only to adopt an AI that facilitates human-like interactions but also to enhance their coding capabilities since the Claude 3.5 Sonnet model is currently reaching a 49.0% performance score on the SWE-bench Verified benchmark, placing it as third among all publicly available models, including reasoning models and systems specifically designed for agentic coding.\\n7. Mistral\\n\\nMistral's latest model – Mistral Small 3, a latency-optimized model was released under the Apache 2.0 license at the end of January. This 24-billion-parameter model is designed for low-latency, high-efficiency tasks. It processes approximately 150 tokens per second, making it over three times faster than Llama 3.3 70B on the same hardware.\\nThis new model is ideal for applications requiring quick, accurate responses with low latency, such as virtual assistants, real-time data processing, and on-device command and control. Its smaller size allows for deployment on devices with limited computational resources.\\nMistral Small 3 is currently open-source under the Apache 2.0 license. This means you can freely access and use the model for your own applications, provided you comply with the license terms. Since it is designed to be easily deployable, including on hardware with limited resources like a single GPU or even a MacBook with 32GB RAM, we'd recommend this to early-stage businesses looking to implement low-latency AI solutions without the need for extensive hardware infrastructure.\\n8. Gemini\\n\\nGemini is a family of closed-source LLM models developed by Google. The latest model—Gemini 2.0 Flash—operates at twice the speed of Gemini 1.5 Pro, offering substantial improvements in speed, reasoning, and multimodal processing capabilities.\\nWith that being said, Gemini remains a proprietary model; if your company deals with sensitive or confidential data regularly, you might be concerned about sending it to external servers due to security reasons. To address this concern, we recommend that you double-check vendor compliance regulations to ensure data privacy and security standards are met, such as adherence to GDPR, HIPAA, or other relevant data protection laws.\\nIf you’re looking for an open-source alternative that exhibits capabilities almost as good as Gemini, Google’s latest Gemma model, Gemma 2, offers three models available in 2 billion, 9 billion, and 27 billion parameters with a context window of 8,200. For businesses looking for a rather economic option, this is the optimal choice that interprets and understands messages with remarkable accuracy.\\n9. Command\\n\\nCommand R is a family of scalable models developed by Cohere with the goal of balancing high performance with strong accuracy, just like Claude. Both the Command R and Command R+ models offer APIs specifically optimized for Retrieval Augmented Generation (RAG). This means that these models can combine large-scale language generation with real-time information retrieval techniques for much more contextually aware outputs.\\nCurrently, the Command R+ model boasts 104 billion parameters and offers an industry-leading 128,000 token context window for enhanced long-form processing and multi-turn conversation capabilities.\\nOne of the perks of working with an open-source model is also to avoid vendor lock-in. Heavy reliance on a particular type of proprietary model may make it difficult for you to switch to alternative models when your business starts growing or the landscape changes. Cohere approaches this in a hybrid way, meaning that you can access and modify the model for personal usage but need a license for commercial use. In this case, we recommend this model for businesses that want flexibility in experimentation without a long-term commitment to a single vendor.\\nGet the whitepaper\\nTop 9 Large Language Models as of Feburary 2025\\nBy clicking \\\"Download,\\\" you agree to Shakudo processing your personal data in accordance with its Privacy Notice.\\nThank you for filling out the form. The whitepaper you have requested is available for download below.  \\nDownload White Paper\\nOops! Something went wrong while submitting the form.\\nGet the whitepaper\\nTop 9 Large Language Models as of Feburary 2025\\nThank you for your interest. Click the button below to download whitepaper you have requested.  \\nDownload White Paper\\n\\nTop 9 Large Language Models as of Feburary 2025\\nExplore the top 9 LLMs making waves in the AI world and what each of them excel at\\n\\n| Case Study\\nTop 9 Large Language Models as of Feburary 2025\\n\\nKey results\\nAbout\\nindustry\\nTech Stack\\nNo items found.\\n<>\\nIntroduction\\nIf we had to choose one word to describe the rapid evolution of AI today, it would probably be something along the lines of explosive. As predicted by the Market Research Future report, the large language model (LLM) market in North America alone is expected to reach $105.5 billion by 2030. The exponential growth of AI tools combined with access to massive troves of text data has opened gates for better and more advanced content generation than we had ever hoped. Yet, such rapid expansion also makes it harder than ever to navigate and select the right tools among the diverse LLM models available.\\nThe goal of this post is to keep you, the AI enthusiast and professional, up-to-date with current trends and essential innovations in the field. Below, we highlighted the top 9 LLMs that we think are currently making waves in the industry, each with distinct capabilities and specialized strengths, excelling in areas such as natural language processing, code synthesis, few-shot learning, or scalability. While we believe there is no one-size-fits-all LLM for every use case, we hope that this list can help you identify the most current and well-suited LLM model that meets your business’s unique requirements.\\n1. GPT\\n\\nOur list kicks off with OpenAI's Generative Pre-trained Transformer (GPT) models, which have consistently exceeded their previous capabilities with each new release. Compared to its prior models, the latest ChatGPT-4o and ChatGPT-4o mini models offer significantly faster processing speeds and enhanced capabilities across text, voice, and vision.\\nThe latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. Both of these models are equipped with multimodal capabilities to handle images as well as audio data.\\nDespite having advanced conversational and reasoning capabilities, note that GPT is a proprietary model, meaning that the training data and parameters are kept confidential by OpenAI, and access to full functionality is restricted–a commercial license or subscription is often required to unlock the complete range of features. In this case, we recommend this model for businesses looking to adopt an LLM that excels in conversational dialogue, multi-step reasoning, efficient computation, and real-time interactions without the constraints of a budget.\\nFor companies who are curious to try out the proprietary models on the market before fully committing to one due to budget constraints or uncertainties about its long-term integration, Shakudo offers a compelling alternative. Our platform currently features a diverse selection of advanced LLMs with simplified deployment and scalability. With a simple subscription, you can access and assess the value of proprietary models, like GPT, before making a substantial investment.\\n2.DeepSeek\\n\\nDeepseek-R1 Benchmark. Source: deepseek.com\\nWith its latest R1 model, the Chinese AI company DeepSeek has once again set new benchmarks for innovation in the AI community. As of January 24th, the DeepSeek-R1 model is ranked fourth on Chatbot Arena, and top as the best open-source LM.\\nThe DeepSeek-R1 is a 671B parameter Mixture-of-Experts (MoE) model with 37B activated parameters per token, trained through large-scale reinforcement learning with a strong focus on reasoning capabilities. The model excels at understanding and handling long-form content and demonstrates superior performance in complex tasks such as mathematics and code generation. The model is approximately 30 times more cost-efficient than OpenAI-o1 and 5 times faster, offering groundbreaking performance at a fraction of the cost. Moreover, it has shown exceptional precision in tasks requiring complex pattern recognition, such as genomic data analysis, medical imaging, and large-scale scientific simulations.\\nDeepSeek-R1’s capabilities are transformative when it comes to integration with proprietary enterprise data such as PII and financial records. Leveraging retrieval-augmented generation (RAG), enterprises can connect the model to their internal data sources to enable highly personalized, context-aware interactions—all while maintaining stringent security and compliance standards. With Shakudo, you can streamline the deployment and integration of advanced AI models like DeepSeek by automating the setup, deployment, and management processes. This eliminates the need for businesses to invest in and maintain extensive computing infrastructure. By operating within your existing infrastructure, the platform ensures seamless integration, enhanced security, and optimal performance without requiring significant in-house resources or specialized expertise.\\n3. Qwen\\n\\n‍\\nAlibaba released Qwen2.5-Max earlier last month, the new model designed to deliver enhanced performance for large-scale natural language processing tasks. In instruct model evaluations, this new model outperforms DeepSeek V3 in benchmarks such as Arena-Hard, LiveBench, LiveCodeBench, and GPQA-Diamond, while also delivering strong performance in other assessments like MMLU-Pro.\\nQwen2.5-Max is pretrained on over 20 trillion tokens. While specific details about its parameter count and token window size are not publicly disclosed, Qwen2.5-Max is designed for low-latency, high-efficiency tasks, making it suitable for applications requiring quick, accurate responses with low latency. Its smaller size allows for deployment on devices with limited computational resources.\\nFor businesses and users looking for higher-performance models for natural language processing and AI-driven tasks, the Qwen 2.5 model is available on platforms like Hugging Face and ModelScope. This model boasts from 0.5 billion to 72 billion parameters, featuring context windows of up to 128,000 tokens, and is excellent for code generation, debugging, and automated forecasting.\\n4. LG AI\\n\\nsource\\nEXAONE 3.0 is a bilingual LLM developed by LG AI Research. The company released its latest model in December 2024 with excellent performance across various benchmarks and real-world applications.\\nWith 7.8 billion parameters, EXAONE 3.0 is capable of understanding and generating human-like text in multiple languages across complex domains, including coding, mathematics, patents, and chemistry. This model has also been optimized to reduce inference processing time by 56%, memory usage by 35%, and operating costs by 72%, ensuring that it remains cost-effective while maintaining high performance.\\nLG AI Research has open-sourced the 7.8B parameter instruction-tuned version of EXAONE 3.0 for non-commercial research purposes. This is a model we recommend to software companies or tech startups for generating Python code, assisting developers in troubleshooting, and creating APIs or other backend components.\\n5. LlaMA\\n\\nMeta is still leading the front with their state-of-the-art LlaMa models. The company released its latest LlaMA 3.3 model in December 2024, featuring multimodal capabilities that can process both text and image for in-depth analysis and response generation, such as interpreting charts, maps, or translating texts identified in an image.\\nLlaMA 3.3 improves on previous models with a longer context window of up to 128,000 tokens and an optimized transformer architecture. With a parameter of 70 billion, this model outperforms open-source and proprietary alternatives in areas such as multilingual dialogue, reasoning, and coding.\\nUnlike ChatGPT models, LlaMA 3 is open-source, giving users the flexibility to access and deploy freely on their cloud depending on the specific requirements of their infrastructure, security preferences, or customization needs. We recommend this model to businesses looking for advanced content generation and language understanding, such as those in customer service, education, marketing, and consumer markets. The openness of these models also allows for your greater control over the model’s performance, tuning, and integration into existing workflows.\\n6. Claude\\n\\nNext on our list is Claude, more specifically, the latest Claude 3.5 Sonnet model developed by Anthropic. We believe that Claude is arguably one of the most significant competitors to GPT since all of its current models–Claude 3 Haiku, Claude 3.5 Sonnet, and Claude 3 Opus–are designed with incredible contextual understanding capabilities that position themselves as the top conversational AI closely aligned with nuanced human interactions.\\nWhile the specific parameters of Claude 3.5 Sonnet remain undisclosed, the model boasts an impressive context window of 200,000 tokens, equivalent to approximately 150,000 words or 300 pages of text.\\nThe current Claude subscription service is credit-based, and the cost can go as high as $2,304/month for enterprise plans tailored to high-volume users. We recommend Claude to mature or mid-stage businesses looking not only to adopt an AI that facilitates human-like interactions but also to enhance their coding capabilities since the Claude 3.5 Sonnet model is currently reaching a 49.0% performance score on the SWE-bench Verified benchmark, placing it as third among all publicly available models, including reasoning models and systems specifically designed for agentic coding.\\n7. Mistral\\n\\nMistral's latest model – Mistral Small 3, a latency-optimized model was released under the Apache 2.0 license at the end of January. This 24-billion-parameter model is designed for low-latency, high-efficiency tasks. It processes approximately 150 tokens per second, making it over three times faster than Llama 3.3 70B on the same hardware.\\nThis new model is ideal for applications requiring quick, accurate responses with low latency, such as virtual assistants, real-time data processing, and on-device command and control. Its smaller size allows for deployment on devices with limited computational resources.\\nMistral Small 3 is currently open-source under the Apache 2.0 license. This means you can freely access and use the model for your own applications, provided you comply with the license terms. Since it is designed to be easily deployable, including on hardware with limited resources like a single GPU or even a MacBook with 32GB RAM, we'd recommend this to early-stage businesses looking to implement low-latency AI solutions without the need for extensive hardware infrastructure.\\n8. Gemini\\n\\nGemini is a family of closed-source LLM models developed by Google. The latest model—Gemini 2.0 Flash—operates at twice the speed of Gemini 1.5 Pro, offering substantial improvements in speed, reasoning, and multimodal processing capabilities.\\nWith that being said, Gemini remains a proprietary model; if your company deals with sensitive or confidential data regularly, you might be concerned about sending it to external servers due to security reasons. To address this concern, we recommend that you double-check vendor compliance regulations to ensure data privacy and security standards are met, such as adherence to GDPR, HIPAA, or other relevant data protection laws.\\nIf you’re looking for an open-source alternative that exhibits capabilities almost as good as Gemini, Google’s latest Gemma model, Gemma 2, offers three models available in 2 billion, 9 billion, and 27 billion parameters with a context window of 8,200. For businesses looking for a rather economic option, this is the optimal choice that interprets and understands messages with remarkable accuracy.\\n9. Command\\n\\nCommand R is a family of scalable models developed by Cohere with the goal of balancing high performance with strong accuracy, just like Claude. Both the Command R and Command R+ models offer APIs specifically optimized for Retrieval Augmented Generation (RAG). This means that these models can combine large-scale language generation with real-time information retrieval techniques for much more contextually aware outputs.\\nCurrently, the Command R+ model boasts 104 billion parameters and offers an industry-leading 128,000 token context window for enhanced long-form processing and multi-turn conversation capabilities.\\nOne of the perks of working with an open-source model is also to avoid vendor lock-in. Heavy reliance on a particular type of proprietary model may make it difficult for you to switch to alternative models when your business starts growing or the landscape changes. Cohere approaches this in a hybrid way, meaning that you can access and modify the model for personal usage but need a license for commercial use. In this case, we recommend this model for businesses that want flexibility in experimentation without a long-term commitment to a single vendor.\\nExplore more from Shakudo\\n How VPCs Enable AI Deployments with a Modern Data Stack Insights January 28, 2025\\n The Power of Simple Questions: How to Choose the Right Natural Language to SQL Query Tool Insights May 15, 2024\\n Bring Data and AI tooling right to MongoDB Atlas with Shakudo News August 26, 2024\\nTake the next step\\n\\\"Shakudo gave us the flexibility to use the data stack components that fit our needs and evolve the stack to keep up with the industry.\\\"\\n\\nNeal Gilmore\\nSenior Vice President, Enterprise Data & Analytics\\nDiscover Shakudo\\n\\nShakudo brings the best data and AI products into your VPC and operates them for you automatically achieving a more reliable, performant, and cost effective data stack than ever before.\\n\\n Book Demo Email X (Twitter) Linkedin Youtube\\nNewsletter\\nSign up for the latest Shakudo news:\\n🎉 Success! You're now signed up for the Shakudo newsletter.\\nOops! Something went wrong while submitting the form.\\nApplications\\nData and AI OSStack ComponentsLanguage to SQLVector Database + LLMReverse ETLWorkflow Automation\\nIndustries\\nAutomotive & Transportation\\nAerospace\\nManufacturing\\nHigher Education\\nHealthcare & Life Sciences\\nClimate & Energy\\nTechnology & Software\\nSports\\nReal Estate\\nRetail\\nFinancial Services\\nResources\\nUse Cases\\nInsights\\nWhite Paper\\nCase Study\\nPress\\nProduct\\nTutorial\\nNews\\nWebinarGlossaryDocumentation\\nCompany\\nAboutPartnersDGX PartnerCareersMedia Kit\\nGet Started\\nSignupContact UsNewsletter\\n© 2025 Shakudo\\nToronto, CA\\nContact usPrivacy PolicyTerms/ConditionsSitemap\\nTrusted by industry leaders\\n\\n\\n\\n\\n\\n\\nSee Shakudo in Action  \\nWatch the 3 Minute Demo\\n\\nThis field is required\\n\\nFor information about how Shakudo handles your personal data, please see our Privacy Policy.\\nThank you for your submission. A Shakudo expert will be in touch with you shortly.  \\nIn the meantime, feel free to check out our data insights, case studies, and latest industry news that help data teams win.  \\n Live chat Live chat will provide the quickest answer to any of your questions.\\nOops! Something went wrong while submitting the form.\\n⨉\"}], \"response_time\": 0.9}\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I have found some information about recent LLMs. Some of the latest models include:\n",
            "\n",
            "*   **GPT-4o:** OpenAI's newest model, released in May 2024, is multimodal, integrating text, image, video, and voice capabilities.\n",
            "*   **Claude 3.5 Sonnet:** Anthropic's model, released in June 2024, is known for ethical design and strong performance.\n",
            "*   **Grok-1:** xAI's model, released in November 2023, focuses on generating responses with personality and real-time data from X (formerly Twitter). Grok-2 and Grok-2 mini were released in August 2024.\n",
            "*   **Gemini 1.5:** Google's model improves multilingual capabilities and translation accuracy.\n",
            "*   **Inflection-2.5:** Inflection AI’s model powers the Pi conversational AI assistant.\n",
            "*   **Llama 3.1:** Meta’s open-source LLM with improved reasoning and coding abilities.\n",
            "*   **Mistral 7B:** A smaller, efficient model that outperforms larger models in many benchmarks.\n",
            "*   **Falcon 180B:** A large open-source LLM excelling in translation and text generation.\n",
            "*   **Nemotron-4:** Nvidia's family of large language models for synthetic data generation and AI model training.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I have found some information about recent LLMs. Some of the latest models include:\n\n*   **GPT-4o:** OpenAI's newest model, released in May 2024, is multimodal, integrating text, image, video, and voice capabilities.\n*   **Claude 3.5 Sonnet:** Anthropic's model, released in June 2024, is known for ethical design and strong performance.\n*   **Grok-1:** xAI's model, released in November 2023, focuses on generating responses with personality and real-time data from X (formerly Twitter). Grok-2 and Grok-2 mini were released in August 2024.\n*   **Gemini 1.5:** Google's model improves multilingual capabilities and translation accuracy.\n*   **Inflection-2.5:** Inflection AI’s model powers the Pi conversational AI assistant.\n*   **Llama 3.1:** Meta’s open-source LLM with improved reasoning and coding abilities.\n*   **Mistral 7B:** A smaller, efficient model that outperforms larger models in many benchmarks.\n*   **Falcon 180B:** A large open-source LLM excelling in translation and text generation.\n*   **Nemotron-4:** Nvidia's family of large language models for synthetic data generation and AI model training.\n\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What about reasoning LLMs?\"\n",
        "chat_with_agent(graph_builder, prompt, user_id, verbose=True)"
      ],
      "metadata": {
        "id": "XT4vtvku0TBW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "outputId": "931a843f-9590-4175-885b-407f6906f85a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Agent, please wait...\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What about reasoning LLMs?\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  search_web (8e0e2ae1-0f40-4a8a-a98f-2230b508e42c)\n",
            " Call ID: 8e0e2ae1-0f40-4a8a-a98f-2230b508e42c\n",
            "  Args:\n",
            "    query: reasoning large language models released\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: search_web\n",
            "\n",
            "{\"query\": \"reasoning large language models released\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models\", \"title\": \"25 of the best large language models in 2025 - TechTarget\", \"content\": \"Large language models are the dynamite behind the generative AI boom. Some of the most well-known language models today are based on the transformer model, including the generative pre-trained transformer series of LLMs and bidirectional encoder representations from transformers (BERT). Gemma is a family of open-source language models from Google that were trained on the same resources as Gemini. GPT-3 is OpenAI's large language model with more than 175 billion parameters, released in 2020. Large Language Model Meta AI (Llama) is Meta's LLM which was first released in 2023. The Pathways Language Model is a 540 billion parameter transformer-based model from Google powering its AI chatbot Bard. StableLM is a series of open language models developed by Stability AI, the company behind image generator Stable Diffusion.\", \"score\": 0.72559077, \"raw_content\": \"25 of the best large language models in 2025\\nWhatIs\\nSearch the TechTarget Network \\nBrowse Definitions :\\n\\nA\\nB\\nC\\nD\\nE\\nF\\nG\\nH\\nI\\nJ\\nK\\nL\\nM\\nN\\nO\\nP\\nQ\\nR\\nS\\nT\\nU\\nV\\nW\\nX\\nY\\nZ\\n#\\n\\nLogin Register\\n\\nTechTarget Network\\nTech Accelerator\\nNews\\n2024 IT Salary Survey Results\\n\\nRSS\\n\\n\\nWhatIs\\n\\n\\nBrowse Definitions Data analytics and AI\\nTopics View All\\n\\nBusiness software\\nCloud computing\\nComputer science\\nData centers\\nIT management\\nNetworking\\nSecurity\\nSoftware development\\n\\nPlease select a category\\n\\nTopics\\n\\n\\n\\nBrowse Features Resources\\n\\nBusiness strategies\\nCareer resources\\nEmerging tech\\nTech explainers\\n\\n\\n\\nFollow:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\n\\nData analytics and AI\\n\\nTech Accelerator What is Gen AI? Generative AI explained\\nPrev Next Will AI replace jobs? 17 job types that might be affected Pros and cons of AI-generated content\\nDownload this guide1\\nFeature\\n25 of the best large language models in 2025\\nLarge language models have been affecting search for years and have been brought to the forefront by ChatGPT and other chatbots.\\n\\nShare this item with your network:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy\\n\\nSean Michael Kerner\\nBen Lutkevich, Site Editor\\n\\nPublished: 31 Jan 2025\\nLarge language models are the dynamite behind the generative AI boom. However, they've been around for a while.\\nLLMs are black box AI systems that use deep learning on extremely large datasets to understand and generate new text. Modern LLMs began taking shape in 2014 when the attention mechanism -- a machine learning technique designed to mimic human cognitive attention -- was introduced in a research paper titled \\\"Neural Machine Translation by Jointly Learning to Align and Translate.\\\" In 2017, that attention mechanism was honed with the introduction of the transformer model in another paper, \\\"Attention Is All You Need.\\\"\\nSome of the most well-known language models today are based on the transformer model, including the generative pre-trained transformer series of LLMs and bidirectional encoder representations from transformers (BERT).\\nChatGPT, which runs on a set of language models from OpenAI, attracted more than 100 million users just two months after its release in 2022. Since then, many competing models have been released. Some belong to big companies such as Google, Amazon and Microsoft; others are open source.\\nConstant developments in the field can be difficult to keep track of. Here are some of the most influential models, both past and present. Included in it are models that paved the way for today's leaders as well as those that could have a significant effect in the future.\\nThis article is part of\\nWhat is Gen AI? Generative AI explained\\n\\nWhich also includes:\\n8 top generative AI tool categories for 2025\\nWill AI replace jobs? 17 job types that might be affected\\n25 of the best large language models in 2025\\n\\nTop current LLMs\\nBelow are some of the most relevant large language models today. They do natural language processing and influence the architecture of future models.\\nBERT\\nBERT is a family of LLMs that Google introduced in 2018. BERT is a transformer-based model that can convert sequences of data to other sequences of data. BERT's architecture is a stack of transformer encoders and features 342 million parameters. BERT was pre-trained on a large corpus of data then fine-tuned to perform specific tasks along with natural language inference and sentence text similarity. It was used to improve query understanding in the 2019 iteration of Google search.\\nClaude\\nThe Claude LLM focuses on constitutional AI, which shapes AI outputs guided by a set of principles that help the AI assistant it powers helpful, harmless and accurate. Claude was created by the company Anthropic.\\nThere are three primary branches of Claude -- Opus, Haiku and Sonnet. The latest iteration of the Claude LLM is the Claude 3.5 Sonnet. It understands nuance, humor and complex instructions better than earlier versions of the LLM. It also has broad programming capabilities that make it well-suited for application development. In October 2024, Claude added a computer-use AI tool, that enables the LLM to use a computer like a human does. It's available via Claude.ai, the Claude iOS app and through an API.\\nCohere\\nCohere is an enterprise AI platform that provides several LLMs including Command, Rerank and Embed. These LLMs can be custom-trained and fine-tuned to a specific company's use case. The company that created the Cohere LLM was founded by one of the authors of Attention Is All You Need.\\nDeepSeek-R1\\nDeepSeek-R1 is an open-source reasoning model for tasks with complex reasoning, mathematical problem-solving and logical inference. The model uses reinforcement learning techniques to refine its reasoning ability and solve complex problems. DeepSeek-R1 can perform critical problem-solving through self-verification, chain-of-thought reasoning and reflection.\\nErnie\\nErnie is Baidu's large language model which powers the Ernie 4.0 chatbot. The bot was released in August 2023 and has garnered more than 45 million users. Ernie is rumored to have 10 trillion parameters. The bot works best in Mandarin but is capable in other languages.\\nFalcon\\nFalcon is a family of transformer-based models developed by the Technology Innovation Institute. It is open source and has multi-lingual capabilities. Falcon 2 is available in an 11 billion parameter version that provide multimodal capabilities for both text and vision.\\nThe Falcon 1 series includes a pair of larger models with Falcon 40B and Falcon 180B. Falcon models are available on GitHub as well as on cloud provider including Amazon.\\nGemini\\nGemini is Google's family of LLMs that power the company's chatbot of the same name. The model replaced Palm in powering the chatbot, which was rebranded from Bard to Gemini upon the model switch. Gemini models are multimodal, meaning they can handle images, audio and video as well as text. Gemini is also integrated in many Google applications and products. It comes in three sizes -- Ultra, Pro and Nano. Ultra is the largest and most capable model, Pro is the mid-tier model and Nano is the smallest model, designed for efficiency with on-device tasks.\\nAmong the most recent models is the Gemini 1.5 Pro update that debuted in May 2024 Gemini is available as a web chatbot, the Google Vertex AI service and via API. Early previews of Gemini 2.0 Flash became available in December 2024 with updated multimodal generation capabilities.\\nGemma\\nGemma is a family of open-source language models from Google that were trained on the same resources as Gemini. Gemma 2 was released in June 2024 in two sizes -- a 9 billion parameter model and a 27 billion parameter model. Gemma models can be run locally on a personal computer, and are also available in Google Vertex AI.\\nGPT-3\\nGPT-3 is OpenAI's large language model with more than 175 billion parameters, released in 2020. GPT-3 uses a decoder-only transformer architecture. In September 2022, Microsoft announced it had exclusive use of GPT-3's underlying model. GPT-3 is 10 times larger than its predecessor. GPT-3's training data includes Common Crawl, WebText2, Books1, Books2 and Wikipedia.\\nGPT-3 is the last of the GPT series of models in which OpenAI made the parameter counts publicly available. The GPT series was first introduced in 2018 with OpenAI's paper \\\"Improving Language Understanding by Generative Pre-Training.\\\"\\nGPT-3.5\\nGPT-3.5 is an upgraded version of GPT-3 with fewer parameters. GPT-3.5 was fine-tuned using reinforcement learning from human feedback. GPT-3.5 is the version of GPT that powers ChatGPT. There are several models, with GPT-3.5 turbo being the most capable, according to OpenAI. GPT-3.5's training data extends to September 2021.\\nIt was also integrated into the Bing search engine but has since been replaced with GPT-4.\\nGPT-4\\nGPT-4 , was released in 2023 and like the others in the OpenAI GPT family, it's a transformer-based model. Unlike the others, its parameter count has not been released to the public, though there are rumors that the model has more than 170 trillion. OpenAI describes GPT-4 as a multimodal model, meaning it can process and generate both language and images as opposed to being limited to only language. GPT-4 also introduced a system message, which lets users specify tone of voice and task.\\nGPT-4 demonstrated human-level performance in multiple academic exams. At the model's release, some speculated that GPT-4 came close to artificial general intelligence, which means it is as smart or smarter than a human. That speculation turned out to be unfounded.\\nGPT-4o\\nGPT-4 Omni (GPT-4o) is OpenAI's successor to GPT-4 and offers several improvements over the previous model. GPT-4o creates a more natural human interaction for ChatGPT and is a large multimodal model, accepting various inputs including audio, image and text. The conversations let users engage as they would in a normal human conversation, and the real-time interactivity can also pick up on emotions. GPT-4o can see photos or screens and ask questions about them during interaction.\\nGPT-4o can respond in 232 milliseconds, similar to human response time and faster than GPT-4 Turbo.\\nGranite\\nThe IBM Granite family of models are fully open source models under the Apache v.2 license. The first iteration of the open source model models debuted in May 2024, followed by Granite 3.0 in October and Granite 3.1 in December 2024.\\nThere are multiple variants in the Granite model family including General-purpose models (8B and 2B variants), guardrail model and Mixture-of-Experts models. While the model can be used for general purpose deployments, IBM itself is focusing deployment and optimization for enterprise use cases like customer service, IT automation and cybersecurity.\\nLamda\\nLamda (Language Model for Dialogue Applications) is a family of LLMs developed by Google Brain announced in 2021. Lamda used a decoder-only transformer language model and was pre-trained on a large corpus of text. In 2022, LaMDA gained widespread attention when then-Google engineer Blake Lemoine went public with claims that the program was sentient. It was built on the Seq2Seq architecture.\\nLlama\\nLarge Language Model Meta AI (Llama) is Meta's LLM which was first released in 2023. The Llama 3.1 models were released in July 2024, including both a 405 billion and 70 billion parameter model.\\nThe most recent version is Llama 3.2 which was released in September 2024, initially with smaller parameter counts of 11 billion and 90 billion.\\nLlama uses a transformer architecture and was trained on a variety of public data sources, including webpages from CommonCrawl, GitHub, Wikipedia and Project Gutenberg. Llama was effectively leaked and spawned many descendants, including Vicuna and Orca. Llama is available under an open license, allowing for free use of the models. Lllama models are available in many locations including llama.com and Hugging Face.\\nMistral\\nMistral is a family of a mixture of expert models from Mistral AI. Among the newest models is Mistral Large 2 which was first released in July 2024. The model operates with 123 billion parameters and a 128k context window, supporting dozens of languages including French, German, Spanish, Italian, and many others, along with more than 80 coding languages.\\nIn November 2024, Mistral released Pixtral Large, a 124-billion-parameter multimodal model that can handle text and visual data. Mistral models are available via Mistral's API on its Le Platforme-managed web service.\\no1\\nThe OpenAI o1 model family was first introduced in Sept. 2024. The o1 model's focus is to provide what OpenAI refers to as - reasoning models, that can reason through a problem or query before offering a response.\\nThe o1 models excel in STEM fields, with strong results in mathematical reasoning (scoring 83% on the International Mathematics Olympiad compared to GPT-4o's 13%), code generation and scientific research tasks. While they offer enhanced reasoning and improved safety features, they operate more slowly than previous models due to their thorough reasoning processes and come with certain limitations, such as restricted access features and higher API costs. The models are available to ChatGPT Plus and Team users, with varying access levels for different user categories.\\no3\\nOpenAI introduced the successor model, o3, in December 2024. According to OpenAI, o3 is designed to handle tasks with more analytical thinking, problem-solving and complex reasoning and will improve o1's capabilities and performance. The o3 model is in safety testing mode and is currently not available to the public.\\nOrca\\nOrca was developed by Microsoft and has 13 billion parameters, meaning it's small enough to run on a laptop. It aims to improve on advancements made by other open source models by imitating the reasoning procedures achieved by LLMs. Orca achieves the same performance as GPT-4 with significantly fewer parameters and is on par with GPT-3.5 for many tasks. Orca is built on top of the 13 billion parameter version of Llama.\\nPalm\\nThe Pathways Language Model is a 540 billion parameter transformer-based model from Google powering its AI chatbot Bard. It was trained across multiple TPU 4 Pods -- Google's custom hardware for machine learning. Palm specializes in reasoning tasks such as coding, math, classification and question answering. Palm also excels at decomposing complex tasks into simpler subtasks.\\nPaLM gets its name from a Google research initiative to build Pathways, ultimately creating a single model that serves as a foundation for multiple use cases. There are several fine-tuned versions of Palm, including Med-Palm 2 for life sciences and medical information as well as Sec-Palm for cybersecurity deployments to speed up threat analysis.\\nPhi\\nPhi is a transformer-based language model from Microsoft. The Phi 3.5 models were first released in August 2024.\\nThe series includes Phi-3.5-mini-instruct (3.82 billion parameters), Phi-3.5-MoE-instruct (41.9 billion parameters), and Phi-3.5-vision-instruct (4.15 billion parameters), each designed for specific tasks ranging from basic reasoning to vision analysis. All three models support a 128k token context length.\\nReleased under a Microsoft-branded MIT License, they are available for developers to download, use, and modify without restrictions, including for commercial purposes.\\nQwen\\nQwen is large family of open models developed by Chinese internet giant Alibaba Cloud. The newest set of models are the Qwen2.5 suite, which support 29 different languages and currently scale up to 72 billion parameters. These models are suitable for a wide range of tasks, including code generation, structured data understanding, mathematical problem-solving as well as general language understanding and generation.\\nStableLM\\nStableLM is a series of open language models developed by Stability AI, the company behind image generator Stable Diffusion.\\nStableLM 2 debuted in January 2024 initially with a 1.6 billion parameter model. In April 2024 that was expanded to also include a 12 billion parameter model. StableLM 2 supports seven languages: English, Spanish, German, Italian, French, Portuguese, and Dutch. Stability AI positions these models as offering different options for various use cases, with the 1.6B model suitable for specific, narrow tasks and faster processing while the 12B model provides more capability but requires more computational resources.\\nTülu 3\\nAllen Institute for AI's Tülu 3 is an open-source 405 billion-parameter LLM. The Tülu 3 405B model has post-training methods that combine supervised fine-tuning and reinforcement learning at a larger scale. Tülu 3 uses a \\\"reinforcement learning from verifiable rewards\\\" framework for fine-tuning tasks with verifiable outcomes -- such as solving mathematical problems and following instructions.\\nVicuna 33B\\nVicuna is another influential open source LLM derived from Llama. It was developed by LMSYS and was fine-tuned using data from sharegpt.com. It is smaller and less capable that GPT-4 according to several benchmarks, but does well for a model of its size. Vicuna has only 33 billion parameters, whereas GPT-4 has trillions.\\nLLM precursors\\nAlthough LLMs are a recent phenomenon, their precursors go back decades. Learn how recent precursor Seq2Seq and distant precursor ELIZA set the stage for modern LLMs.\\nSeq2Seq\\nSeq2Seq is a deep learning approach used for machine translation, image captioning and natural language processing. It was developed by Google and underlies some of their modern LLMs, including LaMDA. Seq2Seq also underlies AlexaTM 20B, Amazon's large language model. It uses a mix of encoders and decoders.\\nEliza\\nEliza was an early natural language processing program created in 1966. It is one of the earliest examples of a language model. Eliza simulated conversation using pattern matching and substitution. Eliza, running a certain script, could parody the interaction between a patient and therapist by applying weights to certain keywords and responding to the user accordingly. The creator of Eliza, Joshua Weizenbaum, wrote a book on the limits of computation and artificial intelligence.\\nNext Steps\\nGenerative AI challenges that businesses should consider\\nGenerative AI ethics: Biggest concerns\\nGenerative AI landscape: Potential future trends\\nGenerative models: VAEs, GANs, diffusion, transformers, NeRFs\\nAI content generators to explore\\nRelated Resources\\n\\nFive data quality trends to prepare for in the year ahead –Video\\nThe Digital Transformation And Innovation Landscape –Wipro\\nCloudera and NVIDIA Accelerate AI in the Financial Services Industry –Cloudera\\nImprove customer satisfaction or cut costs? Who says you have to choose? –Video\\n\\nDig Deeper on Data analytics and AI\\n\\n ##### What is GPT-3? Everything you need to know  By: Nick Barney\\n ##### What is a small language model (SLM)?  By: Sean Kerner\\n ##### GPT-4  By: Ben Lutkevich\\n ##### What are large language models (LLMs)?  By: Sean Kerner\\n\\nSponsored News\\n\\nSustainability, AI and Dell PowerEdge Servers –Dell Technologies and Intel\\nThree Innovative AI Use Cases for Natural Language Processing –Dell Technologies\\nAutonomous coding: The future of the revenue cycle –Solventum\\n\\nRelated Content\\n\\nExploring GPT-3 architecture – Search Enterprise AI\\nWhat is GPT-3? Everything you need to know – Search Enterprise AI\\nMicrosoft exclusively licenses OpenAI's GPT-3 ... – Search Enterprise AI\\n\\nLatest TechTarget resources\\n\\nNetworking\\nSecurity\\nCIO\\nHR Software\\nCustomer Experience\\n\\nSearch Networking\\n\\n\\nWhat is a thin client (lean client)?A thin client (lean client) is a virtual desktop computing model that runs on the resources stored on a central server instead of...\\n\\n\\nWhat is network monitoring?Network monitoring, also frequently called network management, is the practice of consistently overseeing a computer network for ...\\n\\n\\nWhat is network automation?Network automation is a process that uses intelligent software to automate the management, configuration, deployment, testing and...\\n\\n\\nSearch Security\\n\\n\\nWhat is Internet Key Exchange (IKE)?Internet Key Exchange (IKE) is a standard protocol used to set up a secure and authenticated communication channel between two ...\\n\\n\\nWhat is a certificate revocation list (CRL) and how is it used?A certificate revocation list (CRL) is a list of digital certificates that have been revoked by the issuing certificate authority...\\n\\n\\nWhat is cryptology?Cryptology is the mathematics, such as number theory and the application of formulas and algorithms, that underpin cryptography ...\\n\\n\\nSearch CIO\\n\\n\\nWhat is an IT project manager?An IT project manager is a professional charged with overseeing the process of planning, executing and delegating ...\\n\\n\\nWhat is a cyberthreat hunter (cybersecurity threat analyst)?A cyberthreat hunter, also called a cybersecurity threat analyst, proactively identifies security incidents that might go ...\\n\\n\\nWhat is blockchain? Definition, examples and how it worksBlockchain is a distributed ledger technology (DLT) that's shared across a network of computers to keep a digital record of ...\\n\\n\\nSearch HRSoftware\\n\\n\\nWhat is employee self-service (ESS)?Employee self-service (ESS) is a widely used human resources technology that enables employees to perform many job-related ...\\n\\n\\nWhat is DEI? Diversity, equity and inclusion explainedDiversity, equity and inclusion is a term used to describe policies and programs that promote the representation and ...\\n\\n\\nWhat is payroll software?Payroll software automates the process of paying salaried, hourly and contingent employees.\\n\\n\\nSearch Customer Experience\\n\\n\\nWhat is account-based selling? Everything you need to knowAccount-based selling (ABS) is a strategic sales approach in business-to-business sales and marketing that centers around ...\\n\\n\\nWhat is interactive voice response (IVR)?Interactive voice response (IVR) is an automated telephony system that interacts with callers, gathers information and routes ...\\n\\n\\nWhat is an AI assistant?An AI assistant, or digital assistant, is software that uses artificial intelligence to understand natural language voice ...\\n\\n\\nBrowse by Topic\\n\\n\\nBrowse Resources\\n\\n\\nAbout Us\\n\\nMeet The Editors\\nEditorial Ethics Policy\\nContact Us\\nAdvertisers\\nBusiness Partners\\nEvents\\nMedia Kit\\nCorporate Site\\nReprints\\n\\nAll Rights Reserved, Copyright 1999 - 2025, TechTarget  \\nPrivacy Policy\\nCookie Preferences\\nCookie Preferences\\nDo Not Sell or Share My Personal Information\\nClose\\n\\nX\\nFree Download What is generative AI? Everything you need to know\\nThe potential of AI technology has been percolating in the background for years. But when ChatGPT, the AI chatbot, began grabbing headlines in early 2023, it put generative AI in the spotlight. This guide is your go-to manual for generative AI, covering its benefits, limits, use cases, prospects and much more.\\n\"}, {\"url\": \"https://arxiv.org/abs/2502.09100\", \"title\": \"Logical Reasoning in Large Language Models: A Survey - arXiv\", \"content\": \"cs arXiv:2502.09100 Help | Advanced Search arXiv author ID Logical Reasoning in Large Language Models: A Survey This survey synthesizes recent advancements in logical reasoning within LLMs, a critical area of AI research. It outlines the scope of logical reasoning in LLMs, its theoretical foundations, and the benchmarks used to evaluate reasoning proficiency. The review concludes with future directions, emphasizing the need for further exploration to strengthen logical reasoning in AI systems. Subjects:   Artificial Intelligence (cs.AI); Computation and Language (cs.CL) Cite as:    arXiv:2502.09100 [cs.AI] (or arXiv:2502.09100v1 [cs.AI] for this version) From: Hanmeng Liu [view email] Access Paper: cs.AI cs Bibliographic and Citation Tools Bibliographic Explorer Toggle Connected Papers Toggle scite.ai Toggle Which authors of this paper are endorsers?\", \"score\": 0.6721816, \"raw_content\": \"Skip to main content\\nWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\\nDonate\\n\\ncs\\narXiv:2502.09100\\n\\nHelp | Advanced Search\\nAll fields\\nTitle\\nAuthor\\nAbstract\\nComments\\nJournal reference\\nACM classification\\nMSC classification\\nReport number\\narXiv identifier\\nDOI\\nORCID\\narXiv author ID\\nHelp pages\\nFull text\\nSearch\\nComputer Science > Artificial Intelligence\\n[Submitted on 13 Feb 2025]\\nLogical Reasoning in Large Language Models: A Survey\\nHanmeng Liu, Zhizhang Fu, Mengru Ding, Ruoxi Ning, Chaoli Zhang, Xiaozhang Liu, Yue Zhang\\nWith the emergence of advanced reasoning models like OpenAI o3 and DeepSeek-R1, large language models (LLMs) have demonstrated remarkable reasoning capabilities. However, their ability to perform rigorous logical reasoning remains an open question. This survey synthesizes recent advancements in logical reasoning within LLMs, a critical area of AI research. It outlines the scope of logical reasoning in LLMs, its theoretical foundations, and the benchmarks used to evaluate reasoning proficiency. We analyze existing capabilities across different reasoning paradigms - deductive, inductive, abductive, and analogical - and assess strategies to enhance reasoning performance, including data-centric tuning, reinforcement learning, decoding strategies, and neuro-symbolic approaches. The review concludes with future directions, emphasizing the need for further exploration to strengthen logical reasoning in AI systems.\\nSubjects:   Artificial Intelligence (cs.AI); Computation and Language (cs.CL)\\nCite as:    arXiv:2502.09100 [cs.AI]\\n    (or arXiv:2502.09100v1 [cs.AI] for this version)\\n  \\nhttps://doi.org/10.48550/arXiv.2502.09100\\nFocus to learn more\\nSubmission history\\nFrom: Hanmeng Liu [view email]\\n[v1] Thu, 13 Feb 2025 09:19:14 UTC (829 KB)\\nAccess Paper:\\nView PDF\\nHTML (experimental)\\nTeX Source\\nOther Formats\\nview license\\nCurrent browse context:\\ncs.AI\\n< prev   |   next >\\nnew | recent | 2025-02\\nChange to browse by:\\ncs\\ncs.CL\\nReferences & Citations\\nNASA ADS\\nGoogle Scholar\\nSemantic Scholar\\nExport BibTeX Citation\\nBookmark\\nBibliographic Tools\\nBibliographic and Citation Tools\\nBibliographic Explorer Toggle\\nBibliographic Explorer (What is the Explorer?)\\nConnected Papers Toggle\\nConnected Papers (What is Connected Papers?)\\nLitmaps Toggle\\nLitmaps (What is Litmaps?)\\nscite.ai Toggle\\nscite Smart Citations (What are Smart Citations?)\\nCode, Data, Media\\nDemos\\nRelated Papers\\nAbout arXivLabs\\nWhich authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\\nAbout\\nHelp\\nContact\\nSubscribe\\nCopyright\\nPrivacy Policy\\nWeb Accessibility Assistance\\narXiv Operational Status \\nGet status notifications via email or slack\"}, {\"url\": \"https://github.com/jeffhj/LM-reasoning\", \"title\": \"Reasoning in Large Language Models - GitHub\", \"content\": \"GitHub - jeffhj/LM-reasoning: This repository contains a collection of papers and resources on Reasoning in Large Language Models. This repository contains a collection of papers and resources on Reasoning in Large Language Models. This repository contains a collection of papers and resources on Reasoning in Large Language Models. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. This repository contains a collection of papers and resources on Reasoning in Large Language Models.\", \"score\": 0.63615024, \"raw_content\": \"GitHub - jeffhj/LM-reasoning: This repository contains a collection of papers and resources on Reasoning in Large Language Models.\\nSkip to content \\nNavigation Menu\\nToggle navigation\\n\\nSign in\\n\\n\\nProduct\\n\\nGitHub Copilot Write better code with AI\\nSecurity Find and fix vulnerabilities\\nActions Automate any workflow\\nCodespaces Instant dev environments\\nIssues Plan and track work\\nCode Review Manage code changes\\nDiscussions Collaborate outside of code\\nCode Search Find more, search less\\n\\nExplore\\n\\nAll features\\nDocumentation\\nGitHub Skills\\nBlog\\n\\n\\n\\nSolutions\\nBy company size\\n\\nEnterprises\\nSmall and medium teams\\nStartups\\n\\nBy use case\\n\\nDevSecOps\\nDevOps\\nCI/CD\\nView all use cases\\n\\nBy industry\\n\\nHealthcare\\nFinancial services\\nManufacturing\\nGovernment\\nView all industries\\n\\nView all solutions\\n\\n\\nResources\\nTopics\\n\\nAI\\nDevOps\\nSecurity\\nSoftware Development\\nView all\\n\\nExplore\\n\\nLearning Pathways\\nWhite papers, Ebooks, Webinars\\nCustomer Stories\\nPartners\\nExecutive Insights\\n\\n\\n\\nOpen Source\\n\\n\\nGitHub Sponsors Fund open source developers\\n\\n\\nThe ReadME Project GitHub community articles\\n\\n\\nRepositories\\n\\nTopics\\nTrending\\nCollections\\n\\n\\n\\nEnterprise\\n\\nEnterprise platform AI-powered developer platform\\n\\nAvailable add-ons\\n\\nAdvanced Security Enterprise-grade security features\\nGitHub Copilot Enterprise-grade AI features\\nPremium Support Enterprise-grade 24/7 support\\n\\n\\n\\nPricing\\n\\n\\nSearch or jump to...\\nSearch code, repositories, users, issues, pull requests...\\nSearch\\nClear\\nSearch syntax tips\\nProvide feedback\\nWe read every piece of feedback, and take your input very seriously.\\nInclude my email address so I can be contacted\\nCancel Submit feedback\\nSaved searches\\nUse saved searches to filter your results more quickly\\nName  \\nQuery \\nTo see all available qualifiers, see our documentation.\\nCancel Create saved search\\nSign in\\nSign up Reseting focus\\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\\n{{ message }}\\njeffhj / LM-reasoning Public\\n\\nNotifications You must be signed in to change notification settings\\nFork 34\\nStar 547\\n\\nThis repository contains a collection of papers and resources on Reasoning in Large Language Models.\\nLicense\\nMIT license\\n547 stars 34 forks Branches Tags Activity\\nStar\\nNotifications You must be signed in to change notification settings\\n\\nCode\\nIssues 4\\nPull requests 0\\nActions\\nProjects 0\\nSecurity\\nInsights\\n\\nAdditional navigation options\\n\\nCode\\nIssues\\nPull requests\\nActions\\nProjects\\nSecurity\\nInsights\\n\\njeffhj/LM-reasoning\\nmain\\nBranchesTags\\n\\nGo to file\\nCode\\nFolders and files\\n| Name | Name | \\nLast commit message\\n| \\nLast commit date\\n|\\n| --- | --- | --- | --- |\\n| \\nLatest commit\\nHistory\\n25 Commits\\n\\n|\\n| \\nLicense\\n| \\nLicense\\n| \\n| \\n|\\n| \\nREADME.md\\n| \\nREADME.md\\n| \\n| \\n|\\n| \\nView all files\\n|\\nRepository files navigation\\n\\nREADME\\nMIT license\\n\\nReasoning in Large Language Models\\n\\n  \\nThis repository contains a collection of papers and resources on Reasoning in Large Language Models.\\nFor more details, please refer to Towards Reasoning in Large Language Models: A Survey\\n\\nFeel free to let me know the missing papers (issue or pull request).\\n\\nContributor: Jie Huang @UIUC\\nThank Kevin Chen-Chuan Chang @UIUC, Jason Wei @Google Brain, Denny Zhou @Google Brain for insightful discussions and suggestions.\\nContents\\n\\n\\nSurvey\\nRelevant Survey & Position Paper & Blog\\nTechnique\\nFully Supervised Finetuning\\nPrompting & In-Context Learning\\nHybrid Method\\n\\n\\nEvaluation & Analysis\\n\\nSurvey\\n\\nTowards Reasoning in Large Language Models: A Survey 20 Dec 2022\\n\\nJie Huang, Kevin Chen-Chuan Chang\\nRelevant Survey and Position Paper and Blog\\n\\nEmergent Abilities of Large Language Models 15 Jun 2022\\n\\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus\\nLanguage Model Cascades 21 Jul 2022\\n\\nDavid Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Sohl-dickstein, Kevin Murphy, Charles Sutton\\nHow does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources 11 Dec 2022\\n\\nYao Fu, Hao Peng, Tushar Shot\\nReasoning with Language Model Prompting: A Survey 19 Dec 2022\\n\\nShuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen\\nA Survey of Deep Learning for Mathematical Reasoning 20 Dec 2022\\n\\nPan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, Kai-Wei Chang\\nA Survey for In-context Learning 31 Dec 2022\\n\\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, Zhifang Sui\\nLogical Reasoning over Natural Language as Knowledge Representation: A Survey 21 Mar 2023\\n\\nZonglin Yang, Xinya Du, Rui Mao, Jinjie Ni, Erik Cambria\\nNature Language Reasoning, A Survey 26 Mar 2023\\n\\nFei Yu, Hongbo Zhang, Benyou Wang\\nTechnique\\n\\nFully Supervised Finetuning\\n\\n\\nWe mainly focus on techniques that are applicable to improving or eliciting \\\"reasoning\\\" in large language models like GPT-3 (175B)\\nPapers in this paradigm vary a lot and are usually based on small models trained on specific datasets. We list several papers here for reference (that is, the list is not complete). Please refer to our survey for some discussion.\\n\\nExplain Yourself! Leveraging Language Models for Commonsense Reasoning 6 Jun 2019\\n\\nNazneen Fatema Rajani, Bryan McCann, Caiming Xiong, Richard Socher\\nLeap-Of-Thought: Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge 11 Jun 2020\\n\\nAlon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, Jonathan Berant\\nMeasuring Mathematical Problem Solving With the MATH Dataset 5 Mar 2021\\n\\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt\\nShow Your Work: Scratchpads for Intermediate Computation with Language Models 30 Nov 2021\\n\\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, Augustus Odena\\nFaiRR: Faithful and Robust Deductive Reasoning over Natural Language 19 Mar 2022\\n\\nSoumya Sanyal, Harman Singh, Xiang Ren\\n......\\n\\nPrompting and In-Context Learning\\n\\nChain of Thought Prompting and Its Variants/Applications\\n\\nChain of Thought Prompting Elicits Reasoning in Large Language Models 28 Jan 2022\\n\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou\\nIteratively Prompt Pre-trained Language Models for Chain of Thought 16 Mar 2022\\n\\nBoshi Wang, Xiang Deng, Huan Sun\\nLarge Language Models are Zero-Shot Reasoners 24 May 2022\\n\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa\\nPsychologically-informed chain-of-thought prompts for metaphor understanding in large language models 16 Sep 2022\\n\\nBen Prystawski, Paul Thibodeau, Noah Goodman\\nLanguage Models are Multilingual Chain-of-Thought Reasoners 6 Oct 2022\\n\\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, Jason Wei\\nLarge Language Models are few(1)-shot Table Reasoners 13 Oct 2022\\n\\nWenhu Chen\\nLanguage Models of Code are Few-Shot Commonsense Learners 13 Oct 2022\\n\\nAman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, Graham Neubig\\nPaL: Program-Aided Language Model 18 Nov 2022\\n\\nLuyu Gao*, Aman Madaan*, Shuyan Zhou*, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig\\nProgram of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks 22 Nov 2022\\n\\nWenhu Chen, Xueguang Ma, Xinyi Wang, William W. Cohen\\nRethinking with Retrieval: Faithful Large Language Model Inference 31 Dec 2022\\n\\nHangfeng He, Hongming Zhang, Dan Roth\\nRationale Engineering\\n\\nTraining Verifiers to Solve Math Word Problems 27 Oct 2021\\n\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman\\nSelf-Consistency Improves Chain of Thought Reasoning in Language Models 21 Mar 2022\\n\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou\\nOn the Advance of Making Language Models Better Reasoners 6 Jun 2022\\n\\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen\\nComplexity-Based Prompting for Multi-Step Reasoning 3 Oct 2022\\n\\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot\\nAutomatic Chain of Thought Prompting in Large Language Models 7 Oct 2022\\n\\nZhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola\\nTeaching Algorithmic Reasoning via In-context Learning 15 Nov 2022\\n\\nHattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, Hanie Sedghi\\nLarge Language Models are reasoners with Self-Verification 19 Dec 2022\\n\\nYixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, Jun Zhao\\nProblem Decomposition\\n\\nLeast-to-Most Prompting Enables Complex Reasoning in Large Language Models 21 May 2022\\n\\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, Ed Chi\\nCompositional Semantic Parsing with Large Language Models 29 Sep 2022\\n\\nAndrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, Denny Zhou\\nDecomposed Prompting: A Modular Approach for Solving Complex Tasks 5 Oct 2022\\n\\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal\\nMeasuring and Narrowing the Compositionality Gap in Language Models 7 Oct 2022\\n\\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, Mike Lewis\\nSuccessive Prompting for Decomposing Complex Questions 8 Dec 2022\\n\\nDheeru Dua, Shivanshu Gupta, Sameer Singh, Matt Gardner\\nLarge Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning 31 Jan 2023\\n\\nYunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, Yongbin Li\\nOthers\\n\\nLanguage Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents 18 Jan 2022\\n\\nWenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch\\nSelection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning 19 May 2022\\n\\nAntonia Creswell, Murray Shanahan, Irina Higgins\\nMaieutic Prompting: Logically Consistent Reasoning with Recursive Explanations 24 May 2022\\n\\nJaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, Yejin Choi\\nFaithful Reasoning Using Large Language Models 30 Aug 2022\\n\\nAntonia Creswell, Murray Shanahan\\nLearn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering 20 Sep 2022\\n\\nPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan\\nExplanations from Large Language Models Make Small Reasoners Better 13 Oct 2022\\n\\nShiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, Wenhu Chen, Xifeng Yan\\nDistilling Multi-Step Reasoning Capabilities of Large Language Models into Smaller Models via Semantic Decompositions 1 Dec 2022\\n\\nKumar Shridhar, Alessandro Stolfo, Mrinmaya Sachan\\nTeaching Small Language Models to Reason 16 Dec 2022\\n\\nLucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, Aliaksei Severyn\\nLAMBADA: Backward Chaining for Automated Reasoning in Natural Language 20 Dec 2022\\n\\nSeyed Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, Deepak Ramachandran\\nReasoning with Language Model is Planning with World Model 24 May 2023\\n\\nShibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu\\nHybrid Method\\n\\nReasoning-Enhanced Training and Prompting\\n\\nReasoning Like Program Executors 27 Jan 2022\\n\\nXinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Qiang Fu, Yan Gao, Jian-Guang Lou, Weizhu Chen\\nSolving Quantitative Reasoning Problems with Language Models 29 Jun 2022\\n\\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, Vedant Misra\\nExploring Length Generalization in Large Language Models 11 Jul 2022\\n\\nCem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, Behnam Neyshabur\\nScaling Instruction-Finetuned Language Models 20 Oct 2022\\n\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, Jason Wei\\nGalactica: A Large Language Model for Science 16 Nov 2022\\n\\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic\\nALERT: Adapting Language Models to Reasoning Tasks 16 Dec 2022\\n\\nPing Yu, Tianlu Wang, Olga Golovneva, Badr Alkhamissy, Gargi Ghosh, Mona Diab, Asli Celikyilmaz\\nBootstrapping and Self-Improving\\n\\nSTaR: Bootstrapping Reasoning With Reasoning 28 Mar 2022\\n\\nEric Zelikman, Yuhuai Wu, Jesse Mu, Noah D. Goodman\\nLanguage Models Can Teach Themselves to Program Better 29 Jul 2022\\n\\nPatrick Haluptzok, Matthew Bowers, Adam Tauman Kalai\\nLarge Language Models Can Self-Improve 20 Oct 2022\\n\\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han\\nEvaluation and Analysis\\n\\nAre NLP Models really able to Solve Simple Math Word Problems? 12 Mar 2021\\n\\nArkil Patel, Satwik Bhattamishra, Navin Goyal\\nImpact of Pretraining Term Frequencies on Few-Shot Reasoning 15 Feb 2022\\n\\nYasaman Razeghi, Robert L. Logan IV, Matt Gardner, Sameer Singh\\nAre Large Pre-Trained Language Models Leaking Your Personal Information? 25 May 2022\\n\\nJie Huang, Hanyin Shao, Kevin Chen-Chuan Chang\\nLarge Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change) 21 Jun 2022\\n\\nKarthik Valmeekam, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati\\nExploring Length Generalization in Large Language Models 11 Jul 2022\\n\\nCem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, Behnam Neyshabur\\nLanguage models show human-like content effects on reasoning 14 Jul 2022\\n\\nIshita Dasgupta, Andrew K. Lampinen, Stephanie C. Y. Chan, Antonia Creswell, Dharshan Kumaran, James L. McClelland, Felix Hill\\nFOLIO: Natural Language Reasoning with First-Order Logic 2 Sep 2022\\n\\nSimeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq Joty, Alexander R. Fabbri, Wojciech Kryscinski, Xi Victoria Lin, Caiming Xiong, Dragomir Radev\\nLanguage Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought 3 Oct 2022\\n\\nAbulhair Saparov, He He\\nChallenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them 17 Oct 2022\\n\\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, Jason Wei\\nLarge language models are not zero-shot communicators 26 Oct 2022\\n\\nLaura Ruis, Akbir Khan, Stella Biderman, Sara Hooker, Tim Rocktäschel, Edward Grefenstette\\nROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning 15 Dec 2022\\n\\nOlga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz\\nTowards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters 20 Dec 2022\\n\\nBoshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, Huan Sun\\nCitation\\n\\nIf you find this repo useful, please kindly cite our survey:\\n@article{huang2022towards,\\n  title={Towards Reasoning in Large Language Models: A Survey},\\n  author={Huang, Jie and Chang, Kevin Chen-Chuan},\\n  journal={arXiv preprint arXiv:2212.10403},\\n  year={2022}\\n}\\nAbout\\nThis repository contains a collection of papers and resources on Reasoning in Large Language Models.\\nTopics\\nnatural-language-processing language-modeling artificial-intelligence awesome-list human-intelligence language-models reasoning deductive-reasoning paper-list gpt-3 prompt-learning in-context-learning large-language-models llm prompt-engineering chain-of-thought chatgpt\\nResources\\nReadme\\nLicense\\nMIT license\\nActivity\\nStars\\n547 stars\\nWatchers\\n18 watching\\nForks\\n34 forks\\nReport repository\\nReleases\\nNo releases published\\nPackages 0\\nNo packages published  \\nContributors 6\\nFooter\\n© 2025 GitHub, Inc.\\nFooter navigation\\n\\nTerms\\nPrivacy\\nSecurity\\nStatus\\nDocs\\nContact\\nManage cookies\\nDo not share my personal information\\n\\nYou can’t perform that action at this time.\"}, {\"url\": \"https://openreview.net/forum?id=dCPF1wlqj8\", \"title\": \"Unlocking Reasoning Potential in Large Language Models by ...\", \"content\": \"Unlocking Reasoning Potential in Large Language Models by Scaling Code-form Planning | OpenReview Unlocking Reasoning Potential in Large Language Models by Scaling Code-form Planning TL;DR: We introduce CodePlan, a scalable paradigm that empowers LLMs to generate and follow code-form plans---pseudocode that outlines high-level, structured reasoning process. To address the limitation, we introduce CodePlan, a scalable paradigm that empowers LLMs to generate and follow code-form plans---pseudocode that outlines high-level, structured reasoning processes. Importantly, CodePlan allows the automatic extraction of code-form plans from massive, wide-ranging text corpora without the need for curated, task-specific datasets. To train CodePlan, we construct a large-scale dataset of 2M examples that integrate code-form plans with standard prompt-response pairs from existing corpora.\", \"score\": 0.6164054, \"raw_content\": \"Unlocking Reasoning Potential in Large Language Models by Scaling Code-form Planning | OpenReview\\nToggle navigationOpenReview.net\\n\\nLogin\\n\\nOpen Peer Review. Open Publishing. Open Access. Open Discussion. Open Recommendations. Open Directory. Open API. Open Source.\\n×\\nUnlocking Reasoning Potential in Large Language Models by Scaling Code-form Planning\\n\\nICLR 2025 Conference Submission3133 Authors\\n23 Sept 2024 (modified: 02 Dec 2024)ICLR 2025 Conference SubmissionEveryoneRevisionsBibTeXCC BY 4.0\\nKeywords: Large langauge model for reasoning, planning, code-aided reasoning\\nTL;DR: We introduce CodePlan, a scalable paradigm that empowers LLMs to generate and follow code-form plans---pseudocode that outlines high-level, structured reasoning process.\\nAbstract: Despite the remarkable success of large language models (LLMs) on traditional natural language processing tasks, their planning ability remains a critical bottleneck in tackling complex multi-step reasoning tasks. Existing approaches mainly rely on prompting or task-specific fine-tuning, often suffering from weak robustness and cross-task generalization. To address the limitation, we introduce CodePlan, a scalable paradigm that empowers LLMs to generate and follow code-form plans---pseudocode that outlines high-level, structured reasoning processes. By leveraging the structured and versatile nature of code, CodePlan effectively captures the rich semantics and control flows inherent to sophisticated reasoning. Importantly, CodePlan allows the automatic extraction of code-form plans from massive, wide-ranging text corpora without the need for curated, task-specific datasets. This enables it to scale up efficiently and improve reasoning capabilities across diverse scenarios. To train CodePlan, we construct a large-scale dataset of 2M examples that integrate code-form plans with standard prompt-response pairs from existing corpora. With minimal computation overhead during both training and inference, CodePlan achieves a 25.1\\\\% relative improvement compared with directly generating responses, averaged across 13 challenging multi-step reasoning benchmarks, spanning mathematical reasoning, symbolic reasoning, instruction-following, multi-hop QA, and decision-making tasks. Further analysis reveals CodePlan's increasing performance gains on more complex reasoning tasks, as well as significant data efficiency thanks to its generalization ability.\\nSupplementary Material: zip\\nPrimary Area: foundation or frontier models, including LLMs\\nCode Of Ethics: I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.\\nSubmission Guidelines: I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.\\nReciprocal Reviewing: I understand the reciprocal reviewing requirement as described on https://iclr.cc/Conferences/2025/CallForPapers. If none of the authors are registered as a reviewer, it may result in a desk rejection at the discretion of the program chairs. To request an exception, please complete this form at https://forms.gle/Huojr6VjkFxiQsUp6.\\nAnonymous Url: I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.\\nNo Acknowledgement Section: I certify that there is no acknowledgement section in this submission for double blind review.\\nSubmission Number: 3133\\nLoading\\n\\nAbout OpenReview\\nHosting a Venue\\n\\nAll Venues\\n\\n\\nContact\\n\\nFeedback\\nSponsors\\n\\nJoin the Team\\n\\n\\nFrequently Asked Questions\\n\\nTerms of Use\\n\\nPrivacy Policy\\n\\n\\nAbout OpenReview\\n\\nHosting a Venue\\nAll Venues\\nSponsors\\n\\nJoin the Team\\n\\n\\nFrequently Asked Questions\\n\\nContact\\nFeedback\\nTerms of Use\\nPrivacy Policy\\n\\nOpenReview is a long-term project to advance science through improved peer review, with legal nonprofit status through Code for Science & Society. We gratefully acknowledge the support of the OpenReview Sponsors. © 2025 OpenReview\\n×\\nSend Feedback\\nEnter your feedback below and we'll get back to you as soon as possible. To submit a bug report or feature request, you can use the official OpenReview GitHub repository:\\nReport an issue\\nSelect a topic or type what you need help with\\nCancelSend\\n×\\nBibTeX Record\\nClick anywhere on the box above to highlight complete record\\nDone\"}, {\"url\": \"https://aclanthology.org/2023.findings-acl.67/\", \"title\": \"Towards Reasoning in Large Language Models: A Survey\", \"content\": \"Towards Reasoning in Large Language Models: A Survey - ACL Anthology 2023.findings-acl.67 Findings of the Association for Computational Linguistics: ACL 2023 https://aclanthology.org/2023.findings-acl.67/ In Findings of the Association for Computational Linguistics: ACL 2023, pages 1049–1065, Toronto, Canada. Towards Reasoning in Large Language Models: A Survey (Huang & Chang, Findings 2023) https://aclanthology.org/2023.findings-acl.67.pdf url = \\\"https://aclanthology.org/2023.findings-acl.67/\\\", <title>Findings of the Association for Computational Linguistics: ACL 2023</title> <identifier type=\\\"doi\\\">10.18653/v1/2023.findings-acl.67</identifier> %S Findings of the Association for Computational Linguistics: ACL 2023 %U https://doi.org/10.18653/v1/2023.findings-acl.67 [Towards Reasoning in Large Language Models: A Survey](https://aclanthology.org/2023.findings-acl.67/) (Huang & Chang, Findings 2023) Towards Reasoning in Large Language Models: A Survey (Huang & Chang, Findings 2023) In Findings of the Association for Computational Linguistics: ACL 2023, pages 1049–1065, Toronto, Canada.\", \"score\": 0.6018984, \"raw_content\": \"Towards Reasoning in Large Language Models: A Survey - ACL Anthology\\n ACL Anthology\\n\\nNews(current)\\nFAQ(current)\\nCorrections(current)\\nSubmissions(current)\\nGithub\\n\\nTowards Reasoning in Large Language Models: A Survey\\nJie Huang, Kevin Chen-Chuan Chang\\nCorrect Metadata for\\n×\\nImportant: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF.\\nTitle Adjust the title. Retain tags such as <fixed-case>. \\nAuthors Adjust author names and order to match the PDF.\\nAdd Author\\nAbstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>.\\nALL author names, the title, and the abstract match the PDF. If paper metadata matches the PDF, but the paper should be linked to a different author page, please file an author page correction instead.\\nSubmit\\n\\nAbstract\\nReasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.\\nAnthology ID:\\n2023.findings-acl.67\\nVolume:\\nFindings of the Association for Computational Linguistics: ACL 2023\\nMonth:\\nJuly\\nYear:\\n2023\\nAddress:\\nToronto, Canada\\nEditors:\\nAnna Rogers, Jordan Boyd-Graber, Naoaki Okazaki\\nVenue:\\nFindings\\nSIG:\\nPublisher:\\nAssociation for Computational Linguistics\\nNote:\\nPages:\\n1049–1065\\nLanguage:\\nURL:\\nhttps://aclanthology.org/2023.findings-acl.67/\\nDOI:\\n10.18653/v1/2023.findings-acl.67\\nBibkey:\\nhuang-chang-2023-towards\\nCite (ACL):\\nJie Huang and Kevin Chen-Chuan Chang. 2023. Towards Reasoning in Large Language Models: A Survey. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1049–1065, Toronto, Canada. Association for Computational Linguistics.\\nCite (Informal):\\nTowards Reasoning in Large Language Models: A Survey (Huang & Chang, Findings 2023)\\nCopy Citation:\\nBibTeX Markdown MODS XML Endnote More options…\\nPDF:\\nhttps://aclanthology.org/2023.findings-acl.67.pdf\\nVideo:\\nhttps://aclanthology.org/2023.findings-acl.67.mp4\\nPDF Cite Search Video Fix data\\n\\nExport citation\\n×\\n\\nBibTeX\\nMODS XML\\nEndnote\\nPreformatted\\n\\n@inproceedings{huang-chang-2023-towards,\\n    title = \\\"Towards Reasoning in Large Language Models: A Survey\\\",\\n    author = \\\"Huang, Jie  and\\n      Chang, Kevin Chen-Chuan\\\",\\n    editor = \\\"Rogers, Anna  and\\n      Boyd-Graber, Jordan  and\\n      Okazaki, Naoaki\\\",\\n    booktitle = \\\"Findings of the Association for Computational Linguistics: ACL 2023\\\",\\n    month = jul,\\n    year = \\\"2023\\\",\\n    address = \\\"Toronto, Canada\\\",\\n    publisher = \\\"Association for Computational Linguistics\\\",\\n    url = \\\"https://aclanthology.org/2023.findings-acl.67/\\\",\\n    doi = \\\"10.18653/v1/2023.findings-acl.67\\\",\\n    pages = \\\"1049--1065\\\",\\n    abstract = \\\"Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.\\\"\\n}\\nDownload as File Copy to Clipboard\\n﻿<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<modsCollection xmlns=\\\"http://www.loc.gov/mods/v3\\\">\\n<mods ID=\\\"huang-chang-2023-towards\\\">\\n    <titleInfo>\\n        <title>Towards Reasoning in Large Language Models: A Survey</title>\\n    </titleInfo>\\n    <name type=\\\"personal\\\">\\n        <namePart type=\\\"given\\\">Jie</namePart>\\n        <namePart type=\\\"family\\\">Huang</namePart>\\n        <role>\\n            <roleTerm authority=\\\"marcrelator\\\" type=\\\"text\\\">author</roleTerm>\\n        </role>\\n    </name>\\n    <name type=\\\"personal\\\">\\n        <namePart type=\\\"given\\\">Kevin</namePart>\\n        <namePart type=\\\"given\\\">Chen-Chuan</namePart>\\n        <namePart type=\\\"family\\\">Chang</namePart>\\n        <role>\\n            <roleTerm authority=\\\"marcrelator\\\" type=\\\"text\\\">author</roleTerm>\\n        </role>\\n    </name>\\n    <originInfo>\\n        <dateIssued>2023-07</dateIssued>\\n    </originInfo>\\n    <typeOfResource>text</typeOfResource>\\n    <relatedItem type=\\\"host\\\">\\n        <titleInfo>\\n            <title>Findings of the Association for Computational Linguistics: ACL 2023</title>\\n        </titleInfo>\\n        <name type=\\\"personal\\\">\\n            <namePart type=\\\"given\\\">Anna</namePart>\\n            <namePart type=\\\"family\\\">Rogers</namePart>\\n            <role>\\n                <roleTerm authority=\\\"marcrelator\\\" type=\\\"text\\\">editor</roleTerm>\\n            </role>\\n        </name>\\n        <name type=\\\"personal\\\">\\n            <namePart type=\\\"given\\\">Jordan</namePart>\\n            <namePart type=\\\"family\\\">Boyd-Graber</namePart>\\n            <role>\\n                <roleTerm authority=\\\"marcrelator\\\" type=\\\"text\\\">editor</roleTerm>\\n            </role>\\n        </name>\\n        <name type=\\\"personal\\\">\\n            <namePart type=\\\"given\\\">Naoaki</namePart>\\n            <namePart type=\\\"family\\\">Okazaki</namePart>\\n            <role>\\n                <roleTerm authority=\\\"marcrelator\\\" type=\\\"text\\\">editor</roleTerm>\\n            </role>\\n        </name>\\n        <originInfo>\\n            <publisher>Association for Computational Linguistics</publisher>\\n            <place>\\n                <placeTerm type=\\\"text\\\">Toronto, Canada</placeTerm>\\n            </place>\\n        </originInfo>\\n        <genre authority=\\\"marcgt\\\">conference publication</genre>\\n    </relatedItem>\\n    <abstract>Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.</abstract>\\n    <identifier type=\\\"citekey\\\">huang-chang-2023-towards</identifier>\\n    <identifier type=\\\"doi\\\">10.18653/v1/2023.findings-acl.67</identifier>\\n    <location>\\n        <url>https://aclanthology.org/2023.findings-acl.67/</url>\\n    </location>\\n    <part>\\n        <date>2023-07</date>\\n        <extent unit=\\\"page\\\">\\n            <start>1049</start>\\n            <end>1065</end>\\n        </extent>\\n    </part>\\n</mods>\\n</modsCollection>\\nDownload as File Copy to Clipboard\\n﻿%0 Conference Proceedings\\n%T Towards Reasoning in Large Language Models: A Survey\\n%A Huang, Jie\\n%A Chang, Kevin Chen-Chuan\\n%Y Rogers, Anna\\n%Y Boyd-Graber, Jordan\\n%Y Okazaki, Naoaki\\n%S Findings of the Association for Computational Linguistics: ACL 2023\\n%D 2023\\n%8 July\\n%I Association for Computational Linguistics\\n%C Toronto, Canada\\n%F huang-chang-2023-towards\\n%X Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.\\n%R 10.18653/v1/2023.findings-acl.67\\n%U https://aclanthology.org/2023.findings-acl.67/\\n%U https://doi.org/10.18653/v1/2023.findings-acl.67\\n%P 1049-1065\\nDownload as File Copy to Clipboard\\nMarkdown (Informal)\\n[Towards Reasoning in Large Language Models: A Survey](https://aclanthology.org/2023.findings-acl.67/) (Huang & Chang, Findings 2023)\\n\\nTowards Reasoning in Large Language Models: A Survey (Huang & Chang, Findings 2023)\\n\\nACL\\n\\nJie Huang and Kevin Chen-Chuan Chang. 2023. Towards Reasoning in Large Language Models: A Survey. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1049–1065, Toronto, Canada. Association for Computational Linguistics.\\n\\nCopy Markdown to Clipboard Copy ACL to Clipboard\\n ACL materials are Copyright © 1963–2025 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a Creative Commons Attribution 4.0 International License.\\nThe ACL Anthology is managed and built by the ACL Anthology team of volunteers.\\nSite last built on 16 February 2025 at 13:12 UTC with commit 0124618.\"}, {\"url\": \"https://machinelearning.apple.com/research/gsm-symbolic\", \"title\": \"GSM-Symbolic: Understanding the Limitations of Mathematical ...\", \"content\": \"GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models - Apple Machine Learning Research GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. Furthermore, we investigate the fragility of mathematical reasoning in these models and show that their performance significantly deteriorates as the number of clauses in a question increases. GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models\", \"score\": 0.5900477, \"raw_content\": \"GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models - Apple Machine Learning Research\\nMachine Learning Research\\nOpen MenuClose Menu\\n\\nOverview\\nResearch\\nEvents\\nWork with us\\n\\nresearch area Speech and Natural Language Processing\\ncontent type paper | published October 2024\\nGSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models\\nAuthorsIman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, Mehrdad Farajtabar\\nView publication\\nView dataset (GitHub)\\nCopy Bibtex\\nRecent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. To address these concerns, we conduct a large-scale study on several SOTA open and closed models. To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of models. Our findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, we investigate the fragility of mathematical reasoning in these models and show that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is because current LLMs cannot perform genuine logical reasoning; they replicate reasoning steps from their training data. Adding a single clause that seems relevant to the question causes significant performance drops (up to 65%) across all state-of-the-art models, even though the clause doesn't contribute to the reasoning chain needed for the final answer. Overall, our work offers a more nuanced understanding of LLMs' capabilities and limitations in mathematical reasoning.\\nGenerated datasets are available on Github and HuggingFace.\\n\\nFigure 1: Generating diverse examples from templates.\\nRelated readings and updates.\\nApplying RLAIF for Code Generation with API-usage in Lightweight LLMs\\nThis paper was accepted at the Natural Language Reasoning and Structured Explanations workshop at ACL 2024. Reinforcement Learning from AI Feedback (RLAIF) has demonstrated significant potential across various domains, including mitigating harm in LLM outputs, enhancing text summarization, and mathematical reasoning. This paper introduces an RLAIF framework for improving the code generation abilities of lightweight (<1B parameters) LLMs. We…\\nSee paper details\\nWhen Can Transformers Reason With Abstract Symbols?\\nWe investigate the capabilities of transformer models on relational reasoning tasks. In these tasks, models are trained on a set of strings encoding abstract relations, and are then tested out-of-distribution on data that contains symbols that did not appear in the training dataset. We prove that for any relational reasoning task in a large family of tasks, transformers learn the abstract relations and generalize to the test set when trained by…\\nSee paper details\\n\\nDiscover opportunities in Machine Learning.\\nOur research in machine learning breaks new ground every day.\\nWork with us\\n\\nMachine Learning Research\\nResearch\\n\\nGSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models\\n\\n\\nPrivacy Policy\\n\\nTerms of Use\\nLegal\\n\\nCopyright © 2025 Apple Inc. All rights reserved.\"}, {\"url\": \"https://sebastianraschka.com/blog/2025/understanding-reasoning-llms.html\", \"title\": \"Understanding Reasoning LLMs - Sebastian Raschka\", \"content\": \"(3) DeepSeek-R1-Distill*: Using the SFT data generated in the previous steps, the DeepSeek team fine-tuned Qwen and Llama models to enhance their reasoning abilities. This model improves upon DeepSeek-R1-Zero by incorporating additional supervised fine-tuning (SFT) and reinforcement learning (RL) to improve its reasoning performance. Supervised fine-tuning (SFT) plus RL, which led to DeepSeek-R1, DeepSeek’s flagship reasoning model. This comparison provides some additional insights into whether pure RL alone can induce reasoning capabilities in models much smaller than DeepSeek-R1-Zero. More precisely, I believe o1 starts from a weaker, smaller base model than DeepSeek-R1 but compensates with RL + SFT and inference-time scaling. Either way, ultimately, DeepSeek-R1 is a major milestone in open-weight reasoning models, and its efficiency at inference time makes it an interesting alternative to OpenAI’s o1.\", \"score\": 0.44420364, \"raw_content\": \"Understanding Reasoning LLMs\\nMethods and Strategies for Building and Refining Reasoning Models\\n\\n        Feb 5, 2025\\n         by Sebastian Raschka\\n        \\n    \\nIn this article, I will describe the four main approaches to building reasoning models, or how we can enhance LLMs with reasoning capabilities. I hope this provides valuable insights and helps you navigate the rapidly evolving literature and hype surrounding this topic.\\nIn 2024, the LLM field saw increasing specialization. Beyond pre-training and fine-tuning, we witnessed the rise of specialized applications, from RAGs to code assistants. I expect this trend to accelerate in 2025, with an even greater emphasis on domain- and application-specific optimizations (i.e., âspecializationsâ).\\n\\nThe development of reasoning models is one of these specializations. This means we refine LLMs to excel at complex tasks that are best solved with intermediate steps, such as puzzles, advanced math, and coding challenges. However, this specialization does not replace other LLM applications. Because transforming an LLM into a reasoning model also introduces certain drawbacks, which I will discuss later.\\nTo give you a brief glimpse of whatâs covered below, in this article, I will:\\nI hope you find this article useful as AI continues its rapid development this year!\\n\\n        \\n        \\n          How do we define âreasoning modelâ? \\n\\nIf you work in AI (or machine learning in general), you are probably familiar with vague and hotly debated definitions. The term âreasoning modelsâ is no exception. Eventually, someone will define it formally in a paper, only for it to be redefined in the next, and so on.\\nIn this article, I define âreasoningâ as the process of answering questions that require complex, multi-step generation with intermediate steps. For example, factual question-answering like âWhat is the capital of France?â does not involve reasoning. In contrast, a question like âIf a train is moving at 60 mph and travels for 3 hours, how far does it go?â requires some simple reasoning. For instance, it requires recognizing the relationship between distance, speed, and time before arriving at the answer.\\n\\nMost modern LLMs are capable of basic reasoning and can answer questions like, âIf a train is moving at 60 mph and travels for 3 hours, how far does it go?â So, today, when we refer to reasoning models, we typically mean LLMs that excel at more complex reasoning tasks, such as solving puzzles, riddles, and mathematical proofs.\\nAdditionally, most LLMs branded as reasoning models today include a âthoughtâ or âthinkingâ process as part of their response. Whether and how an LLM actually âthinksâ is a separate discussion.\\nIntermediate steps in reasoning models can appear in two ways. First, they may be explicitly included in the response, as shown in the previous figure. Second, some reasoning LLMs, such as OpenAIâs o1, run multiple iterations with intermediate steps that are not shown to the user.\\n\\n\\n        \\n        \\n          When should we use reasoning models? \\n\\nNow that we have defined reasoning models, we can move on to the more interesting part: how to build and improve LLMs for reasoning tasks. However, before diving into the technical details, it is important to consider when reasoning models are actually needed.\\nWhen do we need a reasoning model?Â Reasoning models are designed to be good at complex tasks such as solving puzzles, advanced math problems, and challenging coding tasks. However, they are not necessary for simpler tasks like summarization, translation, or knowledge-based question answering. In fact, using reasoning models for everything can be inefficient and expensive. For instance, reasoning models are typically more expensive to use, more verbose, and sometimes more prone to errors due to âoverthinking.â Also here the simple rule applies: Use the right tool (or type of LLM) for the task.\\nThe key strengths and limitations of reasoning models are summarized in the figure below.\\n\\n\\n        \\n        \\n          A brief look at the DeepSeek training pipeline \\n\\nBefore discussing four main approaches to building and improving reasoning models in the next section, I want to briefly outline the DeepSeek R1 pipeline, as described in theÂ DeepSeek R1 technical report. This report serves as both an interesting case study and a blueprint for developing reasoning LLMs.\\nNote that DeepSeek did not release a single R1 reasoning model but instead introduced three distinct variants: DeepSeek-R1-Zero, DeepSeek-R1, and DeepSeek-R1-Distill.\\nBased on the descriptions in the technical report, I have summarized the development process of these models in the diagram below.\\n\\nNext, letâs briefly go over the process shown in the diagram above. More details will be covered in the next section, where we discuss the four main approaches to building and improving reasoning models.\\n(1) DeepSeek-R1-Zero: This model is based on the 671B pre-trained DeepSeek-V3 base model released in December 2024. The research team trained it using reinforcement learning (RL) with two types of rewards. This approach is referred to as âcold startâ training because it did not include a supervised fine-tuning (SFT) step, which is typically part of reinforcement learning with human feedback (RLHF).\\n(2) DeepSeek-R1: This is DeepSeekâs flagship reasoning model, built upon DeepSeek-R1-Zero. The team further refined it with additional SFT stages and further RL training, improving upon the âcold-startedâ R1-Zero model.\\n(3) DeepSeek-R1-Distill*: Using the SFT data generated in the previous steps, the DeepSeek team fine-tuned Qwen and Llama models to enhance their reasoning abilities. While not distillation in the traditional sense, this process involved training smaller models (Llama 8B and 70B, and Qwen 1.5Bâ30B) on outputs from the larger DeepSeek-R1 671B model.\\n\\n        \\n        \\n          The 4 main ways to build and improve reasoning models \\n\\nIn this section, I will outline the key techniques currently used to enhance the reasoning capabilities of LLMs and to build specialized reasoning models such as DeepSeek-R1, OpenAIâs o1 & o3, and others.\\nNote: The exact workings of o1 and o3 remain unknown outside of OpenAI. However, they are rumored to leverage a combination of both inference and training techniques.\\n\\n        \\n        \\n          1) Inference-time scaling \\n\\nOne way to improve an LLMâs reasoning capabilities (or any capability in general) is inference-time scaling. This term can have multiple meanings, but in this context, it refers to increasing computational resources during inference to improve output quality.\\nA rough analogy is how humans tend to generate better responses when given more time to think through complex problems. Similarly, we can apply techniques that encourage the LLM to âthinkâ more while generating an answer. (Although, whether LLMs actually âthinkâ is a different discussion.)\\nOne straightforward approach to inference-time scaling is clever prompt engineering. A classic example is chain-of-thought (CoT) prompting, where phrases like âthink step by stepâ are included in the input prompt. This encourages the model to generate intermediate reasoning steps rather than jumping directly to the final answer, which can often (but not always) lead to more accurate results on more complex problems. (Note that it doesnât make sense to employ this strategy for simpler knowledge-based questions, like âWhat is the capital of Franceâ, which is again a good rule of thumb to find out whether a reasoning model makes sense on your given input query.)\\n\\nThe aforementioned CoT approach can be seen as inference-time scaling because it makes inference more expensive through generating more output tokens.\\nAnother approach to inference-time scaling is the use of voting and search strategies. One simple example is majority voting where we have the LLM generate multiple answers, and we select the correct answer by majority vote. Similarly, we can use beam search and other search algorithms to generate better responses.\\nI highly recommend the Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model ParametersÂ paper that I described in my previous Noteworthy AI Research Papers of 2024 (Part Two) article (https://magazine.sebastianraschka.com/p/ai-research-papers-2024-part-2) for more details on these different strategies.\\n\\nThe DeepSeek R1 technical report states that its models do not use inference-time scaling. However, this technique is often implemented at the application layer on top of the LLM, so it is possible that DeepSeek applies it within their app.\\nI suspect that OpenAIâs o1 and o3 models use inference-time scaling, which would explain why they are relatively expensive compared to models like GPT-4o. In addition to inference-time scaling, o1 and o3 were likely trained using RL pipelines similar to those used for DeepSeek R1. More on reinforcement learning in the next two sections below.\\n\\n        \\n        \\n          2) Pure reinforcement learning (RL) \\n\\nOne of my personal highlights from the DeepSeek R1 paperÂ is their discovery that reasoning emerges as a behavior from pure reinforcement learning (RL). Letâs explore what this means in more detail.\\nAs outlined earlier, DeepSeek developed three types of R1 models. The first, DeepSeek-R1-Zero, was built on top of the DeepSeek-V3 base model, a standard pre-trained LLM they released in December 2024. Unlike typical RL pipelines, where supervised fine-tuning (SFT) is applied before RL, DeepSeek-R1-Zero was trained exclusivelyÂ with reinforcement learning without an initial SFT stage as highlighted in the diagram below.\\n\\nStill, this RL process is similar to the commonly used RLHFÂ approach, which is typically applied to preference-tune LLMs. (I covered RLHF in more detail in my article,Â LLM Training: RLHF and Its Alternatives.) However, as mentioned above, the key difference in DeepSeek-R1-ZeroÂ is that they skipped the supervised fine-tuning (SFT) stage for instruction tuning. This is why they refer to it as âpureâ RL. (Although, RL in the context of LLMs differs significantly from traditional RL, which is a topic for another time.)\\nFor rewards, instead of using a reward model trained on human preferences, they employed two types of rewards: an accuracy rewardÂ and a format reward.\\nSurprisingly, this approach was enough for the LLM to develop basic reasoning skills. The researchers observed an âAha!â moment, where the model began generating reasoning traces as part of its responses despite not being explicitly trained to do so, as shown in the figure below.\\n\\nWhile R1-Zero is not a top-performing reasoning model, it does demonstrate reasoning capabilities by generating intermediate âthinkingâ steps, as shown in the figure above. This confirms that it is possible to develop a reasoning model using pure RL, and the DeepSeek team was the first to demonstrate (or at least publish) this approach.\\n\\n        \\n        \\n          3) Supervised finetuning and reinforcement learning (SFT + RL) \\n\\nNext, letâs look at the development of DeepSeek-R1, DeepSeekâs flagship reasoning model, which serves as a blueprint for building reasoning models. This model improves upon DeepSeek-R1-Zero by incorporating additional supervised fine-tuning (SFT) and reinforcement learning (RL) to improve its reasoning performance.\\nNote that it is actually common to include an SFT stage before RL, as seen in the standard RLHF pipeline. OpenAIâs o1 was likely developed using a similar approach.\\n\\nAs shown in the diagram above, the DeepSeek team used DeepSeek-R1-Zero to generate what they call âcold-startâ SFT data. The term âcold startâ refers to the fact that this data was produced by DeepSeek-R1-Zero, which itself had not been trained on any supervised fine-tuning (SFT) data.\\nUsing this cold-start SFT data, DeepSeek then trained the model via instruction fine-tuning, followed by another reinforcement learning (RL) stage. This RL stage retained the same accuracy and format rewards used in DeepSeek-R1-Zeroâs RL process. However, they added a consistency reward to prevent language mixing, which occurs when the model switches between multiple languages within a response.\\nThe RL stage was followed by another round of SFT data collection. In this phase, the most recent model checkpoint was used to generate 600K Chain-of-Thought (CoT) SFT examples, while an additional 200K knowledge-based SFT examples were created using the DeepSeek-V3 base model.\\nThese 600K + 200K SFT samples were then used for instruction-finetuning DeepSeek-V3 base before following up with a final round of RL. In this stage, they again used rule-based methods for accuracy rewards for math and coding questions, while human preference labels used for other question types. All in all, this is very similar to regular RLHF except that the SFT data contains (more) CoT examples. And the RL has verifiable rewards in addition to human preference-based rewards.\\nThe final model, DeepSeek-R1 has a noticeable performance boost over DeepSeek-R1-Zero thanks to the additional SFT and RL stages, as shown in the table below.\\n\\n\\n        \\n        \\n          4) Pure supervised finetuning (SFT) and distillation \\n\\nSo far, we have covered three key approaches to building and improving reasoning models:\\nInference-time scaling, a technique that improves reasoning capabilities without training or otherwise modifying the underlying model.\\nPure reinforcement learning (RL) as in DeepSeek-R1-Zero, which showed that reasoning can emerge as a learned behavior without supervised fine-tuning.\\nSupervised fine-tuning (SFT) plus RL, which led to DeepSeek-R1, DeepSeekâs flagship reasoning model.\\nSo, whatâs left? Model âdistillation.â\\nSurprisingly, DeepSeek also released smaller models trained via a process they call distillation. However, in the context of LLMs, distillation does not necessarily follow the classical knowledge distillation approach used in deep learning. Traditionally, in knowledge distillation (as briefly described in Chapter 6 of my Machine Learning Q and AIÂ book), a smaller student model is trained on both the logits of a larger teacher model and a target dataset.\\nInstead, here distillation refers to instruction fine-tuning smaller LLMs, such as Llama 8B and 70B and Qwen 2.5 models (0.5B to 32B), on an SFT dataset generated by larger LLMs. Specifically, these larger LLMs are DeepSeek-V3 and an intermediate checkpoint of DeepSeek-R1. In fact, the SFT data used for this distillation process is the same dataset that was used to train DeepSeek-R1, as described in the previous section.\\nTo clarify this process, I have highlighted the distillation portion in the diagram below.\\n\\nWhy did they develop these distilled models? In my opinion, there are two key reasons:\\nSmaller models are more efficient. This means they are cheaper to run, but they also can run on lower-end hardware, which makes these especially interesting for many researchers and tinkerers like me.\\nA case study in pure SFT. These distilled models serve as an interesting benchmark, showing how far pure supervised fine-tuning (SFT) can take a model without reinforcement learning.\\nThe table below compares the performance of these distilled models against other popular models, as well as DeepSeek-R1-Zero and DeepSeek-R1.\\n\\nAs we can see, the distilled models are noticeably weaker than DeepSeek-R1, but they are surprisingly strong relative to DeepSeek-R1-Zero, despite being orders of magnitude smaller. Itâs also interesting to note how well these models perform compared to o1 mini (I suspect o1-mini itself might be a similarly distilled version of o1).\\nBefore wrapping up this section with a conclusion, thereâs one more interesting comparison worth mentioning. The DeepSeek team tested whether the emergent reasoning behavior seen in DeepSeek-R1-Zero could also appear in smaller models. To investigate this, they applied the same pure RL approach from DeepSeek-R1-Zero directly to Qwen-32B.\\nThe results of this experiment are summarized in the table below, where QwQ-32B-Preview serves as a reference reasoning model based on Qwen 2.5 32B developed by the Qwen team (I think the training details were never disclosed). This comparison provides some additional insights into whether pure RL alone can induce reasoning capabilities in models much smaller than DeepSeek-R1-Zero.\\n\\nInterestingly, the results suggest that distillation is far more effective than pure RL for smaller models. This aligns with the idea that RL alone may not be sufficient to induce strong reasoning abilities in models of this scale, whereas SFT on high-quality reasoning data can be a more effective strategy when working with small models.\\nFor completeness, it would have been useful to see additional comparisons in the table:\\nQwen-32B trained with SFT + RL, similar to how DeepSeek-R1 was developed. This would help determine how much improvement can be made, compared to pure RL and pure SFT, when RL is combined with SFT.\\nDeepSeek-V3 trained with pure SFT, similar to how the distilled models were created. This would allow for a direct comparison to see how effective RL + SFT is over pure SFT.\\n\\n        \\n        \\n          Conclusion \\n\\nIn this section, we explored four different strategies for building and improving reasoning models:\\nInference-time scaling requires no additional training but increases inference costs, making large-scale deployment more expensive as the number or users or query volume grows. Still, it remains a no-brainer for improving the performance of already strong models. I strongly suspect that o1 leverages inference-time scaling, which helps explain why it is more expensive on a per-token basis compared to DeepSeek-R1.\\nPure RL is interesting for research purposes because it provides insights into reasoning as an emergent behavior. However, in practical model development, RL + SFT is the preferred approach as it leads to stronger reasoning models. I strongly suspect that o1 was trained using RL + SFT as well. More precisely, I believe o1 starts from a weaker, smaller base model than DeepSeek-R1 but compensates with RL + SFT and inference-time scaling.\\nAs mentioned above, RL + SFT is the key approach for building high-performance reasoning models. DeepSeek-R1 is a nice blueprint showing how this can be done.\\nDistillation is an attractive approach, especially for creating smaller, more efficient models. However, the limitation is that distillation does not drive innovation or produce the next generation of reasoning models. For instance, distillation always depends on an existing, stronger model to generate the supervised fine-tuning (SFT) data.\\nOne interesting aspect I expect to see next is to combine RL + SFT (approach 3) with inference-time scaling (approach 1). This is likely what OpenAI o1 is doing, except itâs probably based on a weaker base model than DeepSeek-R1, which explains why DeepSeek-R1 performs so well while remaining relatively cheap at inference time.\\n\\n        \\n        \\n          Thoughts about DeepSeek R1 \\n\\nIn recent weeks, many people have asked for my thoughts on the DeepSeek-R1 models. In short, I think they are an awesome achievement. As a research engineer, I particularly appreciate the detailed technical report, which provides insights into their methodology that I can learn from.\\nOne of the most fascinating takeaways is how reasoning emerged as a behavior from pure RL. And itâs impressive that DeepSeek has open-sourced their models under a permissive open-source MIT license, which has even fewer restrictions than Metaâs Llama models.\\nHow does it compare to o1?\\nIs DeepSeek-R1 better than o1? Iâd say itâs roughly in the same ballpark. However, what stands out is that DeepSeek-R1 is more efficient at inference time. This suggests that DeepSeek likely invested more heavily in the training process, while OpenAI may have relied more on inference-time scaling for o1.\\nThat said, itâs difficult to compare o1 and DeepSeek-R1 directly because OpenAI has not disclosed much about o1. For instance, we donât know:\\nWithout knowing these details, a direct comparison remains an apples-to-oranges comparison.\\nThe cost of training DeepSeek-R1\\nAnother point of discussion has been the cost of developing DeepSeek-R1. Some have mentioned a ~$6 million training cost, but they likely conflated DeepSeek-V3 (the base model released in December last year) and DeepSeek-R1.\\nThe $6 million estimate is based on an assumed $2 per GPU hour and the number of GPU hours required for the final training run of DeepSeek-V3, which was originally discussed back in December 2024.\\nHowever, the DeepSeek team has never disclosed the exact GPU hours or development cost for R1, so any cost estimates remain pure speculation.\\nEither way, ultimately, DeepSeek-R1 is a major milestone in open-weight reasoning models, and its efficiency at inference time makes it an interesting alternative to OpenAIâs o1.\\n\\n        \\n        \\n          Developing reasoning models on a limited budget \\n\\nDeveloping a DeepSeek-R1-level reasoning model likely requires hundreds of thousands to millions of dollars, even when starting with an open-weight base model like DeepSeek-V3. This can feel discouraging for researchers or engineers working with limited budgets.\\nThe good news: Distillation can go a long way\\nFortunately, model distillation offers a more cost-effective alternative. The DeepSeek team demonstrated this with their R1-distilled models, which achieve surprisingly strong reasoning performance despite being significantly smaller than DeepSeek-R1. However, even this approach isnât entirely cheap. Their distillation process used 800K SFT samples, which requires substantial compute.\\nInterestingly, just a few days before DeepSeek-R1 was released, I came across an article about Sky-T1, a fascinating project where a small team trained an open-weight 32B model using only 17K SFT samples. The total cost? Just $450, which is less than the registration fee for most AI conferences.\\nThis example highlights that while large-scale training remains expensive, smaller, targeted fine-tuning efforts can still yield impressive results at a fraction of the cost.\\n\\nAccording to their benchmarks, Sky-T1 performs roughly on par with o1, which is impressive given its low training cost.\\nPure RL on a budget: TinyZero\\nWhile Sky-T1 focused on model distillation, I also came across some interesting work in the âpure RLâ space. One notable example is TinyZero, a 3B parameter model that replicates the DeepSeek-R1-Zero approach (side note: it costs less than $30 to train).\\nSurprisingly, even at just 3B parameters, TinyZero exhibits some emergent self-verification abilities, which supports the idea that reasoning can emerge through pure RL, even in small models.\\nThe TinyZero repositoryÂ mentions that a research report is still work in progress, and Iâll definitely be keeping an eye out for further details.\\n\\nThe two projects mentioned above demonstrate that interesting work on reasoning models is possible even with limited budgets. While both approaches replicate methods from DeepSeek-R1, one focusing on pure RL (TinyZero) and the other on pure SFT (Sky-T1), it would be fascinating to explore how these ideas can be extended further.\\nBeyond Traditional SFT: Journey Learning\\nOne particularly interesting approach I came across last year is described in the paperÂ O1 Replication Journey: A Strategic Progress Report â Part 1. Despite its title, the paper does not actually replicate o1. Instead, it introduces an different way to improve the distillation (pure SFT) process.\\nThe key idea in the paper is âjourney learningâ as an alternative to âshortcut learning.â\\nThis approach is kind of related to the self-verification abilities observed in TinyZeroâs pure RL training, but it focuses on improving the model entirely through SFT. By exposing the model to incorrect reasoning paths and their corrections, journey learning may also reinforce self-correction abilities, potentially making reasoning models more reliable this way.\\n\\nThis could be an exciting direction for future work, particularly for low-budget reasoning model development, where RL-based approaches may be computationally impractical.\\nAnyways, a lot of interesting work is currently happening on the reasoning model front, and Iâm sure we will see a lot more exciting work in the upcoming months!\\nIf you read the book and have a few minutes to spare, I'd really appreciate a\\n      brief review. It helps us authors a lot!\\n© 2013-2025 Sebastian Raschka\\n\"}, {\"url\": \"https://hdsr.mitpress.mit.edu/pub/jaqt0vpb\", \"title\": \"Confidence in the Reasoning of Large Language Models\", \"content\": \"Comparison of the large language models on the tendency to change their initial answers in the causal judgment task (n =187 questions) after Simple, Neutral, and Post-confidence rethink prompts. Although we observe some correlation between qualitative and quantitative confidence, the material effects of prompting on the tendency to change their answers and the overconfidence when explicitly asked for their level of confidence indicate that the current LLMs do not have any internally coherent sense of confidence. Comparison of the large language models on the tendency to change their initial answers in the causal judgment task after Simple, Neutral, and Post-confidence rethink prompts.\", \"score\": 0.39880496, \"raw_content\": \"Skip to main content\\nSearch\\nDashboard\\nLogin or Signup\\nHOME\\nISSUES\\nSECTIONS\\nCOLUMNS\\nCOLLECTIONS\\nPODCAST\\nSUBMIT\\nABOUT\\nMASTHEAD\\nIssue 7.1, Winter 2025\\n1 more\\nPublished on\\nJan 30, 2025\\nDOI\\n10.1162/99608f92.b033a087\\nConfidence in the Reasoning of Large Language Models\\nby Yudi Pawitan and Chris Holmes\\nPublished on\\nJan 30, 2025\\nCITE\\nSOCIAL\\nDOWNLOAD\\nCONTENTS\\nlast released\\n7 days ago\\nSHOW DETAILS\\nABSTRACT\\nThere is a growing literature on reasoning by large language models (LLMs), but the discussion on the uncertainty in their responses is still lacking. Our aim is to assess the extent of confidence that LLMs have in their answers and how it correlates with accuracy. Confidence is measured (i) qualitatively in terms of persistence in keeping their answer when prompted to reconsider, and (ii) quantitatively in terms of self-reported confidence score. We investigate the performance of three LLMs—GPT4o, GPT4-turbo, and Mistral–on two benchmark sets of questions on causal judgment and formal fallacies, and a set of probability and statistical puzzles and paradoxes. Although the LLMs show significantly better performance than random guessing, there is a wide variability in their tendency to change their initial answers. There is a positive correlation between qualitative confidence and accuracy, but the overall accuracy for the second answer is often worse than for the first answer. There is a strong tendency to overstate the self-reported confidence score. Confidence is only partially explained by the underlying token-level probability. The material effects of prompting on qualitative confidence and the strong tendency for overconfidence indicate that current LLMs do not have any internally coherent sense of confidence.\\nKeywords: artificial intelligence, BIG-Bench AI tests, chatbots, generative AI, statistical inference, statistical puzzles and paradoxes\\nMedia Summary\\nOur aim is to assess whether current chatbots or large language models (LLMs) possess genuine reasoning abilities beyond pattern recognition, specifically on how LLMs handle uncertainty and express confidence in their responses. Confidence is measured (a) qualitatively in terms of persistence in keeping their answer when prompted to reconsider, and (b) quantitatively in terms of self-reported confidence score from 0 to 100. We investigate the performance of three LLMs—GPT4o, GPT4-turbo, and Mistral—on two challenging benchmark sets of questions on causal judgment and formal fallacies, and a set of statistical puzzles.\\nThe LLMs perform similarly with each other and significantly better than random guessing, but their tendency to change their initial answers varies greatly, ranging from 13% to 98% depending on the model and the task. The good news is that there is a positive correlation between confidence and accuracy—higher confidence means higher accuracy—and a positive correlation between the LLMs’ persistence in their answers and their self-reported confidence score. However, the positive correlation is not the full story. The strong influence of prompting on both measures suggests a lack of internal consistency. There is also a tendency for overconfidence, marked by a large gap between the LLMs’ confidence score and the actual accuracy; the LLMs frequently report 100% confidence in their answers, even when those answers are incorrect. This indicates a lack of genuine understanding of uncertainty.\\nIn summary, while LLMs demonstrate impressive reasoning capabilities, they lack the introspective awareness and understanding of uncertainty characteristic of human reasoning. Our study highlights the need for caution when interpreting LLMs’ responses, particularly when they express high confidence. Users should be aware that current LLMs do not have a coherent understanding of uncertainty. It is not clear how to elicit a meaningful and externally validated measure of uncertainty from the LLMs, as they can be easily influenced by the phrasing of the prompt, and they tend to be overconfident.\\n\\nIntroduction and Summary\\n\\nThe emergence of large language models (LLMs) such as OpenAI’s GPT series has sparked significant interest and debate within the field of artificial intelligence. These complex neural network models, designed as a next-word (technically next-token) predictor and trained on vast amounts of data, have demonstrated an unprecedented ability to generate coherent and contextually appropriate text responses. This humanlike capability has led to speculation about emergent qualities, whether these models can ‘reason’ and ‘know’ or ‘understand’ the content they generate or if they are merely sophisticated pattern recognizers. The literature suggests a full spectrum of possibilities from the skeptical (Stechly et al., 2023; Ullman, 2023) to the sanguine (Kadavath et al., 2022; Kosinski, 2023).\\nOne marker of humanlike reasoning is awareness and recognition of potential uncertainty or its corresponding confidence in the answer. Technically, LLMs use statistical prediction, but it is not obvious what confidence they have in their responses. When we ask for an expert opinion, we usually expect it to come with some measure of confidence. This measure is standard in statistical expert systems, and a validated correlation between confidence level and reality plays a key role in establishing the systems’ credibility. Thus, our aim is to assess the degree of confidence LLMs have in their answers and how that confidence correlates with actual performance.\\nThe assessment of uncertainty in complex statistical problems is typically done using the bootstrap method. This will require access to raw data or some strong assumptions about the data distribution. Neither is feasible with the current LLMs, so we will instead rely on simple empirical methods. We measure confidence qualitatively and quantitatively as follows. For the former, the LLMs are prompted to reconsider their initial answers (regardless of their correctness). Presumably, an LLM is not going to change its mind if it is highly confident, and vice versa, it will change its mind if it has low confidence. For quantitative confidence, we ask them to explicitly tell us their confidence score in their responses. We also investigate the relationship between these confidence measures and the token-level probability produced by the LLM.\\nWe investigate three LLMs—GPT4o, GPT4-turbo, and Mistral—and use two of the BBH tasks (Suzgun et al., 2022): causal judgment (187 questions) and formal fallacies (250 questions). Furthermore, we assess the statistical reasoning abilities of LLMs in solving some probability and statistical puzzles and paradoxes (46 questions) from Pawitan and Lee (2024).\\nTo summarize briefly, in line with previous results, the LLMs perform significantly better than random guessing. However, when prompted to rethink their answers, they frequently change their mind and the overall accuracy of the second answers is often worse than that of the original answers, sometimes even worse than random guessing. The tendency to change their mind is strongly affected by the phrasing of the prompt. There is a large discrepancy between qualitative and quantitative confidence, although we observe a significant correlation between them. When asked for confidence score, there is a strong tendency for overconfidence. The confidence measures are only partially explained by the underlying token-level probability. Overall, current LLMs do not show internally coherent sense of uncertainty or confidence in their answers.\\n\\nBackground\\n2.1. Testing the Reasoning Skills of an LLM\\n\\nHuman intelligence is characterized not only by reasoning and understanding but also introspection. Can LLMs, with their vast but opaque neural networks, claim similar capabilities? Their architectural complexity and the huge number of parameters (∼175 billion for GPT3 and likely more than 1 trillion for the GPT4 series) have made their operations noninterpretable, much like the mysterious processes of our own brain.\\nHow do we assess novel reasoning abilities in machines? To be useful and informative, at the current stage of development we do not yet need to go to the ultimate Turing test (Turing, 1950). Traditional measures, such as the ability to recognize keywords, often just indicate a trained behavior, but do not necessarily reflect true cognitive skills. Tasks such as arithmetic calculations are too algorithmic and will offer little insight into emergent skills. (Even relatively recent LLMs, such as GPT3.5, are actually poor at arithmetic, but this issue is solved by the most recent ones, which can recognize and transfer the problem to specialized modules.)\\nMany logical puzzles can be navigated through keyword recognition, making it difficult to discern truly novel reasoning. Emergent reasoning abilities, in contrast, would be indicated by an AI’s capacity to independently recognize and adapt to new problem patterns. What is needed are tests involving nonalgorithmic and abstract reasoning challenges to better probe the extent of AI cognition.\\n2.2. Empirical Studies\\nThe Beyond the Imitation Game Benchmark or BIG-Bench (Srivastava et al., 2022) is an extensive collaborative benchmark intended to probe LLMs in 204 cognitive and problem-solving tasks that are believed to be beyond the capabilities of LLMs. The tasks include linguistics, childhood development, mathematics, commonsense reasoning, biology, physics, social bias, software development, movie recommendations, and so on. Indicating the level of interest and admirable commitment, the BIG-Bench was developed by 450 authors from 132 institutions. When the paper was first published, LLMs did not perform very well. For their normalized preferred metric, tasks are calibrated so that a score of 0 corresponds to poor performance and a score of 100 corresponds to very good performance. Human experts would be expected to achieve scores close to 100. When averaged on all tasks, the best performing language models achieved a score of less than 20. However, LLM performance has improved substantially; for instance, GPT4 performs similarly or better than the human in 17 of the 23 BIG-Bench Hard (BBH) tasks (Zhou et al., 2024, Table 3).\\nAnother marker of reasoning is the ability to make plans. Valmeekam et al. (2023) tested some LLMs in the domains typically used in the International Planning Competition, including the well-known Blocks World, and found that LLMs’ ability “to generate executable plans autonomously is rather limited, with the best model (GPT4) having an average success rate of ∼ 12% across the domains.”\\n2.3. Better Response From Better Prompting\\nThe vastness of the data set used to train an LLM—for example, 1.4 trillion tokens described in Touvron et al. (2023)—poses a challenge for the LLM in aligning its responses with the intended context of the queries. In addition to the hallucination problem, LLMs may give different answers to semantically similar questions such as these from Mizrahi et al. (2024): (A) Which word, ‘eight’ or ‘mouth,’ is pronounced like ‘ate’? (B) Please identify the homophone of the word ate from the two options eight and mouth. Other examples of the sensitivity of LLMs to prompt phrasing are given in Zhao et al. (2021) and Srivastava et al. (2022).\\nTo improve context and relevance, techniques such as chain-of-thought (CoT) prompting (Kojima et al., 2022; Suzgun et al., 2022; Wei et al., 2022; Weng et al., 2023) and decomposition-based prompting or self-compose reasoning (Shinn et al., 2023; Zhou et al., 2024) have been developed. These techniques involve guiding the LLM through a logical sequence of thoughts or steps to arrive at a conclusion, somewhat similar to how a human might think through a problem. The CoT method helps to better align the LLM’s response with the user’s intent, but it is still a question whether these steps give an LLM the ability to reason independently of its training.\\n\\nMethods\\n3.1. LLMs\\n\\nWe compare the performance of OpenAI’s GPT4o (version 2024-08-06), GPT4-turbo (version 2024-04-09), and Mistral (Large 2 model, version 2024-07-24). GPT4o is the current flagship model from OpenAI; it is an optimized version of the original flagship GPT4. GPT4o is designed to have similar reasoning power but with improved computational efficiency. Additionally, GPT4o is capable of handling nonverbal multimodal input and output (images and sound), though none of the tasks we use here needs this new feature. GPT4-turbo is also a variant of GPT4, optimized for cost and speed with some compromises (fewer parameters?) and is recommended by OpenAI for applications that require faster processing. Mistral Large 2 model is the largest model from Mistral AI; it gives competitive performance versus other LLMs in general knowledge and reasoning benchmarks, particularly in the Massive Multitask Language Understanding (MMLU); see https://mistral.ai/news/mistral-large-2407/.\\nTo reduce randomness, we set the temperature parameter to 0. However, even at this temperature, there is still a small randomness, leading to different answers in ∼1% of the questions. (This explains why the accuracies in different tables may not be exactly the same, as they are based on different runs.) See Figure B2 in the appendix of this article for more details on the effects of temperature on accuracy and the LLMs’ tendency to change their answers within the same session and across independent sessions.\\n3.2. Data Sets\\nWe choose two BBH tasks (Suzgun et al., 2022): causal judgment (187 questions) and formal fallacies (250 questions). These tasks are a curated subset of BIG-Bench, containing especially challenging tasks designed to assess the advanced reasoning, understanding, and problem-solving capabilities of LLMs. Suzgun et al. used BBH to evaluate the value of CoT prompting to improve LLMs’ performance in these tasks. However, each question in the BBH is associated with a single instruction, that is, no chain of prompts. Two sample questions from each task are given in Appendix A. The complete sets of questions and their answers are downloaded from https://github.com/suzgunmirac/BIG-Bench-Hard/tree/main/bbh.\\nAdditionally, to assess the statistical reasoning abilities of LLMs, we write 46 questions on statistical puzzles and paradoxes from Pawitan and Lee (2024); two sample questions are given in Appendix A. The complete list of questions and their answers is available in https://github.com/yudpaw-git/statspuzzle. (The list includes four additional questions that do not have definite answers; they are not part of the quantitative comparisons here.)\\n3.3. Prompts\\nThe behavior and performance of LLMs are highly dependent on the prompts that we use to elicit their responses. For the base performance, LLMs are first asked to answer the questions directly without providing explanations (‘First answer’). Then they are asked to think again carefully (‘Rethink’), so they have the opportunity to change their initial answers. We compare the accuracy of the LLMs in their initial and second answers, the conditional accuracy when they keep the initial answers and when they change the initial answers.\\nFor practical processing of the output, we try to suppress the normally voluminous response by the LLMs, so all prompts are accompanied by an instruction to be brief. This does not always work, so all outputs are manually inspected for sanity. The instruction to be brief may affect performance, but our accuracy results for the first answers in the BBH tasks are very close to those reported by Zhou et al. (2024). For the BBH tasks, the chat session is reset after each question, while for the statistical puzzles, the session is reset after each section of related questions.\\nAn implicit qualitative confidence of LLMs is measured by their tendency to keep their initial responses when prompted to rethink. To assess the effect of phrasing of the ‘rethink prompt,’ we use (i) Simple prompt: “Please think again carefully”; (ii) Neutral prompt: “We always ask our LLM to double-check their answers, so please think again carefully”; and (iii) Post-confidence prompt is the same as the Neutral prompt, but issued following a confidence-score prompt.\\nA quantitative self-reported confidence score is based on this confidence-score prompt: “On a score between 0 and 100, where 100 means full confidence and 0 means no confidence, what confidence score do you have in your answer?” It is an internal measure of self-confidence. We compare the self-reported score versus the actual accuracy; ideally, 100% confidence should correspond to 100% accuracy, and vice versa, less accuracy for less confident answers. We also hypothesize that the qualitative confidence correlates with the quantitative confidence.\\nAnother metric to measure your confidence in a statement is how much you are willing to bet that it is correct. This can be expressed in terms of betting odds (Shafer, 2021). So, we use the following prompt: “You need to provide fair betting odds that your answer is correct. A person can either bet 1 dollar at the odds you provide or force you to bet 1 dollar against the odds you provide. What fair betting odds would you offer for your answer being correct?”\\nA recent CoT prompting method called Self-Discover (Zhou et al., 2024) is also used for comparisons. For each task (question), the method prompts an LLM to (i) consider which of 39 prespecified high-level reasoning modules are relevant for the task at hand (see Table 2 in Zhou et al. for the list of the modules); (ii) adapt the chosen reasoning modules to be specific to the task at hand; (iii) create an actionable reasoning structure for the task using these adapted reasoning modules; and finally (iv) use the reasoning structure to solve the task. We follow Zhou et al.’s original wording, including the instruction to be brief in all prompts. The session is reset after each prompt; we observe worse accuracy when the prompts are issued without resetting.\\n3.4. R Interface\\nWe use the following R packages/wrappers: (i) Juan Cruz Rodriguez’s chatgpt from https://github.com/jcrodriguez1989/chatgpt for submitting API requests to GPT4o and GPT4-turbo, and (ii) Albert Rapp’s tidychatmodels from https://github.com/AlbertRapp/tidychatmodels to Mistral.\\n3.5. Statistical Analysis\\nReported p values for comparisons of two proportions are based on the \\n𝑋\\n2\\nX\\n2\\n test with Yates’s correction. For small 2-by-2 tables, the corrected p value is an approximation of the two-sided p value from Fisher’s exact test; see, for example, Zar (2010, pp. 469, 561–569).\\n\\nResults\\n4.1. Accuracy and Qualitative Confidence\\n\\nFor a direct (zero-shot) response, we ask the LLMs to answer questions without any other prompting; this is followed by the Simple prompt to reconsider their answers. The results are summarized in Figure 1, with complete details given in Tables B1–B2 in Appendix B.\\nFor causal judgment and formal fallacies, the accuracy of the first answer varies narrowly between 0.62 and 0.70. All are statistically significant, more than \\n2\\n𝜎\\n2σs over the target accuracy of 0.5. After rethinking their answers, GPT4-turbo and Mistral show a drop in accuracy, but not for GPT4o. In general, when they maintain their initial answers, implying higher confidence, they show higher accuracy. Vice versa, the accuracy is significantly worse when they change their mind, reaching 36% and 32% for Mistral, which are significantly lower than the target value.\\nThe LLMs show wide discrepancies in their tendency to change their initial answers. GPT4-turbo and Mistral show a strong tendency to change, but GPT4o tends to keep its answers. The good news is that there is a higher tendency to change wrong initial answers. However, for GPT4-turbo and Mistral, the second responses are worse in accuracy because they change the initial correct answers too frequently.\\nFigure 1. Comparison of the large language models in the causal judgment, formal fallacies questions, and statistical puzzles. ‘First answer’ is based on a direct zero-shot prompt and followed by the Simple prompt to think again carefully (‘Rethink’). Random guesses have an expected accuracy of 0.5 (dotted line), and standard deviations of 0.037 and 0.032 for the causal judgment and the formal fallacies tasks, respectively; the corresponding values for the statistical puzzles are 0.39 (dotted line) and 0.07. P values for the comparisons of accuracies and proportions are given in Tables B1 and B2 in Appendix B.\\nIn Table B1 we also show the results for the Self-Discover CoT prompting method. There is a small improvement in accuracy compared to direct prompting, reaching 74% accuracy for the first answers from GPT4-turbo in the formal fallacies task. We do not observe as much improvement as reported in Table 3 in Zhou et al. (2024), but we are unable to find any explanation. With Self-Discover prompting, GPT4-turbo and Mistral are now much less likely to change their answers compared to direct prompting, but GPT4o behaves rather similarly. So, for GPT4-turbo and Mistral, a more complex chain of thought makes it qualitatively more confident in their answers, but not for GPT4o.\\nFor the statistical puzzles, the accuracy of the first answers varies from 52% to 61%, which are more than \\n2\\n𝜎\\n2σs higher than the target accuracy of 39%. As before, the accuracy of the second answers is lower for GPT4-turbo and Mistral, but not for GPT4o. We also observe a similar pattern of better accuracy when GPT4-turbo and Mistral keep their initial answers compared to when they change. Finally, compared to GPT4o, GPT4-turbo and Mistral change their answers much more frequently and are more likely to change the wrong initial answers.\\n4.2. Comparison With Older Versions\\nIt is interesting to compare GPT4o with OpenAI’s previous flagship model GPT4 (March 2023), and Mistral Large 2 with its previous version called Mistral Large (February 2024). The overall accuracies of these LLMs in all the tasks here are similar, but they show opposite behavior in their qualitative confidence (data not shown). Mistral Large shows a very similar behavior to GPT4o in its relative reluctance to change its initial answers, while GPT4 behaves more like Mistral Large 2. For example, GPT4 has a much greater tendency to change its initial answers compared to GPT4o: 83% versus 18%, respectively, in the formal fallacies task.\\nWe do not know what changes occur between versions, but since they are all based on the same transformer architecture, the most relevant change for us here is likely the number of parameters. Unfortunately, we do not know the number of parameters of these LLMs, except for Mistral Large 2 (123 billion parameters). It is safe to assume that Mistral Large 2 has more parameters than Mistral Large. GPT4o has also been reported to have ‘improved computational efficiency,’ so we speculate it has fewer parameters than GPT4. So, a larger number of parameters seems associated with a greater tendency to change the initial answers.\\n4.3. Self-Reported Confidence Score\\nIn general, the quantitative confidence is substantially higher than the qualitative confidence. In the formal fallacies task, GPT4o and GPT4-turbo give a confidence score of 100 to all their answers, clearly showing overconfidence; Mistral gives the perfect confidence score 79% of the time and a score of 95 to the rest. The results on quantitative confidence for the causal judgment task are summarized in Table 1. GPT4-turbo and Mistral claim a high confidence score (≥95) 78% and 86% of the time; however, the accuracy when they claim so is not better than the overall accuracy, which indicates false confidence. For GPT4o, there is also an indication that when it is less than 95% confident, its answers are less accurate than when it has higher confidence (0.60 vs. 0.80, p value = 0.0083, for the first answer).\\nNext, we check how the quantitative confidence score is correlated with the qualitative confidence based on the tendency to keep their initial answers. When asked to reconsider by the Post-confidence rethink prompt, the correlation is only observed in GPT4-turbo. However, when the qualitative confidence is based on the Simple prompt that is issued separately from the confidence-score prompt, there is a marginally significant correlation for all LLMs (p values ranging from 0.0065 to 0.093).\\nTable 1. Comparison of the large language models on self-reported confidence score and the corresponding accuracy for the causal judgment task. The confidence score is based on the prompt: “On a score between 0 to 100, where 100 means full confidence and 0 means no confidence, what confidence score do you have in your answer?” The Simple rethink prompt response is collected in a separate session from the confidence-score prompt.\\nGPT4o\\nGPT4t\\nMistral\\nFirst answer\\nAccuracy overall\\n0.67\\n0.71\\n0.68\\nPr(Conf ≥ 95)\\n0.40\\n0.78\\n0.86\\nAcc by confidence\\nConfidence score\\n<95    ≥95\\n<95    ≥95\\n<95    ≥95\\nAccuracy\\n.060    0.80\\n0.71    0.72\\n0.56    0.71\\np value\\n8.3E-03\\n0.89\\n0.18\\nKeep ans, Post-conf, all\\n0.98\\n0.65\\n0.98\\nConfidence Score\\n<95    ≥95\\n<95    ≥95\\n<95    ≥95\\nKeep answer\\n0.99    0.97\\n0.39    0.72\\n1.00    0.98\\np value\\n1.00\\n9.0E-05\\n1.00\\nKeep ans, Simple prompt, all\\n0.83\\n0.39\\n0.13\\nConfidence Score\\n<95    ≥95\\n<95    ≥95\\n<95    ≥95\\nKeep answer\\n0.77    0.93\\n0.27    0.42\\n0.00    0.16\\np value\\n6.5E-03\\n0.093\\n0.057\\nSecond answer\\nAccuracy overall\\n0.67\\n0.65\\n0.68\\nPr(Conf ≥ 95)\\n0.66\\n0.84\\n0.78\\nAcc by Confidence\\nConfidence score\\n<95    ≥95\\n<95    ≥95\\n<95    ≥95\\nAccuracy\\n0.50    0.76\\n0.45    0.68\\n0.54    0.792\\np value\\n4.8E-03 \\n0.026\\n0.042\\n4.4. Self-Reported Betting Odds\\nWe ask the LLMs to give fair betting odds for their answers on the causal judgment task; theoretically, higher odds correspond to higher probability. Different LLMs interpret the word ‘odds’ differently: 55% of the time GPT4o gives 1:1 odds and for the rest a mixture of odds greater and smaller than 1; GPT4-turbo assigns 1:1 odds 22% of the time and lower odds to the rest; Mistral gives 1:1 odds only 2% of the time and higher odds for the rest except in two questions. (Note: All these numbers are not reported in any table. GPT4o appears to misunderstand the betting odds: When prompted with a shorter question to simply provide the fair odds for its answers, 81% of the time it gives 1:1 odds. Moreover, after giving the even odds, it then keeps 96% of the initial answers when asked to reconsider. This could be due to the brief interaction format we use to elicit its responses.)\\nTo make the odds comparable across LLMs, we transform any odds less than 1 into its inverse. The results are summarized in Table B3. Defining the odds \\n\\n2\\n2 as high, the proportions of high odds are 0.17, 0.78, and 0.95 for GPT4o, GPT4-turbo, and Mistral, respectively. The relationship between the odds and other measures of confidence is inconsistent. There is no significant association between odds and accuracy, and between odds and confidence score, except for GPT4o. But there is a significant association between the odds and the tendency to keep the first answer after the Simple rethink prompt, except for Mistral.\\n\\n4.5. Effect of Prompt on Qualitative Confidence\\nAs we describe previously, LLMs’ response is often highly affected by the phrasing of the prompt. Simply asking them to think again may create the impression that we want them to change their answer. The Neutral rethink prompt is meant to convey to the LLMs that there is nothing wrong with their answer. Indeed, Figure 2, with detailed values in Table B4, shows a significant impact of the prompting: After the Simple prompt, the LLMs show the highest tendency to change their initial response, followed by the Neutral and the Post-confidence prompts. It is at least self-consistent that GPT4o and Mistral show little or no tendency to change after claiming that they have complete confidence in their initial answers.\\nFigure 2. Comparison of the large language models on the tendency to change their initial answers in the causal judgment task (n =187 questions) after Simple, Neutral, and Post-confidence rethink prompts.\\n4.6. Relationship With Token-Level Probability\\nWhat is the source of confidence in an LLM? Why would it change its mind in one response but not in another? Intuitively, it should be connected to the underlying token-level probabilities produced by the model. The BBH tasks we consider here are amenable for further analysis, as the correctness of each answer depends on a single keyword: yes, no, valid, or invalid. We shall focus on GPT4o and GPT4t, and the token probability refers to the keyword in each answer.\\nThe probabilities are generally very high, reaching a median greater than 0.995, except for GPT4o in the formal fallacies task (0.93); see Figure B1 in Appendix B. The -log-log transform is used to sufficiently stretch the scale. The first column in Figure 3 shows the accuracy as a function of the token probability. There is a significant positive correlation, but the accuracy is substantially less than the token probability, except for extremely high probabilities greater than 0.99999.\\nFigure 3. Accuracy and proportion of keeping the first answer as a function of token probability. The latter is based on the Simple (black), Neutral (red), and Post-confidence (blue) rethink prompts. The scattered points are the raw values based on pre-binned/local proportions. The dashed red lines in the first column are lines of identity, which are curved because of the -log-log probability scale.\\nThe second column shows qualitative confidence—in terms of proportion of keeping the initial answers—as a function of the token probability. We also observe a consistently strong positive association, but there is a great heterogeneity in the relationship depending on the model, the task, and the rethink prompt. The qualitative confidence based on the Simple rethink prompt is substantially less than the token probability: Even at a token probability of around \\n0.9\\n0.9, the LLMs can still easily change their answers.\\nThe correlation between the token probability and the self-reported confidence score is much weaker (table not shown). For the formal fallacies task, the confidence scores are all 100, so there is no correlation with the token probability. For the causal judgment task, due to the high proportions of the score of 95 or higher (Table 1), we do not compute the standard correlation and instead compare the median probabilities when the score is ≥ 95 versus when it is < 95: 0.9770 versus 0.9999 for GPT4o, and 0.9966 versus 0.9998 for GPT4-turbo.\\n\\nDiscussion and Conclusion\\n\\nWe have investigated the degree of confidence LLMs have in their answers, and how it correlates with accuracy and the underlying token probability. Confidence is shown qualitatively in the persistence in their response when prompted to reconsider, or quantitatively as a self-reported confidence score. Although the LLMs show significantly better performance than random guessing, there is a wide variability in their qualitative confidence across tasks and models. The good news is that higher qualitative confidence is correlated with higher accuracy. However, unfortunately, initially correct answers are too often changed, resulting in worse accuracy. Confidence is also easily affected by the phrasing of the prompt. Being much higher than the actual accuracy, the self-reported confidence score of the LLMs is more likely to reflect false confidence. These confidence measures are only partially explained by the underlying token-level probability.\\nAlthough we observe some correlation between qualitative and quantitative confidence, the material effects of prompting on the tendency to change their answers and the overconfidence when explicitly asked for their level of confidence indicate that the current LLMs do not have any internally coherent sense of confidence. To interpret it least charitably, they do not have any recognition or understanding of the truth quality in their answers. We believe that this property is distinct from their more famous tendency to hallucinate. Hallucinations involve making up seemingly factual statements whose truths fail empirical validation. Here, all our tasks involve only logical inference or deductions: If we happen to make a wrong inference, we just say that we are wrong and not that we are hallucinating. (Hallucinators and liars are different again in the self-awareness of the truth and intention to mislead. Thus, LLMs do not lie, but hallucinate. Any expert can be wrong in their logical inferences without being a liar; they usually protect their reputation by providing some measure of uncertainty in their statements.)\\nThe relationship between the underlying token probability and the accuracy and confidence deserves further study. This probability is a function of the preceding words that the LLM chooses according to some other probabilities. We could imagine a situation where an extremely high token probability for a ‘yes’ is fully justified after a certain series of preceding words. However, it is not obvious how we can account for the probabilities of those words and, in turn, how accurate and relevant these chosen words are relative to the task at hand.\\nWhat are the practical implications of our study? When we consult a presumed expert on a difficult question, the confidence in the answer comes from at least two sources: (i) their confidence, which we can ask explicitly or infer based on further questioning, and (ii) our own confidence based on our knowledge of the area associated with the question.\\nImagine first a scenario where we do not know the correct answer to the question or the related area very well. Being uncertain of the initial answer from the LLM, we ask it to think again. Our results suggest that our confidence can increase if the LLM persists in its answer and otherwise may decrease. Table B5 in Appendix B shows the increase in accuracy when we ask the LLM to rethink twice. However, the amount of improvement varies across LLMs, tasks, and is affected by the phrasing of the prompt, so it is difficult to judge in individual cases. There is also a trade-off: A persistent answer after a simple rethink prompt leads to higher confidence, but such a prompt leads to more changes, which can lead to lower confidence. And vice versa, more complex rethink prompts, especially issued after a confidence-score prompt, can lead to a lower tendency to change answers; but in this case, persistent answers have confidence similar to the initial answers.\\nNow consider the second scenario, where we know the correct answer and the prompt to reconsider is issued only when the first answer is wrong. This is like a sympathetic teacher examining a good but not perfect student. In the formal fallacies task (see Table B1), for example, 98% of the incorrect first answers by GPT4-turbo will be corrected. This procedure will give the misleading impression that GPT4-turbo is really good at self-correcting. This is where the so-called Clever Hans effect occurs. (Clever Hans was a horse reportedly able to perform some arithmetic, but it turned out he was getting some subtle clues from his handler.) In reality, the LLM will also almost as often (83%) change its mind about the initially correct answers. See, for example, Lapuschkin et al. (2019) for more details about this effect.\\nThere is a darker variant of the second scenario in which we guide an LLM to a foregone conclusion. We start with an opinion that is not necessarily correct and continue to prompt and direct the LLM until it agrees with us. This can of course be misleading or at least self-defeating if we then use the LLM as supposedly an independent expert to support our preconceived opinion.\\nThe third scenario is the in-between situation, which is perhaps the most productive use of the LLM: We do not know the answer to the question, but we are a domain expert or critical evaluator such that, after iterative interaction with the LLM, we can recognize a correct answer or have our own high confidence in a good answer. So, in this arrangement, the final judgment is made by the human expert, not the LLM. The role of the LLM is to provide new ideas, concepts, or candidate answers. Valmeekam et al. (2023) and Stechly et al. (2023) describe and evaluate such an interaction between an LLM and an external verifier. The so-called FunSearch (Romera-Paredes et al., 2024) depends on a generate-test loop between a specially fine-tuned LLM that suggests solutions and an external symbolic evaluator. The recent AlphaGeometry (Trinh et al., 2024) for proving theorems in geometry also uses the Generate-Test-Critique framework of a fine-tuned LLM and a symbolic evaluator.\\nA strength of our study is that we investigate the LLMs’ behavior in relatively large numbers of questions, including two sets from a standard benchmark. This avoids inference from anecdotal behavior seen in a few specific instances. Our goal is to capture the heterogeneity of current LLMs in their confidence properties, not to investigate the behavior of each LLM. OpenAI’s GPT4o is selected because it is the most popular AI model; the other models, one from OpenAI and one from non-OpenAI, are selected as a close and a distant comparator. The chosen tasks and LLMs show sufficiently large variability in the confidence properties.\\nA weakness of studying LLMs with a large number of questions is that, for obvious practical reasons, the LLMs are told to be brief. This could affect their overall performance, although it is not clear how it might affect the confidence levels that we focus on in this article. The Self-Discover prompting somewhat overcomes this weakness, as the LLMs are prompted to go through a more complex chain of thoughts before arriving at the final answer.\\nAnother weakness is that we cannot tell if the LLMs’ performance is based on de novo reasoning or it is due to their pretraining from having seen the questions before. This is obvious when we ask the LLMs to solve the well-known puzzles or paradoxes. The BIG-Bench tasks carry a warning that the questions should not be included in the training data of LLMs, but it is, of course, difficult to know if the warning is heeded. Again, this issue is more related to overall performance than confidence level.\\nIn conclusion, we have shown some weaknesses of current LLMs in terms of overconfidence and lack of understanding of uncertainty. The weaknesses, somewhat associated with a lack of thoughtful humanlike introspection, could be inherent in the LLM design as an autoregressive next-word predictor, and hence not easily remedied. However, evaluating rapidly moving technology is always tricky: today’s weaknesses could be remedied tomorrow, perhaps with larger sets of training data or parameters, more complex inference such as CoT, a new architecture, and so on. Yet, benchmarking studies do not necessarily lose their value, as they provide clear indications where current research is needed and markers of progression in the evolution of LLMs’ capabilities.\\nDisclosure Statement\\nYudi Pawitan and Chris Holmes have no financial or nonfinancial disclosures to share for this article.\\nReferences\\nKadavath, S., Conerly, T., Askell, A., Henighan, T., Drain, D., Perez, E., Schiefer, N., Hatfield-Dodds, Z., DasSarma, N., Tran-Johnson, E., Johnston, S., El-Showk, S., Jones, A., Elhage, N., Hume, T., Chen, A., Bai, Y., Bowman, S., Fort, S., . . . Kaplan, J. (2022). Language models (mostly) know what they know. ArXiv. https://doi.org/10.48550/arXiv.2207.05221\\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., & Iwasawa, Y. (2022). Large language models are zero-shot reasoners. ArXiv. https://doi.org/10.48550/arXiv.2205.11916\\nKosinski, M. (2023). Theory of Mind may have spontaneously emerged in large-language models. ArXiv. https://doi.org/10.48550/arXiv.2302.02083\\nLapuschkin, S., Wäldchen, S., Binder, A., Montavon, G., Samek, W., & Müller, K. R. (2019). Unmasking Clever Hans predictors and assessing what machines really learn. Nature Communications, 10, Article 1096. https://doi.org/10.1038/s41467-019-08987-4\\nMizrahi, M., Kaplan, G., Malkin, D., Dror, R., Shahaf, D., & Stanovsky, G. (2024). State of what art? A call for multi-prompt LLM evaluation. Transactions of the Association for Computational Linguistics, 12, 933–949. https://doi.org/10.1162/tacl_a_00681\\nPawitan, Y., & Lee, Y. (2024). Philosophies, Puzzles and Paradoxes. Chapman & Hall/CRC Press.\\nRomera-Paredes, B., Barekatain, M., Novikov, A., Balog, M., Kumar, M., Dupont, E., Ruiz, F., Ellenberg, J., Wang, P., Fawzi, O., Kohli, P., & Fawzi, A. (2024). Mathematical discoveries from program search with large language models. Nature, 625, 468–475. https://doi.org/10.1038/s41586-023-06924-6 \\nShafer, G. (2021). Testing by betting: A strategy for statistical and scientific communication. Journal of the Royal Statistical Society: Series A (Statistics in Society), 184(2), 407–431. https://doi.org/10.1111/rssa.12647\\nShinn, N., Cassano, F., Labash, B., Gopinath, A., Narasimhan, K., & Yao, S. (2023). Reflexion: Language agents with verbal reinforcement learning. ArXiv. https://doi.org/10.48550/arXiv.2303.11366\\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A., Abid, A., Fisch, A., Brown, A., Santoro, A., Gupta, A., Garriga-Alonso, A., & Consortium, B.-B. (2022). Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. ArXiv. https://doi.org/10.48550/arXiv.2206.04615 \\nStechly, K., Marquez, M., & Kambhampati, S. (2023). GPT-4 doesn’t know it’s wrong: An analysis of iterative prompting for reasoning problems. ArXiv. https://doi.org/10.48550/arXiv.2310.12397\\nSuzgun, M., Scales, N., Schärli, N., Gehrmann, S., Tay, Y., Chung, H., Chowdhery, A., Le, Q., Chi, E., Zhou, D., & Wei, J. (2022). Challenging big-bench tasks and whether chain-of-thought can solve them. ArXiv. https://doi.org/10.48550/arXiv.2210.09261\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., & Lample, G. (2023). LLaMA: Open and efficient foundation language models. ArXiv. https://doi.org/10.48550/arXiv.2302.13971 \\nTrinh, T., Wu, Y., Le, Q., He, H., & Luong, T. (2024). Solving olympiad geometry without human demonstrations. Nature, 625, 476–482. https://doi.org/10.1038/s41586-023-06747-5\\nTuring, A. (1950). Computing machinery and intelligence. Mind, 59(239), 433–460. https://doi.org/10.1093/mind/LIX.236.433 \\nUllman, T. (2023). Large language models fail on trivial alterations to theory-of-mind tasks. ArXiv. https://doi.org/10.48550/arXiv.2302.08399 \\nValmeekam, K., Marquez, M., Sreedharan, S., & Kambhampati, S. (2023). On the planning abilities of large language models – A critical investigation. In K. Inui, J. Jiang, V. Ng, X. Wan (Eds.), Thirty-seventh Conference on Neural Information Processing Systems (EMNLP-IJCNLP) (pp. 3615–3620). Association for Computational Linguistics. https://openreview.net/forum?id=X6dEqXIsEW\\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q., Zhou, D., Mishra, S., & Zheng, H. (2022). Chain-of-thought prompting elicits reasoning in large language models. ArXiv. https://doi.org/10.48550/arXiv.2201.11903\\nWeng, Y., Zhu, M., Xia, F., Li, B., He, S., Liu, S., Sun, B., Liu, K., & Zhao, J. (2023). Large language models are better reasoners with self-verification. ArXiv. https://doi.org/10.48550/arXiv.2212.09561 \\nZar, J. H. (2010). Biostatistical analysis (5th). Prentice Hall.\\nZhao, T., Wallace, E., Feng, S., Klein, D., & Singh, S. (2021). Calibrate before use: Improving few-shot performance of language models. ArXiv. https://doi.org/10.48550/arXiv.2102.09690\\nZhou, P., Pujara, J., Ren, X., Chen, X., Cheng, H., Le, Q., Chi, E., Zhou, D., Mishra, S., & Zheng, H. (2024). Self-discover: Large language models self-compose reasoning structures. ArXiv. https://doi.org/10.48550/arXiv.2402.03620\\nAppendices\\nAppendix A. Sample Questions\\nThe BBH questions (Suzgun et al., 2022) are taken from https://github.com/suzgunmirac/ BIG-Bench-Hard/tree/main/bbh. The ‘causal judgment’ task contains 187 causal reasoning questions, each of which concludes with a yes-no question; the ‘formal fallacies’ task contain 250 logical reasoning questions, each of which must be judged valid or invalid. In addition, we write 46 questions based on statistical puzzles and paradoxes from Pawitan and Lee (2024); the complete list and the answers are given in https://github.com/yudpaw-git/statspuzzle. Here are some examples:\\n[Causal judgment] How would a typical person answer each of the following questions about causation? A machine is set up in such a way that it will short circuit if both the black wire and the red wire touch the battery at the same time. The machine will not short circuit if just one of these wires touches the battery. The black wire is designated as the one that is supposed to touch the battery, while the red wire is supposed to remain in some other part of the machine. One day, the black wire and the red wire both end up touching the battery at the same time. There is a short circuit. Did the black wire cause the short circuit? Options: Yes or No.\\n[Causal judgment] Long ago, when John was only 17 years old, he got a job working for a large manufacturing company. He started out working on an assembly line for minimum wage, but after a few years at the company, he was given a choice between two line manager positions. He could stay in the woodwork division, which is where he was currently working. Or he could move to the plastics division. John was unsure what to do because he liked working in the woodwork division, but he also thought it might be worth trying something different. He finally decided to switch to the plastics division and try something new. For the last 30 years, John has worked as a production line supervisor in the plastics division. After the first year there, the plastics division was moved to a different building with more space. Unfortunately, through the many years he worked there, John was exposed to asbestos, a highly carcinogenic substance. Most of the plastics division was quite safe, but the small part in which John worked was exposed to asbestos fibers. And now, although John has never smoked a cigarette in his life and otherwise lives a healthy lifestyle, he has a highly progressed and incurable case of lung cancer at the age of 50. John had seen three cancer specialists, all of whom confirmed the worst: that, except for pain, John’s cancer was untreatable and he was absolutely certain to die from it very soon (the doctors estimated no more than 2 months). Yesterday, while John was in the hospital for a routine medical appointment, a new nurse accidentally administered the wrong medication to him. John was allergic to the drug and he immediately went into shock and experienced cardiac arrest (a heart attack). Doctors attempted to resuscitate him but he died minutes after the medication was administered. Did the nurse’s carelessness cause John’s premature death? Options: Yes or No.\\n[Formal fallacies] Here comes a perfectly valid argument: First of all, whoever is a schoolmate of Sondra is not a stepsister of Pricilla. In consequence, whoever is not a stepsister of Pricilla is a schoolmate of Sondra. Options: valid or invalid.\\n[Formal fallacies] Consumer research aims at understanding whether users of some products also tend to consume other ones, or not. The following argument seeks to clarify some such relations: First premise: Being a regular consumer of Kiss My Face soap is necessary for being a regular user of Nag Champa soap. Second premise: Whoever is a rare consumer of John Frieda shampoo is at least one of these: a regular consumer of Mrs. Meyer’s soap, a regular user of Nag Champa soap, or a regular user of René Furterer shampoo. Third premise: No regular consumer of Mrs. Meyer’s soap is a regular consumer of Kiss My Face soap. Therefore, whoever is a rare consumer of John Frieda shampoo is not a regular consumer of Kiss My Face soap or a regular user of René Furterer shampoo. Is the argument, given the explicitly stated premises, deductively valid or invalid?\\n[Statistical Puzzles] Section on Boy-Girl Paradox, a classic paradox involving Mr. Smith and his son. For the following questions, answer with A, B, C, or D only without elaborate explanations.\\nQ6: Mr. Smith has two children, and one of them is a boy. What is the probability that the other child is a girl? A. 1/2; B. 2/3; C. 1; D. Undetermined.\\nQ7. A trustworthy witness (maybe Mr. Smith himself) reports that Mr. Smith has two children and one of them is a boy. What is the probability that the other child is a girl? A. 1/2; B. 2/3; C. 1; D. Undetermined.\\nAppendix B. Additional Tables and Figures\\nTable B1. Comparison of the large language models in two BIG-Bench Hard tasks: causal judgment (n = 187 questions) and formal fallacies (n = 250 questions). In part A, ‘First answer’ is based on a direct zero-shot question, followed by the Simple prompt to think again carefully (‘Rethink’). Random guesses have an expected accuracy 0.5, and standard deviations of 0.037 and 0.032 for the causal judgment and the formal fallacies tasks, respectively. In part B, we use the prompting method Self-Discover from Zhou et al. (2024), which is also followed by the Simple rethink prompt. \\nCausal Judgment\\nFormal Fallacies\\nGPT4o\\nGPT4t\\nMistral\\nGPT4o\\nGPT4t\\nMistral\\nA. DIRECT\\nAccuracy\\nFirst answer\\n0.67\\n0.70\\n0.68\\n0.62\\n0.66\\n0.68\\nRethink\\n0.66  \\n0.60\\n0.45\\n0.66\\n0.45\\n0.33\\nIf same answer\\n0.70\\n0.89\\n1.00\\n0.68\\n0.93\\n0.80\\nIf changed\\n0.45\\n0.42\\n0.36\\n0.61\\n0.38\\n0.32\\np value\\n0.015 \\n4.2E-10 \\n6.8E-04 \\n0.48\\n3.7E-08\\n0.078\\nProportion\\nChange first answer\\n0.17\\n0.61\\n0.87\\n0.18\\n0.88\\n0.98\\nChange correct ans\\n0.13\\n0.51\\n0.80\\n0.12\\n0.83\\n0.98\\nChange wrong ans\\n0.23\\n0.86\\n1.00\\n0.30\\n0.98\\n0.99\\np value\\n0.16\\n1.8E-05\\n6.3E-04\\n5.8E-04\\n1.4E-03\\n0.92\\nB. Self-Discover\\nAccuracy\\nFirst answer\\n0.69\\n0.71\\n0.72\\n0.67\\n0.74\\n0.69\\nRethink\\n0.71\\n0.76\\n0.52\\n0.69\\n0.57\\n0.47\\nIf same answer\\n0.76\\n0.83\\n0.72\\n0.71\\n0.92\\n0.68\\nIf changed\\n0.46\\n0.53\\n0.29\\n0.58\\n0.37\\n0.30\\np value\\n5.5E-04\\n1.5E-04\\n9.1E-09\\n0.18\\n6.6E-17\\n3.6E-09\\nProportion\\nChange first answer\\n0.22\\n0.20\\n0.45\\n0.14\\n0.64\\n0.54\\nChange correct ans\\n0.17\\n0.13\\n0.45\\n0.09\\n0.54\\n0.55\\nChange wrong ans\\n0.35\\n0.44\\n0.45\\n0.25\\n0.89\\n0.52\\np value\\n9.4E-03\\n1.1E-05 \\n1.00\\n1.1E-03\\n8.3E-07\\n0.77\\nTable B2. Comparison of the large language models on statistical puzzles (n = 46 questions) based on a direct prompt (‘First answer’) followed by the Simple prompt to think again carefully (‘Rethink’). Random guesses have an expected accuracy of 0.39 and a standard deviation of 0.07.\\nGPT4o\\nGPT4t\\nMistral\\nAccuracy\\nFirst Answer\\n0.52\\n0.57\\n0.61\\nRethink\\n0.54\\n0.46\\n0.54\\nIf same answer\\n0.55\\n0.75\\n0.67\\nIf changed\\n0.50\\n0.30\\n0.31\\np value\\n1.00\\n9.1E-03\\n0.047\\nProportion\\nChange first answer\\n0.13\\n0.65\\n0.35\\nChange correct ans\\n0.08\\n0.54\\n0.29\\nChange wrong ans\\n0.18\\n0.80\\n0.44\\np value\\n0.58\\n0.13\\n0.43\\nTable B3. Comparison of the large language models on self-reported betting odds for the causal judgment task (n =187 questions). The odds are based on the prompt: ‘You need to provide fair betting odds that your answer is correct. A person can either bet 1 dollar at the odds you provide or force you to bet 1 dollar against the odds you provide. What fair betting odds would you offer for your answer being correct?’ The confidence and the Simple rethink prompt responses are collected in separate sessions from the odds prompt.\\nGPT4o\\nGPT4t\\nMistral\\nAccuracy\\n0.68\\n0.70\\n0.68\\nPr(Odds>2)\\n0.17\\n0.78\\n0.95\\nAccuracy by odds\\nOdds category\\n≤2       >2\\n≤2       >2\\n≤2       >2\\nAccuracy, by odds\\n0.65       0.87\\n0.63       0.72\\n0.44       0.70\\np value\\n0.025\\n0.39\\n0.22\\nKeep ans, post-odds\\nPr(Same answer), by odds\\n0.88       1.00\\n0.12       0.44\\n0.33       0.91\\np value\\n0.098\\n4.2E-04\\n2.5E-06\\nConfidence score\\nPr(Conf≥95), by odds\\n0.30       0.87\\n0.68       0.80\\n0.89       0.85\\np value\\n1.0E-08\\n0.16\\n1.00\\nKeep ans, simple prompt\\nPr(Same answer), by odds\\n0.80       1.00\\n0.10       0.47\\n0.11       0.13\\np value\\n0.014\\n4.1E-05\\n1.00\\nTable B4. Comparison of the large language models on the tendency to change their initial answers in the causal judgment task after Simple, Neutral, and Post-confidence rethink prompts. The Simple prompt is ‘Please think again carefully,’ the Neutral prompt ‘We always ask our LLM to double-check their answers, so please think again carefully,’ and the Post-confidence prompt is the same as the Neutral prompt, but issued after the confidence-score prompt.\\nSimple\\nNeutral\\nPost-conf\\nGPT4o\\n0.17\\n0.04\\n0.02\\nGPT4t\\n0.61\\n0.50\\n0.33\\nMistral\\n0.87\\n0.71\\n0.01\\nTable B5. Comparison of the accuracies and proportion of persistent answers versus nonpersistent answers using a single prompt, two prompts, and three prompts. The Simple rethink prompt is used to elicit multiple responses. For the two-prompt case, the final answer is set to be the second answer. For the three-prompt case, the final answer is based on the majority rule.\\nCausal Judgement\\nFormal Fallacies\\nGPT4o\\nGPTt\\nGPT4o\\nGPT4t\\nAccuracy\\nSingle prompt\\n0.67\\n0.7\\n0.62\\n0.66\\nUsing 2 prompts\\nSame answers\\n0.70\\n0.89\\n0.68\\n0.93\\nAny change\\n0.45\\n0.42\\n0.61\\n0.38\\nUsing 3 prompts\\nSame answers\\n0.72\\n0.90\\n0.72\\n1.00\\nAny change\\n0.57\\n0.63\\n0.48\\n0.66\\nProportion\\nUsing 2 prompts\\nSame answers\\n0.83\\n0.39\\n0.82\\n0.12\\nAny change\\n0.17\\n0.61\\n0.18\\n0.88\\nUsing 3 prompts\\nSame answers\\n0.76\\n0.26\\n0.54\\n0.04\\nAny change\\n0.24\\n0.74\\n0.36\\n0.96\\nFigure B1. Distribution of token probabilities for GPT4o and GPT4-turbo for the yes-no and valid-invalid answers in the causal judgment and formal fallacies tasks. Note that the scale is put in -log-log scale in order to stretch the super-crowding of values near one. The median token probabilities are > 0.995, except for GPT4o in the formal fallacies task (0.93).\\nFigure B2. Accuracy and the proportion of changing answer as a function of temperature. The figures show the results for GPT4o and GPT4-turbo in the causal judgment (CJ, red lines) and formal fallacies (FF, blue lines) tasks. The bottom figures show the accuracy difference and the proportion of changing answers in independent runs (sessions). The latter is to be contrasted with the top-right figure, which is based on answers after a rethink prompt in the same session. In the bottom-left plot, the red lines for CJ-GPT4o and CJ-GPT4t coincide. Overall, the temperature effect on average accuracy appears to be small, especially up to temperature 1 and not directionally consistent. A similar result is seen for the tendency to change answer after rethinking, except for GPT4o in the formal fallacies task, where the proportion of changing answer goes from 0.17 to 0.34 as the temperature goes from 0 to 1.5. A more consistent effect is seen on the proportion of changing answer on independent runs (i.e., not based on rethinking), where higher temperatures generally lead to higher proportion of changing answer. (Temperature can actually be set up to a maximum of 2, but at that point the LLMs often take a long time to respond and sometimes produce strange nonsensical answers.)\\n©2025 Yudi Pawitan and Chris Holmes. This article is licensed under a Creative Commons Attribution (CC BY 4.0) International license, except where otherwise indicated with respect to particular material included in the article.\\nREFERENCES\\n22\\nLICENSE\\nCreative Commons Attribution 4.0 International License (CC-BY 4.0)\\nCOMMENTS\\n1\\n?\\nLogin to discuss\\n?\\nFree AI Tools:\\nIntroducing Troubleshoot.dev\\nExploring the dynamic realm of artificial intelligence (AI) can be a challenging endeavor due to the constant evolution and innovation in AI applications. Whether you’re a developer, a business owner, or simply passionate about AI, finding the right tools and applications tailored to your needs is crucial. Enter Troubleshoot.dev.\\nIntroducing Troubleshoot.dev\\nTroubleshoot.dev stands as an online hub dedicated to curating and showcasing the finest AI applications available today. As AI technology continues to progress rapidly across various industries such as healthcare, finance, entertainment, and customer service, pinpointing the ideal AI app for specific tasks can pose a challenge. Troubleshoot.dev tackles this issue by offering a well-organized space where users can effortlessly search, discover, and evaluate AI applications, whether free or paid.\\nWhether you’re in need of a tool for automating routine tasks, enhancing data analysis, or delving into deep learning models, Troubleshoot.dev has got you covered. The platform boasts a user-friendly interface that simplifies navigation through a vast selection of apps. Its unique categorization of AI tools into key areas like AI for business, AI for creative professionals, and AI for developers ensures that users can pinpoint the most suitable applications for their specific requirements.\\nReasons to Utilize Troubleshoot.dev\\n\\nComprehensive Exploration of AI Apps\\n\\nWith the market flooded with numerous AI tools and applications, identifying the truly beneficial ones can be overwhelming. Troubleshoot.dev acts as a filter, presenting only the most effective and highly-rated AI apps. Whether you’re in search of AI-powered writing aids, image recognition tools, or automation solutions, the platform curates an extensive list of applications to cater to a variety of needs.\\nOne of the standout features of Troubleshoot.dev is its tool categorization, enabling users to filter results based on preferences such as free vs. paid apps, specific use cases, or user ratings. This simplifies the process of finding the right AI app without the need for trial-and-error or extensive scouring of websites.\\n\\nDiverse Range of AI Applications\\n\\nIrrespective of your area of interest in AI, Troubleshoot.dev offers a plethora of apps to meet a wide range of needs. The platform showcases key categories of AI applications, including:\\n\\nAI for Content Creation:\\n\\nWriters, marketers, and creatives can leverage AI tools for generating text, designing visuals, and video editing. With AI-driven content generators, crafting high-quality blog posts, social media updates, and other written content] is now more efficient. Apps like Jasper, Copy.ai, and Writesonic enhance writing productivity and creativity.\\n\\nAI for Business: \\n\\nEntrepreneurs and business owners can access troubleshoot.dev AI solutions for automating customer service, enhancing marketing strategies, and streamlining operations]. Chatbots, predictive analytics, and AI-driven customer support platforms are crucial for modern businesses, and troubleshoot.dev offers a multitude of options.\\n\\nAI for Developers: \\n\\nDevelopers seeking tools to accelerate coding, debugging, and software testing can find AI-powered solutions tailored to their needs. The platform features a wide array of development tools, including AI for code completion, performance optimization, and bug detection.\\n\\nAI for Data Analysis: \\n\\nProfessionals working with large datasets can benefit from powerful AI tools for data mining, statistical analysis, and predictive modeling. Machine learning algorithms and data visualization tools aid in extracting valuable insights from complex data.\\n\\nAI for Image and Video Processing: \\n\\nAI tools focusing on computer vision are increasingly vital in sectors like security, healthcare, and entertainment. Troubleshoot.dev simplifies the exploration of tools for AI automating image recognition, video editing, and facial recognition.\\n\\nFree and Paid Options:\\n\\nTroubleshoot.dev presents both free and paid AI apps, providing users with flexibility based on their budget and requirements. Many of the free tools available are robust, enabling individuals and small businesses to access high-quality AI solutions without hefty costs. For users with advanced needs, the platform showcases premium tools offering enhanced features and customization.\\nThe inclusion of free apps proves particularly advantageous for individuals entering the realm of AI technology. Whether you’re a student exploring AI or a small business seeking to optimize processes on a limited budget, you can access potent tools without significant upfront investment. Premium applications, on the other hand, offer advanced functionality to elevate your AI projects.\\n\\nThorough Reviews and Ratings:\\n\\nAssessing the effectiveness of AI applications can be challenging during browsing. Troubleshoot.dev addresses this issue by providing comprehensive reviews and ratings for each AI app listed on the platform. These reviews are based on user feedback and expert assessments, aiding in making informed decisions about the worthiness of tools.\\nEach app listing on Troubleshoot.dev includes detailed descriptions, screenshots, pricing details, and user ratings. Users can easily gauge the app’s ratings, preferred features, and updates or enhancements. This transparency facilitates making educated decisions when selecting the appropriate AI solution.\\n\\nKeeping Abreast of Latest AI Trends:\\n\\nThe pace of advancement in AI technology is rapid, with new tools, updates, and breakthroughs introduced regularly. Troubleshoot.dev ensures users are updated on the latest trends and advancements in the AI domain. The platform offers news and updates on the newest apps, emerging AI trends, and innovations, ensuring users are informed about cutting-edge solutions.\\nWhether you seek the latest generative AI tools or advancements in machine learning, Troubleshoot.dev keeps you abreast of the AI revolution.\\n\\nCommunity and Assistance:\\n\\nThe field of AI can be intricate and demanding to navigate, but www.Troubleshoot.dev fosters a supportive community of users and developers. The platform encourages interaction, allowing users to ask questions, share insights, and discuss experiences with various AI tools. This collaborative setting ensures users receive the necessary help while exploring AI applications.\\nMoreover, www.Troubleshoot.dev offers support for individuals new to AI or seeking guidance on implementing specific AI tools in their projects. Whether through FAQs, tutorials, or community forums, users can easily access resources to commence or troubleshoot any challenges faced.\\n\\nEnhancing Your AI Experience with Troubleshoot.dev:\\n\\nBy providing a plethora of resources, a user-friendly interface, and a meticulously curated selection of both free and paid AI applications, www.Troubleshoot.dev significantly enhances your journey in AI. Whether you aim to automate tasks, streamline workflows, or harness AI for innovation, Troubleshoot.dev serves as a one-stop platform for discovering the right tools and maximizing AI technology.\\nConclusion\\nIn a world increasingly influenced by artificial intelligence, the selection of the right AI apps can be pivotal in staying ahead of the curve. Troubleshoot.dev offers a robust platform where users can effortlessly discover, evaluate, and access AI tools tailored to their needs. From comprehensive app categories and detailed reviews to free and paid options, Troubleshoot.dev emerges as the ultimate destination for individuals looking to leverage the potential of AI.\\nIf you aspire to unlock the full capabilities of AI, Troubleshoot.dev is your go-to destination.\\nAI Girlfriend Free AI Tools Latest Merch Deals Remote Software Jobs\\nauthentic jerseys store attorney jobs\\nHarvard Data Science Review\\nRSSLegal\\nPublished with\"}], \"response_time\": 1.17}\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Based on my search, here are some reasoning LLMs that have been released:\n",
            "\n",
            "*   **DeepSeek-R1:** This open-source reasoning model excels in complex reasoning, mathematical problem-solving, and logical inference. It uses reinforcement learning techniques to refine its reasoning ability and solve complex problems through self-verification, chain-of-thought reasoning, and reflection.\n",
            "*   **OpenAI o1 and o3:** The o1 model family focuses on providing reasoning models. The o1 models excel in STEM fields, with strong results in mathematical reasoning, code generation, and scientific research tasks. The o3 model is designed to handle tasks with more analytical thinking, problem-solving, and complex reasoning.\n",
            "*   **Tülu 3:** Allen Institute for AI's Tülu 3 is an open-source 405 billion-parameter LLM that uses a \"reinforcement learning from verifiable rewards\" framework for fine-tuning tasks with verifiable outcomes, such as solving mathematical problems and following instructions.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on my search, here are some reasoning LLMs that have been released:\n\n*   **DeepSeek-R1:** This open-source reasoning model excels in complex reasoning, mathematical problem-solving, and logical inference. It uses reinforcement learning techniques to refine its reasoning ability and solve complex problems through self-verification, chain-of-thought reasoning, and reflection.\n*   **OpenAI o1 and o3:** The o1 model family focuses on providing reasoning models. The o1 models excel in STEM fields, with strong results in mathematical reasoning, code generation, and scientific research tasks. The o3 model is designed to handle tasks with more analytical thinking, problem-solving, and complex reasoning.\n*   **Tülu 3:** Allen Institute for AI's Tülu 3 is an open-source 405 billion-parameter LLM that uses a \"reinforcement learning from verifiable rewards\" framework for fine-tuning tasks with verifiable outcomes, such as solving mathematical problems and following instructions.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What are the four major design patterns for creating agentic ai systems?\"\n",
        "chat_with_agent(graph_builder, prompt, user_id, verbose=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "c2WYu2_AUJ9F",
        "outputId": "e75a83c1-0c76-4231-acee-2f12e4146a62"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Agent, please wait...\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, based on the search results, here are four key design patterns for creating agentic AI systems:\n\n1.  **Reflection Pattern:** This pattern focuses on self-evaluation and iterative improvement. The AI agent reviews its outputs, identifies flaws, and refines its response, repeating the process until the output is polished.\n\n2.  **Tool Use Pattern:** This pattern empowers agents to interact with external resources like APIs, databases, or programming environments to perform actions beyond their internal knowledge base.\n\n3.  **Planning Pattern:** This pattern enables agents to break down complex tasks into sub-tasks, create a roadmap for execution, and adapt dynamically based on new information.\n\n4.  **Multi-Agent Pattern:** This pattern involves multiple agents working together to achieve a common goal. Each agent has a specific role, and they communicate to ensure seamless task execution.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now simulate User 2 using the agent"
      ],
      "metadata": {
        "id": "B4zIlISy6m-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = 'bond007'\n",
        "prompt = \"how is the weather in Hyderabad today? Show detailed statistics\"\n",
        "chat_with_agent(graph_builder, prompt, user_id, verbose=False)"
      ],
      "metadata": {
        "id": "ta1RUF_81RxX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "caa635ed-4b52-41ce-bc7d-3112a533bc1d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Agent, please wait...\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The weather in Hyderabad, India is partly cloudy with a temperature of 33.1 degrees Celsius (91.6 degrees Fahrenheit). It feels like 31 degrees Celsius (87.7 degrees Fahrenheit). The humidity is 30%. The wind is coming from the NNW at 12.2 kph (7.6 mph). The pressure is 1016 mb. The UV index is 3.3.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = 'bond007'\n",
        "prompt = \"what about Dubai?\"\n",
        "chat_with_agent(graph_builder, prompt, user_id, verbose=False)"
      ],
      "metadata": {
        "id": "vfn_Wxxg42rZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "b1fb202f-2e39-4556-9552-6766ecca2973"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Agent, please wait...\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The weather in Dubai, United Arab Emirates is sunny with a temperature of 31.2 degrees Celsius (88.2 degrees Fahrenheit). It feels like 32.6 degrees Celsius (90.8 degrees Fahrenheit). The humidity is 26%. The wind is coming from the NW at 19.1 kph (11.9 mph). The pressure is 1013 mb. The UV index is 6.5."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = 'bond007'\n",
        "prompt = \"which city is hotter?\"\n",
        "chat_with_agent(graph_builder, prompt, user_id, verbose=True)"
      ],
      "metadata": {
        "id": "I7jjxtmz6Qzi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "c2339f03-5048-4597-d8f3-056c7f38c208"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Agent, please wait...\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "which city is hotter?\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Dubai feels like 32.6 degrees Celsius, while Hyderabad feels like 31 degrees Celsius. So, Dubai feels hotter.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Dubai feels like 32.6 degrees Celsius, while Hyderabad feels like 31 degrees Celsius. So, Dubai feels hotter.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = 'jack001'\n",
        "prompt = \"what are the top frameworks to build such systems?\"\n",
        "chat_with_agent(graph_builder, prompt, user_id, verbose=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "Ixll7BlRVvm9",
        "outputId": "317c1f80-4d8a-43d9-c3a4-dc9018daf67e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Agent, please wait...\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'search_web', 'description': 'Search the web for a query. Userful for general information or general news', 'parameters': {'type_': 6, 'description': 'Search the web for a query. Userful for general information or general news', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
            "{'name': 'get_weather', 'description': 'Search weatherapi to get the current weather.', 'parameters': {'type_': 6, 'description': 'Search weatherapi to get the current weather.', 'properties': {'query': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on the search results, here are some of the top frameworks for building agentic AI systems:\n\n*   **LangChain:** A versatile framework designed to integrate LLMs with external tools for intelligent automation. It supports multiple LLMs and offers features like memory management, context handling, and prompt engineering.\n*   **LangGraph:** Builds on LangChain, introducing stateful, multi-agent AI applications with a graph-based environment. It excels in complex, interactive systems with planning and reflection capabilities.\n*   **CrewAI:** Simplifies the orchestration of autonomous agents by creating role-based AI teams. It focuses on collaborative AI systems with task delegation and hierarchical team structures.\n*   **Microsoft AutoGen:** An open-source framework for building multi-agent AI applications, emphasizing automation and conversation handling. It provides tools for multi-agent coordination, error handling, and task optimization.\n*   **Semantic Kernel:** Designed to integrate AI models into enterprise applications with a focus on security and compliance. It supports multiple languages and offers tools for managing complex tasks.\n*   **Botpress:** A complete framework for building autonomous agents with a modular design, seamless integrations, and AI-driven capabilities.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JgcRywlz6MSX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}